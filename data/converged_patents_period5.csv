"국가코드","DB종류","특허/실용 구분","문헌종류 코드","발명의 명칭","요약","대표청구항","청구항 수","출원번호","출원일","공개번호","공개일","등록번호","등록일","출원인","발명자","우선권 번호","우선권 국가","우선권 주장일","Original CPC Main","Original CPC All","Original IPC Main","Original IPC All","Original US Class Main","Original US Class All","Original FI[JP]","Original F-term[JP]","Original Theme Code [JP]","WIPS ON key"
"US","US","P","B2","Method for evaluating a laser cut edge, mobile terminal and system","A method for evaluating a laser cut edge of a workpiece includes capturing image data of the laser cut edge and its surroundings, segmenting the image data, and identifying a segment of interest of the image data. The segment of interest comprises image data of the laser cut edge. The method further includes carrying out an image quality detection for the segment of interest and generating, based on the image quality detection, an output for a user.","1. A method for evaluating a laser cut edge of a workpiece, the method comprising: A) capturing image data of the laser cut edge and its surroundings, wherein the laser cut edge is formed by laser cutting of the workpiece using a laser cutting machine;B) segmenting the image data and identifying a segment of interest of the image data performed by one or more processors, wherein the segment of interest comprises image data of the laser cut edge;C) carrying out an image quality detection for the segment of interest performed by the one or more processors, wherein the image quality detection comprises detecting an image sharpness of the segment of interest;D) determining by the one or more processors, based on the image quality detection, whether the image sharpness is sufficient;E) performing by the one or more processors the following steps: upon determining that the image sharpness is not sufficient, generating an output for the user on the output interface, the output recommending recording new image data of the laser cut edge and its surroundings; andupon determining that the image sharpness is sufficient, evaluating a quality of the laser cut edge based on the image data of the laser cut edge;determining at least one method parameter based on the quality of the laser cut edge; andforwarding the at least one method parameter to a controller for controlling the laser cutting machine to perform laser cutting of further workpieces.","13","17/553872","2021-12-17","2022-0105590","2022-04-07","12179283","2024-12-31","TRUMPF WERKZEUGMASCHINEN SE + CO. KG","Leonie Felica Tatzel | Manuel Kiefer | Jens Ottnad","10-2019-209088","DE","2019-06-24","B23K-0026/032","B23K-0026/032 | B23K-0026/123 | B23K-0026/1437 | B23K-0031/125 | G06F-0003/01 | G06F-0003/048 | G06F-0003/04847 | G06F-0003/14 | G06T-0005/73 | G06T-0007/0002 | G06T-0007/0004 | G06T-0007/10 | G06T-0007/11 | G06T-0007/12 | G06V-0010/25 | G06V-0010/26 | H04N-0021/4854 | H04N-0023/62 | H04N-0023/675 | A61F-0009/00814 | B23K-0026/142 | B23K-0026/38 | B23K-2101/18 | G05B-2219/36199 | G05B-2219/40613 | G05B-2219/45041 | G05B-2219/45165 | G05B-2219/49353 | G06T-2200/24 | G06T-2207/10148 | G06T-2207/20048 | G06T-2207/30164 | G06T-2207/30168","B23K-026/03","B23K-026/03 | B23K-026/12 | B23K-026/14 | B23K-031/12 | G06F-003/01 | G06F-003/048 | G06F-003/04847 | G06F-003/14 | G06T-005/73 | G06T-007/00 | G06T-007/10 | G06T-007/11 | G06T-007/12 | G06V-010/25 | G06V-010/26 | H04N-021/485 | H04N-023/62 | H04N-023/67 | A61F-009/008 | B23K-026/142 | B23K-026/38 | B23K-101/18","","","","","","4924053002121"
"US","US","P","B1","Method and system for accurately tracking and informing of health and safety for group settings","A user score of a test for assessing a risk of a user of having or developing a contagious illness may be obtained, for each user of a plurality of users entering in a group setting. A group score based on the user scores associated with the plurality of users in the group setting may be generated. The group score indicating a group risk of a presence or development of the contagious illness in the group setting may be communicated for display. Group scores associated with a plurality of group settings may be obtained and communicated for display. A message may be communicated to a mobile user device indicating a group setting as a current high risk group setting, being associated with a score that is outside a limit set by a threshold value and with a location in proximity to a current location of the mobile user device.","1. A method comprising: providing a plurality of access control systems, each access control system being associated with a group setting of a plurality of group settings, wherein the access control system is operative to permit entry to the group setting to one or more users of a plurality of users;providing a first server associated with each access control system, the first server managing entry into the respective group setting for the one or more users of the plurality of users;providing a second server in communication with the first server associated with each access control system for the group setting of the plurality of group settings, the second server: obtaining a score indicating a group risk of a presence or development of a contagious illness in the group setting, for each group setting of the plurality of group settings; andcommunicating for display the scores associated with the plurality of group settings;wherein the first server is configured to permit or deny entry to the respective group setting to each of the one or more users based on a user score reading obtained by the access control system from a mobile user device of the user by communicating a signal to an entry mechanism of the access control system to open or close the entry mechanism;wherein the score associated with the group setting is stored in association with a location of the group setting, for each group setting of the plurality of group settings, the method further comprising:at the server, monitoring a current location of a mobile user device of the one or more users of the plurality of users operating in a mobile network;comparing the current location of the mobile user device to one or more locations of the plurality of group settings;upon identifying a match between the current location and the one or more locations of the plurality of group settings, communicating to the mobile user device a message indicating at least one group setting of the plurality of group settings as a current high risk group setting that is associated with a score that is outside a limit set by a threshold value and with a location in proximity to the current location of the mobile user device; andwherein the threshold value for the score associated with the group setting is set by the user of the mobile user device.","14","17/164332","2021-02-01","","","12183469","2024-12-31","UNITED SERVICES AUTOMOBILE ASSOCIATION (USAA)","Mark Paxman Warnick | Will Kerns Maney, Jr. | Phillip E. Marks | David Jason Anderson James | Elena Marie Carrasco | Quian Antony Jones | Sumita T. Jonak","","","","G16H-0050/80","G16H-0050/80 | A61B-0005/7275 | G06Q-0010/0635 | G06Q-0050/265 | G06V-0040/168 | G07C-0009/00 | G10L-0025/66 | G16H-0015/00 | G16H-0040/67 | G16H-0050/30 | H04W-0004/029 | A61B-0005/14551","G16H-050/80","G16H-050/80 | A61B-005/00 | A61B-005/1455 | G06Q-010/0635 | G06Q-050/26 | G06V-040/16 | G07C-009/00 | G10L-025/66 | G16H-015/00 | G16H-040/67 | G16H-050/30 | H04W-004/029","","","","","","4924053006261"
"US","US","P","B2","Measuring representational motions in a medical context","A method includes receiving data representing graphomotor motion during a succession of executions of graphomotor diagnostic tasks performed in a medical context by a subject, processing the received data using a computer, including determining a first set of quantitative features from a first execution of a task by the subject, and determining a second set of quantitative features from a second execution of a task by the subject, determining one or more metrics based on a comparison to the successive executions, including using at least the first set of quantitative features and the second set of quantitative features to determine said metrics, and providing a diagnostic report associated with neurocognitive mechanisms underlying the subject's execution of the tasks based on the determined metrics.","1. A computer-implemented method comprising: providing a task to a subject;monitoring a performance of the task by the subject using one or more sensors;collecting first data corresponding to a plurality of elements drawn using individual representational motions by the subject during the performance of the task; andcausing to be displayed an analysis of the first data for evaluation of one or more medical characteristics of the subject based on the plurality of elements of the individual representational motions,wherein the analysis uses a machine learning algorithm trained with a set of training data, to identify the plurality of elements of the individual representational motions.","26","18/505786","2023-11-09","2024-0079106","2024-03-07","12176084","2024-12-24","MASSACHUSETTS INSTITUTE OF TECHNOLOGY | LAHEY CLINIC FOUNDATION, INC.","Randall Davis | Dana L. Penney","","","","G16H-0015/00","G16H-0015/00 | A61B-0005/165 | A61B-0005/4088 | A61B-0005/7264 | A61B-0005/7475 | G06Q-0010/101 | G06Q-0050/01 | G16H-0040/63 | G16H-0050/00 | A61B-0005/002 | G16H-0050/20","G16H-015/00","G16H-015/00 | A61B-005/00 | A61B-005/16 | G06Q-010/101 | G06Q-050/00 | G16H-040/63 | G16H-050/00 | G16H-050/20","","","","","","4924052005699"
"US","US","P","B2","Personalized avatar responsive to user physical state and context","Systems and methods are disclosed that facilitate providing guidance to a user during performance of a program or routine using a personalized avatar. In an aspect, a system includes a reception component configured to receive biochemical information about a physiological state or condition of a user, including information identifying a presence or a status of one or more biomarkers. The system further includes an analysis component configured to determine or infer one or more characteristics of the physiological state or condition of the user based on the information identifying the presence or the status of the one or more biomarkers, and a visualization component configured to adapt an appearance of an avatar presented to the user based on the one or more characteristics to reflect the one or more characteristics.","1. A system, comprising: a processor; anda memory that stores executable instructions that, when executed by the processor, facilitate performance of operations, comprising: monitoring user adherence to a defined program followed by a plurality of users based on feedback information, associated with respective performances of the defined program by the plurality of users, received over a course of the defined program, wherein the feedback information comprises at least one of physical movement information related to the plurality of users, or physiological information related to the plurality of users;determining whether the plurality of users are deviating or are likely to deviate from requirements of the defined program based on the feedback information;generating respective reactions to be performed by respective avatars of the plurality of users, based on a determination that the plurality of users are deviating or are likely to deviate from the requirements of the defined program, to guide the plurality of users to adhere to the defined program, wherein the respective reactions are generated by employing machine learning techniques to analyze learned behaviors of the plurality of users, and wherein individual ones of the respective avatars are responsive to the feedback information from the plurality of users; anddisplaying the respective avatars of the plurality of users at a shared user interface generated and presented to the plurality of users at a device.","20","17/806124","2022-06-09","2022-0319692","2022-10-06","12176102","2024-12-24","KC HOLDINGS I","Robert Louis Kaleal, III","","","","G16H-0040/63","G16H-0040/63 | A61B-0005/0205 | A61B-0005/02055 | A61B-0005/11 | A61B-0005/165 | A61B-0005/20 | A61B-0005/40 | A61B-0005/41 | A61B-0005/42 | A61B-0005/43 | A61B-0005/45 | A61B-0005/486 | A61B-0005/7264 | A61B-0005/744 | A61B-0005/7445 | G06F-0001/1626 | G06Q-0010/0639 | G06Q-0010/10 | G06Q-0050/01 | G06T-0013/40 | G06T-0019/00 | G09B-0005/02 | G09B-0005/06 | G09B-0019/00 | G09B-0019/0092 | G16H-0020/40 | G16H-0020/70 | A61B-0005/08 | A61B-0005/44 | A61B-0005/7246 | G06T-2219/2012","A61B-005/20","A61B-005/20 | A61B-005/00 | A61B-005/0205 | A61B-005/11 | A61B-005/16 | G06F-001/16 | G06Q-010/0639 | G06Q-010/10 | G06Q-050/00 | G06T-013/40 | G06T-019/00 | G09B-005/02 | G09B-005/06 | G09B-019/00 | G16H-020/40 | G16H-020/70 | G16H-040/63 | A61B-005/08","","","","","","4924052005717"
"US","US","P","B2","Methods and systems for display of patient data in computer-assisted surgery","Methods and systems for performing computer-assisted image-guided surgery, including robotically-assisted surgery. A method of displaying image data includes displaying image data of a patient on a handheld display device, tracking the handheld display device using a motion tracking system, and modifying the image data displayed in response to changes in the position and orientation of the handheld display device. Further embodiments include a sterile case for a handheld display device, display devices on a robotic arm, and methods and systems for performing image-guided surgery using multiple reference marker devices fixed to a patient.","1. A method for performing image-guided surgery using multiple reference marker devices fixed to a patient, the method comprising: obtaining image data of a patient using an imaging device;associating a first portion of the image data with a first reference marker device fixed to a first location on the patient;associating a second portion of the image data with a second reference marker device fixed to a second location on the patient;displaying at least a portion of the image data of the patient on a display screen of a movable display device;tracking movement of the movable display device with respect to at least one of the first reference marker device fixed to the first location on the patient and the second reference marker device fixed to the second location on the patient; andmodifying the image data displayed on the display screen of the movable display device in response to relative movement occurring between the movable display device and one or more of the first reference marker device fixed to the first location on the patient and the second reference marker device fixed to the second location on the patient.","20","18/348130","2023-07-06","2023-0355347","2023-11-09","12167940","2024-12-17","MOBIUS IMAGING, LLC","Eugene A. Gregerson | Scott Coppen | Todd Furlong | Edward Daley | Russell Stanton | Adeline Harris | Paul Sebring","","","","A61B-0090/37","A61B-0090/37 | A61B-0005/055 | A61B-0005/1127 | A61B-0006/032 | A61B-0006/461 | A61B-0034/20 | A61B-0034/30 | A61B-0034/35 | A61B-0090/00 | A61B-0090/361 | A61B-0090/50 | G06T-0007/248 | G06T-0007/74 | G06T-0015/08 | G06T-0015/20 | G06T-0019/006 | A61B-0005/0035 | A61B-0005/7425 | A61B-0006/463 | A61B-2034/105 | A61B-2034/2048 | A61B-2034/2057 | A61B-2034/2059 | A61B-2034/2065 | A61B-2034/2072 | A61B-2090/365 | A61B-2090/367 | A61B-2090/372 | A61B-2090/373 | A61B-2090/374 | A61B-2090/376 | A61B-2090/3762 | A61B-2090/378 | A61B-2090/3983 | A61B-2560/0223 | G06F-0003/04883 | G06F-0003/1446 | G06T-2207/30012","A61B-090/00","A61B-090/00 | A61B-005/00 | A61B-005/055 | A61B-005/11 | A61B-006/03 | A61B-006/46 | A61B-034/10 | A61B-034/20 | A61B-034/30 | A61B-034/35 | A61B-090/50 | G06F-003/04883 | G06F-003/14 | G06T-007/246 | G06T-007/73 | G06T-015/08 | G06T-015/20 | G06T-019/00","","","","","","4924051000752"
"US","US","P","B2","Data processing system with machine learning engine to provide output generation functions","Methods, computer-readable media, systems, and/or apparatuses are provided for providing offer and insight generation functions. User input requesting an offer or insight may be received and an image of a photographic identification of a user may be requested. The image of the photographic identification may be captured and stored. A self-captured image of the user may be captured (e.g., via an image capture device of the computing device) and compared to an image of a user from the photographic identification. Responsive to determining that the images match, displaying an instruction to capture a vehicle identification number. The vehicle identification number may be captured. Data, including location data, may be extracted and an archive including the extracted data may be generated and the data may be transmitted to an entity computing system for processing. The entity computing system may evaluate the data and generate one or more insights and/or outputs.","1. A computing system, comprising: a processor; anda memory storing computer-executable instructions, wherein the computer-executable instructions, when executed by the processor, cause the processor to: receive, in a user interface of an application executing on the computing system, user input requesting initiation of a process to extract data of a user and generate an output based on the extracted data;responsive to receiving the user input, confirm that location services are enabled on the computing system;receive an instruction to capture an image of photographic identification of the user;capture, via an image capture device of the computing system, the image of the photographic identification of the user;generate, using facial recognition image processing, an isolated image of the user from the image of the photographic identification;receive an instruction to self-capture an image of the user;capture, via the image capture device, the image of the user,obscure, by the computing system and based on a selection from the user, a facial image in image data of the self-captured image of the user and the isolated image of the user, wherein the self-captured image and the isolated image are obscured by using object recognition;transmit the obscured image data of the-self-captured image of the user and the isolated image of the user to a computing platform for comparison and user authentication;receive an indication that the user is authenticated based on the comparison of the obscured image data of the isolated image of the of the user and the self-captured image of the user;responsive to receiving the indication that the user is authenticated, receive an instruction to capture a vehicle identification number of a vehicle associated with the user and for which the output is being generated;capture, via the image capture device, the vehicle identification number;extract location data associated with the user, the location data including global positioning system coordinates for a plurality of location entries corresponding to a plurality of locations of the computing system captured over a period of time;transmit the location data and the captured vehicle identification number to the computing platform;receive, from the computing platform, a plurality of user insights including the output including at least one offer, wherein the plurality of user insights is generated based on a filtered set of the extracted location data; anddisplay, in the user interface of the application on the computing system, the plurality of user insights and the output including the at least one offer.","20","18/086072","2022-12-21","2023-0230121","2023-07-20","12169847","2024-12-17","ALLSTATE INSURANCE COMPANY","Sunil Chintakindi | Timothy W. Gibson | Howard Hayes | Regina Madigan | Soton Ayodele Rosanwo | Caleb Johnson | Aleksandr Likhterman | Srinivas Nainala","","","","G06Q-0030/0239","G06Q-0030/0239 | A61B-0005/024 | G06F-0016/337 | G06F-0021/31 | G06F-0021/32 | G06N-0020/00 | G06Q-0030/0222 | G06Q-0030/0236 | G06Q-0030/0269 | G16H-0010/60 | H04W-0004/029 | G06Q-0030/0255 | G06Q-0030/0261","G06Q-030/00","G06Q-030/00 | A61B-005/024 | G06F-016/335 | G06F-021/31 | G06F-021/32 | G06N-020/00 | G06Q-030/0207 | G06Q-030/0251 | G16H-010/60 | H04W-004/029","","","","","","4924051002646"
"US","US","P","B2","System and a method for alerting on vision impairment","The present invention discloses a technique for alerting on vision impairment. The system comprises a processing unit configured and operable for receiving scene data being indicative of a scene of at least one consumer in an environment, identifying in the scene data a certain consumer, identifying an event being indicative of a behavioral compensation for vision impairment, and, upon identification of such an event, sending a notification relating to the vision impairment.","1. A system for alerting on vision impairment, said system comprising a server connectable, via a communication network, to subscribers's computer entities comprising medical clinics and stores communication devices and user'ss communication devices, the server comprising: a communication interface utility configured and operable for data communication with the user'ss communication devices for communicating request and notification data, and with said medical clinics and stores communication devices; anda processing unit configured and operable for: upon identifying a consumer in the medical clinic or store, via the user'ss communication device of the consumer, and obtaining consumer'ss permission, initiating passive observation of the consumer within the medical clinic or store, said passive observation comprising:receiving, from at least one sensing unit in said medical clinic or store, a scene data;processing the scene data to identify the consumer in environment of the medical clinic or store and determine a condition of the consumer with respect to at least one object of consumer'ss interest in the environment of the medical clinic or store being indicative of visual behavior of the consumer;analyzing the scene data and the condition of the consumer with respect to the at least one object of consumer'ss interest to identify an event being indicative of an unconscious behavioral compensation for vision impairment of said consumer, by carrying out at least one of: identifying the event upon comparison between the scene data and reference data; or determining a probability for a vision impairment of the consumer based on comparison between the scene data and reference data; andupon identification of the event, operating the communication interface utility to send a notification, relating to the vision impairment, to at least one of said user'ss communication device of the consumer, and said medical clinics and stores communication devices.","13","17/310852","2020-02-27","2022-0189010","2022-06-16","12169927","2024-12-17","SHAMIR OPTICAL INDUSTRY LTD.","Amnon Kanter","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0003/085 | A61B-0003/113 | A61B-0003/1176 | A61B-0005/0022 | A61B-0005/74 | G02B-0027/0093 | G06Q-0030/0251 | G06T-0007/292 | G06T-0007/70 | G06V-0040/20 | G06T-2207/20076 | G06T-2207/30041 | G06T-2207/30201 | G06V-0040/172","A61B-003/08","A61B-003/08 | A61B-003/113 | A61B-003/117 | A61B-005/00 | G02B-027/00 | G06Q-030/0251 | G06T-007/00 | G06T-007/292 | G06T-007/70 | G06V-040/20 | G06V-040/16","","","","","","4924051002726"
"US","US","P","B2","Systems and methods of interactive goal setting tools","Examples described herein relate to determining an actual, updated goal of a user based on emotional response data captured while an initial goal is visualized or otherwise perceived by the user, including determining an initial goal of a user, displaying an initial video depicting the initial goal, tracking, with an emotion-tracking device, emotions of the user while the video is displayed by the output circuit by determining emotional response data captured by the emotion-tracking device, updating the initial goal based on the emotional response data to determine an updated goal, and displaying the updated goal.","1. A method, comprising: generating a first portion of a video, wherein the first portion depicts an initial object corresponding to an initial goal, wherein the first portion of the video is displayed on an output circuit of a user device;generating a second portion of the video based on emotional response data captured by an emotion-tracking device of the user device, wherein the second portion depicts an updated object corresponding to an updated goal, the updated goal is updated from the initial goal, and the updated object being different from the initial object, wherein the emotional response data is captured by the emotion-tracking device while a user is viewing the first portion using the output circuit, and wherein the emotional response data indicates that the user reacted negatively to the initial object corresponding to the initial goal;determining a cutoff point of the first portion; andstitching the second portion of the video to the first portion of the video at the cutoff point, wherein the output circuit of the user device continuously plays the video from the first portion to the second portion.","20","18/372486","2023-09-25","2024-0015362","2024-01-11","12170817","2024-12-17","WELLS FARGO BANK, N.A.","Marjorie S. Anzalone | Darius A. Miranda | Wairnola Marria Rhodriquez | Samundra Timilsina | Paul Vittimberga","","","","H04N-0021/44218","H04N-0021/44218 | A61B-0005/0205 | A61B-0005/165 | G06F-0003/011 | G06F-0003/012 | G06F-0003/14 | G06N-0005/04 | G06Q-0040/06 | G06V-0040/174 | G10L-0025/63 | A61B-0005/0077 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0533 | A61B-0005/0816 | A61B-0005/369 | A61B-0007/04 | A61B-2503/12 | G06F-0003/015 | G06F-0003/016 | G06F-2203/011 | G06N-0003/126","H04N-021/442","H04N-021/442 | A61B-005/0205 | A61B-005/16 | G06F-003/01 | G06F-003/14 | G06N-005/04 | G06Q-040/06 | G06V-040/16 | G10L-025/63 | A61B-005/00 | A61B-005/021 | A61B-005/024 | A61B-005/0533 | A61B-005/08 | A61B-005/369 | A61B-007/04 | G06N-003/126","","","","","","4924051003604"
"US","US","P","B2","Creative camera","The present disclosure generally relates to displaying visual effects in image data. In some examples, visual effects include an avatar displayed on a user's face. In some examples, visual effects include stickers applied to image data. In some examples, visual effects include screen effects. In some examples, visual effects are modified based on depth data in the image data.","1. An electronic device, comprising: a display apparatus;one or more input devices;one or more processors; andmemory storing one or more programs configured to be executed by the one or more processors, the one or more programs including instructions for: displaying, via the display apparatus, a representation of previously captured image data, including: displaying a representation of a subject in the representation of the previously captured image data; anddisplaying a representation of a first virtual avatar over at least a first portion of the representation of the subject in the representation of the previously captured image data;while displaying the representation of the previously captured image data, displaying, via the display apparatus, a set of one or more virtual avatar options;detecting, via the one or more input devices, a selection of a respective avatar option of the set of one or more avatar options, wherein the respective avatar option corresponds to a second virtual avatar, different from the first virtual avatar; andin response to detecting the selection of the respective avatar option: ceasing display of the representation of the first virtual avatar over at least the first portion of the representation of the subject in the representation of the previously captured image data; anddisplaying, at a location that was previously occupied by the first virtual avatar, a representation of the second virtual avatar over at least the first portion of the representation of the subject in the representation of the previously captured image data.","39","18/197242","2023-05-15","2023-0283884","2023-09-07","12170834","2024-12-17","Apple Inc.","Marcel Van Os | Lee Broughton | Nicholas V. King | Grant Paul | William A. Sorrentino, III","","","","H04N-0023/631","H04N-0023/631 | G06F-0003/04845 | G06T-0013/20 | G06T-0013/40 | H04M-0001/72436 | H04M-0001/72439 | H04N-0005/2226 | H04N-0005/2621 | H04N-0023/611 | H04N-0023/62 | H04N-0023/632 | A61B-0005/744 | G06F-0003/04842 | G06T-0007/50 | G06T-0007/70 | G06T-2207/30196 | H04N-2005/2726 | H04N-2007/145","H04N-023/63","H04N-023/63 | A61B-005/00 | G06F-003/04842 | G06F-003/04845 | G06T-007/50 | G06T-007/70 | G06T-013/20 | G06T-013/40 | H04M-001/72436 | H04M-001/72439 | H04N-005/222 | H04N-005/262 | H04N-005/272 | H04N-007/14 | H04N-023/611 | H04N-023/62","","","","","","4924051003621"
"US","US","P","B2","Surgical support system, data processing apparatus and method","A surgical support system including: an eye behaviour monitoring apparatus that monitors eye behaviour of a surgeon performing a surgical procedure to obtain eye behaviour data of the surgeon; a surgical data generating apparatus that generates surgical data associated with the surgical procedure being performed by the surgeon; a surgical intervention apparatus that performs an intervention procedure for intervening in the surgeon's performance of the surgical procedure; and a data processing apparatus that: determines a value of an intervention parameter associated with the surgical procedure using the obtained eye behaviour data; determines an acceptable range of the value of the intervention parameter using the generated surgical data, and if the determined value of the intervention parameter is outside the determined acceptable range of the value of the intervention parameter, controls the surgical intervention apparatus to perform the intervention procedure for intervening in the surgeon's performance of the surgical procedure.","1. A surgical support system comprising: an eye monitor to monitor eye behaviour of a surgeon performing a surgical procedure to obtain eye behaviour data of the surgeon;a surgical data sensor to generate surgical data associated with the surgical procedure being performed by the surgeon;a surgical intervention circuit configured to perform an intervention procedure for intervening in the surgeon'ss performance of the surgical procedure; anda processing circuit configured to:determine a value of an intervention parameter associated with the surgical procedure using the obtained eye behaviour data, wherein the intervention parameter is indicative of a likelihood of a potential error event in the surgical procedure based on eye behaviour data of the surgeon;determine an acceptable range of the value of the intervention parameter using the generated surgical data and the potential error event, wherein the acceptable range of the value of the intervention parameter being indicative of an acceptable likelihood of the surgeon making the potential error event, andif the determined value of the intervention parameter is outside the determined acceptable range of the value of the intervention parameter, control the surgical intervention circuit to perform the intervention procedure for intervening in the surgeon'ss performance of the surgical procedure, wherein the intervention procedure includes:outputting an alert signal indicating that the determined likelihood of the surgeon making the potential error event in the surgical procedure exceeds the acceptable likelihood; and/orproviding an adjustment signal to a surgical apparatus used by the surgeon to perform a surgical function in the surgical procedure to control the surgical apparatus to adjust the performed surgical function to pre-empt the potential error event in the surgical procedure.","20","17/261589","2019-09-27","2021-0259789","2021-08-26","12161430","2024-12-10","SONY CORPORATION","Christopher Wright | Matthew Lawrenson | Naoyuki Hirota","2018-200263","EP","2018-10-12","A61B-0034/25","A61B-0034/25 | A61B-0005/163 | A61B-0034/30 | A61B-0090/08 | G06F-0003/013 | A61B-2017/00119 | A61B-2017/00216 | A61B-2034/254 | A61B-2034/258 | A61B-2090/0807","A61B-034/00","A61B-034/00 | A61B-005/16 | A61B-034/30 | A61B-090/00 | G06F-003/01 | A61B-017/00","","","","","","4924050001046"
"US","US","P","B2","Biometric sensor","An access control unit having a novel structure and arrangement, including a first layer comprising an electrostimulation contact interface, a second layer including a biometric sensor coupled to the electrostimulation contact interface, and a third layer including a microprocessor unit in communication with the electrostimulation contact interface. The second layer is sandwiched between the first layer and the third layer. The electrostimulation contact interface comprises one or more anode/cathode arrays configured to deliver neurostimulative excitations to the electrostimulation contact interface to elicit behavior modification.","1. A self-contained fully intergraded access control unit comprising: a first layer comprising an electrostimulation contact interface;a second layer comprising a biometric sensor coupled to the electrostimulation contact interface; anda third layer comprising a microprocessor unit in communication with the electrostimulation contact interface, whereinthe second layer is sandwiched between the first layer and the third layer of said access control unit,the electrostimulation contact interface comprises one or more anode-cathode arrays, andthe one or more anode-cathode arrays are configured to deliver neurostimulative excitations to the electrostimulation contact interface when a received biometric data by the biometric sensor does not match with a prestored biometric data.","19","18/129876","2023-04-02","2023-0248971","2023-08-10","12161867","2024-12-10","INTELLISHOT HOLDINGS INC.","Steven Wayne Goldstein","","","","A61N-0001/36028","A61N-0001/36028 | A61B-0005/0533 | A61N-0001/0456 | A61N-0001/0476 | A61N-0001/36031 | A61N-0001/36034 | G06F-0003/041 | G07C-0009/00563 | G06F-2203/04101","A61N-001/36","A61N-001/36 | A61B-005/0533 | A61N-001/04 | G06F-003/041 | G07C-009/00","","","","","","4924050001479"
"US","US","P","B2","Acquiring user-specific customization data for a medical device","A computer-implemented method, a medical device, and a system for acquiring user-specific customization data, i.e. user-specific language packages, are provided. The computer-implemented method comprises: determining a first set of user-specific customization data stored on the medical device; comparing the first set of user-specific customization data with a second set of user-specific customization data that is required by a user; and upon determining that the second set of user-specific customization data is different from the first set of user-specific customization data, acquiring, from a data storage external to the medical device, the second set of user-specific customization data or a delta between the first set and the second set of user-specific customization data.","1. A computer-implemented method for resource-efficient customization of an operating system of a medical device, the method comprising: identifying, by the medical device, a first set of user-specific customization data stored on the medical device;determining, by the medical device, a second set of user-specific customization data that is required by a user;comparing, by the medical device, the first set of user-specific customization data with the second set of user-specific customization data;in response to determining that the second set of user-specific customization data is different from the first set of user-specific customization data stored on the medical device: determining, by the medical device, a delta consisting of one or more items of data present in the second set of user-specific customization data and absent from the first set of user-specific customization data stored on the medical device;acquiring a first subset of the second set of user-specific customization data from the first set of user-specific customization data stored on the medical device; andacquiring a second subset of the second set of user-customization data from a data storage external to the medical device, the second subset consisting of the delta between the first set of user-specific customization data and the second set of user-specific customization data;customizing, based on the acquired delta, an output generated by the operating system of the medical device during treatment of the user, wherein customizing the output comprises changing a graphical layout of one or more graphical user interface (GUI) elements during the treatment of the user based on the second subset of the second set of user-customization data acquired from the external data storage; andcausing the medical device to provide the customized output to the user.","20","17/383524","2021-07-23","2023-0022816","2023-01-26","12159130","2024-12-03","FRESENIUS MEDICAL CARE DEUTSCHLAND GMBH | FRESENIUS MEDICAL CARE HOLDINGS, INC.","Karsten Fischer | Stefan Perplies","","","","G06F-0008/65","G06F-0008/65 | G06F-0003/0481 | G06F-0009/4451 | G06F-0009/451 | G16H-0010/65 | G16H-0040/40 | G16H-0040/60 | G16H-0040/63 | H04L-0063/08 | A61M-0001/14 | H04L-2463/082","G06F-008/65","G06F-008/65 | G06F-003/0481 | G06F-009/445 | G06F-009/451 | G16H-010/65 | G16H-040/40 | G16H-040/60 | G16H-040/63 | H04L-009/40 | A61M-001/14","","","","","","4924049003292"
"US","US","P","B2","Intelligent joint prosthesis","Medical devices coupled to a sensor, and systems including such devices, can generate data and analysis based on that data, which may be used to identify and/or address problems associated with the implanted medical device, including incorrect placement of the device, unanticipated degradation of the device, and undesired movement of the device. Also provided are medical devices coupled to a sensor, and devices and methods to address problems that have been identified with an implanted medical device.","1. A method for determining loosening of a prosthesis comprising at least one sensor, wherein the prosthesis is implanted in or on a body part of a patient, the method comprising: a) during a first monitoring session: obtaining data comprising one or both of acceleration data and velocity data from the at least one sensor,bandpass filtering the data to obtain filtered data within a frequency range relevant to a movement of the prosthesis relative to the body part, anddetermining a standardized norm of the movement based on the filtered data;b) during one or more second monitoring sessions that occur subsequent to the first monitoring session: obtaining data comprising one or both of acceleration data and velocity data from the at least one sensor,bandpass filtering the data to obtain filtered data within the frequency range, anddetermining a current description of the movement based on the filtered data; andc) comparing the current description of the movement to the standardized norm of the movement, to thereby identify loosening of the prosthesis relative to the body part.","9","17/490400","2021-09-30","2022-0031238","2022-02-03","12159714","2024-12-03","Canary Medical Inc.","Jeffrey M. Gross | Peter J. Schiller | William L. Hunter | Fred Cushner | Patrick M. Aubin","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/0031 | A61B-0005/076 | A61B-0005/11 | A61B-0005/1122 | A61B-0005/4851 | A61B-0005/6811 | A61B-0005/686 | A61B-0005/725 | A61B-0005/7282 | A61B-0005/746 | A61B-0005/747 | A61F-0002/32 | A61F-0002/38 | A61F-0002/40 | A61F-0002/4657 | G01C-0019/00 | G01P-0015/08 | G01P-0015/18 | G06Q-0040/08 | G16H-0040/63 | A61B-0005/0022 | A61B-2560/0209 | A61B-2560/0475 | A61B-2562/0219 | A61B-2562/08 | A61F-2002/3067 | A61F-0002/389 | H04L-0067/12 | H04W-0004/38","G16H-040/67","G16H-040/67 | A61B-005/00 | A61B-005/07 | A61B-005/11 | A61F-002/32 | A61F-002/38 | A61F-002/40 | A61F-002/46 | G01C-019/00 | G01P-015/08 | G01P-015/18 | G06Q-040/08 | G16H-040/63 | A61F-002/30 | H04L-067/12 | H04W-004/38","","","","","","4924049003872"
"US","US","P","B2","Intercommunication and cooperative operation of surgical devices","Examples described herein any include a surgical module for use within the surgical system. The surgical module may include a first port connected to a surgical hub; a second port connected to an additional surgical module; and a controller. The controller may be configured to receive surgical data; determine if the surgical data is a first type of data or a second type of data; and instruct the surgical module to send the surgical data to the first port if the surgical data is the first type of data or to the second port if the surgical data is the second type of data.","1. A first surgical module for use within a surgical system, comprising: a first port connected to a surgical hub, wherein the first port comprises a control plane, a data plane, and a backplane that facilitates data communication between the first surgical module and the surgical hub;a second port connected to a second surgical module, wherein the second port comprises an external wire that facilitates data communication between the first surgical module and the second surgical module; anda controller configured to: receive, via the first port, a first indication from the surgical hub that the second surgical module is energized to perform a first surgical task;based on receiving the first indication, prevent the first surgical module from performing a second surgical task;based on receiving the first indication, instruct the first surgical module to perform a third surgical task;send, via a second port, a second indication to the second surgical module that the first surgical module is prevented from performing the second surgical task and is instructed to perform the third surgical task; andsend, via the first port, a third indication to the surgical hub that the first surgical module is prevented from performing the second surgical task and is performing the third surgical task.","20","17/384455","2021-07-23","2023-0026893","2023-01-26","12154683","2024-11-26","CILAG GMBH INTERNATIONAL","Frederick E. Shelton, IV | Shane R. Adams | Kevin Fiebig | Jeffrey D. Messerly","","","","G16H-0040/20","G16H-0040/20 | A61B-0017/00 | A61B-0018/1206 | A61B-0034/10 | A61B-0034/20 | A61B-0034/25 | A61B-0034/30 | A61B-0034/32 | A61B-0090/08 | A61B-0090/37 | G05B-0013/0265 | G06F-0003/14 | G06F-0003/1423 | G06F-0003/167 | G06F-0009/4881 | G06F-0009/542 | G06F-0013/4068 | G06F-0016/211 | G06F-0016/284 | G06F-0016/285 | G06N-0020/00 | G06Q-0010/30 | G06T-0011/60 | G08B-0005/22 | G10L-0015/22 | G16H-0010/60 | G16H-0015/00 | G16H-0020/40 | G16H-0030/40 | G16H-0040/40 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | G16H-0050/70 | H04L-0001/22 | H04L-0041/12 | H04L-0065/80 | H04L-0067/12 | H04L-0067/125 | H04N-0005/272 | H04N-0007/15 | A61B-0008/06 | A61B-2017/00221 | A61B-2018/00702 | A61B-2018/00994 | A61B-2034/2072 | A61B-2034/254 | A61B-2090/364 | A61B-2090/365 | A61B-2090/373 | G06F-0021/6245 | G06F-0040/169 | G10L-2015/223 | G16H-0030/20 | H02J-0007/0063","G16H-040/40","G16H-040/40 | A61B-017/00 | A61B-018/12 | A61B-034/00 | A61B-034/10 | A61B-034/20 | A61B-034/30 | A61B-034/32 | A61B-090/00 | G05B-013/02 | G06F-003/14 | G06F-003/16 | G06F-009/48 | G06F-009/54 | G06F-013/40 | G06F-016/21 | G06F-016/28 | G06N-020/00 | G06Q-010/30 | G06T-011/60 | G08B-005/22 | G10L-015/22 | G16H-010/60 | G16H-015/00 | G16H-020/40 | G16H-030/40 | G16H-040/20 | G16H-040/63 | G16H-040/67 | G16H-050/20 | G16H-050/70 | H04L-001/22 | H04L-041/12 | H04L-065/80 | H04L-067/12 | H04L-067/125 | H04N-005/272 | H04N-007/15 | A61B-008/06 | A61B-018/00 | G06F-021/62 | G06F-040/169 | G16H-030/20 | H02J-007/00","","","","","","4924048004907"
"US","US","P","B2","Methods and system for monitoring and assessing employee moods","Methods and system for monitoring and assessing employee moods are disclosed. A proposed enterprise employee monitoring system includes surveillance cameras, a facial recognition module, an emotional analyzer module, and an employee database. The surveillance cameras capture image data including employee individuals within the enterprise. The facial recognition module identifies the individuals in the image data, and the emotional analyzer module determines an emotional state of the individuals based upon the image data. The employee database stores employee information and the emotional state information from the emotional analyzer module, based upon the identification performed by the facial recognition module.","1. A system, comprising: surveillance cameras configured to capture image data of a scene within an enterprise, the scene including an individual; anda device comprising: a memory storing instructions thereon; andat least one processor coupled with the memory and configured by the instructions to: determine, based on the image data, emotional state information defining an emotional state of the individual;determine, based on the image data and the emotional state information, an emotion level for the individual corresponding to the emotional state information of the individual, the emotion level quantifying the emotional state according to a defined scale, wherein the defined scale discretizes a range of intensity of a type of the emotional state;determine individual information based on the image data in response to the emotional state information corresponding to a particular type of the emotional state; andsend, to a second device, the emotional state information, the emotion level, the individual information, and a desk location of a desk of the individual within a building of the enterprise in response to the emotion level for the individual achieving a predefined threshold level in the defined scale, wherein the predefined threshold level exceeds a lower limit of the range of intensity,wherein the second device is configured to: request records for other individuals with matching desk locations of the desk location of the individual within the building of the enterprise; andperform statistical analysis of the emotional state information and the emotion level of the individual and of the other individuals to detect a group level trend in an emotional state of a group including the individual and the other individuals.","15","16/756905","2018-11-02","2021-0196169","2021-07-01","12155665","2024-11-26","TYCO FIRE & SECURITY GMBH","Peter Alexander Ainsworth | Ian C. Westmacott | Martin J. Donaghy | Derek Boyes | Terry Neill | John McKenna | Anne Gallagher | Mark Paterson | Ashish Italiya","","","","H04L-0063/102","H04L-0063/102 | A61B-0005/1176 | A61B-0005/165 | G06Q-0010/0631 | G06Q-0010/105 | G06Q-0010/1093 | G06V-0040/166 | G06V-0040/172 | G07C-0009/00904 | G07C-0009/257 | G07C-0009/30 | G07C-0009/37 | G08B-0013/19608 | H04L-0063/105 | H04N-0007/181 | H04W-0004/023 | H04W-0004/029 | A61B-2503/24 | G06Q-0050/26 | G06V-0040/16 | G06V-0040/174","G06Q-010/0631","G06Q-010/0631 | A61B-005/1171 | A61B-005/16 | G06Q-010/105 | G06Q-010/1093 | G06V-040/16 | G07C-009/00 | G07C-009/25 | G07C-009/30 | G07C-009/37 | G08B-013/196 | H04L-009/40 | H04N-007/18 | H04W-004/02 | H04W-004/029 | G06Q-050/26","","","","","","4924048005884"
"US","US","P","B2","Systems and methods for visualizing anatomy, locating medical devices, or placing medical devices","A medical device-placing system including a medical-device tip-location sensor (""TLS"") configured for placement on a chest of a patient, an ultrasound probe, a console, and an alternative-reality headset. The ultrasound probe can be configured to emit ultrasound signals into the patient and receive echoed ultrasound signals from the patient. The console can be configured to transform the echoed ultrasound signals to produce ultrasound-image segments corresponding to anatomical structures of the patient, as well as transform TLS signals from the TLS into location information for a medical device within the patient. The alternative-reality headset can include a display screen through which a wearer of the alternative-reality headset can see the patient. The display screen can be configured to display over the patient a virtual medical per the location information for the medical device within objects of virtual anatomy corresponding to the ultrasound-image segments.","1. A wireless medical device-placing system comprising: an ultrasound probe configured to emit ultrasound signals into a patient and receive echoed ultrasound signals from the patient by way of a piezoelectric sensor array;a medical-device tip-location sensor (""TLS"") configured for placement on a chest of the patient; andan alternative-reality headset configured to wirelessly communicate with the ultrasound probe and the TLS, the alternative-reality headset including:a frame having electronic circuitry including memory and a processor configured to: transform the echoed ultrasound signals to produce ultrasound-image segments corresponding to anatomical structures of the patient; andtransform TLS signals from the TLS into location information for a medical device within the patient when the TLS is placed on the chest of the patient; anda display screen coupled to the frame through which a wearer of the alternative-reality headset can see an environment including the patient, the display screen configured to: display a virtual medical device in accordance with the location information for the medical device within objects of virtual anatomy corresponding to the ultrasound-image segments;display one or more graphical-control-element windows including output corresponding to one or more processes of the medical device-placing system; ordisplay both the virtual medical device within the objects of virtual anatomy and the one or more windows.","27","16/430414","2019-06-03","2019-0307419","2019-10-10","12144675","2024-11-19","BARD ACCESS SYSTEMS, INC.","Tyler L. Durfee","","","","A61B-0008/0841","A61B-0008/0841 | A61B-0005/066 | A61B-0005/283 | A61B-0008/14 | A61B-0008/463 | A61B-0008/469 | A61B-0008/5253 | G02B-0027/0172 | G06F-0003/017 | G06T-0007/37 | G02B-2027/0138 | G02B-2027/014 | G02B-2027/0141 | G06T-2207/10132","A61B-008/00","A61B-008/00 | A61B-005/06 | A61B-005/283 | A61B-008/08 | A61B-008/14 | G02B-027/01 | G06F-003/01 | G06T-007/37","","","","","","4924047001086"
"US","US","P","B2","Distributed vehicle system control system and method","A distributed control system includes a remote control system configured to be communicatively coupled with plural separate vehicle systems. The remote control system is configured to remotely control operation of the vehicle systems and/or communicate with the local vehicle control system or operator. The remote control system also is configured to one or more of change how many of the vehicle systems are concurrently controlled by the remote control system or change how many remote operators of the remote control system concurrently control the same vehicle system of the vehicle systems.","1. A distributed control system comprising: a remote control system configured to be communicatively coupled with plural separate vehicle systems, the remote control system configured to remotely control different operations of each of the vehicle systems,the remote control system configured to determine that a remote operator ratio needs to change based at least in part on one or more characteristics of a common vehicle system of the vehicle systems, the remote operator ratio representative of a number of remote operators of the remote control system configured to concurrently control one or more different operations of the common vehicle system,the remote control system configured to change the remote operator ratio responsive to determining that the remote operator ratio needs to change, wherein changing the remote operator ratio changes one or more of how many of the remote operators concurrently control the one or more of the different operations of the common vehicle system or which of the remote operators are assigned to concurrently control the one or more of the different operations of the common vehicle system.","20","16/885318","2020-05-28","2020-0363797","2020-11-19","12147228","2024-11-19","TRANSPORTATION IP HOLDINGS, LLC","James D Brooks | Harry Kirk Mathews, Jr. | Paul Houpt","","","","G05D-0001/0027","G05D-0001/0027 | A61B-0005/0077 | A61B-0005/0205 | A61B-0005/11 | A61B-0005/18 | A61B-0005/318 | A61B-0005/369 | B61L-0027/14 | B61L-0027/40 | G05D-0001/0077 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0816 | B61L-0003/127 | G05D-0001/0016 | G06Q-0010/0631 | G06Q-0050/40 | G08G-0009/00 | H04L-0067/12 | H04L-0067/52","G05D-001/00","G05D-001/00 | A61B-005/00 | A61B-005/0205 | A61B-005/11 | A61B-005/18 | A61B-005/318 | A61B-005/369 | B61L-027/14 | B61L-027/40 | A61B-005/021 | A61B-005/024 | A61B-005/08 | B61L-003/12 | G06Q-010/0631 | G06Q-050/40 | G08G-009/00 | H04L-067/12 | H04L-067/52","","","","","","4924047003608"
"US","US","P","B2","Interactive electronic content delivery in coordination with rapid decoding of brain activity","A method and system for providing a user with virtual objects within an environment, characterizing interactions with the virtual objects using a brain computer interface, and modulating features of the virtual objects based upon improved classifiers associated with the interactions. The method and system can be used to rapidly customize virtual objects to a specific user in applications related to increasing engagement with traditional and new media content, virtual and augmented reality products, streamlining interactions with input devices in digital and physical environments, providing user authentication tools, providing more secure cybersecurity features, and delivering tailored content to users.","1. A method for improving decoding of neurological activities, the method comprising: providing a digital object to a user within a virtual environment;detecting a neural signal stream from a brain computer interface (BCI) coupled to the user, as the user interacts with the digital object;in a first reinforcement loop implemented in a first time window: generating a first classification of a neurological activity of the user upon processing the neural signal stream with a decoding algorithm,in response to the first classification indicating a stressed state, modulating a first set of a plurality of modulation features of the digital object to induce an unstressed state, andin response to the first classification indicating an unstressed state, modulating a second set of the plurality of modulation features of the digital object to maintain the unstressed state, andin a second reinforcement loop implemented in a second time window subsequent to the first time window: generating a second classification of neurological activity of the user as the user interacts with the modulated digital object, andmodulating a set of parameters of the decoding algorithm based on a comparison of the first classification to the second classification.","20","18/303382","2023-04-19","2023-0259208","2023-08-17","12147603","2024-11-19","ARCTOP LTD","Daniel Furman | Eitan Kwalwasser","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/372 | G06F-0001/163 | G06F-0021/31 | G06N-0020/00 | G06T-0019/20 | G16H-0040/60 | G06F-2221/2103 | G06F-2221/2139 | G06T-2219/2021","G06F-003/01","G06F-003/01 | A61B-005/00 | A61B-005/372 | G06F-001/16 | G06F-021/31 | G06N-020/00 | G06T-019/20 | G16H-040/60","","","","","","4924047003979"
"US","US","P","B2","Systems and methods for selecting, activating, or selecting and activating transducers","A graphical representation may be displayed including at least a plurality of transducer graphical elements, each transducer graphical element of the plurality of transducer graphical elements representative of a respective transducer of a plurality of transducers of a transducer-based device. A set of user input may be received including an instruction set to reposition a first transducer graphical element in a state in which the first transducer graphical element is located at a first location in the graphical representation and a second transducer graphical element is located at a second location in the graphical representation, the second location closer to a predetermined location in the graphical representation than the first location. In response to conclusion of receipt of the set of user input, the first transducer graphical element may be repositioned from the first location in the graphical representation to the predetermined location in the graphical representation.","1. A medical system comprising: a data processing device system;an input-output device system communicatively connected to the data processing device system; anda memory device system communicatively connected to the data processing device system and storing a program executable by the data processing device system, the program comprising:first reception instructions configured to cause reception of transducer data via the input-output device system, the transducer data indicating data acquired by at least a first transducer set;two-dimensional graphical representation instructions configured to cause display, via the input-output device system and based at least on an analysis of the transducer data, of a two-dimensional graphical representation including a two-dimensional map of intra-cardiac information;second reception instructions configured to cause reception of a set of user input via the input-output device system;graphical path display instructions configured to cause generation and display, at least in response to receiving at least part of the set of user input, and via the input-output device system, of a two-dimensional graphical path among the intra-cardiac information in the two-dimensional map of intra-cardiac information; andthree-dimensional graphical representation instructions configured to cause display of a three-dimensional graphical representation including a three-dimensional map of intra-cardiac information with a three-dimensional graphical path displayed among the intra-cardiac information in the three-dimensional map of intra-cardiac information, the three-dimensional graphical path corresponding to the generated two-dimensional graphical path, but visually represented as three dimensional,wherein the displayed two-dimensional graphical path comprises an arcuate portion.","38","17/177517","2021-02-17","2021-0186438","2021-06-24","12133745","2024-11-05","KARDIUM INC.","Saar Moisa | Michael Hermann Weber","","","","A61B-0005/743","A61B-0005/743 | A61B-0005/015 | A61B-0005/02 | A61B-0005/283 | A61B-0005/287 | A61B-0005/6859 | A61B-0005/6869 | A61B-0005/72 | A61B-0005/7435 | A61B-0005/748 | G06F-0003/04815 | G06F-0003/0482 | G06F-0003/04842 | G06F-0003/04847 | A61B-0005/150022 | A61B-0005/150969 | A61B-0005/157 | A61B-2018/00214 | A61B-2018/00267 | A61B-2018/00357 | A61B-2018/00577 | A61B-2018/00642 | A61B-2018/00702 | A61B-2018/00791 | A61B-2018/00839 | A61B-2018/00875 | A61B-2018/1467 | A61B-0018/1492 | A61B-2034/107 | A61B-2560/0475 | G16H-0040/63","A61B-005/00","A61B-005/00 | A61B-005/01 | A61B-005/02 | A61B-005/283 | A61B-005/287 | G06F-003/04815 | G06F-003/0482 | G06F-003/04842 | G06F-003/04847 | A61B-005/15 | A61B-005/157 | A61B-018/00 | A61B-018/14 | A61B-034/10 | G16H-040/63","","","","","","4924045001401"
"US","US","P","B2","Wearable device network system","In one embodiment, a method to track cardiac health of a patient is described. The method includes communicating, via a mobile device, to a cardiac device worn by the patient, wherein the cardiac device includes at least one sensor. The method also includes receiving cardiac device data from the cardiac device and filtering the cardiac device data based at least in part on patient logic rules. The method includes pushing the cardiac device data to one or more personnel based at least in part on the patient logic rules.","1. A method to track cardiac health of a patient, the method comprising: communicating, via a mobile device, to a cardiac device worn by the patient, wherein the cardiac device includes at least one sensor;receiving cardiac device data from the cardiac device;filtering the cardiac device data based, at least in part, on patient logic rules, wherein the patient logic rules comprise rules to restrict and grant access to portions of the cardiac device data; andpushing the filtered cardiac device data to one or more personnel based, at least in part, on the patient logic rules.","19","17/518573","2021-11-03","2022-0225949","2022-07-21","12127860","2024-10-29","KESTRA MEDICAL TECHNOLOGIES, INC. | WEST AFFUM HOLDINGS DAC","Traci S. Umberger | Brian D. Webster | Phillip D. Foshee, Jr. | Gordon P. Teddy | Krystyna Szul","","","","A61B-0005/747","A61B-0005/747 | A61B-0005/02438 | A61B-0005/725 | A61B-0005/7264 | A61B-0005/746 | A61B-0005/749 | G06F-0021/6254 | G16H-0010/60 | G16H-0040/67 | G16H-0050/30 | A61N-0001/3904 | A61N-0001/3925","A61B-005/00","A61B-005/00 | A61B-005/024 | G06F-021/62 | G16H-010/60 | G16H-040/67 | G16H-050/30 | A61N-001/39","","","","","","4924044001281"
"US","US","P","B2","Packaging assembly","A computing device comprises a memory and a data interface for connecting to a corresponding interface in a storage assembly. The memory comprises instructions which, on connection with the storage assembly, cause a processor of the storage assembly to: detect, using a device sensor of the storage assembly, one or more medicament delivery devices stored in the storage assembly; and display, on a display screen provided on the storage assembly, information relating to a medicament stored in the storage assembly.","1. A storage assembly configured to store one or more medicament delivery devices, the storage assembly comprising a processor, an interface, a device sensor and a Bluetooth module, wherein when a computing device is connected with the storage assembly via the interface, the storage assembly is configured to receive instructions from the computing device, the instructions causing the processor of the storage assembly to:detect, using the device sensor of the storage assembly, the one or more medicament delivery devices stored in the storage assembly; andsend, using the Bluetooth module of the storage assembly, information relating to at least one of the one or more medicament delivery devices stored in the storage assembly to an external device.","20","18/347076","2023-07-05","2023-0338641","2023-10-26","12128209","2024-10-29","SANOFI","Michael Helmer","2017-306949","EP","2017-12-28","A61M-0005/002","A61M-0005/002 | A61M-0005/20 | F25D-0011/00 | G06K-0007/10297 | G06K-0007/10861 | G06K-0007/1413 | G06Q-0010/0875 | G16H-0020/17 | G16H-0040/20 | G16H-0040/40 | G16H-0040/67 | H04W-0076/10 | A61M-2205/3576 | A61M-2205/3606 | A61M-2205/502 | A61M-2205/52 | A61M-2205/6054 | A61M-2205/6063 | A61M-2205/6072 | G06K-2007/10504","A61M-005/00","A61M-005/00 | A61M-005/20 | F25D-011/00 | G06K-007/10 | G06K-007/14 | G06Q-010/0875 | G16H-020/17 | G16H-040/20 | G16H-040/40 | G16H-040/67 | H04W-076/10","","","","","","4924044001629"
"US","US","P","B2","System and method for adapting a control function based on a user profile","The vehicle control system/method for adapting a control function based on a user profile may comprise: a gesture recognition module; a user profile module; a function control module; a processor; a non-transitory storage element coupled to the processor; encoded instructions stored in the non-transitory storage element, wherein the encoded instructions when implemented by the processor, configure the system to: identify a user; retrieve a user profile for the identified user; receive at a gesture recognition module, an input indicating a gesture from the user; identify a control function request corresponding to the gesture input; send a verification of the control function request; and receive at a function control module characteristics parsed from the user profile that effect the control function request by the user profile module to adapt a control function command for an adapted control function output by the function control module.","1. A system, comprising: memory including machine-readable instructions; andone or more processors configured, in response to executing the machine-readable instructions, to perform operations comprising: determining a gesture of a user located within a cabin of a vehicle;generating a control signal based on the gesture, the control signal causing a preview of a modification of a function or setting of a vehicle subsystem to the user for a predetermined duration; andin response to receiving neither a confirmation indication nor a cancelation indication associated with the modification from the user during the predetermined duration, maintaining the modification of the function or setting after expiration of the predetermined duration.","20","17/581459","2022-01-21","2022-0147578","2022-05-12","12130870","2024-10-29","AUTOCONNECT HOLDINGS LLC","Christopher P. Ricci","","","","G06F-0016/951","G06F-0016/951 | A61B-0005/0077 | A61B-0005/4809 | A61B-0005/6808 | A61B-0005/7405 | A61B-0005/742 | A61B-0007/04 | B60C-0001/00 | B60H-0001/00742 | B60K-0035/00 | B60N-0002/0244 | B60Q-0009/00 | B60R-0025/00 | B60R-0025/01 | B60R-0025/1004 | B60R-0025/102 | B60R-0025/20 | B60R-0025/25 | B60W-0050/085 | B60W-0050/10 | B60W-0050/14 | G01C-0021/3484 | G01C-0021/365 | G01C-0021/3667 | G01C-0021/3691 | G01C-0021/3697 | G01S-0019/42 | G05D-0001/0016 | G05D-0001/0276 | G05D-0023/1917 | G06F-0003/013 | G06F-0003/017 | G06F-0003/0481 | G06F-0003/0482 | G06F-0003/04842 | G06F-0003/04886 | G06F-0003/0622 | G06F-0003/0637 | G06F-0003/0673 | G06F-0009/451 | G06F-0016/183 | G06F-0016/24575 | G06F-0016/25 | G06F-0016/252 | G06F-0016/583 | G06F-0021/00 | G06F-0021/31 | G06F-0021/32 | G06Q-0010/00 | G06Q-0010/02 | G06Q-0010/20 | G06Q-0030/00 | G06Q-0030/012 | G06Q-0030/0265 | G06Q-0030/0266 | G06Q-0030/0633 | G06Q-0030/0639 | G06Q-0030/0641 | G06Q-0030/0645 | G06Q-0050/40 | G06V-0020/59 | G06V-0020/593 | G06V-0040/166 | G06V-0040/168 | G06V-0040/172 | G06V-0040/20 | G06V-0040/28 | G07C-0005/02 | G07C-0005/08 | G07C-0005/0825 | G07C-0005/0833 | G07C-0009/00563 | G08B-0013/19647 | G08B-0021/0205 | G08B-0021/06 | G08B-0021/18 | G08B-0025/016 | G08B-0029/188 | G08G-0001/01 | G08G-0001/07 | G08G-0001/096725 | G08G-0001/096741 | G08G-0001/096775 | G08G-0001/0968 | G08G-0001/096805 | G08G-0001/096811 | G08G-0001/096844 | G08G-0001/164 | G08G-0001/207 | G09G-0005/37 | H04L-0051/02 | H04L-0063/0236 | H04L-0063/0428 | H04L-0063/102 | H04L-0067/10 | H04L-0067/12 | H04L-0067/306 | H04L-0067/55 | H04N-0021/2225 | H04N-0021/2265 | H04N-0021/2393 | H04N-0021/25816 | H04N-0021/43615 | H04N-0021/43637 | H04N-0021/454 | H04N-0021/6408 | H04N-0021/64322 | H04W-0004/021 | H04W-0004/12 | H04W-0004/21 | H04W-0004/30 | H04W-0004/40 | H04W-0004/48 | H04W-0004/60 | H04W-0004/70 | H04W-0004/80 | H04W-0012/06 | H04W-0012/088 | H04W-0036/34 | H04W-0048/04 | H04W-0076/11 | H04W-0076/19 | H04W-0084/18 | A61B-2503/04 | B60K-0035/10 | B60K-0035/20 | B60K-0035/28 | B60K-0035/81 | B60K-2360/11 | B60K-2360/146 | B60Q-0001/52 | B60R-0011/04 | B60R-0025/2081 | B60R-0025/257 | B60W-2050/0067 | B60W-2050/0085 | G01C-0021/362 | G02B-0027/0093 | G05D-0001/021 | G06F-0003/0488 | G06F-2203/04803 | G06V-0040/15 | G06V-0040/16 | G09G-2380/10 | H04L-0067/34 | H04N-0007/181 | H04W-0012/68 | H04W-0084/005","H04W-048/04","H04W-048/04 | A61B-005/00 | A61B-007/04 | B60C-001/00 | B60H-001/00 | B60K-035/00 | B60N-002/02 | B60Q-009/00 | B60R-025/00 | B60R-025/01 | B60R-025/10 | B60R-025/102 | B60R-025/20 | B60R-025/25 | B60W-050/08 | B60W-050/10 | B60W-050/14 | G01C-021/34 | G01C-021/36 | G01S-019/42 | G05D-001/00 | G05D-023/19 | G06F-003/01 | G06F-003/0481 | G06F-003/0482 | G06F-003/04842 | G06F-003/04886 | G06F-003/06 | G06F-009/451 | G06F-016/182 | G06F-016/2457 | G06F-016/25 | G06F-016/583 | G06F-016/951 | G06F-021/00 | G06F-021/31 | G06F-021/32 | G06Q-010/00 | G06Q-010/02 | G06Q-010/20 | G06Q-030/00 | G06Q-030/012 | G06Q-030/0251 | G06Q-030/0601 | G06Q-030/0645 | G06Q-050/40 | G06V-020/59 | G06V-040/16 | G06V-040/20 | G07C-005/02 | G07C-005/08 | G07C-009/00 | G08B-013/196 | G08B-021/02 | G08B-021/06 | G08B-021/18 | G08B-025/01 | G08B-029/18 | G08G-001/00 | G08G-001/01 | G08G-001/07 | G08G-001/0967 | G08G-001/0968 | G08G-001/16 | G09G-005/37 | H04L-009/40 | H04L-051/02 | H04L-067/10 | H04L-067/12 | H04L-067/306 | H04L-067/55 | H04N-021/2225 | H04N-021/226 | H04N-021/239 | H04N-021/258 | H04N-021/436 | H04N-021/4363 | H04N-021/454 | H04N-021/6408 | H04N-021/643 | H04W-004/021 | H04W-004/12 | H04W-004/21 | H04W-004/30 | H04W-004/40 | H04W-004/48 | H04W-004/60 | H04W-004/70 | H04W-004/80 | H04W-012/06 | H04W-012/088 | H04W-036/34 | H04W-076/11 | H04W-076/19 | H04W-084/18 | B60K-035/10 | B60K-035/20 | B60K-035/28 | B60K-035/81 | B60Q-001/52 | B60R-011/04 | B60W-050/00 | G02B-027/00 | G06F-003/0488 | G06V-040/10 | H04L-067/00 | H04N-007/18 | H04W-012/68 | H04W-084/00","","","","","","4924044004271"
"US","US","P","B2","System and methods for validating and performing operations on homomorphically encrypted data","Systems, methods and devices for validating and performing operations on homomorphically encrypted data are described herein. The methods include securely transmitting and extracting information from encrypted data without fully decrypting the data. A data request may include an encrypted portion including a set of confidential data. One or more sets of encrypted comparison data may be then retrieved from a database in response to the data request. The encrypted set of confidential data from the data request is then compared with each set of encrypted comparison data using one or more homomorphic operations to determine which set of encrypted comparison data matches the encrypted set of confidential data. If there is a match, this validates the set of confidential data. An encrypted indicator is then generated indicating success or failure in validating the set of confidential data, which may then be forwarded to a party associated with the data request.","1. A method for securely transmitting and extracting information from encrypted data without fully decrypting the data, comprising: receiving a data request, with at least an encrypted first portion of the data request encrypted according to a homomorphic encryption scheme, and the encrypted first portion of the data request comprising at least one set of confidential data comprising a plurality of strings corresponding to a first plurality of data fields in the at least one set of confidential data;retrieving one or more sets of encrypted comparison data from a database, each set of the one or more sets of encrypted comparison data encrypted according to the homomorphic encryption scheme and comprising a second plurality of data fields corresponding to the first plurality of data fields in the at least one set of confidential data;comparing the encrypted first portion of the data request comprising the at least one set of confidential data from the data request with each set of the one or more sets of encrypted comparison data using homomorphic operations on the plurality of strings collectively to determine which set of the one or more sets of encrypted comparison data matches the at least one set of confidential data of the encrypted first portion of the data request and validating the at least one set of confidential data upon a match, wherein the homomorphic operations combine to form an XNOR operation, wherein the XNOR operation is implemented following an XNOR model of z=(1?x?y)(1?x?y);generating an encrypted indicator indicating success or failure of validating the at least one set of confidential data;forwarding the encrypted indicator to a party associated with the data request; andwherein a second portion of the data request is compared to each set of the one or more sets of encrypted comparison cardholder card data to reduce a size of the one or more sets of encrypted comparison cardholder card data prior to comparing the one or more sets of encrypted comparison data with the at least one set of encrypted confidential cardholder data,wherein the at least one set of confidential data is never decrypted during the method.","16","17/571458","2022-01-08","2022-0129892","2022-04-28","12131319","2024-10-29","LORICA CYBERSECURITY INC.","Glenn Gulak | Alhassan Khedr","","","","G06Q-0020/3829","G06Q-0020/3829 | A61B-0005/72 | G06N-0020/00 | G06Q-0020/382 | G06Q-0020/38215 | G16H-0010/40 | G16H-0010/60 | H04L-0009/008 | H04L-0009/3093 | H04L-0063/0414 | H04L-0063/0428 | G06Q-0020/08 | G06Q-2220/00","G06Q-020/38","G06Q-020/38 | A61B-005/00 | G06N-020/00 | G06Q-020/08 | G16H-010/40 | G16H-010/60 | H04L-009/00 | H04L-009/30 | H04L-009/40","","","","","","4924044004719"
"US","US","P","B2","Brain-computer interface for facilitating direct selection of multiple-choice answers and the identification of state changes","Methods, systems, apparatus, and non-transitory computer readable media are disclosed utilizing brain-computer interfaces (BCIs). Various embodiments are disclosed to allow a user to directly select multiple-choice answers, to provide motorized wheelchair controls, and to allow a user to play a game via the BCI. When used in a cognitive assessment test, embodiments include the administration of unmodified standardized tests with results in the same or a similar format as those taken without a BCI. Various embodiments are disclosed to improve the accuracy of BCI test administration using a three-step process for each test question, which includes determining whether the user intends to select an answer, monitoring user brain activity to determine a selected answer, and verifying the selected answer. In addition, the selected answer may be verified by monitoring user brain activity in accordance with a hold-release process to determine whether a user intends to initiate a state change.","1. A method implemented in a brain-computer interface (BCI) computer, the method comprising: calculating, by one or more processors, a first and a second range of classifier values based upon a user'ss electroencephalograph (EEG) signals while the user is exposed to a target and to a cancellation stimuli, respectively;calculating, by one or more processors, a first and a second training classifier threshold to separate the first and the second range of classifier values from one another;classifying, by one or more processors, received EEG signals while the user is subsequently exposed to the target or the cancellation stimuli as being within the first or the second range of classifier values based upon the first and a second training classifier thresholds;determining, by one or more processors, whether the user has been exposed to the target stimuli or to the cancellation stimuli based upon the classifying of the subsequently received EEG signals into one of the first or the second range of classifier values;calculating, by one or more processors, a hold-release classifier value based upon EEG signals received after determining whether the user has been subsequently exposed to the target stimuli or to the cancellation stimuli; andidentifying, by one or more processors, whether the user has decided to hold an action associated with the target stimuli or to release the action by switching to the cancellation stimuli based on a comparison between the hold-release classifier value and the first and second training classifier thresholds.","10","17/368207","2021-07-06","2021-0330242","2021-10-28","12121360","2024-10-22","THE REGENTS OF THE UNIVERSITY OF MICHIGAN","Jane E. Huggins | Seth Warschausky | Ramses Eduardo Alcaide","","","","A61B-0005/4088","A61B-0005/4088 | A61B-0005/372 | A61B-0005/374 | A61B-0005/378 | A61B-0005/7221 | A61B-0005/7264 | A61B-0005/7267 | A61B-0005/7282 | A61B-0005/742 | A61B-0005/7435 | A61F-0002/72 | A61F-0004/00 | G06F-0003/015 | G09B-0007/06","G06F-003/01","G06F-003/01 | A61B-005/00 | A61B-005/372 | A61B-005/374 | A61B-005/378 | A61F-002/72 | A61F-004/00 | G09B-007/06","","","","","","4924043001715"
"US","US","P","B2","Mixed-reality endoscope and surgical tools with haptic feedback for integrated virtual-reality visual and haptic surgical simulation","An apparatus has a device representing an endoscope, the device being either an endoscope or a dummy endoscope having shape and feel resembling an endoscope, and includes a tracker adapted to operate with a three-dimensional tracking system to track location and orientation of the device in three dimensions in a simulated operating-room environment. The apparatus also has a physical head model comprising hard and soft components, the device representing an endoscope configured to be inserted into the physical head model to provide a haptic feedback of endoscopic surgery.","1. A multimode VR apparatus comprising: an endoscope device selected from an endoscope and a dummy endoscope having shape and feel resembling that of an endoscope,a first wireless tracker operating with a three-dimensional tracking system to track location and orientation of the endoscope device in three dimensions in a simulated operating-room environment,a video modeling and display machine loaded with a computer-aided design (CAD) model of a head, providing a simulated endoscope view of a simulated head environment; anda physical head model comprising hard and soft physical components, and a second wireless tracker where the computer-aided design (CAD) model of the head is registered to a tracked position of the physical head model, the endoscope device being configured to be inserted into the physical head model to provide haptic feedback during manipulation resembling haptic feedback present during manipulation of an endoscope in a head to a person handling the endoscope device.","8","17/048991","2019-04-18","2021-0244474","2021-08-12","12118895","2024-10-15","ARIZONA BOARD OF REGENTS ON BEHALF OF THE UNIVERSITY OF ARIZONA","Samuel Barber | Saurabh Jain | Young-Jun Son | Eugene Chang","","","","G09B-0023/285","G09B-0023/285 | A61B-0017/24 | A61B-0034/10 | A61B-0034/20 | A61B-0090/36 | B33Y-0080/00 | G06F-0003/011 | G06F-0003/012 | G06F-0003/016 | G06F-0003/0346 | G06T-0007/11 | G09B-0023/30 | G16H-0020/40 | G16H-0040/63 | G16H-0050/50 | A61B-2017/00119 | A61B-2017/00216 | A61B-2017/00707 | A61B-2017/00716 | A61B-0017/29 | A61B-0017/320016 | A61B-0017/3201 | A61B-2018/00297 | A61B-0018/14 | A61B-2034/101 | A61B-2034/105 | A61B-2034/107 | A61B-2090/365 | A61B-2090/372 | A61B-2090/502 | G06T-2207/10068 | G06T-2207/10072 | G06T-2207/30004","G09B-023/28","G09B-023/28 | A61B-017/24 | A61B-034/10 | A61B-034/20 | A61B-090/00 | B33Y-080/00 | G06F-003/01 | G06F-003/0346 | G06T-007/11 | G09B-023/30 | G16H-020/40 | G16H-040/63 | G16H-050/50 | A61B-017/00 | A61B-017/29 | A61B-017/32 | A61B-017/3201 | A61B-018/00 | A61B-018/14 | A61B-090/50","","","","","","4924042005273"
"US","US","P","B2","Optimal multi-electrode transcutaneous stimulation with high focality and intensity","Methods, apparatus, and systems are disclosed for optimization techniques and a realistic 3D model to design optimal parameters for transcutaneous stimulation to achieve focalized stimulation of a target tissue such as the spinal cord, brain or other internal organ. The methods, apparatus, and systems include generation of a 3D model from a CT/MRI image, as well as an optimization algorithm that enables stimulation of any target location (e.g., on the dorsal root, or on the dorsal column) with any orientation at high precision.","1. A system comprising: a stimulation array comprising multiple stimulation electrodes arranged in an array;a stimulator circuit configured to provide independent stimulation signals to individual electrodes in the stimulator electrode array at a plurality of frequencies, intensities, and/or waveforms;an external device comprising: a non-transitory memory storing instructions, anda processor configured to access the non-transitory memory and execute the instructions to at least: receive one or more structural images of a target treatment region of a patient when the stimulation array is placed within the target treatment region;generate a 3D model of the target treatment region based on the one or more structural images of the target treatment region;calculate, based on the 3D model of the target treatment region, a lead field matrix associated with the stimulation electrode array and the target treatment region;determine a safety limit for transcutaneous stimulation of the target treatment region;generate a set of stimulation parameters for a stimulation to be delivered by the multiple stimulation electrodes based on the lead field matrix and the safety limit, wherein the set of stimulation parameters provides both high intensity and focal accuracy within the safety limit by assigning a relative weight to directional intensity and focality of stimulation at the target treatment region; andconfigure the stimulator circuit to operate utilizing the generated set of stimulation parameters to deliver the stimulation with the set of stimulation parameters to the patient transcutaneously.","16","18/332091","2023-06-09","2023-0402189","2023-12-14","12119120","2024-10-15","THE REGENTS OF THE UNIVERSITY OF CALIFORNIA","Ying Li | Wentai Liu | Yi-Kai Lo","","","","G16H-0050/50","G16H-0050/50 | A61B-0005/055 | A61B-0005/369 | A61B-0005/389 | A61B-0005/4848 | A61B-0006/032 | A61N-0001/08 | A61N-0001/205 | A61N-0001/36003 | A61N-0001/36007 | A61N-0001/36014 | A61N-0001/36031 | G06F-0017/11 | G06F-0030/23 | G16H-0020/40 | G16H-0030/40 | G16H-0070/20 | G06F-2111/10","G16H-050/50","G16H-050/50 | A61B-005/00 | A61B-005/055 | A61B-005/369 | A61B-005/389 | A61B-006/03 | A61N-001/08 | A61N-001/20 | A61N-001/36 | G06F-017/11 | G06F-030/23 | G16H-020/40 | G16H-030/40 | G16H-070/20 | G06F-111/10","","","","","","4924042005497"
"US","US","P","B2","Feedback device and method for providing thermal feedback using the same","A method for providing a thermal feedback, performed by a feedback device. The feedback device outputs the thermal feedback, by transmitting a heat generated by a thermoelectric element, to a user via a contact surface contacting with a body part of the user. The method may include obtaining a feedback start message including a type of the thermal feedback, and when the type of the thermal feedback is a thermal grill feedback, outputting the thermal grill feedback by performing a thermal grill operation in which a heat generating operation and a heat absorbing operation is combined. The outputting of the thermal grill feedback may include applying a forward power to the thermoelectric element to perform the heat generating operation, applying a reverse power of which a current direction is opposite to the forward power to the thermoelectric element to perform the heat absorbing operation, and repeating the applying the forward power and the applying the reverse power alternatively.","1. An electronic device comprising: a thermoelectric module comprising a plurality of thermoelectric groups and a contact surface, wherein the thermoelectric module is configured to contact with a body part of a user, transferring heat to the user or absorbing heat from the user, via the contact surface, wherein the contact surface comprises at least a first region and a second region, wherein each of the plurality of thermoelectric groups comprises a plurality of thermoelectric elements, and wherein the plurality of thermoelectric groups includes at least a first thermoelectric group placed on the first region and a second thermoelectric group placed on the second region; anda controller configured to control the plurality of thermoelectric groups individually by controlling an operation power applied to the thermoelectric module, andwherein the controller is configured to apply, during a first-time period, a first voltage to the first thermoelectric group to perform a heat absorbing operation, and apply, during a second-time period following the first-time period, a second voltage corresponding to the first voltage to the second thermoelectric group to perform the heat absorbing operation.","14","18/373024","2023-09-26","2024-0023897","2024-01-25","12102452","2024-10-01","TEGWAY CO., LTD.","Kyoungsoo Yi | Ockkyun Oh","10-2016-0157732 | 10-2016-0157733 | 10-2016-0157734 | 10-2016-0157735 | 10-2016-0157736 | 10-2016-0157737","KR | KR | KR | KR | KR | KR","2016-11-24 | 2016-11-24 | 2016-11-24 | 2016-11-24 | 2016-11-24 | 2016-11-24","A61B-0005/7271","A61B-0005/7271 | A61B-0005/01 | A61B-0005/015 | A61B-0018/1477 | F25B-0021/02 | G05D-0023/192 | G05D-0023/20 | G06F-0003/011 | G06F-0003/016 | H05B-0001/0227 | F25B-2321/0212 | Y02B-0030/70","F25B-029/00","F25B-029/00 | A61B-005/00 | A61B-005/01 | A61B-018/14 | F25B-021/02 | G05D-023/19 | G05D-023/20 | G06F-003/01 | H05B-001/02","","","","","","4924040001582"
"US","US","P","B2","Enhanced confirmations for touchscreen infusion pump","Enhanced confirmations for programming of touchscreen infusion pumps reduce the likelihood of users confirming pump parameters that have been mistakenly programmed. Following programming of a given pump parameter, an icon or image intuitively related to the pump parameter can be displayed on the touchscreen. To confirm programming of the parameter, the user can be required to trace the icon on the touchscreen. The user will therefore necessarily associate the confirmation step with the parameter being programmed and be far more likely to realize if a mistake has been made because the user was intending to program a different parameter than with simple confirmations involving the press of a confirmation button.","1. A method of delivering medicament to a user with a portable infusion pump, comprising: receiving user input programming a bolus delivery of medicament through a touchscreen display;displaying a first confirmation for the bolus delivery of medicament after receiving the user input programming the bolus delivery of medicament and determining an amount of the bolus delivery of medicament, the first confirmation displaying the amount of the bolus delivery of medicament and a touch-selectable first confirmation input comprising a pictorial icon having a shape related to the bolus delivery of medicament;receiving a touch-selection of the first confirmation input;displaying a second confirmation after receiving the first confirmation input, the second confirmation requiring a biometric authentication as a second confirmation input prior to delivering the bolus;receiving a biometric input as the second confirmation input;authenticating that the biometric input belongs to an authorized user;causing the bolus delivery of medicament to be delivered to the authorized user after authenticating the biometric input.","18","17/306022","2021-05-03","2021-0252218","2021-08-19","12102795","2024-10-01","TANDEM DIABETES CARE, INC.","Michael J. Rosinko","","","","A61M-0005/172","A61M-0005/172 | A61M-0005/142 | A61M-0005/1723 | G06F-0003/0484 | G06F-0021/32 | G16H-0020/17 | G16H-0020/60 | G16H-0040/63 | G16H-0050/30 | A61B-0005/1171 | A61B-0005/1172 | A61M-2005/14208 | A61M-2205/18 | A61M-2205/3592 | A61M-2205/50 | A61M-2205/505 | A61M-2205/52 | A61M-2205/581 | A61M-2205/609","A61M-005/172","A61M-005/172 | A61M-005/142 | G06F-003/0484 | G06F-021/32 | G16H-020/17 | G16H-020/60 | G16H-040/63 | G16H-050/30 | A61B-005/1171 | A61B-005/1172","","","","","","4924040001921"
"US","US","P","B2","External medical device that identifies a response activity","An external medical device is provided. The device can include monitoring circuitry configured to sense physiological information of a patient and a controller with one or more input components. The controller can be configured to: detect one or more patient events based, at least in part, on the physiological information; notify the patient of the detection of the one or more patient events; and receive a patient response to the notification. The patient response can include a response activity identifiable by the input component, which is configured to test a psychomotor ability of the patient, cognitive ability of the patient, strength, balance, stability, and flexibility of the patient, and/or to substantially confirm that a person performing the activity is the patient.","1. A wearable monitoring and treatment device for testing patient abilities, the device comprising: a garment configured to be worn on a torso of a patient;therapy electrodes supported by the garment and operatively connected to a shock generator configured to deliver an electrical therapy to the patient;one or more sensors supported by the garment configured to detect physiological information of the patient;at least one response button;a touch screen display; anda controller operatively connected to the shock generator, the one or more sensors, the at least one response button, and the touch screen display, wherein the controller is configured to: select at least one patient response action configured to test cognitive ability of the patient from a plurality of possible response actions that test cognitive ability and an associated level of difficulty for the at least one patient response action based on information provided by at least one of a service technician, a patient service representative, or a caregiver;identify a patient event treatable by shock therapy based, at least in part, on physiological information detected by the one or more sensors;upon detection of the patient event, instruct the patient to perform, with the at least one response button or the touch screen display, the selected at least one patient response action configured to test the cognitive ability of the patient; andsuspend the shock therapy when a complete or partial patient response action is received.","20","17/336433","2021-06-02","2021-0282722","2021-09-16","12089967","2024-09-17","ZOLL MEDICAL CORPORATION","Shane Volpe","","","","A61B-0005/7282","A61B-0005/7282 | A61B-0005/1124 | A61B-0005/162 | A61B-0005/349 | A61B-0005/6804 | A61B-0005/6823 | A61B-0005/74 | A61B-0005/7475 | A61N-0001/3904 | A61N-0001/3925 | G06F-0003/04817 | G06F-0003/04883 | A61N-0001/046 | A61N-0001/0484 | A61N-0001/3993 | G10L-0015/22 | G10L-0025/66","A61B-005/00","A61B-005/00 | A61B-005/11 | A61B-005/16 | A61B-005/349 | A61N-001/04 | A61N-001/39 | G06F-003/04817 | G06F-003/04883 | G10L-015/22 | G10L-025/66","","","","","","4924038001721"
"US","US","P","B2","Ocular system to optimize learning","A method to measure a cognitive load based upon ocular information of a subject includes the steps of: providing a video camera configured to record a close-up view of at least one eye of the subject; providing a computing device electronically connected to the video camera and the electronic display; recording, via the video camera, the ocular information of the at least one eye of the subject; processing, via the computing device, the ocular information to identify changes in ocular signals of the subject through the use of convolutional neural networks; evaluating, via the computing device, the changes in ocular signals from the convolutional neural networks by a machine learning algorithm; determining, via the machine learning algorithm, the cognitive load for the subject; and displaying, to the subject and/or to a supervisor, the cognitive load for the subject.","1. A method to measure a cognitive load based upon ocular information of a subject, the method comprising the steps of: providing a video camera configured to record a close-up view of at least one eye of the subject;providing a computing device electronically connected to the video camera and the electronic display;recording, via the video camera, the ocular information of the at least one eye of the subject;processing, via the computing device, the ocular information to identify changes in ocular signals of the subject through the use of convolutional neural networks;evaluating, via the computing device, the changes in ocular signals from the convolutional neural networks by a machine learning algorithm;determining, via the machine learning algorithm, the cognitive load for the subject; anddisplaying, to the subject and/or to a supervisor, the cognitive load for the subject.","4","18/128987","2023-03-30","2023-0306341","2023-09-28","12093871","2024-09-17","SENSEYE, INC.","David Zakariaie | Kathryn McNeil | Alexander Rowe | Joseph Brown | Patricia Herrmann | Jared Bowden | Taumer Anabtawi | Andrew R. Sommerlot | Seth Weisberg | Veronica Choi","","","","G06Q-0010/0635","G06Q-0010/0635 | A61B-0003/0025 | A61B-0003/0041 | A61B-0003/0091 | A61B-0003/112 | A61B-0003/113 | A61B-0003/145 | A61B-0005/1103 | A61B-0005/161 | A61B-0005/163 | A61B-0005/165 | A61B-0005/4845 | A61B-0005/4863 | A61B-0005/6898 | A61B-0005/7246 | G06N-0003/045 | G06N-0003/08 | G06Q-0010/06398 | G06Q-0010/10 | G06T-0007/73 | G06V-0010/143 | G06V-0010/454 | G06V-0010/764 | G06V-0020/46 | G06V-0040/18 | G06V-0040/19 | G06V-0040/193 | G16H-0015/00 | G16H-0030/20 | G16H-0050/20 | G16H-0050/50 | G16H-0050/70 | A61B-0005/7267 | A61B-2503/20 | G06N-0003/088 | G06T-2207/10016 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30041 | G16H-0050/30","G06K-009/62","G06K-009/62 | A61B-003/00 | A61B-003/11 | A61B-003/113 | A61B-003/14 | A61B-005/00 | A61B-005/11 | A61B-005/16 | G06N-003/045 | G06N-003/08 | G06Q-010/0635 | G06Q-010/0639 | G06Q-010/10 | G06T-007/73 | G06V-010/143 | G06V-010/44 | G06V-010/764 | G06V-020/40 | G06V-040/18 | G06V-040/19 | G16H-015/00 | G16H-030/20 | G16H-050/20 | G16H-050/50 | G16H-050/70 | G06N-003/088 | G16H-050/30","","","","","","4924038005571"
"US","US","P","B2","Determine image processing candidate medical images based on a number of the selected one or more medical images","An image processing apparatus selects one or a plurality of examinations to which a medical image belongs, determines image processing candidate examinations based on the selected one or plurality of examinations, displays medical images belonging to the determined image processing candidate examinations on a display unit, and executes image processing using, of the displayed medical images, a plurality of medical images selected by a user, wherein, when the one examination is selected, the selected one examination and one or a plurality of examinations obtained by a search based on the selected one examination are determined as the image processing candidate examinations, and when the plurality of examinations are selected, in the determining, the selected plurality of examinations are determined as the image processing candidate examinations.","1. An image processing apparatus comprising: a data acquisition unit configured to acquire information of a medical image including a plurality of medical images and information of examination in association with the plurality of medical images;a reception unit configured to receive at least any one of selection information in the medical image out from the plurality of medical images specified by a user and selection information in the examination out from a plurality of examinations specified by the user;a selection unit configured to acquire information of a reference examination and a comparative examination based on at least any one of a capturing date and time of the medical image in association with the examination and an order of selection from the user in association with the selection information, wherein the reference examination is configured to include one or more candidate reference images and the comparative examination is configured to include one or more candidate comparative images;a determination unit configured to determine a couple of candidate medical images including a reference image and a comparative image based on at least any one of a number of medical images selected in the selection information in the medical image and a number of examinations selected in the selection information in the examination;a display control unit configured to display the couple of candidate medical images determined by the determination unit on a display unit; andan image processing unit configured to execute image subtraction processing using, of the couple of candidate displayed medical images,wherein, when the number of medical images that are selected is one, the determination unit is configured to determine the reference image in association with the selected one medical image and the comparative image by making a search out from the plurality of medical images based on the selected one medical image,when the number of medical images that are selected is two or more and the number of examinations in association with the selected medical images is one, the determination unit is configured to determine the reference image out from the selected medical images based on any one of the selection order and the reception specified from the user and the comparative image by making a search out from the plurality of medical images based on the reference image, andwhen the number of medical images that are selected is two or more and the number of examinations in association with the selected medical images is two or more, the determination unit is configured to determine the reference image and the comparative image selected out from the selected medical images based on any one of the selection order and the reception specified from the user.","7","18/060478","2022-11-30","2023-0095776","2023-03-30","12094120","2024-09-17","Canon Kabushiki Kaisha","Itaru Otomaru | Toshimizu Yamane","2018-077138","JP","2018-04-12","G06T-0007/0016","G06T-0007/0016 | A61B-0005/7425 | G06F-0003/14 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/10101 | G06T-2207/10104 | G06T-2207/10108 | G06T-2207/10136 | G06T-2207/30004","G06K-009/00","G06K-009/00 | A61B-005/00 | G06F-003/14 | G06T-007/00","","","","","","4924038005818"
"US","US","P","B2","Systems and methods for conducting/defending digital warfare or conflict","In exemplary embodiments, an AI (Artificial Intelligence) enabled system is developed, configured, and/or intended to facilitate the offensive, defensive, and logistical capabilities of a person engaged in warfare in some direct form. This encompasses scenarios ranging from the solo unit behind enemy lines, the operational deployment of forces and resources, to strategic theater. Exemplary embodiments include building or configuring a highly advanced and aware system, facilitated by AI, driven by cloud and edge computing, to allow decisions and tactical advantage be deployed and calculated at the level it needs first. Exemplary embodiments may be configured to address both the kinetic real world as well as digital cyberspace, incorporate offensive and defensive technology, and be primarily driven by the concept to augment forces to support and counter threats as well as malicious enemy AI.","1. A system comprising a plurality of devices, sensors, and communication network(s), the system configured to: determine, through a plurality of measurements/readings taken by the plurality of devices, sensors, and communication network(s), underlying root cause(s) and motivation(s) of behavior(s) of at least one asset of a first group of one or more assets and (a) context(s) associated with the behavior(s) of the at least one asset; or(b) location and the context(s) associated with the behavior(s) of the at least one asset;analyze the measurements/readings, the underlying root cause(s) and motivation(s) of the behavior(s) of the at least one asset, and (a) the context(s) associated with the behavior(s) of the at least one asset or (b) the location and the context(s) associated with the behavior(s) of the at least one asset at the location, to thereby determine risk(s) associated with the behavior(s) of the at least one asset relative to their impact(s) on behavior(s) and associated motivation(s) by a second group of one or more assets that are not part of the first group of one or more assets; andfacilitate one or more actions to lower the risk of negative impact(s) of behavior(s) of the first group of one or more assets on the second group of one or more assets when compared to one or more threshold(s).","39","17/541707","2021-12-03","2022-0116736","2022-04-14","12096308","2024-09-17","CONQUER YOUR ADDICTION LLC","David H. Williams | Adam H. Williams","","","","H04W-0004/021","H04W-0004/021 | A61B-0005/0022 | A61B-0005/4842 | A61B-0005/6802 | A61B-0005/7264 | A61B-0005/747 | A63F-0009/12 | G06F-0021/36 | G06Q-0010/0635 | G06Q-0020/3224 | G06Q-0020/401 | G06Q-0020/4014 | G06Q-0020/4016 | G07F-0017/3237 | G16H-0040/67 | G16H-0050/30 | H04L-0063/107 | H04L-0063/20 | H04L-0067/12 | H04W-0004/029 | H04W-0012/08 | G06F-2221/2111 | H04W-0012/06","H04W-004/021","H04W-004/021 | A61B-005/00 | A63F-009/12 | G06F-021/36 | G06Q-010/0635 | G06Q-020/32 | G06Q-020/40 | G07F-017/32 | G16H-040/67 | G16H-050/30 | H04L-009/40 | H04L-067/12 | H04W-004/029 | H04W-012/08 | H04W-012/06","","","","","","4924038007978"
"US","US","P","B2","Systems and methods for positioning","Systems and methods for positioning in medical systems are provided. The system may obtain data associated with a scanning range of a subject. The system may also obtain an image of the subject on a couch of a medical radiation device. The image may be acquired by an imaging device when the couch is at a first position. The system may determine a position of the scanning range of the subject in the image. The system may further cause, based on the position of the scanning range in the image, the couch to move to a second position. The scanning range of the subject may be in a radiation region of the medical radiation device when the couch is located at the second position.","1. A system, comprising: at least one storage device including a set of instructions; andat least one processor configured to communicate with the at least one storage device, wherein when executing the set of instructions, the at least one processor is configured to direct the system to perform operations including: obtaining data associated with a scanning range of a subject, wherein the scanning range of the subject includes to a first anatomical location of the subject and a second anatomical location of the subject;obtaining an image of the subject on a couch of a medical radiation device, the image being acquired by an imaging device when the couch is at a first position;determining, based on the data associated with the scanning range of the subject, a position of the scanning range of the subject in the image; andcausing, based on the position of the scanning range in the image, the couch to move to a second position, including:obtaining a transforming relationship between a first coordinate system applied to the image and a second coordinate system, wherein the second coordinate system is a spatial coordinate system which is established based on a position of the medical radiation device, the transforming relationship including a transforming ratio of a number of pixels in the image to a corresponding distance in the second coordinate system;determining, based on the transforming relationship, the first position of the couch in the second coordinate system;determining, based on the position of the scanning range in the image, a first count of pixels from the first anatomical location to a radiation region of the medical radiation device in the image along a long axis of the couch;determining, based on the position of the scanning range in the image, a second count of pixels from the second anatomical location to the radiation region of the medical radiation device in the image along the long axis of the couch;determining, based on the first count of pixels, a first distance from the first anatomical location to the radiation region in the second coordinate system;determining, based on the second count of pixels, a second distance between the second anatomical location to the radiation region in the second coordinate system; anddetermining, based on the first distance, the second distance, the first position, and the transforming ratio, the second position of the couch.","20","17/446835","2021-09-03","2022-0061781","2022-03-03","12082953","2024-09-10","SHANGHAI UNITED IMAGING HEALTHCARE CO., LTD.","Xiaofen Zhao","2020-10916972","CN","2020-09-03","A61B-0006/0487","A61B-0006/0487 | A61B-0006/0407 | A61B-0006/465 | A61B-0006/467 | A61B-0006/488 | A61B-0006/54 | A61B-0006/582 | G06F-0003/0482 | G06F-0003/04847 | G06T-0007/0012 | G06T-0007/50 | G06T-0007/70 | A61B-0005/055 | G06T-2200/24 | G06T-2207/30004 | G06T-2207/30196","A61B-006/04","A61B-006/04 | A61B-006/00 | A61B-006/46 | A61B-006/58 | G06F-003/0482 | G06F-003/04847 | G06T-007/00 | G06T-007/50 | G06T-007/70 | A61B-005/055","","","","","","4924037001130"
"US","US","P","B2","Systems and methods for using artificial intelligence for skin condition diagnosis and treatment options","Methods, systems, and storage media for determining a numerical classification of human skin color, determining one or more characteristics of skin based on images, and determining a personalized treatment plan for one or more skin conditions or issues are disclosed. The system can receive images of skin and access user profile information, such as biometric information, medical record information, and other clinically relevant information. The system can determine a classification of a skin color of the user using the image and the biometric information by providing the image and the biometric information as input to a skin color classifier. Using the skin color, the system can determine one or more characteristics of the skin in the image and, if needed, determine and provide at least one personalized treatment plan to a computing device of a user.","1. A system for determining a numerical classification of human skin color, comprising: a data processing system comprising one or more processors and a memory, the data processing system configured to: receive an image captured by a camera, the image depicting a portion of skin of a user having a skin color;access biometric information of the user from a computing device of the user;determine a classification of the skin color of the user using the image and the biometric information by: providing the image and the biometric information as input to a skin color classifier,receiving, from an output of the skin color classifier, a plurality of skin color classification output values, andselecting the classification of the skin color from the plurality of skin color classification output values having a value greater than another of the plurality of skin color classification output values; andprovide the classification of the skin color of the user to the computing device.","17","17/398971","2021-08-10","2022-0051409","2022-02-17","12086986","2024-09-10","CORTINA HEALTH, INC.","Reid Maclellan | Paul Roossin","","","","G06T-0007/0016","G06T-0007/0016 | A61B-0005/441 | G06F-0017/18 | G06T-0007/90 | G16H-0010/60 | G06T-2200/24 | G06T-2207/10024 | G06T-2207/30088","G06T-007/90","G06T-007/90 | A61B-005/00 | G06F-017/18 | G06T-007/00 | G16H-010/60","","","","","","4924037005130"
"US","US","P","B2","Facilitation of conditional do not resuscitate orders","This disclosure describes a solution to enable more useful and responsive methods for a person's wishes for resuscitation actions to be canceled or discontinued in the event of a medical event. In this solution, a person can record their do not resuscitate (DNR) wishes with more specificity. For instance, they can specify conditions for treatment or non-treatment in the event of a medical emergency that would otherwise call for live-saving procedures or the use of an automated external defibrillator (AED) device. Conditional DNR data can be recorded in an electronic device (e.g., emergency pendant or smart watch, or in an electronic device) implanted within or on the person's body. This data can also be stored in a database accessible via a network.","1. A method, comprising: receiving, by a server device comprising a processor, from a first user equipment, do not resuscitate data representative of a condition that determines execution of a do not resuscitate order applicable to a specified person;receiving, by the server device, status data representative of a status of the specified person, wherein the status data comprises medical data associated with the specified person; andin response to determining that the status data does not satisfy the condition, sending, by the server device via a network, an attempt to resuscitate instruction for presentation via a second user equipment within a defined distance of the first user equipment, the sending further comprises sending a command to a defibrillator device to control resuscitation of the specified person at least in part using the defibrillator device.","20","18/064962","2022-12-13","2023-0113034","2023-04-13","12087441","2024-09-10","AT&T INTELLECTUAL PROPERTY I, L.P.","Brittaney Zellner | Sameena Khan | Ryan Schaub | Barrett Kreiner | Ari Craine | Robert Koch","","","","G16H-0040/67","G16H-0040/67 | A61H-0031/007 | A61N-0001/3904 | G06Q-0010/109 | G06Q-0050/265 | G16H-0010/60 | G16H-0040/20 | H04W-0004/023 | H04W-0004/20 | A61H-2201/501","G16H-040/67","G16H-040/67 | A61H-031/00 | A61N-001/39 | G06Q-010/109 | G06Q-050/26 | G16H-010/60 | G16H-040/20 | H04W-004/02 | H04W-004/20","","","","","","4924037005580"
"US","US","P","B1","Automatic in-home senior care system augmented with internet of things technologies","The in-home care of seniors is augmented using Internet of Things (IOT) technologies. In-home sensors monitor a senior and their caregiver. Physical conditions and psychological conditions may be monitored. In some implementations, a machine learning system has a classifier trained to detect a specified condition, such as depression. The system may perform various transformations of raw sensor data into a format indicative of a particular condition. In one implementation, a psychological or medical condition has symptoms in which each symptom has one or more measurable events. Mappings between symptoms, events, sensor data, and sensor transformation functions may be supported.","1. A system for providing in-home care for seniors, comprising: a machine learning subsystem including a processor configured to: generate a normalized feature vector from a set of sensor outputs from sensors disposed within a living area of a senior according to a sensor floorplan, including identifying a first relationship between symptoms of depression and events, wherein the events are measurable physical or mental features associated with at least one of the symptoms of depression, identifying a second relationship between the events and the set of sensor outputs based at least in part on the locations of each sensor and a sensor type of each sensor; and identifying a third relationship of sensor transformations required to transform sensor data into a format indicative of events, wherein the first relationship, the second relationship, and the third relationship is used to generate the normalized feature vector;input the normalized feature vector to a logistic regression classifier of a machine learning model, wherein the logistic regression classifier is trained to determine thresholds for identifying depression based on a training data set of a set of seniors; anddetermine a likelihood that the senior has depression.","8","18/304669","2023-04-21","","","12076108","2024-09-03","CLEARCARE, INC.","Geoffrey Nudd | David Cristman | Jonathan J. Hull | Bala Krishna Nakshatrala","","","","A61B-0005/0022","A61B-0005/0022 | A61B-0005/165 | G06F-0009/542 | G06F-0017/18 | G06N-0020/10 | G08B-0021/0423 | G10L-0015/16 | G10L-0025/30 | G10L-0019/00","G10L-015/00","G10L-015/00 | A61B-005/00 | A61B-005/16 | G06F-009/54 | G06F-017/18 | G06N-020/10 | G08B-021/04 | G10L-015/16 | G10L-025/30 | G10L-019/00","","","","","","4924036001055"
"US","US","P","B2","Systems and methods for providing digital health services","The present disclosure is directed to providing digital health services. In some embodiments, systems and methods for conducting virtual or remote sessions between patients and clinicians are disclosed. During the sessions, media content (e.g., images, video content, audio content, etc.) may be captured as the patient performs one or more tasks. The media content may be presented to the clinician and used to evaluate a condition of the patient or a state of the condition, adjust treatment parameters, provide therapy, or other operations to treat the patient. The analysis of the media content may be aided by one or more machine learning/artificial intelligence models that analyze various aspects of the media content, augment the media content, or other functionality to aid in the treatment of the patient.","1. A method of remotely programming an implantable medical device that provides therapy to a patient, comprising: establishing a first communication between a patient controller (PC) device and the implantable medical device, wherein the implantable medical device provides therapy to the patient according to one or more programmable parameters, the PC device communicates signals to the implantable medical device to set or modify the one or more programmable parameters, and the PC device comprises a video camera;establishing a video connection between the PC device and a clinician programmer (CP) device of a clinician for a remote programming session in a second communication that includes an audio/video (NV) session;communicating a value for a respective programmable parameter of the implantable medical device from the CP device to the PC device during the remote programming session; andmodifying, by the PC device, the respective programming parameter of the implantable medical device according to the communicated value from the CP device during the remote programming session; wherein the method further comprises:automatically analyzing, by one or more processors, patient data generated during the remote programming session;applying the patient data to a trained neural network to perform a classification operation;calculating a probability of a correct classification of the classification operation for the patient according to the patient data;comparing the calculated probability to a minimum accuracy value;displaying, via the CP device during the remote programming session, a graphical user interface (GUI) display of a patient value related to the classification operation when the calculated probability satisfies the minimum accuracy value.","17","17/891078","2022-08-18","2023-0054172","2023-02-23","12076552","2024-09-03","ADVANCED NEUROMODULATION SYSTEMS, INC.","Mary Khun Hor-Lao | Binesh Balasingh | Scott DeBates | Douglas Alfred Lautner | Kyuwan Choi | Anahita Kyani | Erika Ross","","","","A61N-0001/37282","A61N-0001/37282 | A61B-0005/1101 | A61B-0005/1114 | A61B-0005/1118 | A61B-0005/112 | A61B-0005/4082 | A61B-0005/742 | A61B-0005/7455 | A61N-0001/36135 | A61N-0001/37247 | A61N-0001/37264 | G06F-0021/6254 | G06T-0019/006 | G06V-0010/82 | G06V-0040/25 | G16H-0020/30 | G16H-0040/40 | G16H-0040/67 | G16H-0050/20 | G16H-0080/00 | H04N-0005/272 | H04N-0007/141 | A61N-0001/36067 | A61N-0001/36071 | A61N-0001/36132","A61N-001/372","A61N-001/372 | A61B-005/00 | A61B-005/11 | A61N-001/36 | G06F-021/62 | G06T-019/00 | G06V-010/82 | G06V-040/20 | G16H-020/30 | G16H-040/40 | G16H-040/67 | G16H-050/20 | G16H-080/00 | H04N-005/272 | H04N-007/14","","","","","","4924036001497"
"US","US","P","B2","Recommendation and selection of personalized output actions in a vehicle","The present embodiments relate to selection and execution of one or more output actions relating to a modification of at least one feature of a vehicle. A series of sensors on a vehicle can acquire data that can be used to identify vehicle environment characteristics indicative of a status of a vehicle environment and an emotional state of the user. The vehicle environment characteristics and the emotional state can be processed using a user model that corresponds to a user to generate one or more selected output actions. The output actions can be executed on the vehicle to increase user experience. The output actions can relate to any of entertainment features, safety features, and/or comfort features of the vehicle.","1. A computer-implemented method for generating a user-specific profile model and implementing an output action based on the user-specific profile model, the method comprising: retrieving a series of predetermined profile types, each predetermined profile type including a series of features common among a subset of users and each predetermined profile type including mappings between each of a plurality of vehicle states and one or more output actions generated in response to the mapped vehicle state;acquiring a stream of user characteristic data indicative of modifications to settings of a vehicle;for each portion of user characteristic data of the stream of user characteristic data, comparing the user characteristic data with the series of predetermined profile types to determine a first predetermined profile type with features that corresponds to the user characteristic data;for each portion of user characteristic data of the stream of user characteristic data, modifying a user-specific profile model using the first predetermined profile type to include the mappings between the vehicle states and the output actions associated with the first predetermined profile type;receiving a set of vehicle environmental data indicative of a current state of the vehicle;processing the set of vehicle environmental data using the modified user-specific profile model to generate an output action that modifies one or more vehicle features, wherein said processing includes: identifying a subset of vehicle states included in the modified user-specific profile model that are within a threshold similarity of the vehicle environmental data; andderiving an output action from the mappings between the subset of vehicle states and the one or more output actions associated with the modified user-specific profile model; andperforming the derived output action to modify one or more features of the vehicle.","20","17/648752","2022-01-24","2022-0144194","2022-05-12","12077113","2024-09-03","Cobalt Industries Inc.","Rana June Sobhany","","","","B60R-0016/037","B60R-0016/037 | A61B-0005/18 | B60H-0001/00878 | B60H-0003/0007 | B60K-0035/00 | B60N-0002/976 | B60W-0030/025 | B60W-0040/08 | B60W-0050/08 | B60W-0050/085 | G05B-0013/028 | G05D-0001/0088 | G06F-0003/011 | G06F-0003/017 | G06F-0016/2379 | G06F-0016/24575 | G06F-0018/24 | G06F-0018/254 | G06N-0005/04 | G06N-0020/00 | G06V-0010/809 | G06V-0020/597 | G06V-0040/176 | G06V-0040/70 | G10L-0025/63 | H04L-0012/40 | H04L-0067/306 | B60W-2040/0872 | B60W-2040/089 | B60W-2050/0028 | B60W-2050/0088 | B60W-2540/22 | B60W-2540/221 | B60W-2710/30 | B60W-2754/20 | H04L-2012/40215","B60R-016/037","B60R-016/037 | A61B-005/18 | B60H-001/00 | B60H-003/00 | B60K-035/00 | B60N-002/90 | B60W-030/02 | B60W-040/08 | B60W-050/08 | G05B-013/02 | G05D-001/00 | G06F-003/01 | G06F-016/23 | G06F-016/2457 | G06F-018/24 | G06F-018/25 | G06N-005/04 | G06N-020/00 | G06V-010/80 | G06V-020/59 | G06V-040/16 | G06V-040/70 | G10L-025/63 | H04L-012/40 | H04L-067/306 | B60W-050/00","","","","","","4924036002051"
"US","US","P","B2","Forecast revision","A data processing apparatus is provided that includes forecast circuitry for generating a forecast of an aspect of a system for a next future time and for one or more subsequent future times following the next future time. Measurement circuitry generates, at the next future time, a new measurement of the aspect of the system. Aggregation circuitry produces an aggregation of the forecast of the aspect of the system for the next future time and of the new measurement of the aspect of the system. The forecast circuitry revises the forecast of the aspect of the system for the one or more subsequent future times using the aggregation.","1. A data processing apparatus to improve upon data comprising: forecast circuitry configured to generate a forecast of an aspect of a system for a next future time and for one or more subsequent future times following the next future time;measurement circuitry configured to generate, at the next future time, a new measurement of the aspect of the system;aggregation circuitry configured to produce an aggregation by aggregating the forecast of the aspect of the system for the next future time andthe new measurement of the aspect of the system;residual calculation circuitry configured to calculate residuals of the forecast of the aspect of the system for the next future time and for the one or more subsequent future times following the next future time; andconfidence interval generation circuitry configured to generate, for each forecast of the aspect of the system and for the one or more subsequent future times following the next future time, confidence intervals based on the residuals;wherein the forecast circuitry is configured to update the forecast of the aspect of the system for the one or more subsequent future times using the aggregation; andwherein the confidence intervals are generated so as to encompass forecasts whose residuals fall within a predefined percentile.","18","17/559246","2021-12-22","2023-0195846","2023-06-22","12079309","2024-09-03","ARM LIMITED","Michael Bartling","","","","G06F-0018/2193","G06F-0018/2193 | A61B-0005/14532 | G06F-0011/3409 | G06F-0011/3452 | G06F-0017/00 | G06F-0017/18 | G06Q-0030/0202","G06F-017/00","G06F-017/00 | A61B-005/145 | G06F-011/34 | G06F-017/18 | G06F-018/21 | G06Q-030/0202 | A61B-005/02 | A61B-005/24 | G06F-016/906 | G06N-020/00 | G06Q-030/02 | G06V-010/774","","","","","","4924036004230"
"US","US","P","B2","Task execution order determination system and task execution method","A technique for evaluating human cognitive and motor functions by a plurality of hand movement tasks is disclosed. A task execution method determines the execution order of a plurality of tasks which a test subject is caused to execute to acquire a characteristic quantity. A test subject group task database includes scores given in advance and characteristic quantities obtained from a plurality of tasks stored as past data corresponding to each of a plurality of test subjects. In a storage device, (1) a differentiation precision database for a case in which test subjects are divided into two groups by predetermined threshold value scores differentiated by the characteristic quantities, or (2) an estimation precision database for a case in which a score is estimated using the characteristic quantity for a predetermined score value is prepared for each of the tasks on the basis of the test subject group task database.","1. A method in a task execution order determination system including a processor and a storage device, the storage device storing task data of a plurality of different tasks performed by subjects to evaluate motor function and cognitive function of a current test subject by comparison of test results of a current test subject with test results of a group of test subjects previously tested, the method comprising: storing in the storage device test results of the group of test subjects previously tested including scores of a characteristic quantity associated with the performance of each of the plurality of different tasks by each subject of the group of test subjects previously tested;obtaining test results of the group of test subjects previously tested and of a currently tested subject with magnetic sensors that obtain hand motion data of the subject being tested for each of the plurality of different tasks,wherein the magnetic sensors are coupled to an analog/digital conversion circuit that converts an analog waveform signal detected by the magnetic sensors into a digital waveform signal by sampling, the digital waveform signal is input to a motion sensor control part that includes an alternating current (AC) generation circuit, a current generation amplifier circuit, a preamplifier circuit, a detection circuit, an LPF circuit, a phase adjustment circuit, an amplifier circuit, and an output signal terminal, the motion sensor control part outputting a time-series waveform signal from which the characteristic quantity associated with the performance of the plurality of different tasks is obtained;dividing the scores of the previously tested subjects into two groups, by discriminating between a first group of the previously tested subjects having a score less than a threshold N, N being an integer in a predetermined range of integers, and a second group of the previously tested subjects having a score equal to or more than the threshold N, based on an appearance frequency distribution with respect to the characteristic quantity of the first group and an appearance frequency distribution with respect to the characteristic quantity of the second group;calculating a discrimination accuracy value (AUC) for each of a plurality of the thresholds N in the predetermined range corresponding to each reference task of the plurality of different tasks and storing a combination of the reference task and AUC value for each of the thresholds N for the predetermined range into an evaluation accuracy database;selecting from the evaluation accuracy database one of the plurality of the thresholds N, for each reference task stored in the storage device, having a maximum accuracy value as a maximum accuracy value threshold Nmax; andapplying the maximum accuracy value threshold Nmax to the score of the characteristic quantity associated with the performance of the reference task by a current test subject to discriminate the score on the basis of the threshold Nmax and determine in which one of the first and second groups that the score of the characteristic quantity associated with the performance of the reference task is grouped as a representation of the motor function and cognitive function of a current test subject.","5","17/885918","2022-08-11","2022-0382554","2022-12-01","12079633","2024-09-03","Maxell, Ltd.","Yuko Sano | Akihiko Kandori | Mitsunobu Watanabe","2016-191129","JP","2016-09-29","G06F-0009/3856","G06F-0009/3856 | A61B-0005/7275 | G06F-0003/011 | G06F-0003/017 | G06F-0009/30094 | G06F-0009/48","G06F-009/38","G06F-009/38 | A61B-005/00 | G06F-003/01 | G06F-009/30 | G06F-009/48","","","","","","4924036004551"
"US","US","P","B2","Medical assistant","A wearable device can present virtual content to the wearer for many applications in a healthcare setting. The wearer may be a patient or a healthcare provider (HCP). Such applications can include, but are not limited to, access, display, and modification of patient medical records and sharing patient medical records among authorized HCPs.","1. A display system for facilitating a medical procedure, the display system comprising: a head-mountable three-dimensional (3D) display configured to present virtual content to a user of the display system while the display is being worn on a head of the user, wherein the display is at least partly transparent to provide the user a view of an environment in proximity to the user while the display presents the virtual content to augment the view of the environment, and wherein the display is configured to present the virtual content at a plurality of depth planes by outputting, to an eye of the user, light that conveys the virtual content with different levels of wavefront curvature corresponding to different depth planes of the plurality of depth planes;one or more environmental sensors configured to collect data associated with the environment; andat least one hardware processor in communication with the display and the one or more environmental sensors, the at least one hardware processor programmed to: analyze the data collected by the one or more environmental sensors to update a map describing at least a portion of the environment;receive information describing the medical procedure to be performed on a patient currently present in the environment and described in the map;identify a region within the portion of the environment;based on the data collected by the one or more environmental sensors, identify a medical instrument that is currently present in the region and described in the map and determine a current 3D position of the medical instrument in the region;detect an anomaly based on the medical instrument being currently present in the region, wherein the anomaly is detected based at least partly on comparing the information describing the medical procedure to semantic information that describes the medical instrument; andresponsive to detecting the anomaly, cause the display to present an alert indicating the anomaly, wherein the alert is included in the virtual content presented through the display, wherein the alert is presented at one of the plurality of depth planes supported by the display such that the alert appears to be located in proximity to the 3D position of the medical instrument and at a distance corresponding to a depth of the 3D position of the medical instrument, wherein presenting the alert includes presenting a focus indicator that is a halo presented around the medical instrument, and wherein the halo has a color based on the semantic information that describes the medical instrument.","20","18/056164","2022-11-16","2023-0075466","2023-03-09","12080393","2024-09-03","MAGIC LEAP, INC.","Nastasja U. Robaina | Nicole Elizabeth Samec | Mark Baerenrodt | Christopher M. Harrises","","","","G16H-0010/60","G16H-0010/60 | A61B-0003/0041 | A61B-0003/10 | A61B-0003/113 | A61B-0005/0077 | A61B-0005/06 | A61B-0005/1171 | A61B-0005/1176 | A61B-0005/339 | A61B-0005/4803 | A61B-0090/37 | G02B-0027/0093 | G02B-0027/0172 | G06F-0016/22 | G06F-0016/2379 | G06F-0021/32 | G06F-0040/205 | G06F-0040/289 | G06V-0020/20 | G10L-0015/26 | G16H-0040/67 | A61B-2017/00207 | A61B-2017/00216 | A61B-2034/2048 | A61B-2090/365 | A61B-2090/372 | A61B-2090/502 | G02B-2027/0127 | G02B-2027/0138 | G02B-2027/014 | G02B-2027/0141 | G06F-0021/6245 | G16H-0030/40","G16H-010/60","G16H-010/60 | A61B-003/00 | A61B-003/10 | A61B-003/113 | A61B-005/00 | A61B-005/06 | A61B-005/1171 | A61B-005/339 | A61B-090/00 | G02B-027/00 | G02B-027/01 | G06F-016/22 | G06F-016/23 | G06F-021/32 | G06F-021/62 | G06F-040/205 | G06F-040/289 | G06V-020/20 | G10L-015/26 | G16H-040/67 | A61B-017/00 | A61B-034/20 | A61B-090/50 | G16H-030/40","","","","","","4924036005297"
"US","US","P","B2","Area of interest overlay on dental site using augmented reality","A system comprises an image capture device, an augmented reality (AR) display to display, and a processing device. The processing deice receives image data of a dental arch from the image capture device and processes the image data using a plurality of detection rules, where each detection rule detects one or more dental conditions. The processing device determines a dental condition for the dental arch based on the processing, determines a position of an area of interest on the dental arch, wherein the area of interest is associated with the dental condition, generates a visual overlay comprising an indication of the dental condition at the position of the area of interest, and outputs the visual overlay to the AR display, wherein the visual overlay is superimposed over a view of the dental arch on the AR display at the position of the area of interest.","1. A method comprising: receiving image data of a dental arch from an image capture device associated with an augmented reality (AR) display;processing, by a processing device, the image data using a plurality of detection rules, where each detection rule of the plurality of detection rules detects one or more dental conditions;determining, by the processing device, a dental condition for the dental arch based on the processing;determining, by the processing device, a position of an area of interest on the dental arch, wherein the area of interest is associated with the dental condition;generating, by the processing device, a visual overlay comprising an indication of the dental condition at the position of the area of interest; andoutputting the visual overlay to the AR display, wherein the visual overlay is superimposed over a view of the dental arch on the AR display at the position of the area of interest.","24","18/151260","2023-01-06","2023-0141733","2023-05-11","12070375","2024-08-27","ALIGN TECHNOLOGY, INC.","Avi Kopelman | Adi Levin | Eric Paul Meyer | Elad Zeiri | Amir Ashkenazi | Ron Ganot | Sergei Ozerov | Inna Karapetyan","","","","A61C-0009/0053","A61C-0009/0053 | A61B-0001/000094 | A61B-0001/000096 | A61B-0005/0088 | A61B-0034/10 | A61B-0034/20 | A61B-0034/25 | A61B-0090/36 | A61C-0001/0015 | A61C-0001/082 | A61C-0005/44 | A61C-0007/002 | A61C-0009/004 | A61C-0009/0046 | A61C-0019/04 | A61C-0019/05 | G06F-0003/011 | G06T-0017/00 | G06T-0019/006 | G06V-0010/758 | G06V-0040/171 | G09B-0023/283 | G16H-0020/40 | G16H-0030/20 | G16H-0030/40 | G16H-0040/60 | G16H-0050/20 | G16H-0050/50 | A61B-0001/00009 | A61B-0001/00045 | A61B-0001/00172 | A61B-0001/24 | A61B-0005/1032 | A61B-0005/4542 | A61B-0005/4547 | A61B-0005/4552 | A61B-0005/7455 | A61B-0006/466 | A61B-0006/51 | A61B-0006/5217 | A61B-0006/5247 | A61B-2017/00216 | A61B-2034/102 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2048 | A61B-2034/2065 | A61B-2034/252 | A61B-2034/254 | A61B-2034/256 | A61B-2034/258 | A61B-0034/76 | A61B-2090/062 | A61B-2090/309 | A61B-2090/3612 | A61B-2090/365 | A61B-2090/371 | A61B-2090/372 | A61B-2090/373 | A61B-2090/502 | G02B-0027/0093 | G02B-0027/017 | G02B-2027/0178 | G06F-0003/013 | G06N-0003/08 | G06N-0020/10 | G06T-2207/30036 | G06T-2210/41 | Y02A-0090/10","G06T-019/00","G06T-019/00 | A61B-001/00 | A61B-005/00 | A61B-034/00 | A61B-034/10 | A61B-034/20 | A61B-090/00 | A61C-001/00 | A61C-001/08 | A61C-005/44 | A61C-007/00 | A61C-009/00 | A61C-019/04 | A61C-019/05 | G06F-003/01 | G06T-017/00 | G06V-010/75 | G06V-040/16 | G09B-023/28 | G16H-020/40 | G16H-030/20 | G16H-030/40 | G16H-040/60 | G16H-050/20 | G16H-050/50 | A61B-001/24 | A61B-005/103 | A61B-006/00 | A61B-006/46 | A61B-006/51 | A61B-017/00 | A61B-090/30 | A61B-090/50 | G02B-027/00 | G02B-027/01 | G06N-003/08 | G06N-020/10","","","","","","4924035001105"
"US","US","P","B2","Infusion device and methods","Medical devices, systems, and methods related thereto a glucose monitoring system having a first display unit in data communication with a skin-mounted assembly, the skin-mounted assembly including an in vivo sensor and a transmitter. The first display unit and a second display unit are in data communication with a data management system. The first display unit comprises memory that grants a first user first access level rights and the second display unit comprises memory that grants a second individual second access level rights.","1. A computing device in data communication with a skin-mounted assembly comprising an in vivo glucose sensor and a transmitter unit, the computing device comprising: one or more processors;one or more memory units operatively coupled to the one or more processors and including program instructions stored therein which, when executed by the one or more processors, causes the one or more processors to: receive, via user input, individual hierarchical access level rights including at least first access level rights and second access level rights, wherein the first access level rights enable a first individual to view first level parameters of a plurality of parameters of the computing device, and wherein the second access level rights enable a second individual to view second level parameters of the plurality of parameters using the computing device, the second level parameters at least being different than the first level parameters, wherein the first level parameters and the second level parameters relate to data indicative of a glucose level, the data received from the skin-mounted assembly;transmit the first access level rights to a data management system, wherein the first access level rights are associated with a first device associated with the first individual; andtransmit the second access level rights to the data management system, wherein the second access level rights are associated with a second device associated with the second individual;receive, from the skin-mounted assembly, the data indicative of the glucose level; andtransmit the data indicative of the glucose level to the data management system,wherein the first access level rights are configured to cause the data management system to transmit a notification including the data indicative of the glucose level to the first device.","20","17/984524","2022-11-10","2023-0064839","2023-03-02","12073941","2024-08-27","ABBOTT DIABETES CARE INC.","Christopher V. Reggiardo | Namvar Kiaie | James Thomson","","","","G16H-0040/63","G16H-0040/63 | A61M-0005/14244 | A61M-0005/14276 | A61M-0005/1723 | G06F-0021/30 | G06F-0021/44 | G16H-0020/17 | G16H-0040/67 | G16Z-0099/00 | H04L-0063/104 | H04L-0063/105 | H04W-0012/08 | A61B-0005/14532 | A61M-0005/31525 | A61M-0005/31546 | A61M-2205/276 | A61M-2205/3561 | A61M-2205/3569","G06F-021/30","G06F-021/30 | A61M-005/142 | A61M-005/172 | G06F-021/44 | G16H-020/17 | G16H-040/63 | G16H-040/67 | G16Z-099/00 | H04L-009/40 | H04W-012/08 | A61B-005/145 | A61M-005/315","","","","","","4924035004614"
"US","US","P","B2","Methods for providing an alert or an alarm to a user of a mobile communications device","Methods, devices and systems are disclosed for inter-app communications between software applications on a mobile communications device. In one aspect, a computer-readable medium on a mobile computing device comprising an inter-application communication data structure to facilitate transitioning and distributing data between software applications in a shared app group for an operating system of the mobile computing device includes a scheme field of the data structure providing a scheme id associated with a target software app to transition to from a source software app, wherein the scheme id is listed on a scheme list stored with the source software app; and a payload field of the data structure providing data and/or an identification where to access data in a shared file system accessible to the software applications in the shared app group, wherein the payload field is encrypted.","1. A mobile communications device, comprising: a memory comprising a first software module and a second software module;a processor in data communication with the memory and configured to: receive, via the first software module, glucose measurements from a glucose monitoring system;determine, via the second software module, an insulin dosage based on the received glucose measurements, wherein: the first and second software modules are different software modules,the second software module is operable to receive patient specific input data and calculate the insulin dosage based on the patient specific input data and the received glucose measurements, andthe second software module is a plug-in for a software application comprising the first software module; andtransmit a command indicative of the determined insulin dosage to an insulin delivery device to cause the insulin delivery device to administer the insulin dosage to a patient.","16","18/465839","2023-09-12","2023-0414873","2023-12-28","12064601","2024-08-20","DEXCOM, INC.","Gary A. Morris | Scott M. Belliveau | Esteban Cabrera, Jr. | Anna Leigh Davis | Rian W. Draeger | Laura J. Dunn | Timothy Joseph Goldsmith | Hari Hampapuram | Christopher Robert Hannemann | Apurv Ullas Kamath | Katherine Yerre Koehler | Patrick Wile McBride | Michael Robert Mensinger | Francis William Pascual | Philip Mansiel Pellouchoud | Nicholas Polytaridis | Philip Thomas Pupa | Kevin Shoemaker | Brian Christopher Smith | Benjamin Elrod West | Atiim Joseph Wiley","","","","A61M-0005/1723","A61M-0005/1723 | A61B-0005/14532 | A61B-0005/4839 | A61B-0005/7405 | A61B-0005/742 | G06F-0009/54 | G06F-0009/546 | G06F-0016/2228 | G06F-0016/955 | G06F-0021/606 | G08B-0021/0453 | G08B-0025/08 | G16H-0010/60 | G16H-0020/17 | G16H-0040/63 | H04L-0009/0637 | H04L-0009/0822 | H04L-0009/0833 | H04L-0009/0861 | H04L-0009/0891 | H04L-0009/14 | H04L-0009/30 | H04L-0063/0428 | H04M-0001/72409 | H04W-0012/084 | A61M-2205/3584 | A61M-2205/50 | A61M-2205/502 | A61M-2230/201 | H04L-2209/80 | H04M-0001/724095 | H04M-0001/72412","A61M-005/172","A61M-005/172 | A61B-005/00 | A61B-005/145 | G06F-009/54 | G06F-016/22 | G06F-016/955 | G06F-021/60 | G08B-021/04 | G08B-025/08 | G16H-010/60 | G16H-020/17 | G16H-040/63 | H04L-009/06 | H04L-009/08 | H04L-009/14 | H04L-009/30 | H04L-009/40 | H04M-001/72409 | H04M-001/72412 | H04W-012/084","","","","","","4924034001290"
"US","US","P","B2","Devices, systems, and methods to emphasize regions of interest across multiple imaging modalities","One or more devices, systems, methods and storage mediums for optical imaging medical devices, such as, but not limited to, Optical Coherence Tomography (OCT), single mode OCT, and/or multi-modal OCT apparatuses and systems, and methods and storage mediums for use with same, for viewing, controlling, updating, and emphasizing multiple imaging modalities are provided herein. One or more embodiments provide at least one intuitive Graphical User Interface (GUI), method, device, apparatus, system, or storage medium to comprehend information, including, but not limited to, molecular structure of a vessel, and to provide an ability to manipulate the vessel information. In addition to controlling multiple imaging modalities, the GUI may operate for one or more applications, including, but not limited to, expansion/underexpansion (e.g., for a stent) and/or apposition/malapposition (e.g., for a stent), co-registration and imaging.","1. An imaging device comprising: one or more processors that operate to:display an image for each of multiple imaging modalities on a display;receive an input request during a rotational, scrolling, or sliding movement of an interaction, performed on or performed using the display, with one or more of the displayed images where at least the display of the one or more images is changed during the interaction, and/or receive an input request during an interaction, performed on or performed using the display, with one or more moveable control bars or tools displayed on or in the one or more of the displayed images where the one or more moveable control bars or tools are rotationally moved or are scrolled or slid during the interaction to receive the input request, the input request operating to change the one or more of the displayed images of at least one of the multiple imaging modalities; andsynchronously update each displayed image for each of the multiple imaging modalities based on the received input request such that data is changed in each of the displays for each of the multiple imaging modalities based on the received input request and such that: (i) an orientation or position for each displayed image(s) for each of the other multiple imaging modalities is updated synchronously by rotating, scrolling, or sliding each of the displayed image(s) by an amount equal or corresponding to the rotational, scrolling, or sliding movement of the interaction and/or the moveable control bars or tools and displayed using the updated orientation or position based on the rotational, scrolling, or sliding movement of the interaction and/or the moveable control bars or tools, or (ii) a frame for each displayed image(s) for each of the other multiple imaging modalities is updated synchronously to be a different frame corresponding to or located at a position selected by the scrolling or sliding along a direction of an image of the one or more images where the movement is a scrolling or sliding movement and/or where the moveable control bars or tools are scrolled or slid,wherein the one or more processors further operate to at least:(i) determine whether at least one of two cases occurs, where a first case of the two cases is where the input request is received during the rotational, sliding, or scrolling movement of the interaction with a displayed image for one imaging modality of the multiple imaging modalities and where a second case of the two cases is where the input request is received during the interaction with one or more rotational, scrolling, or sliding moveable control bars or tools displayed on or in the displayed image for the one imaging modality of the multiple imaging modalities, and, where the first case and/or the second case of the two cases is/are determined to occur, perform the synchronous update for the other displayed image(s) for each of the multiple imaging modalities; and(ii) determine whether at least one of two other cases occurs, where a first other case of the two other cases is where the input request is received during the rotational, sliding, or scrolling movement of the interaction with a displayed image for another imaging modality of the multiple imaging modalities and where a second other case of the two other cases is where the input request is received during the interaction with one or more rotational, scrolling, or sliding moveable control bars or tools displayed on or in the displayed image for the another imaging modality of the multiple imaging modalities, and, where the first other case and/or the second other case of the two other cases is/are determined to occur, perform the synchronous update for the other displayed image(s) for each of the multiple imaging modalities, where the one imaging modality is different from the another imaging modality.","27","16/401390","2019-05-02","2019-0339850","2019-11-07","12067225","2024-08-20","CANON USA, INC.","Christina Ho","","","","G06F-0003/04847","G06F-0003/04847 | A61B-0005/0066 | A61B-0005/0071 | A61B-0005/7425 | A61B-0005/7435 | A61B-0005/748 | G06F-0003/04845 | G06F-0003/0485 | G06F-0003/04883 | G06F-2203/04808","G06F-003/00","G06F-003/00 | A61B-005/00 | G06F-003/04845 | G06F-003/04847 | G06F-003/0485 | G06F-003/04883 | G06F-009/00 | G06F-017/00","","","","","","4924034003884"
"US","US","P","B2","System, method and apparatus for performing real-time virtual medical examinations","Disclosed herein is a method for permitting a real-time virtual medical examination using a patient device and at least one diagnostic device including receiving, at the patient device, a signal transmitted from the at least one diagnostic device; generating diagnostic information based on the received signal; encrypting the diagnostic information; establishing communication over a network between the patient device and a first remote server; establishing a video conferencing session via a second remote server; and transmitting the encrypted diagnostic information to the first remote server.","1. A method for permitting a real-time virtual medical examination, comprising: receiving, at a patient device associated with a patient, a signal transmitted from at least one diagnostic device;generating, by the patient device, diagnostic information based on the received signal;encrypting, by the patient device, the diagnostic information within the patient device using a hypertext transfer protocol secure method, wherein the encryption is based upon a unique user identifier corresponding to a wireless signal generated by the diagnostic device;establishing communication over a network between the patient device and a physician device associated with a physician;establishing communication over the network between the patient device and a video conferencing server;receiving, by the patient device, a video conferencing platform from a content manager server separate from the video conferencing server;receiving, by the patient device, an advertisement for a health-related product configured to treat symptoms experienced by the patient based on the diagnostic information;enabling, by the patient device, selective purchase of the health-related product advised to treat symptoms experienced by the patient based on the diagnostic information;establishing a video conferencing session on the video conferencing platform via the video conferencing server in communication with the patient device to allow the physician and patient to discuss the diagnostic information and the advertised health-related product as part of the real-time virtual medical examination;encrypting and transmitting, by the patient device, first voice and video signals generated during the video conferencing session to the video conferencing server;receiving and decrypting, by the patient device, encrypted second voice and video signals generated during the video conferencing session from the video conferencing server; andtransmitting, by the patient device, the encrypted diagnostic information to the physician device.","18","16/840624","2020-04-06","2021-0183507","2021-06-17","12069407","2024-08-20","SMART SOLUTIONS IP, LLC","Fawzi Shaya","","","","H04N-0007/141","H04N-0007/141 | A61B-0005/0022 | A61B-0005/02233 | A61B-0005/14551 | A61B-0005/1468 | A61B-0005/7465 | A61B-0008/00 | G06Q-0010/10 | G06Q-0030/0241 | G16H-0010/60 | G16H-0040/63 | G16H-0040/67 | H04L-0063/0428 | H04L-0065/1069 | H04N-0007/15 | A61B-0005/021 | A61B-0005/087 | A61B-0005/14532 | A61B-0005/14542 | G16H-0015/00","G16H-040/67","G16H-040/67 | A61B-005/00 | A61B-005/022 | A61B-005/1455 | A61B-005/1468 | A61B-008/00 | G06Q-010/10 | G06Q-030/0241 | G16H-010/60 | G16H-040/63 | H04L-009/40 | H04L-065/1069 | H04N-007/14 | H04N-007/15 | A61B-005/021 | A61B-005/087 | A61B-005/145 | G16H-015/00","","","","","","4924034006043"
"US","US","P","B2","Medical image processing device, treatment system, and storage medium","A medical image processing device includes a region-of-interest acquirer, a treatment plan acquirer, a degree-of-influence calculator, and a display controller. The region-of-interest acquirer is configured to acquire a partial region within a body of a patient as a region of interest. The treatment plan acquirer is configured to acquire treatment plan information determined in a planning stage of radiation treatment to be performed on the patient. The degree-of-influence calculator is configured to calculate a degree of influence representing an influence on the region of interest up to a range until radiation with which the patient is irradiated reaches a target portion to be treated within the body of the patient. The display controller is configured to generate a display image in which information regarding the degree of influence is superimposed on a current fluoroscopic image of the patient and causes a display to display the display image.","1. A medical image processing device comprising: circuitry configured to: acquire a partial region within a body of a patient as a region of interest;acquire treatment plan information determined in a planning stage of radiation treatment to be performed on the patient;calculate a degree of influence representing an influence of radiation with which the patient is irradiated on the region of interest up to a range in which the radiation reaches a target portion to be treated within the body of the patient;generate a display image in which information regarding the degree of influence is superimposed on a current fluoroscopic image of the patient and cause a display to display the display image;calculate the degree of influence on the basis of a degree of overlap between a passage path of the radiation that is radiated and the region of interest; andcalculate the degree of influence on the basis of a ratio between a volume in which the passage path of the radiation that is radiated overlaps the region of interest and a volume of the region of interest.","16","16/979619","2019-03-07","2021-0038917","2021-02-11","12059579","2024-08-13","TOSHIBA ENERGY SYSTEMS & SOLUTIONS CORPORATION","Kenichi Karasawa | Ryusuke Hirai | Tomoya Okazaki | Yasunori Taguchi | Keiko Okaya | Shinichiro Mori","2018-053470","JP","2018-03-20","A61N-0005/1049","A61N-0005/1049 | A61B-0006/463 | A61B-0006/487 | A61N-0005/1037 | G06F-0003/14 | G06T-0007/0012 | G06T-0007/70 | G06T-0011/003 | G06T-2207/10064 | G06T-2207/10081 | G06T-2207/10124 | G06T-2207/30096","A61N-005/10","A61N-005/10 | A61B-006/00 | A61B-006/46 | G06F-003/14 | G06K-009/32 | G06T-007/00 | G06T-007/70 | G06T-011/00","","","","","","4924033001308"
"US","US","P","B2","Systems and methods for determining spinal cord stimulation parameters based on patient feedback","The present disclosure provides a grip sensor for quantifying pain experienced by a patient during spinal cord stimulation (SCS). The grip sensor includes an electronics enclosure, an annular outer shell substantially surrounding the electronics enclosure and sized to be held by the patient, a pressure sensor embedded in the outer shell and communicatively coupled to the electronics enclosure, the pressure sensor configured to measure a grip strength of the patient as SCS is applied to the patient, and a plurality of galvanic skin response sensors communicatively coupled to the electronics enclosure and configured to measure an electrical impedance of the skin of the patient as SCS is applied to the patient.","1. A method for determining spinal cord stimulation (SCS) therapy parameters for a patient, the method comprising: first determining a tonic stimulation (TS) configuration by: i) applying tonic stimulation at a fixed frequency; andii) varying at least one parameter of the applied tonic stimulation (TS), until first patient feedback indicates that paresthesia coverage of a target area of the patient is achieved; andsecond determining at least one of a burst or high-frequency stimulation configuration by at least one of: i) dividing each pulse of the TS into a plurality of pulses; orii) adjusting a frequency of the first TS parameter set to a threshold frequency,until second patient feedback indicates pain relief with reduced paresthesia is achieved.","10","17/369035","2021-07-07","2021-0330979","2021-10-28","12053633","2024-08-06","PACESETTER, INC.","Alexander Kent | Edward Karst | Gene A. Bornzin","","","","A61N-0001/36132","A61N-0001/36132 | A61B-0005/0533 | A61B-0005/225 | A61B-0005/4824 | A61N-0001/36062 | A61N-0001/3614 | A61B-0005/4836 | A61B-2562/046 | A61N-0001/0551 | A61N-0001/36021 | A61N-0001/36071 | A61N-0001/37247 | A63B-0021/4035 | G06F-0003/015","A61N-001/36","A61N-001/36 | A61B-005/00 | A61B-005/0533 | A61B-005/22 | A61N-001/05 | A61N-001/372 | A63B-021/00 | G06F-003/01","","","","","","4924032001677"
"US","US","P","B1","Street greening quality detection method based on physiological activation recognition","A street greening quality detection method based on physiological activation recognition is provided. The street greening quality detection method includes establishing a greening quality factor index system, and obtaining and uniformly processing street greening images; collecting raw data, and performing reclassification and differential wave processing on the raw data to obtain valid physiological data that can be used for activation feature recognition of greening quality factors; calculating physiological activation feature parameters, training the physiological activation feature parameters by transfer learning fusion to determine importance of physiological activation features, and recognizing weighted average greening activation indexes of the greening quality factors; analyzing weighted average greening activation index data of the greening quality factors to form a street greening quality detection model; and inputting annotated street samples to be analyzed into the street greening quality detection model to obtain annotated results of street greening quality grading detection target data.","1. A street greening quality detection method based on a physiological activation recognition, comprising the following steps: establishing a greening quality factor index system according to high-frequency street landscape characteristics, and obtaining and uniformly processing street greening images for greening stimulus physiological experiments;collecting EEG, ECG, EDA and EMG raw data stimulated by the street greening images, and performing reclassification and differential wave processing on the raw data according to greening quality factor indexes to obtain valid physiological data that can be used for activation feature extraction of greening quality factors;calculating EEG, ECG, EDA and EMG physiological activation feature parameters of the greening quality factors according to the obtained valid physiological data, training the physiological activation feature parameters by transfer learning fusion to determine importance of physiological activation features, and recognizing weighted average greening activation indexes of the greening quality factors;analyzing weighted average greening activation index data of the greening quality factors to form a street greening quality detection model for contrast detection of street greening quality; andinputting annotated street samples to be analyzed into the street greening quality detection model to obtain annotated results of street greening quality grading detection target data.","20","18/565501","2023-04-04","2024-0260885","2024-08-08","12048549","2024-07-30","SOUTHEAST UNIVERSITY","Zhe Li | Liya Wang | Xiao Han | Jie Li | Qixin Zhang | Mingjing Dong | Mingchen Xu | Shuang Wu | Yi Shi | Haini Chen | Qiaochu Wang","2022-11390493","CN","2022-11-08","A61B-0005/378","A61B-0005/378 | A61B-0005/0205 | A61B-0005/0533 | A61B-0005/352 | A61B-0005/397 | G06N-0020/00 | G06V-0010/26 | G06V-0010/764 | G06V-0020/39 | A61B-2503/12 | G06Q-0050/26","G06V-010/00","G06V-010/00 | A61B-005/0205 | A61B-005/0533 | A61B-005/352 | A61B-005/378 | A61B-005/397 | G06N-020/00 | G06V-010/26 | G06V-010/764 | G06V-020/00 | G06Q-050/26","","","","","","4924031001117"
"US","US","P","B2","Dynamically controlled treatment protocols for autonomous treatment systems","Systems, and methods relate to a medical device receiving a treatment parameter operating point within a first operating region defined by a first set of operating points for which automatic incremental adjustment of a parameter in the current operation is permitted. In an illustrative example, incremental adjustment may use artificial intelligence based on patient feedback and sensor measurement of outcomes. Some exemplary devices may receive a request to alter the current treatment parameter operating point to a second treatment parameter operating point outside the first operating region and in a second operating region in a known safe operation zone, bounded by a known unsafe zone unavailable to the user. In the second operating region, some examples may restrict the step size of incremental adjustments requested by the user. Data may be collected for cloud-based analysis, for example, to facilitate discovery of more effective treatment protocols.","1. A therapeutic delivery apparatus comprising: a therapy delivery mechanism operably coupled to a processor and adapted to deliver a predetermined therapy to a patient according to a treatment parameter operating point defined by a set of parameters associated with the delivery of the therapy in response to receiving a command signal transmitted by the processor;a communication interface configured for communicating information about the patient to a remote or local server, the communicated information including information about a result of the therapy applied to the patient by the therapy delivery mechanism;the processor operably coupled to control the therapy delivery mechanism according to the treatment parameter operating point, and operably coupled to the communication interface to generate a message that includes the result of the therapy applied to the patient by the therapy delivery mechanism; and,a memory device operably coupled to the processor and containing instructions, that when executed by the processor, cause the processor to perform operations to dynamically adjust the treatment parameter operating point, wherein the operations comprising: receive a current treatment parameter operating point that lies within a first predetermined operating region defined by a first set of operating points for which automatic incremental adjustment of a parameter in the current operation is permitted;receive a request to alter the current treatment parameter operating point to a second treatment parameter operating point that lies outside of the first predetermined operating region and in a second predetermined operating region defined by a second set of operating points for which authorization is required, wherein the second predetermined operating region lies outside of the first predetermined operating region in a space of operating points.","23","18/183763","2023-03-14","2023-0211084","2023-07-06","12048832","2024-07-30","NEXTERN INNOVATION, LLC","Ryan J. Douglas | Steven M. Gigl","","","","A61M-0005/1723","A61M-0005/1723 | A61H-0009/0085 | G06Q-0010/0639 | G06Q-0010/10 | G06Q-0050/22 | G16H-0020/10 | G16H-0020/30 | G16H-0040/40 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30 | A61H-0009/0092 | A61H-2201/1207 | A61H-2201/1635 | A61H-2201/5005 | A61H-2201/5007 | A61H-2201/5012 | A61H-2201/5043 | A61H-2201/5046 | A61H-2201/5071 | A61H-2201/5089 | A61H-2201/5097 | A61H-2230/00 | A61H-2230/065 | A61H-2230/203 | A61H-2230/208 | A61H-2230/255 | A61H-2230/305 | A61H-2230/505 | A61H-2230/655 | A61M-0001/74 | A61M-0001/90 | A61M-0016/0051 | A61M-2205/3553 | A61M-2205/3561 | A61M-2205/52 | A61M-2230/005 | A61M-2230/06 | A61M-2230/205 | A61M-2230/30 | A61M-2230/50 | G16H-0020/17","G16H-050/20","G16H-050/20 | A61H-009/00 | A61M-005/172 | G06Q-010/0639 | G06Q-010/10 | G06Q-050/22 | G16H-020/10 | G16H-020/17 | G16H-020/30 | G16H-040/40 | G16H-040/63 | G16H-040/67 | G16H-050/30 | A61M-001/00 | A61M-016/00","","","","","","4924031001400"
"US","US","P","B2","Virtual guidance for orthopedic surgical procedures","An example method includes displaying, via a visualization device and overlaid on a portion of an anatomy of a patient viewable via the visualization device, a virtual model of the portion of the anatomy obtained from a virtual surgical plan for an orthopedic joint repair surgical procedure to attach a prosthetic to the anatomy; and displaying, via the visualization device and overlaid on the portion of the anatomy, a virtual guide that guides at least one of preparation of the anatomy for attachment of the prosthetic or attachment of the prosthetic to the anatomy.","1. A method comprising: storing a virtual surgical plan for an orthopedic surgery, wherein the virtual surgical plan for the orthopedic surgery includes information defining a plurality of steps of the orthopedic surgery and includes information that specifies that a first surgical item is to be used in a specific step of the orthopedic surgery, wherein the plurality of steps defined by the virtual surgical plan include the specific step;determining, by a processing device, that a surgeon is currently using a second surgical item;selecting, by the processing device, based on the information in the virtual surgical plan that specifies that the first surgical item is to be used in the specific step and based on the determination that the surgeon is currently using the second surgical item, the first surgical item in a kit that includes a plurality of surgical items, wherein the first surgical item is usable for the specific step of the orthopedic surgery and the plurality of surgical items in the kit includes one or more of surgical tools or surgical implants; andcausing, by the processing device, a mixed reality (MR) visualization device to present virtual information that identifies to a user the first surgical item among the plurality of surgical items as being usable for the specific step of the orthopedic surgery, wherein the virtual information is presented on or adjacent a position of the first surgical item visible via the MR visualization device and the virtual information comprises one or more of a virtual halo, an outline, or highlighting.","13","17/221320","2021-04-02","2021-0290319","2021-09-23","12050999","2024-07-30","HOWMEDICA OSTEONICS CORP.","Sergii Poltaretskyi | Jean Chaoui | Damien Cariou | Yannick Morvan | Vincent Abel Maurice Simoes | Vincent Gaborit | Benjamin Dassonville","","","","G16H-0020/40","G16H-0020/40 | A61B-0005/1114 | A61B-0005/1121 | A61B-0005/1127 | A61B-0005/681 | A61B-0017/142 | A61B-0017/1604 | A61B-0017/1626 | A61B-0017/1659 | A61B-0017/1684 | A61B-0017/1703 | A61B-0034/10 | A61B-0034/25 | A61B-0034/76 | A61B-0090/08 | A61B-0090/36 | A61B-0090/361 | A61B-0090/37 | A61B-0090/39 | A61B-0090/92 | A61F-0002/40 | A61F-0002/4081 | G02B-0027/0075 | G02B-0027/017 | G02B-0027/0172 | G06F-0003/011 | G06F-0003/04815 | G06F-0003/0482 | G06F-0018/2163 | G06F-0030/10 | G06N-0003/08 | G06N-0020/00 | G06T-0007/0012 | G06T-0007/11 | G06T-0007/55 | G06T-0011/00 | G06T-0019/006 | G06T-0019/20 | G09B-0005/06 | G09B-0009/00 | G09B-0019/003 | G09B-0023/28 | G16H-0030/20 | G16H-0030/40 | G16H-0040/20 | G16H-0040/60 | G16H-0040/63 | G16H-0040/67 | G16H-0050/30 | G16H-0050/50 | G16H-0050/70 | G16H-0070/20 | G16H-0070/60 | G16H-0080/00 | H04N-0013/122 | H04N-0013/332 | A61B-0005/744 | A61B-2017/00115 | A61B-2017/00119 | A61B-2017/00123 | A61B-0017/151 | A61B-0017/1775 | A61B-0017/1778 | A61B-2034/102 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/108 | A61B-2034/2048 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2065 | A61B-2034/2068 | A61B-2034/252 | A61B-2034/254 | A61B-2090/062 | A61B-2090/067 | A61B-2090/0801 | A61B-2090/0807 | A61B-2090/365 | A61B-2090/366 | A61B-2090/367 | A61B-2090/368 | A61B-2090/373 | A61B-2090/374 | A61B-2090/3762 | A61B-2090/378 | A61B-2090/3937 | A61B-2090/3945 | A61B-2090/397 | A61B-2090/502 | A61B-2505/05 | A61B-2562/0219 | A61F-2002/4011 | A61F-0002/4606 | A61F-0002/4612 | A61F-2002/4633 | A61F-2002/4658 | A61F-2002/4668 | G02B-2027/0141 | G02B-2027/0174 | G06F-0003/0483 | G06N-0003/04 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/20036 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30008 | G06T-2207/30052 | G06T-2207/30204 | G06T-2210/41 | G06T-2219/2004 | G06V-2201/03 | G16H-0050/20","G06N-003/084","G06N-003/084 | A61B-005/00 | A61B-005/11 | A61B-017/14 | A61B-017/16 | A61B-017/17 | A61B-034/00 | A61B-034/10 | A61B-090/00 | A61B-090/92 | A61F-002/40 | G02B-027/00 | G02B-027/01 | G06F-003/01 | G06F-003/04815 | G06F-003/0482 | G06F-018/21 | G06F-030/10 | G06N-003/08 | G06N-020/00 | G06T-007/00 | G06T-007/11 | G06T-007/55 | G06T-011/00 | G06T-019/00 | G06T-019/20 | G09B-005/06 | G09B-009/00 | G09B-019/00 | G09B-023/28 | G16H-020/40 | G16H-030/20 | G16H-030/40 | G16H-040/20 | G16H-040/60 | G16H-040/63 | G16H-040/67 | G16H-050/30 | G16H-050/50 | G16H-050/70 | G16H-070/20 | G16H-070/60 | G16H-080/00 | H04N-013/122 | H04N-013/332 | A61B-017/00 | A61B-017/15 | A61B-034/20 | A61B-090/50 | A61F-002/46 | G06F-003/0483 | G06N-003/04 | G16H-050/20","","","","","","4924031003545"
"US","US","P","B2","Operating room black-box device, system, method and computer readable medium for event and error prediction","A multi-channel recorder/encoder for collecting, integrating, synchronizing and recording medical or surgical data received as independent live or real-time data streams from a plurality of hardware units. The medical or surgical data relating to a live or real-time medical procedure. Example hardware units include a control interface, cameras, sensors, audio devices, and patient monitoring hardware. Further example systems may include a cloud based platform incorporating the encoder.","1. A computer implemented system for automatically processing audio-visual recordings of one or more surgical procedures to automatically generate data structures representing durations of time where predicted errors are estimated to augment rendering of a surgical decision support graphical user interface associating digital recordings of one or more surgical procedures with the predicted errors based on predictions generated by an artificial neural network, the system comprising: a plurality of hardware units for collecting real-time medical or surgical data streams having a control interface coupled by a network to cameras, sensors, audio devices, and patient monitoring hardware, the real-time medical or surgical data streams relating to a real-time medical procedure within an operating or clinical site;an encoder for synchronizing and recording the real-time medical or surgical data streams to a common clock or timeline to generate a session container data structure storing at least one of video or audio data streams;a processor, coupled with a non-transitory computer-readable memory maintaining a trained predictive data model, the processor configured to: identify, using the trained predictive data model, one or more potential time-stamped clinical error events within the session container data structure;generate a time-synchronized metadata track representing timestamp markers corresponding to specific timestamps or timestamp ranges of the durations of time corresponding to the at least one of video or audio data streams of the session container data structure, each timestamp marker of the timestamp markers corresponding to a specific time-stamped potential clinical error event;train the predictive data model using reviewer validation inputs representing one or more labels corresponding to at least one of the time-stamped potential clinical error events, the one or more labels each indicating whether the corresponding time-stamped potential clinical error event was correct or false; andcontrol rendering of the surgical decision support graphical user interface by overlaying a visual rendering of the time-synchronized metadata track upon a rendering of a timeline chart, one or more interactive visual interface elements, each interactive visual interface element corresponding to the corresponding specific time-stamped potential clinical error event.","20","17/734834","2022-05-02","2022-0270750","2022-08-25","12051500","2024-07-30","SST CANADA INC.","Teodor Pantchev Grantcharov | Kevin Lee Yang","PCT-CA2015-000504","WO","2015-09-23","G16H-0040/20","G16H-0040/20 | A61B-0005/0022 | G06F-0001/12 | G06F-0017/40 | G06N-0020/00 | G16H-0020/40 | G16H-0030/20 | G16H-0040/63 | G16H-0040/67 | G16H-0050/50 | H04L-0063/0421","G16H-040/20","G16H-040/20 | A61B-005/00 | G06F-001/12 | G06F-017/40 | G06N-020/00 | G16H-020/40 | G16H-030/20 | G16H-040/63 | G16H-040/67 | G16H-050/50 | H04L-009/40","","","","","","4924031004044"
"US","US","P","B2","Remote patient management and monitoring systems and methods","Systems and methods are provided for remote patient management and monitoring. The patient is monitored with a wireless sensor system connected to an application executing on a patient user computing device. The system continuously monitors physiological parameters, such as, but not limited to, blood oxygen saturation (SpO2), pulse rate, perfusion index, pleth variability index, and/or respiration rate from the photoplethysmograph. The system triggers alarms if the patient physiological data violates thresholds. Care providers review patient data and associated alarm(s) with graphical user interfaces.","1. A method of establishing a monitoring environment for a user suspected of having a contagious respiratory infection where the user is to be monitored remotely from a care provider, said monitoring environment including one or more sensors worn by the user, a wearable device worn by the user configured to communicate with the one or more sensors and to process information responsive to output from the one or more sensors, a user computing device configured to wirelessly communicate with the wearable device and to communicate with a remote care provider system over a network, the care provider system configured to be monitored by the care provider, the method comprising: providing a user monitoring kit to the user, said user monitoring kit including said wearable device and at least some of said one or more sensors, said wearable device configured to process sensor signals to determine measurement values of blood oxygen saturation of the user over a monitoring period;providing a user a first software application for said user computing device, said first software application configured to aggregate medical information of said user, said medical information including received said measurement values of said blood oxygen saturation and received one or more measurement values of a temperature of said user; andproviding a care provider a second software application for said care provider system, said second software application configured to receive medical information from said first software application, to process said medical information and to output to a display viewable by said care provider indicia responsive to said measurement values of said blood oxygen saturation and temperature of said user during said monitored period, said indicia including a variance from a baseline for said user at least when said user should receive further screening for said contagious respiratory infection.","12","18/447121","2023-08-09","2023-0380701","2023-11-30","12042252","2024-07-23","MASIMO CORPORATION","Omar Ahmed | Nicholas Evan Barker | Keith Ward Indorf | Sungwhan Cha | Sebastian T. Frey | Hyejin Cho","","","","A61B-0005/02055","A61B-0005/02055 | A61B-0005/0004 | A61B-0005/0008 | A61B-0005/002 | A61B-0005/0022 | A61B-0005/01 | A61B-0005/0205 | A61B-0005/08 | A61B-0005/0816 | A61B-0005/14542 | A61B-0005/6801 | A61B-0005/6814 | A61B-0005/6823 | A61B-0005/6824 | A61B-0005/6826 | A61B-0005/7275 | A61B-0005/7282 | A61B-0005/7435 | A61B-0005/746 | A61B-0005/7465 | A61B-0005/747 | A61B-0005/7475 | G06F-0009/451 | G16H-0010/60 | G16H-0015/00 | G16H-0040/20 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | G16H-0050/80 | H04B-0017/318 | H04W-0004/021 | A61B-0005/024 | A61B-0005/026 | A61B-0005/14551 | A61B-2562/0271 | G06F-0003/04842","A61B-005/00","A61B-005/00 | A61B-005/01 | A61B-005/0205 | A61B-005/08 | A61B-005/145 | G06F-009/451 | G16H-010/60 | G16H-015/00 | G16H-040/20 | G16H-040/67 | G16H-050/20 | G16H-050/30 | G16H-050/70 | G16H-050/80 | H04B-017/318 | H04W-004/021 | A61B-005/024 | A61B-005/026 | A61B-005/1455 | G06F-003/04842","","","","","","4924030001163"
"US","US","P","B2","Facial model for generation of post-treatment images of teeth and soft facial tissues","A processing device receives a pre-treatment model of upper and lower dental arches of a person, the pre-treatment model comprising first positions of teeth, and further receives a post-treatment model of the upper and lower dental arches, the post-treatment model comprising second positions of the teeth. The processing device determines a first mapping between first positions of first landmarks in the pre-treatment model and second positions of the first landmarks in the post-treatment model, and receives an image of a face of the person, comprising at least the teeth and soft facial tissues. The processing device generates a second mapping between the first landmarks and second landmarks associated with the soft facial tissues from the image, and generates a facial model comprising the first mapping and the second mapping, wherein the facial model generates post-treatment images of the teeth and the soft facial tissues based on pre-treatment images.","1. A method comprising: receiving a pre-treatment virtual three-dimensional (3D) model of an upper dental arch and a lower dental arch of a person, the pre-treatment virtual 3D model comprising first positions and orientations of teeth of the upper dental arch and the lower dental arch;receiving a post-treatment virtual 3D model of the upper dental arch and the lower dental arch of the person, the post-treatment virtual 3D model comprising second positions and orientations of the teeth of the upper dental arch and the lower dental arch, wherein for at least one tooth the second positions and orientations are different from the first positions and orientations;determining a first mapping between first positions of a first plurality of landmarks associated with the teeth in the pre-treatment virtual 3D model and second positions of the first plurality of landmarks associated with the teeth in the post-treatment virtual 3D model;receiving an image of a face of the person, wherein the image comprises at least a portion of the teeth and soft facial tissues;generating a second mapping between the first plurality of landmarks associated with the teeth and a second plurality of landmarks associated with the soft facial tissues from the image of the face of the person; andgenerating a facial model comprising the first mapping and the second mapping, wherein the facial model generates post-treatment images of the teeth and the soft facial tissues based on pre-treatment images of the teeth and the soft facial tissues.","24","17/234214","2021-04-19","2021-0236241","2021-08-05","12042350","2024-07-23","ALIGN TECHNOLOGY, INC.","Andrew Jang","","","","A61C-0007/002","A61C-0007/002 | A61B-0005/4542 | A61B-0034/10 | G06V-0040/168 | G06V-0040/174 | A61B-2017/00216 | A61B-2034/105 | A61B-2034/2065 | A61B-0090/36 | G06F-0003/011","A61C-007/00","A61C-007/00 | A61B-005/00 | A61B-017/00 | A61B-034/10 | A61B-034/20 | A61B-090/00 | G06F-003/01 | G06V-040/16","","","","","","4924030001261"
"US","US","P","B2","Scheduling device for customizable electronic notifications","A user interface is provided to schedule a sleep schedule associated with an alarm. Various user interface elements may be presented using the user interface. Responsive to user selection of these user interface elements, certain actions may be performed to configure aspects of the sleep schedule.","1. A computer-implemented method, comprising: presenting, at a display of a user device, a user interface for configuring a sleep schedule associated with an alarm;responsive to receiving, via the user interface, a first user selection of one or more first user interface elements presented by the user interface, adding one or more days to the sleep schedule;responsive to receiving, via the user interface, a second user selection of a second user interface element presented by the user interface, scheduling a wake up time for each of the one or more days,responsive to receiving, via the user interface, a third user selection of a third user interface element presented by the user interface, configuring a bed time for each of the one or more days; andresponsive to receiving, via the user interface, a fourth user selection of a fourth user interface element presented by the user interface, configuring an alarm option associated with presentation of the alarm on each of the one or more days.","20","17/542122","2021-12-03","2022-0091565","2022-03-24","12045017","2024-07-23","Apple Inc.","Roy J. E. M. Raymann | Jay Kriz Blahnik | Stephanie M. Greer | Aroon Pahwa | Jonathan T. Varbel | Kevin M. Lynch","","","","G04F-0003/06","G04F-0003/06 | A61B-0005/4812 | A61B-0005/4815 | A61B-0005/742 | G04G-0009/0064 | G04G-0013/02 | G04G-0021/025 | G06F-0003/04847 | G06F-0009/451 | G06Q-0010/109 | G06Q-0050/22 | A61B-0005/0205 | A61B-0005/282 | A61B-0005/681","G04F-003/06","G04F-003/06 | A61B-005/00 | A61B-005/0205 | A61B-005/282 | G04G-009/00 | G04G-013/02 | G04G-021/02 | G06F-003/04847 | G06F-009/451 | G06Q-010/109 | G06Q-050/22","","","","","","4924030003898"
"US","US","P","B2","Medical image reading assistant apparatus and method providing hanging protocols based on medical use artificial neural network","Disclosed herein is a medical image reading assistant apparatus that provides hanging protocols based on a medical artificial neural network. The medical image reading assistant apparatus includes a computing system, and the computing system includes at least one processor. The at least one processor is configured to acquire or receive a first analysis result obtained through the inference of a first artificial neural network from a first medical image, to generate a first display setting based on the first analysis result, and to execute the first display setting so that the first medical image and the first analysis result are displayed on a screen based on the first display setting.","1. A medical image reading assistant apparatus providing hanging protocols based on a medical artificial neural network, the medical image reading assistant apparatus comprising a computing system, the computing system comprising at least one processor, wherein the at least one processor is configured to: acquire or receive a first analysis result obtained through an inference of a first artificial neural network from a first medical image;generate a first display setting based on the first analysis result; andexecute the first display setting by controlling a display device so that the first medical image and the first analysis result are displayed on a screen of the display device based on the first display setting,wherein the first display setting includes a layout of a plurality of visual expressions of the first medical image.","18","16/896801","2020-06-09","2021-0035687","2021-02-04","12046367","2024-07-23","CORELINE SOFT CO., LTD.","Jaeyoun Yi | Donghoon Yu | Hyeon Ji","10-2019-0092909","KR","2019-07-31","G16H-0050/20","G16H-0050/20 | A61B-0006/5217 | G06F-0003/0481 | G06F-0003/14 | G06T-0007/0012 | G06V-0010/82 | G16H-0015/00 | G16H-0030/40 | G16H-0050/70 | A61B-0005/055 | A61B-0006/032 | G06T-2200/24 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30004 | G06T-2207/30096","G16H-050/00","G16H-050/00 | A61B-006/00 | G06F-003/0481 | G06F-003/14 | G06T-007/00 | G06V-010/82 | G16H-015/00 | G16H-030/40 | G16H-050/20 | G16H-050/70 | A61B-005/055 | A61B-006/03","","","","","","4924030005241"
"US","US","P","B2","Customization of individualized implant","A system for customizing an implant is provided. The system includes a processor configured to: i) obtain one or more medical image stacks of a joint; ii) obtain a three-dimensional image representation of the joint based on at least one of said medical image stacks; iii) determine damage to the joint by analyzing said medical image stacks; iv) select an implant template from a predefined set of implant templates having predetermined types and sizes; v) generate a 3D model, in which the marked damage is visualized together with the selected implant template in a proposed position; vi) display the 3D model; vii) receive an approval for said selected implant template in said proposed position; and viii) determine the final shape and dimensions of a customized implant based on said selected implant template and said proposed position.","1. A system for customizing an individualized implant suitable for an anatomical joint of a patient, the system comprising a plurality of types of authorizations for different users and at least one processor configured to: i) obtain one or more medical image stacks of at least a part of the anatomical joint, wherein each of the medical image stacks are generated using a medical imaging system;ii) obtain a three-dimensional image representation of the at least part of the anatomical joint which is based on at least one of said medical image stacks;iii) select an implant template to be used as a basis for a customized implant for said anatomical joint;iv) generate a 3D model for visualization based on the three-dimensional image representation, in which 3D model the selected implant template is visualized in a proposed position;v) display the 3D model with functionality to enable manipulation of the 3D model;vi) receive an approval for said selected implant template in said proposed position from a user having an approval authorization from the plurality of types of authorizations; andvii) determine the final shape and dimensions of a customized implant suitable for said anatomical joint based on said selected implant template and said proposed position.","20","18/128453","2023-03-30","2023-0317298","2023-10-05","12046376","2024-07-23","EPISURF IP-MANAGEMENT AB","Jeanette Sp?ngberg | Felicia Aldrin Bernhardt | Katarina Flodstr?m","","","","G16H-0050/50","G16H-0050/50 | A61F-0002/30942 | G06F-0003/04815 | G06F-0021/30 | G06F-0030/10 | G06T-0007/0012 | G06T-0019/20 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | A61F-2002/30952 | A61F-2002/30963 | G06F-2111/16 | G06Q-0050/04 | G06T-2200/24 | G06T-2207/30008 | G06T-2210/41","G16H-050/50","G16H-050/50 | A61F-002/30 | G06F-003/04815 | G06F-021/30 | G06F-030/10 | G06F-111/16 | G06Q-050/04 | G06T-007/00 | G06T-019/20 | G16H-030/20 | G16H-030/40 | G16H-050/20","","","","","","4924030005250"
"US","US","P","B1","Communication devices, methods, and systems","Exemplary aspects of a treatment device are described, such as a body and a plurality of energy generator elements. The body may be provided with a processing unit and a power source. The plurality of energy generator elements may be independently operable to convert electricity from the power source into a plurality of different energy types transmittable towards an area of skin of a user. The plurality of energy generator elements may be arranged coaxially about an axis. The body may include a grip arranged to be grasped by a hand of the user applying a gripping force to maintain the plurality of energy generator elements on or adjacent the area of skin. Related apparatus, methods, and systems are described.","1. A treatment device, comprising: a body provided with a processing unit and a power source; anda plurality of energy generator elements being independently operable to convert electricity from the power source into a plurality of different energy types transmittable towards an area of skin of a user, the plurality of energy generator elements being arranged coaxially about an axis,wherein the body includes a grip arranged to be grasped by a hand of the user applying a gripping force to maintain the plurality of energy generator elements on or adjacent the area of skin,wherein the plurality of energy generator elements includes a first energy generator element and a second energy generator element, and wherein the first energy generator element is an impact generator element having a tissue contact surface that is linearly actuatable along the axis to contact and cause corresponding physical movement of the area of skin.","28","18/526980","2023-12-01","","","12036174","2024-07-16","DATAFEEL INC.","Matthew Robert Leaper","","","","A61H-0023/006","A61H-0023/006 | A61B-0005/486 | A61H-0023/02 | A61N-0001/0456 | A61N-0001/36014 | A61B-0005/7435 | A61H-2201/0153 | A61H-2201/0207 | A61H-2201/123 | A61H-2201/1664 | A61H-2205/065 | A61H-2230/065","A61B-005/11","A61B-005/11 | A61B-005/00 | A61H-023/00 | A61H-023/02 | A61N-001/04 | A61N-001/36 | G06F-003/01","","","","","","4924029001396"
"US","US","P","B2","Contactless operation of devices using a pointing apparatus","Contactless operation of a device is provided via a mobile pointing apparatus and a receiving arrangement associated with the device. The mobile pointing apparatus includes a signal emitter for emitting an optical or electromagnetic signal, and the receiving arrangement associated with the device determines a pointing target of the mobile pointing apparatus relative to the device based on the signal emitted by the mobile pointing apparatus and triggers a function of the device based on the pointing target of the mobile pointing apparatus.","1. A system for contactless operation of a device, comprising: an electromagnetic pointing apparatus, comprising at least one electromagnetic signal emitter configured to emit a radar signal;at least three receivers configured to receive the radar signal emitted by the at least one electromagnetic signal emitter; anda processor configured to: determine a pointing target of the electromagnetic pointing apparatus relative to the device based on the received radar signal; andtrigger a function of the device based on the determined pointing target of the electromagnetic pointing apparatus.","20","18/198737","2023-05-17","2023-0288995","2023-09-14","12039112","2024-07-16","FRESENIUS MEDICAL CARE AG | FRESENIUS MEDICAL CARE DEUTSCHLAND GMBH","Christoph Hitzelberger | Waldemar Witt | Martin Pospiech","2021-167557","EP","2021-04-09","G06F-0003/0346","G06F-0003/0346 | G06F-0003/0304 | G06F-0003/03542 | G06F-0003/03545 | G06F-0003/0383 | G16H-0040/63 | A61B-0005/7475 | A61M-0005/172 | A61M-2205/3306 | A61M-2205/502 | G06V-0040/13","G06F-003/0346","G06F-003/0346 | G06F-003/03 | G06F-003/0354 | G06F-003/038 | G16H-040/63 | A61B-005/00 | A61M-005/172 | G06V-040/13","","","","","","4924029004314"
"US","US","P","B2","Systems and methods for data synchronization","A system for data synchronization is provided. The system may include a workstation, a user terminal, and a storage device that is accessible to the user terminal and the workstation. The workstation may be configured to generate an identifier relating to a subject. The identifier may point to a data address in the storage device that stores data relating to the subject. The user terminal may be configured to acquire image data of the subject from the data address based on the identifier, receive a user input with respect to the image data of the subject, and update the image data stored in the data address based on the user input.","1. A system for data synchronization, comprising: a workstation;a user terminal; anda storage device that is a cache region of the workstation shared with the user terminal, being accessible to the user terminal and the workstation, wherein the workstation is configured to generate an identifier relating to a subject, the identifier pointing to a data address in the storage device that stores data relating to the subject, the identifier includes at least one of a barcode, a quick response (QR) code, or an image identifier; and the workstation is configured to display the identifier on a display of the workstation;the user terminal is configured to acquire image data of the subject from the data address by scanning the identifier, receive a user input with respect to the image data of the subject, generate updated image data based on the user input, and transmit the updated image data to the storage device to be stored in the data address by scanning the identifier again; andthe updated image data is automatically synchronized with the workstation after the updated image data is stored in the data address, and the workstation is further configured to generate a processing result by processing the updated image data, the processing result includes a treatment plan that includes at least one of a total dose or a dose distribution in the subject, the workstation is configured to generate the treatment plan by using an RT software.","16","17/172095","2021-02-10","2021-0257108","2021-08-19","12040074","2024-07-16","SHANGHAI UNITED IMAGING HEALTHCARE CO., LTD.","Chengcheng Rong","2020-10099414","CN","2020-02-18","G16H-0030/20","G16H-0030/20 | G06K-0007/1417 | G06T-0007/0012 | G06V-0010/25 | G16H-0030/40 | G16H-0080/00","A61N-005/00","A61N-005/00 | G06K-007/14 | G06T-007/00 | G06V-010/25 | G16H-030/20 | G16H-030/40 | G16H-080/00","","","","","","4924029005266"
"US","US","P","B2","Fitness activity monitoring systems and methods","Apparatus, systems, and methods for tracking the location of an individual during a fitness activity are disclosed. A method of tracking a participant engaged in a fitness activity includes determining a location of the participant during the fitness activity based on data received at a portable fitness device used by the participant; determining a location of a spectator during the fitness activity based on data received at a mobile spectator device used by the spectator; from a server, sending an alert to a spectator at a spectator device during the fitness activity indicating that the participant is within a predetermined distance of the spectator; and sending an alert to the portable fitness device during the fitness activity indicating that the spectator is within a predetermined distance of the participant.","1. A method for tracking an athlete engaged in a fitness activity, the method comprising: receiving, at a portable fitness monitoring device, data based upon which a dynamic location of the athlete during the fitness activity can be determined;receiving, at a spectator device associated with a spectator registered to track the athlete associated with the portable fitness monitoring device, data based upon which a spectator location of a spectator can be determined;displaying, on the portable fitness monitoring device, a map of a route of the fitness activity; anddisplaying, on the portable fitness monitoring device, a number of spectators registered to track the athlete, a first symbol of the dynamic location of the athlete, and a second symbol of the spectator location along the route,wherein the first symbol moves in real-time based on the dynamic location of the athlete along the route.","20","17/811372","2022-07-08","2022-0342081","2022-10-27","12032070","2024-07-09","adidas AG","Jon Harald Werner | Christian Dibenedetto | Stephen John Black | Amy Jones Vaterlaus","","","","G01S-0019/19","G01S-0019/19 | A61B-0005/02055 | A61B-0005/4875 | A63B-0024/0062 | G06Q-0010/10 | G06Q-0030/0241 | G06Q-0050/01 | H04L-0067/12 | H04L-0067/52 | A61B-0005/02438 | A61B-0005/0816 | A61B-0005/1112 | A61B-0005/112 | A61B-2562/0219 | A61B-2562/0223 | A63B-2220/12 | A63B-2220/17 | A63B-2220/40 | A63B-2230/06 | A63B-2230/42 | A63B-2230/50 | G16H-0020/30 | G16H-0040/67 | G16Y-0010/65 | G16Y-0020/10 | G16Y-0040/60","G06Q-099/00","G06Q-099/00 | A61B-005/00 | A61B-005/0205 | A63B-024/00 | G01S-019/19 | G06Q-010/10 | G06Q-030/0241 | G06Q-050/00 | H04L-067/12 | H04L-067/52 | A61B-005/024 | A61B-005/08 | A61B-005/11 | G16H-020/30 | G16H-040/67 | G16Y-010/65 | G16Y-020/10 | G16Y-040/60","","","","","","4924028004376"
"US","US","P","B2","Time and location-based linking of captured medical information with medical records","Systems, methods, and computer readable media related to aggregating and associated captured medical data are disclosed. They involve receiving an ID of a piece of equipment in a medical facility, location information for the equipment, and medical information captured by the equipment; and ascertaining a time of information capture by the equipment. They further involve performing a lookup in a data record to determine an identity of a particular patient assigned to a location associated with the location information and performing a lookup in a data structure to identify a medical record of the particular patient. They further involve establishing an association between the medical information captured by the equipment and the medical record to thereby enable access to the medical information through access to the medical record of the particular patient.","1. A non-transitory computer readable medium including instruction that, when executed by at least one processor, cause the at least one processor to perform operations for updating medical records, the operations comprising: receiving an ID of a piece of equipment in a medical facility;receiving location information for the piece of equipment in the medical facility;receiving medical information captured by the piece of equipment during a medical procedure;ascertaining a time of information capture by the piece of equipment;accessing at least one data record that associates scheduled medical procedures with locations of medical procedures and patient information;using at least the location information and the ascertained time, performing a look-up in the data record to determine an identity of a patient assigned to a location associated with the location information;accessing a medical record data structure;using the identity of the patient, performing a lookup in the medical record data structure to identify a medical record of the patient; andestablishing an association between the medical information captured by the piece of equipment and at least a portion of information in the medical record of the particular patient to thereby enable access to the medical information captured by the piece of equipment through access to the medical record of the patient.","20","17/643248","2021-12-08","2022-0165403","2022-05-26","12033104","2024-07-09","THEATOR INC.","Tamir Wolf | Dotan Asselmann | Evgeny Makrinich | Asaf Rafaeli | Barak Ariel","","","","G06Q-0010/06398","G06Q-0010/06398 | A61B-0034/20 | A61B-0034/25 | A61B-0090/37 | G06N-0020/00 | G06T-0007/0014 | G06T-0007/20 | G06T-0007/70 | G06T-0007/90 | G06V-0010/764 | G06V-0020/40 | G06V-0020/41 | G06V-0020/46 | G06V-0020/48 | G06V-0020/49 | G11B-0027/102 | G11B-0027/11 | G11B-0027/34 | G16H-0010/60 | G16H-0015/00 | G16H-0020/40 | G16H-0030/20 | G16H-0030/40 | G16H-0040/20 | G16H-0050/30 | G16H-0050/70 | G16H-0070/20 | A61B-2034/2065 | A61B-2034/256 | A61B-2560/0487 | G06F-0003/0482 | G06T-2207/10016 | G06T-2207/20081 | G06T-2207/20084 | G06V-0020/44 | G06V-2201/03 | G06V-2201/034 | G16H-0050/20","A61B-005/00","A61B-005/00 | A61B-034/00 | A61B-034/20 | A61B-090/00 | G06N-020/00 | G06Q-010/0639 | G06T-007/00 | G06T-007/20 | G06T-007/70 | G06T-007/90 | G06V-010/764 | G06V-020/40 | G11B-027/10 | G11B-027/11 | G11B-027/34 | G16H-010/60 | G16H-015/00 | G16H-020/40 | G16H-030/20 | G16H-030/40 | G16H-040/20 | G16H-050/30 | G16H-050/70 | G16H-070/20 | G06F-003/0482 | G16H-050/20","","","","","","4924028005406"
"US","US","P","B2","Ungrounded master control devices and methods of use","A mechanically ungrounded master control device comprises a rigid chassis for engagement with one or more fingers of a user's hand and at least one pivotable finger engagement device for controlling movement of a tool end effector. The control device also includes at least one sensor for detecting a position and orientation of the rigid chassis in a surgical environment and a switch coupled to the rigid chassis and manipulatable by a finger of the user's hand while another finger of the user's hand is engaged with the at least one pivotable finger engagement device.","1. A teleoperated medical system comprising: a mechanically ungrounded master control device including: a rigid chassis engageable with one or more fingers of a user'ss hand,at least one pivotable finger engagement device operative to control at least one of: movement of a virtual tool or a physical tool end effector,at least one sensor of a position and an orientation of the rigid chassis in a surgical environment,a switch coupled to the rigid chassis and manipulatable by a second finger of the user'ss hand while a first finger of the user'ss hand is engaged with the at least one pivotable finger engagement device, anda plate pivotally coupled to the rigid chassis and configured to contact a backside of the user'ss hand, the plate being pivotable about a rotational axis of the plate with reference to the rigid chassis;a console including: a mechanically grounded master control device operative to control at least one of: movement of the virtual tool or movement of the physical tool end effector, anda display operative to display at least one of the virtual tool or the physical tool end effector; anda manipulator operative to manipulate the physical tool end effector in response to at least one of: movement of the mechanically grounded master control device or movement of the mechanically ungrounded master control device.","22","17/833667","2022-06-06","2022-0313379","2022-10-06","12023122","2024-07-02","INTUITIVE SURGICAL OPERATIONS, INC.","Anthony M. Jarc","","","","A61B-0034/37","A61B-0034/37 | A61B-0001/04 | A61B-0005/11 | A61B-0034/35 | A61B-0034/74 | A61B-2017/00438 | A61B-2034/742 | G06F-0003/0346","A61B-034/37","A61B-034/37 | A61B-001/04 | A61B-005/11 | A61B-017/00 | A61B-034/00 | A61B-034/35 | G06F-003/0346","","","","","","4924027001252"
"US","US","P","B2","Device interoperation","A surgical robotic system comprising: a surgical robot; a user interface console coupled to the surgical robot whereby a user can control motion of the surgical robot; a data logger for logging data from a surgical procedure performed by means of the robot; and a portable user terminal; the system further comprising: a display controllable by one of the data logger and the user terminal, that one of the data logger and the user terminal being configured for displaying a machine-readable code whereby the data logger or a user of the user terminal can be identified; and a camera coupled to the other of the data logger and the user terminal; the said other of the data logger and the user terminal being configured to, on receiving from the camera an image of a machine-readable code, decode that code to identify the data logger or a user of the user terminal and to cause the identified entity to be logged in association with a procedure performed by means of the robot.","1. A method of analysing a performance of a surgical robotic system, the method comprising: performing a surgical procedure using a surgical robot;measuring, during the surgical procedure, a state of the surgical robot at multiple times;storing log data indicative of the measured state of the surgical robot at the multiple times, and including an indication of a user involved in the surgical procedure and their role in the surgical procedure;storing a dataset defining a set of metrics, each metric defining one or more states of the surgical robot and indicating a role;applying one metric of the set of metrics to the stored log data by identifying the defined one or more states of the surgical robot in the log data and thereby deriving an indicator of performance of the surgical robot; andselectively reporting the indicator of performance to the user indicated by the log data as having had in the surgical procedure the role indicated by the metric.","13","18/197228","2023-05-15","2023-0277265","2023-09-07","12023123","2024-07-02","CMR SURGICAL LIMITED","Rupert Menzies | Paul Christopher Roberts","2018016165","GB","2018-10-03","A61B-0034/37","A61B-0034/37 | B25J-0009/1697 | B25J-0013/06 | G06F-0021/44 | A61B-0005/117 | A61B-2017/00221 | A61B-2034/258 | A61B-2090/3612 | A61B-2090/373 | A61B-0090/50 | A61B-0090/96 | G05B-2219/40414 | G05B-2219/50391 | G06F-0021/31 | H04N-0023/54","A61B-034/37","A61B-034/37 | A61B-005/117 | A61B-017/00 | A61B-034/00 | A61B-090/00 | A61B-090/50 | A61B-090/96 | B25J-009/16 | B25J-013/06 | G06F-021/31 | G06F-021/44 | H04N-023/54","","","","","","4924027001253"
"US","US","P","B2","System and method for eye tracking","A system and method for eye tracking includes receiving an input image including at least a portion of a person's retina, finding a spatial transformation between the input image and a reference image, the reference image including at least a portion of the person's retina and calculating a change in orientation of an eye of the person corresponding to the spatial transformation. A processor may calculate, from the change in orientation an orientation of the person's eye associated with the input image, a direction of gaze associated with the input image, a gaze target associated with the input image and/or a ray of gaze associated with the input image.","1. A method comprising: receiving an input image including at least a portion of a person'ss retina;finding a spatial transformation between the input image and a reference image, the reference image including at least a portion of the person'ss retina, the reference image associated with a known direction of gaze of the person;converting the spatial transformation to a corresponding spherical rotation having three degrees of freedom;using the spherical rotation to rotate the known direction of gaze associated with the reference image to calculate a change in orientation of an eye of the person corresponding to the spatial transformation; andoutputting a signal based on the change of orientation.","19","17/420166","2020-01-05","2022-0148218","2022-05-12","12023128","2024-07-02","IMMERSIX LTD.","Ori Weitz | Saar Wilf","","","","A61B-0005/0077","A61B-0005/0077 | A61B-0003/0008 | A61B-0003/0025 | A61B-0003/113 | A61B-0003/12 | A61B-0003/14 | A61B-0005/0062 | G06F-0003/013 | G06T-0003/4038 | G06T-0007/0012 | G06T-0007/248 | G06T-0007/50 | G06T-0007/74 | G06V-0010/141 | G06V-0010/75 | G06V-0010/761 | G06V-0040/18 | G06V-0040/197 | H04N-0023/56 | H04N-0023/90 | A61B-0005/6821 | G06F-0018/22 | G06T-2207/10048 | G06T-2207/20081 | G06T-2207/30041","A61B-003/14","A61B-003/14 | A61B-003/00 | A61B-003/113 | A61B-003/12 | A61B-005/00 | G06F-003/01 | G06T-003/4038 | G06T-007/00 | G06T-007/246 | G06T-007/50 | G06T-007/73 | G06V-010/141 | G06V-010/74 | G06V-010/75 | G06V-040/18 | H04N-023/56 | H04N-023/90 | G06F-018/22","","","","","","4924027001258"
"US","US","P","B2","Method and system for image processing to determine blood flow","Embodiments include a system for determining cardiovascular information for a patient. The system may include at least one computer system configured to receive patient-specific data regarding a geometry of the patient's heart, and create a three-dimensional model representing at least a portion of the patient's heart based on the patient-specific data. The at least one computer system may be further configured to create a physics-based model relating to a blood flow characteristic of the patient's heart and determine a fractional flow reserve within the patient's heart based on the three-dimensional model and the physics-based model.","1. A method for identifying blood flow within a vasculature, comprising: determining a physiological model of an anatomical region of a patient containing blood vessels, the physiological model including at least one geometric parameter and functional parameters;adjusting the physiological model on the basis of patient-specific 3D image data of the anatomical region;simulating a 3D perfusion of the anatomical region on the basis of the adjusted physiological model;determining a boundary condition based on the simulating, the boundary condition indicating movement of blood in the blood vessels and into at least a part of a myocardium of the anatomical region;calculating fractional flow reserve (FFR) at the boundary condition; andidentifying blood flow into the anatomical region on the basis of the simulation.","19","17/330521","2021-05-26","2021-0282860","2021-09-16","12016635","2024-06-25","HEARTFLOW, INC.","Charles A. Taylor","","","","A61B-0034/10","A61B-0034/10 | A61B-0005/0035 | A61B-0005/004 | A61B-0005/0044 | A61B-0005/02 | A61B-0005/02007 | A61B-0005/02028 | A61B-0005/021 | A61B-0005/024 | A61B-0005/026 | A61B-0005/0263 | A61B-0005/029 | A61B-0005/055 | A61B-0005/1075 | A61B-0005/1118 | A61B-0005/22 | A61B-0005/4848 | A61B-0005/6852 | A61B-0005/7246 | A61B-0005/7275 | A61B-0005/7278 | A61B-0005/745 | A61B-0006/03 | A61B-0006/032 | A61B-0006/481 | A61B-0006/503 | A61B-0006/504 | A61B-0006/507 | A61B-0006/5205 | A61B-0006/5217 | A61B-0006/5229 | A61B-0008/02 | A61B-0008/04 | A61B-0008/06 | A61B-0008/065 | A61B-0008/481 | A61B-0008/5223 | A61B-0008/5261 | A61B-0034/25 | A61M-0005/007 | G01R-0033/5601 | G01R-0033/5635 | G01R-0033/56366 | G06F-0017/10 | G06F-0018/10 | G06F-0018/22 | G06F-0018/24 | G06F-0030/20 | G06F-0030/23 | G06F-0030/28 | G06G-0007/60 | G06T-0007/0012 | G06T-0007/0014 | G06T-0007/11 | G06T-0007/12 | G06T-0007/13 | G06T-0007/149 | G06T-0007/20 | G06T-0007/60 | G06T-0007/62 | G06T-0007/70 | G06T-0007/73 | G06T-0007/74 | G06T-0011/00 | G06T-0011/001 | G06T-0011/008 | G06T-0011/20 | G06T-0011/60 | G06T-0015/10 | G06T-0017/00 | G06T-0017/005 | G06T-0017/20 | G06V-0010/40 | G06V-0010/42 | G06V-0010/44 | G06V-0020/698 | G16B-0005/00 | G16B-0045/00 | G16H-0010/40 | G16H-0010/60 | G16H-0030/20 | G16H-0030/40 | G16H-0050/30 | G16H-0050/50 | G16H-0050/70 | G16H-0070/00 | A61B-0005/6868 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/108 | A61B-2090/374 | A61B-2090/3762 | A61B-2090/3764 | A61B-2576/00 | A61B-2576/023 | G06T-0007/10 | G06T-2200/04 | G06T-2207/10012 | G06T-2207/10072 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/10104 | G06T-2207/10108 | G06T-2207/20036 | G06T-2207/20124 | G06T-2207/30016 | G06T-2207/30048 | G06T-2207/30104 | G06T-2210/41 | G06T-2211/404 | G06V-0010/467 | Y02A-0090/10","G06T-007/00","G06T-007/00 | A61B-005/00 | A61B-005/02 | A61B-005/021 | A61B-005/024 | A61B-005/026 | A61B-005/029 | A61B-005/055 | A61B-005/107 | A61B-005/11 | A61B-005/22 | A61B-006/00 | A61B-006/03 | A61B-006/50 | A61B-008/02 | A61B-008/04 | A61B-008/06 | A61B-008/08 | A61B-034/00 | A61B-034/10 | A61M-005/00 | G01R-033/56 | G01R-033/563 | G06F-017/10 | G06F-018/10 | G06F-018/22 | G06F-018/24 | G06F-030/20 | G06F-030/23 | G06F-030/28 | G06G-007/60 | G06T-007/11 | G06T-007/12 | G06T-007/13 | G06T-007/149 | G06T-007/20 | G06T-007/60 | G06T-007/62 | G06T-007/70 | G06T-007/73 | G06T-011/00 | G06T-011/20 | G06T-011/60 | G06T-015/10 | G06T-017/00 | G06T-017/20 | G06V-010/40 | G06V-010/42 | G06V-010/44 | G06V-020/69 | G16B-005/00 | G16B-045/00 | G16H-010/40 | G16H-010/60 | G16H-030/20 | G16H-030/40 | G16H-050/30 | G16H-050/50 | G16H-050/70 | G16H-070/00 | A61B-090/00 | G06T-007/10 | G06V-010/46","","","","","","4924026001246"
"US","US","P","B2","Determination of stimulation parameters for muscle activation","Computer-implemented systems and methods for determining epidural spinal stimulation parameters that promote muscle activation use spectral analysis and machine learning techniques to characterize electromyography data.","1. A method for determining a spinal cord epidural stimulation (scES) pattern effective in promoting muscle action, comprising: detecting a first diagnostic signal from at least one muscle of a subject during performance of a first muscle action during a first application of scES to the subject;detecting a second diagnostic signal from the at least one muscle of the subject during performance of a second muscle action during a second application of scES to the subject;extracting at least one feature from the first diagnostic signal and extracting the same at least one feature from the second diagnostic signal;classifying, using a machine learning model, a third diagnostic signal collected from the at least one muscle during a third application of scES to the subject as representative of the first muscle action or the second muscle action based at least in part on the at least one feature.","17","18/175874","2023-02-28","2023-0200713","2023-06-29","12016695","2024-06-25","UNIVERSITY OF LOUISVILLE RESEARCH FOUNDATION, INC.","Susan J. Harkema | Enrico Rejc | Samineh Mesbah","","","","A61B-0005/395","A61B-0005/395 | A61N-0001/36062 | A61N-0001/36135 | G06F-0017/142 | G16H-0020/30 | G16H-0050/70","A61B-005/395","A61B-005/395 | A61N-001/36 | G06F-017/14 | G16H-020/30 | G16H-050/70","","","","","","4924026001306"
"US","US","P","B2","Information processing device, information processing method, information processing system, display device, and reservation system","Provided is an information processing device including a reaction information use unit configured to use reaction information indicating a reaction of a user to presented information in a case where use of the reaction information has been permitted.","1. An information processing device, comprising: at least one sensor configured to monitor a first user within a specific range of the at least one sensor; anda central processing unit (CPU) configured to: change a setting value of the at least one sensor based on a change in a brightness of presented information on a display screen,wherein the change in the brightness of the presented information is in synchronization with a progress of the presented information, andthe at least one sensor is further configured to generate sensor information based on the changed setting value;estimate, based on the sensor information, reaction information that indicates a reaction of the first user to the presented information; anduse the reaction information in a case where the use of the reaction information has been permitted by the first user.","16","16/629343","2018-07-30","2021-0158228","2021-05-27","12020180","2024-06-25","SONY CORPORATION","Itaru Shimizu | Makoto Koike","2017-199063","JP","2017-10-13","G06Q-0010/02","G06Q-0010/02 | A61B-0005/165 | G06Q-0030/0201 | A61B-0005/6803 | A61B-2503/12 | G06V-0040/174","G06Q-010/02","G06Q-010/02 | A61B-005/00 | A61B-005/16 | G06Q-030/0201 | G06V-040/16","","","","","","4924026004759"
"US","US","P","B2","Method to mitigate allergen symptoms in a personalized and hyperlocal manner","A system and method of determining an allergy impact profile of an individual are disclosed. The system and method may be employed to predict allergy impact environmental conditions may have on allergy symptoms of an individual and to recommend treatment of the individual in response to the predicted allergy impact.","1. A method of using a computerized processing device to determine an impact that an environmental condition may have on allergy symptom of an individual and to determine a therapeutic option for the individual, comprising: tracking said individual'ss behavior data over time, wherein said individual'ss behavior data includes said individual'ss allergy symptom; behavior, environmental conditions; medications; and quality of life;measuring an outcome for each of said individual'ss behavior data at multiple time points; wherein said individual'ss behavior data includes said individual'ss treatment regime (including mediation response (no response) data), adherence to use of one or more of said medications and symptom perception,wherein measuring an outcome of said individual'ss behavior data includes: calculating, for the individual, on the computerized processing device, the change in symptom and allergen outcome relationship over time (SnPn/dt) (or the modeling of nested/clustered data, wherein there are multiple individuals in each cluster) using a multivariate regression or probabilistic approach;generating and including on the computerized processing device an analysis engine for analyzing potential treatment options based on the individual'ss behavior data and data regarding the individual'ss response (or no response) to medication in combination with the calculated change in symptom and allergen outcome relationship over time (SnPn/dt) data by applying at least a portion of the crowdsourced information acquired from the Sn/Pn/dt relationship between user symptoms (Sn) and allergens (P) over time (dt) and the individual'ss data tracked over time;employing the analysis engine to train machine learning models for increasing the individual'ss pollen threshold, wherein such machine learning models are selected from support vector machines, k-nearest neighbors, random forests or mixtures thereof; andcomparing said individual'ss behavior data to said machine learning models, using the computerized processing device, to determine one or more of the individual'ss treatment recommendation(s) for increasing the individual'ss poller threshold.","6","17/031474","2020-09-24","2021-0090700","2021-03-25","12020792","2024-06-25","JOHNSON & JOHNSON CONSUMER INC.","Russell Gould | Russel Walters | Matthew Richtymyer | Thomas Shyr | Christina I. Lee | Jennifer Callaghan | Jessica L. Lienert | Grant David Hou","","","","G16H-0020/10","G16H-0020/10 | G01D-0021/00 | G06N-0020/00 | G06Q-0030/0631 | G16H-0010/60 | G16H-0050/20 | G16H-0050/50 | G16H-0050/70 | A61B-0005/411","G16H-020/00","G16H-020/00 | G01D-021/00 | G06N-020/00 | G06Q-030/0601 | G16H-010/60 | G16H-020/10 | G16H-050/20 | G16H-050/50 | G16H-050/70 | A61B-005/00","","","","","","4924026005366"
"US","US","P","B2","Product consumption recommendations","Disclosed herein are techniques related to product consumption recommendations. In some embodiments, the techniques may involve receiving activity data from an activity monitoring device. The activity monitoring device may comprise an activity sensor for tracking movement of a user during an activity. The techniques may also involve receiving glucose data from a continuous glucose monitoring device. The techniques may further involve determining a product consumption recommendation based on the glucose data and the activity data. The product consumption recommendation may include a recommendation of when the user should consume a carbohydrate-containing product in order to maintain glucose levels with a specified target range during the activity. Additionally, the technique may involve causing display of the product consumption recommendation on a display device.","1. A system, comprising: one or more processors; andone or more non-transitory processor-readable media storing instructions which, when executed by the one or more processors, cause performance of: receiving activity data from an activity monitoring device, the activity monitoring device comprising an activity sensor for tracking movement of a user during an activity;receiving glucose data from a continuous glucose monitoring device;determining a product consumption recommendation based on the glucose data and the activity data, wherein the product consumption recommendation includes a recommendation of when the user should consume a carbohydrate-containing product in order to maintain glucose levels within a specified target range during the activity; andcausing display of the product consumption recommendation on a display device.","20","18/135464","2023-04-17","2023-0253093","2023-08-10","12020802","2024-06-25","MEDTRONIC MINIMED, INC.","Matthew N. Haggerty | Erik R. Hoeg","","","","G16H-0020/60","G16H-0020/60 | A61B-0005/1112 | A61B-0005/1118 | A61B-0005/14532 | A61B-0005/1477 | A61B-0005/681 | A61B-0005/7264 | A61B-0005/742 | G06K-0007/10297 | G06K-0007/10366 | G06K-0007/10722 | G06K-0007/1413 | G06K-0007/1417 | G06N-0020/00 | G16H-0010/60 | G16H-0040/67 | A61B-2562/0219","G16H-020/60","G16H-020/60 | A61B-005/00 | A61B-005/11 | A61B-005/145 | A61B-005/1477 | G06K-007/10 | G06K-007/14 | G06N-020/00 | G16H-010/60 | G16H-040/67","","","","","","4924026005376"
"US","US","P","B2","System and method of gesture detection and device positioning","A system according to at least one embodiment of the present disclosure includes an imaging source; an imaging detector; a depth sensor; and a controller, where the controller receives image information from the depth sensor, determines a gesture in relation to a working volume, and moves the imaging source and the imaging detector relative to the working volume based on the gesture.","1. A system comprising: an imaging source;an imaging detector separate from the imaging source;a depth sensor; anda controller,wherein the controller receives image information from the depth sensor, determines a gesture in relation to a working volume, and moves the imaging source and the imaging detector relative to the working volume based on the gesture, andwherein the imaging source is moved in a complementary motion relative to the imaging detector such that the imaging source is substantially aligned with the imaging detector after the imaging source and the imaging detector have been moved.","20","17/331081","2021-05-26","2022-0378521","2022-12-01","12011237","2024-06-18","MAZOR ROBOTICS LTD.","Avi Turgeman | Yonatan Ushpizin | Eli Zehavi | Ido Zucker","","","","A61B-0034/30","A61B-0034/30 | A61B-0017/86 | A61B-0034/25 | G06F-0003/017 | G06T-0007/50 | A61B-2034/743 | A61B-2034/744 | G06T-2207/10028 | G06T-2207/30012","A61B-034/30","A61B-034/30 | A61B-017/86 | A61B-034/00 | G06F-003/01 | G06T-007/50","","","","","","4924025001248"
"US","US","P","B2","Patient positioning support apparatus with fail-safe connector attachment mechanism","A patient support apparatus for supporting a patient in a prone position during a surgical procedure is provided, including an open fixed frame suspended above a floor and a pair of spaced opposed radially sliding joints cooperating with the frame, each joint including a virtual pivot point and an arc of motion spaced from the virtual pivot point, the joints being movable along the arc providing a pivot ship mechanism for a pair of pelvic pads attached to the joints. A base for supporting and suspending a patient support structure above the floor, for supporting a patient during a surgical procedure, the base including a pair of spaced opposed vertical translation subassemblies reversibly attachable to a patient support structure, a cross-bar, and a rotation subassembly having two degrees of rotational freedom; wherein a location of each vertical translation subassembly is substantially constant during operation of the patient support structure.","1. A surgical system for supporting a patient during surgery, the surgical system comprising: a surgical table including a support portion and a platform portion, the support portion spacing the platform portion from a surface that the surgical table rests upon, the platform portion including a first end, an opposite second end, a first lateral side, an opposite second lateral side, and a mid-longitudinal axis extending through the first end and the second end of the platform portion,a padding system including at least one chest-support portion and at least one pelvic-support portion supported by the platform portion, the at least one chest-support portion being closer to the first end of the platform portion than the at least one pelvic-support portion, and the at least one pelvic-support portion being closer to the second end of the platform portion than the at least one chest-support portion, the at least one pelvic-support portion including a first pelvic-support portion and a second pelvic-support portion each having a first end, an opposite second end, a pad, and a pad-support portion supporting the pad, the first ends of the first pelvic-support portion and the second pelvic-support portion being oriented toward the first end of the platform portion, the pad-support portion of the first pelvic-support portion being attached relative to the platform portion adjacent the first lateral side, the pad-support portion of the second pelvic-support portion being attached relative to the platform portion adjacent the second lateral side,wherein the pad-support portions and the pads of the first pelvic-support portion and the second pelvic-support portion are each moveable relative to the platform portion in a horizontal first direction aligned with the mid-longitudinal axis when the first ends of each of the first pelvic-support portion and the second pelvic-support portion are moved from a first position closer to the second end of the platform portion to a second position closer to the first end of the platform portion.","20","18/351839","2023-07-13","2023-0355455","2023-11-09","12011399","2024-06-18","JACKSON, ROGER P., JACK | WARSAW ORTHOPEDIC, INC","Roger P. Jackson | Lawrence E. Guerra | Trevor A. Waggoner | Steven R. Walton | Michael A. Herron","","","","A61G-0013/04","A61G-0013/04 | A61B-0005/4836 | A61B-0005/704 | A61B-0005/7425 | A61G-0007/001 | A61G-0007/005 | A61G-0007/008 | A61G-0007/012 | A61G-0007/015 | A61G-0007/018 | A61G-0013/0036 | A61G-0013/0054 | A61G-0013/02 | A61G-0013/06 | A61G-0013/08 | A61G-0013/101 | A61G-0013/104 | A61G-0013/1205 | A61G-0013/121 | A61G-0013/122 | A61G-0013/123 | A61G-0013/1235 | G06F-0003/04842 | G16H-0020/40 | G16H-0040/63 | A61B-2017/00022 | A61G-0007/002","A61G-013/04","A61G-013/04 | A61B-005/00 | A61G-007/00 | A61G-007/005 | A61G-007/008 | A61G-007/012 | A61G-007/015 | A61G-007/018 | A61G-013/00 | A61G-013/02 | A61G-013/06 | A61G-013/08 | A61G-013/10 | A61G-013/12 | G06F-003/04842 | G16H-020/40 | G16H-040/63 | A61B-017/00 | A61G-007/002","","","","","","4924025001409"
"US","US","P","B2","Multi-sensor platform for health monitoring","A mechanism is provided in a data processing system to implement a multi-sensor health monitoring platform. The mechanism applies a machine learning model to predict patient needs and patient activity trends based on physiological features and activity features of the patient. The mechanism applies the machine learning model to predict energy requirements for a plurality of medical sensors based on the predicted patient needs and patient activity trends. The mechanism schedules recharging of the plurality of medical sensors based on the predicted energy requirements and identifying one or more sensors to set to an activate state based on the predicted patient needs and patient activity trends. The mechanism collecting sensor data from the one or more sensors and applies the machine learning model to generate a point-of-care recommendation based on the collected sensor data.","1. A method, in a data processing system comprising a processor and a memory, the memory comprising instructions that are executed by the processor to configure the processor to implement a multi-sensor health monitoring platform, the method comprising: applying a machine learning model to predict patient needs and patient activity trends based on physiological features and activity features of the patient;applying the machine learning model to predict energy requirements and energy availability for a plurality of medical sensors based on the predicted patient needs and patient activity trends, wherein the predicted energy requirements indicates an amount of energy needed to power a subset of medical sensors, of the plurality of medical sensors, to monitor at least one predicted patient need or activity at a future time point based on the predicted patient needs and patient activity trends, wherein the subset of medical sensors are medical sensors that monitor physiological features or activity features of the patient specific to the at least one predicted patient need or patient activity;scheduling recharging of the subset of medical sensors, in the plurality of medical sensors, based on the predicted energy requirements and predicted energy availability of the subset of medical sensors, wherein the scheduling schedules recharging of the subset of medical sensors at a time point prior to the future time point such that the recharging causes at least one power source of the subset of medical sensors to have available electrical power to satisfy the predicted energy requirements at the future time point;automatically controlling the plurality of medical sensors based on the predicted patient needs, patient activity trends, and predicted energy requirements, to set the subset of medical sensors to a recharge state in response to a current time being the time point prior to the future time point, to set the subset of medical sensors to an activate state in response to the current time being the future time point, and to set one or more other medical sensors in the subset of medical sensors to an inactive state in response to the current time being the future time point;collecting sensor data from the subset of medical sensors in response to the current time being the future time point; andapplying the machine learning model to generate a point-of-care recommendation based on the collected sensor data.","20","17/130534","2020-12-22","2022-0199235","2022-06-23","12014816","2024-06-18","INTERNATIONAL BUSINESS MACHINES CORPORATION","John Knickerbocker | Bing Dang | Qianwen Chen | Leanna Pancoast","","","","G16H-0040/20","G16H-0040/20 | A61B-0005/7264 | A61B-0005/7275 | A61B-0005/747 | G06N-0020/00 | G06Q-0010/1095 | G06Q-0050/06 | G06Q-0050/26 | G16H-0010/60 | G16H-0040/40 | G16H-0040/67 | G16H-0050/20 | G16H-0050/70 | H04L-0009/32 | A61B-2560/0214","G16H-040/20","G16H-040/20 | A61B-005/00 | G06N-020/00 | G06Q-010/1093 | G06Q-050/06 | G06Q-050/26 | G16H-010/60 | G16H-040/40 | G16H-040/67 | G16H-050/20 | G16H-050/70 | H04L-009/32","","","","","","4924025004791"
"US","US","P","B2","Computer-assisted patient navigation and information systems and methods","A computer-assisted patient navigational communication system for receiving electronic and oral communications from a patient, scanning data to determine the medical needs of the patient, and displaying relevant information to appropriate medical personnel who can immediately advise the patient of the most appropriate source of medical assistance relating to the patient's identified symptoms.","1. A virtual medical system for efficiently navigating a patient through many levels of health care to an appropriate level and healthcare provider, said virtual medical system comprising: a) a call center having navigation personnel and communication facilities providing for communications between said patient and said navigation personnel of the call center, said navigation personnel comprising healthcare providers who either (i) provide assessment, diagnosis, treatment, and/or education to said patient, or (ii) further navigate the patient to the most appropriate healthcare provider within or outside the call center for assessing, diagnosing and/or treating the patient'ss symptoms;b) said call center being arranged and configured for receiving, recording, and displaying patient physiological data from one or more of the following sources: physiological monitors possessed by the patient, remote facilities with patient physiological monitors and/or patient physiological data in the patient'ss electronic health records, and health care personnel associated with the patient; andc) said communications and said patient physiological data enabling said navigation personnel to quickly assess, diagnose, treat, educate, and/or refer said patient to one or more of the following: a laboratory, imaging facility, pharmacy, home health personnel, therapists, ambulance services, hospitals, emergency room, physicians and/or medical specialists for the patient'ss medical condition.","6","18/165287","2023-02-06","2023-0181038","2023-06-15","12004839","2024-06-11","Forge Laboratories, LLC","Randall S. Hickle | Christopher K. Allen | Jason Paul Derouen","","","","A61B-0005/0022","A61B-0005/0022 | A61B-0005/021 | A61B-0005/746 | A61B-0005/7465 | A61B-0005/747 | G06Q-0010/10 | G16H-0010/60 | G16H-0040/20 | G16H-0040/63 | G16H-0040/67 | H04M-0003/5183 | H04M-0003/5232 | A61B-0005/024 | A61B-0005/14532 | A61B-0005/14542 | G06Q-0040/08 | G16H-0050/20","H04M-003/51","H04M-003/51 | A61B-005/00 | A61B-005/021 | G06Q-010/10 | G16H-010/60 | G16H-040/20 | G16H-040/63 | G16H-040/67 | G16H-050/20 | H04M-003/523 | A61B-005/024 | A61B-005/145 | G06Q-040/08","","","","","","4924024001445"
"US","US","P","B1","Methods and systems for detection of errors in medical reports","Systems for preparing a medical report, and related methods, generate and output feedback indicative or potential errors in the medical report. A system for preparing a medical report is configured to receive image data for a medical image of a patient, display the medical image, monitor content of the medical report during preparation of the medical report, process the image data to detect one or more organs of the patient imaged in the medical image, compare the content of the medical report with the one or more organs imaged in the medical image to detect one or more potential errors in the medical report, and output feedback indicative of the one or more potential errors in the medical report.","1. A system for preparing a medical report, the system comprising: one or more displays operable to display a medical image of a patient;one or more input devices;one or more processors; andone or more computer-readable media that store non-transitory instructions executable by the one or more processors to cause the one or more processors to: receive image data for a medical image of a patient, wherein the image data is generated via a medical imaging system;cause the one or more displays to display the medical image of the patient;receive user input for preparation of the medical report via the one or more input devices;display content of the medical report via the one or more displays during preparation of the medical report;identify a subject organ of the patient referred to in the medical report by monitoring content of the medical report during preparation of the medical report;process the image data to detect whether an image of the subject organ of the patient is included in the displayed medical image of the patient; andoutput feedback via the one or more displays that indicates whether an image of the subject organ of the patient is included in the displayed medical image of the patient.","20","17/217675","2021-03-30","","","12004891","2024-06-11","AMAZON TECHNOLOGIES, INC.","Sharon Alpert | Antonio Criminisi","","","","A61B-0006/5211","A61B-0006/5211 | A61B-0005/055 | A61B-0006/032 | A61B-0008/5215 | G06F-0003/0482 | G06F-0003/1423 | G06F-0018/22 | G06T-0007/0012 | G06T-0007/70 | G16H-0015/00 | G16H-0030/20 | G16H-0030/40 | G16H-0050/70 | G06T-2200/24 | G06T-2207/30004 | G06V-2201/031 | G10L-0015/18 | G10L-0015/22 | G10L-2015/223 | G16H-0040/20 | G16H-0050/20 | G16H-0070/20 | G16H-0070/60","A61B-006/00","A61B-006/00 | A61B-005/055 | A61B-006/03 | A61B-008/08 | G06F-003/0482 | G06F-003/14 | G06F-018/22 | G06T-007/00 | G06T-007/70 | G16H-015/00 | G16H-030/20 | G16H-030/40 | G16H-050/70 | G10L-015/18 | G10L-015/22 | G16H-040/20 | G16H-050/20 | G16H-070/20 | G16H-070/60","","","","","","4924024001497"
"US","US","P","B2","Automated clinical documentation system and method","A method, computer program product, and computing system for compartmentalizing a virtual assistant is executed on a computing device and includes obtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module. One or more additional functionalities are added to the compartmentalized virtual assistant on an as-needed basis.","1. A computer-implemented method for compartmentalizing a virtual assistant, executed on a computing device, comprising: obtaining encounter information via a compartmentalized virtual assistant during a patient encounter, wherein the compartmentalized virtual assistant includes a core functionality module that verbally interacts with an encounter participant of the patient encounter, wherein the core functionality module verbally interacts with at least one of a patient and a medical professional as the encounter participant during a portion of the patient encounter;determining, by the compartmentalized virtual assistant, that a medical treatment topic was mentioned by at least one of the patient and the medical professional while verbally interacting with the virtual assistant during the portion of the patient encounter;downloading and activating one or more additional functionalities associated with the medical treatment topic to the compartmentalized virtual assistant during the patient encounter based upon, at least in part, the compartmentalized virtual assistant determining that the medical treatment topic was mentioned during the portion of the patient encounter, wherein at least one of the one or more additional functionalities added and activated to the compartmentalized virtual assistant during the patient encounter includes an adverse interaction functionality configured to identify any potential adverse interactions associated with the medical treatment topic, wherein the one or more additional functionalities associated with the medical treatment topic has not been downloaded and activated prior to the compartmentalized virtual assistant determining that one of the medical treatment topic was mentioned, and wherein the one or more additional functionalities associated with the medical treatment topic is downloaded and activated after the compartmentalized virtual assistant determines that the medical treatment topic was mentioned; andproviding feedback to the medical professional from the compartmentalized virtual assistant based upon the one or more additional functionalities associated with the medical treatment topic and the medical treatment topic being mentioned.","21","17/846355","2022-06-22","2022-0319653","2022-10-06","12008310","2024-06-11","MICROSOFT TECHNOLOGY LICENSING, LLC","Donald E. Owen | Mehmet Mert ?z | Garret N. Erskine","","","","G06F-0040/174","G06F-0040/174 | A61B-0005/7405 | G06F-0003/16 | G06F-0016/637 | G06F-0016/685 | G06F-0016/904 | G06F-0018/25 | G06F-0021/6245 | G06F-0040/30 | G06F-0040/40 | G06K-0019/07762 | G06N-0003/006 | G06T-0007/00 | G06V-0020/10 | G06V-0020/52 | G06V-0040/103 | G06V-0040/16 | G06V-0040/172 | G06V-0040/23 | G10L-0015/08 | G10L-0017/00 | G10L-0021/0232 | G11B-0027/10 | G16B-0050/00 | G16H-0010/20 | G16H-0010/60 | G16H-0015/00 | G16H-0030/00 | G16H-0030/20 | G16H-0030/40 | G16H-0040/20 | G16H-0040/60 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | G16H-0080/00 | G16Y-0020/00 | H04L-0051/02 | H04L-0051/222 | H04N-0007/183 | H04R-0001/326 | H04R-0003/005 | H04R-0003/12 | G06T-2207/10024 | G06T-2207/10044 | G06T-2207/10048 | G06T-2207/10116 | G06T-2207/10132 | G10L-0015/1815 | G10L-0015/22 | G10L-0015/26 | G10L-2021/02082 | H04N-0007/181 | H04R-0001/406 | H04R-0003/02 | H04R-2420/07 | H04S-2400/15","G16H-040/20","G16H-040/20 | A61B-005/00 | G06F-003/16 | G06F-016/635 | G06F-016/683 | G06F-016/904 | G06F-018/25 | G06F-021/62 | G06F-040/174 | G06F-040/30 | G06F-040/40 | G06K-019/077 | G06N-003/006 | G06T-007/00 | G06V-020/10 | G06V-020/52 | G06V-040/10 | G06V-040/16 | G06V-040/20 | G10L-015/08 | G10L-017/00 | G10L-021/0232 | G11B-027/10 | G16B-050/00 | G16H-010/20 | G16H-010/60 | G16H-015/00 | G16H-030/00 | G16H-030/20 | G16H-030/40 | G16H-040/60 | G16H-040/63 | G16H-050/20 | G16H-050/30 | G16H-080/00 | G16Y-020/00 | H04L-051/02 | H04L-051/222 | H04N-007/18 | H04R-001/32 | H04R-003/00 | H04R-003/12 | G10L-015/18 | G10L-015/22 | G10L-015/26 | G10L-021/0208 | H04R-001/40 | H04R-003/02","","","","","","4924024004908"
"US","US","P","B2","Personalized health system, method and device having a lifestyle function","A personal health system, method and device that maintains a health knowledge base, inputs user characteristics, generates health scores based on the user characteristics and provides recommendations based on the user characteristics, health scores and knowledge base, wherein the recommendations are indicated by the knowledge base to be likely to improve the user's health.","1. A personal health system for promoting healthy choices to one or more users, the personal health system comprising: a non-transitory computer readable memory storing a knowledge base including a plurality of tuples, wherein each tuple includes a condition, one or more factors that affect the condition, and a relationship that describes how the one or more factors affect the condition, wherein one or more of the factors are occupations; andat least one computing device including a processor and a non-transitory computer-readable medium coupled with the processor and storing a personal health platform having a user interface, wherein when executed by the processor the personal health platform is operable to: generate an input graphical user interface on the device having one or more input features configured to input personal characteristics of the user, the personal characteristics including physical activity characteristics, social activity characteristics, sleep characteristics, nutrition characteristics and stress characteristics, wherein the stress characteristics comprise an occupation of the user, a method of transport to the occupation, and a work commute distance of the user, and further wherein the factors and the conditions of the tuples are one or more of the personal characteristics, the non-transitory computer-readable medium storing a user profile and a lifestyle score digital image, the user profile including a physical activity score that is based on the physical activity characteristics, a social activity score that is based on the social activity characteristics, a sleep score that is based on the sleep characteristics, a nutrition score that is based on the nutrition characteristics and a stress score that is based on the stress characteristics and the lifestyle score digital image indicating a lifestyle score that is based on a combination of the physical activity score, the social activity score, the sleep score, the stress score and the nutrition score; andprovide a navigation graphical user interface that: enables the user to select one or more of the conditions as desired conditions;generates a navigation digital image that concurrently displays one or more relationship strength features adjacent to a network web, wherein the network web graphically illustrates nodes representing the desired conditions and visible connections between each of the nodes and one or more related factors of the factors that have an identified relationship with the desired conditions according to the tuples that include the desired conditions;enables the user to selectively input changes to one or more relationship strength threshold values associated with the desired conditions using the relationship strength features; anddynamically alters the network web as concurrently displayed based on the input changes to the relationship strength threshold values, wherein the altering of the network web is such that the one or more related factors included in the network web is modified to only be a set of the related factors whose identified relationship to the desired conditions, according to the tuples that include the desired conditions, falls within one or more relationship strength ranges defined by the relationship strength threshold values for the desired conditions.","30","17/705160","2022-03-25","2022-0310264","2022-09-29","12009075","2024-06-11","VYDIANT, INC.","James Kaput | Corrado Priami | Melissa Morine | Terry Carlone | John Green","","","","G16H-0010/60","G16H-0010/60 | A61B-0005/742 | A61B-0005/7465 | A61B-0005/7475 | G06F-0003/04847 | G06F-0009/453 | G06N-0005/04 | G16H-0010/20 | G16H-0015/00 | G16H-0020/17 | G16H-0020/30 | G16H-0020/60 | G16H-0020/70 | G16H-0040/67 | G16H-0050/30 | G16H-0070/40 | G16H-0070/60","G16H-050/30","G16H-050/30 | A61B-005/00 | G06F-003/04847 | G06F-009/451 | G06N-005/04 | G16H-010/20 | G16H-010/60 | G16H-015/00 | G16H-020/17 | G16H-020/30 | G16H-020/60 | G16H-020/70 | G16H-040/67 | G16H-070/40 | G16H-070/60","","","","","","4924024005669"
"US","US","P","B2","Remote physical therapy and assessment of patients","Systems and methods for physical therapy and training delivery are presented herein. These technologies may comprise notifying a patient of a scheduled prescribed activity via an on-location at least one client device or console; identifying the patient with one or more sensors connected to or part of the at least one client device or console; confirming, via the at least one client device or console, the patient's acknowledgment of the notification; demonstrating, via a graphical interactive avatar displayed on the at least one client device or console, the prescribed activity to be carried out by the patient; confirming, via the at least one client device or console, that the patient is undertaking or will be undertaking the prescribed activity; capturing, via the one or more sensors, frames of the patient undertaking the prescribed activity; and processing frames of the patient undertaking the prescribed activity.","1. A method for remote physical therapy and assessment, the method executed by an at least one processor, comprising: notifying a patient of a scheduled prescribed activity via an on-location at least one client device or console;identifying the patient with one or more sensors connected to or part of the at least one client device or console;confirming, via the at least one client device or console, the patient'ss acknowledgment of the notification;demonstrating, via a graphical interactive avatar displayed on the at least one client device or console, the prescribed activity to be carried out by the patient;confirming, via the at least one client device or console, that the patient is undertaking or will be undertaking the prescribed activity;capturing, via the one or more sensors, frames of the patient undertaking the prescribed activity;processing the frames of the patient undertaking the prescribed activity;the capturing of the frames being undertaken by a left sensor and a right sensor; andthe processing of the frames comprising:transmitting the captured frames to an internal depth image processor, wherein the captured frames consist of pixel data of one or more captured scenes;calculating, by the internal depth image processor, a depth of each pixel, to produce depth pixel values;processing, by the internal depth image processor, of the depth pixel values to create a depth frame;combining, by the internal depth image processor, the depth frames into a depth video stream;applying a point cloud over the depth video stream; andestimating position and orientation of the patient'ss body joint centers.","17","17/526839","2021-11-15","2022-0157427","2022-05-19","12009083","2024-06-11","ELECTRONIC CAREGIVER, INC.","David W. Keeley | Anthony Dohrmann | Robert Wood","","","","G16H-0020/30","G16H-0020/30 | A61B-0005/744 | G06F-0003/048 | G06N-0020/00 | G16H-0010/60 | G16H-0030/00 | G16H-0040/67","G16H-020/30","G16H-020/30 | A61B-005/00 | G06F-003/048 | G06N-020/00 | G16H-010/60 | G16H-030/00 | G16H-040/67","","","","","","4924024005677"
"US","US","P","B2","Systems and methods for approximating musculoskeletal dynamics","A system and method for controlling a device, such as a virtual reality (VR) and/or a prosthetic limb are provided. A biomimetic controller of the system comprises a signal processor and a musculoskeletal model. The signal processor processes M biological signals received from a residual limb to transform the M biological signals into N activation signals, where M and N are integers and M is less than N. The musculoskeletal model transforms the N activation signals into intended motion signals. A prosthesis controller transforms the intended motion signals into three or more control signals that are outputted from an output port of the prosthesis controller. A controlled device receives the control signals and performs one or more tasks in accordance with the control signals.","1. A system comprising: a biomimetic controller comprising a processor and a memory;machine-readable instructions stored in the memory that, when executed by the processor, cause the biomimetic controller to at least: receive a biological signal from a residual limb?expand the biological signal into one or more activation signals based at least in part on a scaling coefficient and a number of degrees of freedom (DOFs) associated with the biological signal;perform a modeling algorithm that transforms the one or more activation signals into one or more intended motion signals; and perform a control algorithm that transforms the intended motion signals into control signals; andsend the control signals to a controlled device.","20","17/787701","2020-12-19","2023-0021860","2023-01-26","11998460","2024-06-04","UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION | WEST VIRGINIA UNIVERSITY","Anton Sobinov | Sergiy Yakovenko | Valeriya Gritsenko | Matthew Boots | Robert Gaunt | Jennifer Collinger | Lee Fisher","","","","A61F-0002/72","A61F-0002/72 | A61B-0005/1107 | A61B-0005/4519 | A61F-0002/582 | A61F-0002/70 | B25J-0009/1075 | G06F-0003/011 | G06F-0003/015 | G16H-0030/20 | A61B-0005/389 | A61F-2002/0894 | A61F-2002/5066 | A61F-2002/6827 | A61F-2002/6872 | A61F-2002/704 | A61H-2230/605 | A63B-2230/605","A61F-002/72","A61F-002/72 | A61B-005/00 | A61B-005/11 | A61B-005/389 | A61F-002/58 | A61F-002/70 | B25J-009/10 | G06F-003/01 | G16H-030/20 | A61F-002/08 | A61F-002/50 | A61F-002/68","","","","","","4924023001274"
"US","US","P","B2","Medical monitoring system with monitoring camera","There is provided a recognition system adaptable to a portable device or a wearable device. The recognition system senses a body heat using a thermal sensor, and performs functions such as the living body recognition, image denoising and body temperature prompting according to detected results.","1. A medical monitoring system, comprising: a wearable accessory, comprising: a thermal sensor, configured to measure a body temperature of a patient; anda transmitter, configured to send a temperature message of the body temperature and a label message of the wearable accessory;a central computer, configured to receive the temperature message from the wearable accessory, and comprising: a display, configured to show a temporal distribution of the body temperature,a camera, configured to be turned on by the central computer to monitor the patient when the body temperature exceeds a predetermined range, wherein the camera is turned off when the body temperature does not exceed the predetermined range to protect patient privacy,wherein the central computer is further configured to automatically turn on a dosing equipment to dose when the body temperature exceeds the predetermined range, andthe display is further configured to show a relationship between dosage of the dosing equipment and a variation of the body temperature.","10","18/132421","2023-04-10","2023-0245496","2023-08-03","12002288","2024-06-04","PixArt Imaging Inc.","Nien-Tse Chen | Yi-Hsien Ko | Yen-Min Chang","","","","G06V-0040/161","G06V-0040/161 | A61B-0005/0008 | A61B-0005/01 | A61B-0005/742 | A61B-0005/746 | G01J-0005/0025 | G01J-0005/026 | G06F-0018/22 | G06F-0018/251 | G06F-0021/32 | G06V-0010/143 | G06V-0010/761 | G06V-0010/803 | G06V-0040/113 | G06V-0040/168 | G06V-0040/172 | G06V-0040/20 | G06V-0040/28 | G06V-0040/45 | G16H-0040/63 | G01J-2005/0077 | G06V-0040/117 | G06V-0040/16","G06K-009/00","G06K-009/00 | A61B-005/00 | A61B-005/01 | G01J-005/00 | G01J-005/02 | G06F-018/22 | G06F-018/25 | G06F-021/32 | G06V-010/143 | G06V-010/74 | G06V-010/80 | G06V-040/10 | G06V-040/16 | G06V-040/20 | G06V-040/40 | G16H-040/63","","","","","","4924023005065"
"US","US","P","B2","Home dialysis machine network including walk-up service centers","A home dialysis machine network is provided that includes a home dialysis machine and one or more walk-up service centers. The walk-up service center can be equipped with a diagnostic tool, parts, or supplies for the home dialysis machine. Methods of maintaining a home dialysis machine are also provided as are service centers equipped with a transportation service or office, a diagnostics department, an education and training department, a consultation department, an exercise and spa department, an entertainment department, a sales department, a repair shop, a new technologies department, a pharmacy, and a doctor's office.","1. A home dialysis machine network comprising a home dialysis machine, one or more walk-up service centers, and a robot at one or more of the one or more walk-up service centers, wherein: each walk-up service center is equipped with (i) a diagnostic tool configured to diagnose the home dialysis machine, a component of the home dialysis machine, a peripheral device to be used with the home dialysis machine, or a combination thereof, (ii) parts for servicing, repairing, or both, the home dialysis machine, a component of the home dialysis machine, a peripheral device to be used with the home dialysis machine, or a combination thereof, and (iii) supplies to be consumed before, during, or subsequent to operation of the home dialysis machine, andthe robot is configured with an artificial intelligence (AI) system and is configured to assist a home dialysis patient, a home dialysis patient caregiver, a service provider, or a combination thereof, with a method performed at one or more of the one or more walk-up service centers.","23","17/232660","2021-04-16","2021-0358608","2021-11-18","11996187","2024-05-28","FRESENIUS MEDICAL CARE AG | FRESENIUS MEDICAL CARE HOLDINGS, INC.","Stacy L. Blasberg | Zdenek Cerman","","","","G16H-0040/40","G16H-0040/40 | A61M-0001/1654 | G05B-0023/0267 | G06Q-0010/087 | G06Q-0010/20 | G06Q-0030/012 | G06Q-0040/08 | G09B-0019/0092 | G16H-0010/60 | G16H-0020/10 | G16H-0020/40 | G16H-0040/20 | G16H-0040/67 | G16H-0050/20 | G16H-0080/00 | H04L-0067/12 | G16H-0070/20","G16H-040/40","G16H-040/40 | A61M-001/16 | G05B-023/02 | G06Q-010/087 | G06Q-010/20 | G06Q-030/012 | G06Q-040/08 | G09B-019/00 | G16H-010/60 | G16H-020/10 | G16H-020/40 | G16H-040/20 | G16H-040/67 | G16H-050/20 | G16H-080/00 | H04L-067/12 | G16H-070/20","","","","","","4924022005280"
"US","US","P","B2","Treatment information display device and method for displaying treatment history on image of teeth in accumulated manner","Disclosed are treatment information display device and method for displaying treatment history on an image of teeth in an accumulated manner. The treatment information display method enables a user to recognize at once a past treatment history, a current treatment status, and a future treatment plan by displaying, on the image of teeth, treatment information for the past, present, and future. In addition, the treatment information display method can provide the user with pieces of treatment information displayed on different images of teeth, by displaying, in an accumulated manner, the pieces of treatment information displayed on a plurality of teeth images.","1. A treatment information display method, the method comprising: receiving a plurality of patient images of teeth for a plurality of patients at different times;displaying at least one of the plurality of images from a first patient of the plurality of patients;displaying area dividing lines on the displayed image of the teeth, each of the area dividing lines for dividing an area of each of the teeth included in the image of the teeth;receiving, through an interface, a user selection of one of a plurality of areas divided according to the area dividing lines;in response to the user selection, displaying an additional image of a tooth corresponding to the area;identifying a received direction selected by a user on the additional image of the tooth through the interface; anddisplaying at least one type of treatment information from different types of treatment information based on the identified direction selected by the user on the additional image of the tooth;wherein the different types of treatment information of the tooth comprises treatment information of the tooth collected from each of the patient'ss images of the teeth generated at different times in an accumulated manner,wherein the displaying the at least one type of treatment information of the tooth comprises:displaying summary information if the number of pieces of the treatment information of the tooth in the accumulated manner is equal to or greater than a threshold value wherein the summary information is reduced in size compared to detailed information, and displaying detailed information in the accumulated manner if the number of pieces of the treatment information of the tooth is less than the threshold value.","16","17/618726","2020-06-10","2022-0240868","2022-08-04","11986325","2024-05-21","OSSTEMIMPLANT CO., LTD.","Kyoo Ok Choi | Soo Gil Kim","10-2019-0069915 | 10-2020-0069786","KR | KR","2019-06-13 | 2020-06-09","A61B-0005/743","A61B-0005/743 | G06F-0003/0482 | G06F-0003/04855 | G16H-0030/40","G06F-017/00","G06F-017/00 | A61B-005/00 | G06F-003/0482 | G06F-003/04855 | G16H-030/40","","","","","","4924021001493"
"US","US","P","B2","Vehicle usage fee determination system and vehicle usage fee determination method","A vehicle usage fee determination system includes a reception unit that receives a usage application by a user who has an intention to drive a vehicle, a wakefulness level estimation unit that estimates a wakefulness level of the user, and a fee determination unit that sets a usage fee of the vehicle when a target wakefulness level that is the wakefulness level at a time a first predetermined time before a scheduled driving start time of the vehicle by the user is a predetermined value to be equal to or less than a usage fee of the vehicle when the target wakefulness level is lower than the predetermined value.","1. A vehicle usage fee determination system comprising: a reception unit that receives a usage application by a user who has an intention to drive a vehicle;a wakefulness level estimation unit that estimates a wakefulness level of the user based on data received from a sensor configured to capture image data of the user;a fee determination unit that sets a usage fee of the vehicle when a target wakefulness level that is the wakefulness level at a time a first predetermined time before a scheduled driving start time of the vehicle by the user is a predetermined value to be equal to or lower than a usage fee of the vehicle when the target wakefulness level is lower than the predetermined value; anda warning generation unit configured to generate a warning in response to the wakefulness level being lower than the predetermined value, and in response to said warning, activate a speaker control unit configured to audibly relay said warning to the user or a display to output the warning to the user.","20","17/685319","2022-03-02","2022-0366466","2022-11-17","11989761","2024-05-21","TOYOTA JIDOSHA KABUSHIKI KAISHA","Reiko Yamamoto | Gen Fukuyama | Masao Tajima | Mikio Inoue","2021-081154","JP","2021-05-12","G06Q-0030/0283","G06Q-0030/0283 | A61B-0005/18 | A61B-0005/4809","G06Q-010/02","G06Q-010/02 | A61B-005/00 | A61B-005/18 | G06Q-030/0283","","","","","","4924021004909"
"US","US","P","B2","Adjusting diabetes alerts and therapy based on gesture-based detection of vehicle operation","One or more processors may be configured to detect whether a patient with diabetes is operating a vehicle based on one or more detected gestures. Based on the detection of operating the vehicle, the one or more processors may cause a patient device to output alerts according to a driving alert protocol. In another example, based on the detection of operating the vehicle, one or more processors may cause an insulin pump to operate according to a driving therapy protocol.","1. A system for outputting patient alerts related to diabetes therapy, the system comprising: a patient device including a user interface configured to output one or more alerts, the one or more alerts including alerts related to the diabetes therapy; andone or more processors configured to: detect a gesture of a patient using a wearable device communicatively coupled, via the patient device, to a medical device configured to provide the diabetes therapy;determine, based on a connection to a short-range communication system of a vehicle and by the patient device, that the patient is in the vehicle and determine, based at least in part on the gesture, that the patient is operating the vehicle;instruct the patient device to output the one or more alerts according to a driving alert protocol based on the determination that the patient is operating the vehicle, wherein the driving alert protocol reduces an output frequency of a portion of the one or more alerts relative to a default alert protocol; anddetermine a driving therapy protocol based on the determination that the patient is operating the vehicle, wherein the driving therapy protocol causes the diabetes therapy to be managed differently when the patient is operating the vehicle relative to when the patient is a passenger in the vehicle.","20","17/004981","2020-08-27","2021-0065894","2021-03-04","11990236","2024-05-21","MEDTRONIC MINIMED, INC.","Patrick E. Weydt | Pratik J. Agrawal | Louis J. Lintereur | Lavie Golenberg | David Dunleavy","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/14532 | A61B-0005/7405 | A61B-0005/746 | A61K-0038/28 | A61M-0005/14244 | A61M-0005/14248 | A61M-0005/1723 | G06F-0003/017 | G06N-0003/08 | G16H-0020/17 | G16H-0020/60 | G16H-0040/20 | G16H-0070/40 | H04W-0004/027 | A61M-2205/3584 | A61M-2205/502 | A61M-2205/505 | A61M-2205/52 | A61M-2205/581 | A61M-2205/582 | A61M-2205/583 | A61M-2205/8206 | A61M-2230/201 | A61M-2230/63 | H04M-0001/72454 | H04M-0001/72463","G16H-040/67","G16H-040/67 | A61B-005/00 | A61B-005/145 | A61K-038/28 | A61M-005/142 | A61M-005/172 | G06F-003/01 | G06N-003/08 | G16H-020/17 | G16H-020/60 | G16H-040/20 | G16H-070/40 | H04M-001/72454 | H04M-001/72463 | H04W-004/02","","","","","","4924021005381"
"US","US","P","B2","Method, device and system for correlating at least one additional 2D-image to a 3D-representation of at least a part of tooth","The present disclosure provides a computer-implemented method for correlating at least one infrared 2D-image to a 3D-representation of at least a part of a tooth displayed in a graphical user-interface, of a hand-held scanning device, on a screen, comprising the steps of: obtaining a first set of 2D-images of the at least part of the tooth; forming a 3D-representation of the at least a part of the tooth from the first set of 2D-images; displaying, in the graphical user-interface, the 3D-representation; obtaining a second set of 2D-images, wherein the second set of 2D images are infrared 2D-images acquired within the at least part of the tooth; displaying, in the user-interface, at least one of the 2D-images from the second set of 2D-images; displaying, in the user-interface, a manipulator configured to change between 2D-images in the second set of 2D-images.","1. A computer-implemented method for correlating at least one infrared 2D-image to a 3D-representation of at least a part of a tooth displayed in a graphical user-interface, of a hand-held scanning device, on a screen, comprising the steps of: obtaining a first set of 2D-images of the at least part of the tooth;forming a 3D-representation of the at least a part of the tooth from the first set of 2D-images;displaying, in the graphical user-interface, the 3D-representation;obtaining a second set of 2D-images, wherein the second set of 2D images are infrared 2D-images acquired within the at least part of the tooth;displaying, in the user-interface, at least one of the 2D-images from the second set of 2D-images;displaying, in the user-interface, a manipulator configured to change between 2D-images in the second set of 2D-images;based on input as associated to the manipulator, changing between the 2D-images in the second set of 2D-images until a desired 2D-image is displayed; andcorrelating the desired 2D-image to the 3D-representation.","15","17/253710","2019-06-14","2021-0264600","2021-08-26","11983873","2024-05-14","3SHAPE A/S","Anders Robert Jellinggaard | Christoph Vannahme | Mike Van Der Poel | Karl-Josef Hollenbeck | Anders Gaarde | Mads Brøkner Christiansen","2018-179027","EP","2018-06-21","G06T-0007/0014","G06T-0007/0014 | A61B-0005/0088 | A61B-0034/74 | G06F-0003/048 | A61B-2560/0487 | G06T-2207/10048 | G06T-2207/20024 | G06T-2207/30036","G06T-007/00","G06T-007/00 | A61B-005/00 | A61B-034/00 | G06F-003/048","","","","","","4924020004711"
"US","US","P","B2","User scanning and one-way augmented reality viewing system","An augmented reality customer interaction system includes a transparent panel having a first side and a second side that is opposite to the first side, and a camera device configured to capture visual data from an area adjacent to the second side of the transparent panel. The visual data includes identifying features of a customer located in the area with respect to the second side of the transparent panel. The system further includes a projection system configured to project information on the first side of the transparent panel. The information projected on the first side of the transparent panel may include customer interaction data retrieved from a data store based on the identifying features of the customer.","1. An augmented reality viewing system, comprising: at least one sensor configured detect entry of a customer into a location;a set of sensors configured to collect multiple characteristics of the customer;a tracking and monitoring system configured to: track a position of the customer in a first area at the location;track movements and positions of multiple available service representatives in a second area different from the first area;determine a preferred customer service representative capable of fulfilling a specific need of the customer from the multiple available service representatives to interact with the customer based on the position of the customer and the position of the preferred service representative; andassign the customer to the preferred service representative capable of fulfilling the specific need of the customer;a processor configured to: receive the collected multiple characteristics;compare the collected multiple characteristics with the characteristics stored in a customer database; andsuggest customer interaction strategies for interacting with the customer;a customer identification system configured to identify the customer to a threshold level of confidence based on the comparison of the collected multiple characteristics with the characteristics stored in the customer database, wherein the processor is further configured to suggest the customer interaction strategies based on an identification of the customer; anda projection system configured to project the suggested customer interaction strategies on a first surface of a transparent panel such that the suggested customer interaction strategies are visible to the service representative located in an area adjacent to the first surface of the transparent panel and are not visible to the customer located in an area adjacent to a second surface of the transparent panel opposite the first surface.","20","17/671699","2022-02-15","2022-0172468","2022-06-02","11978253","2024-05-07","TRUIST BANK","Michael Anthony Dascola | Jacob Atticus Grady | Kaitlyn Stahl","","","","G06V-0020/20","G06V-0020/20 | A61B-0005/112 | G06F-0003/013 | G06F-0021/32 | G06Q-0030/0269 | G06T-0007/207 | G06T-0007/74 | G06V-0040/171 | H04L-0063/105 | H04L-2463/082","G06V-020/20","G06V-020/20 | A61B-005/11 | G06F-003/01 | G06F-021/32 | G06Q-030/0251 | G06T-007/207 | G06T-007/73 | G06V-040/16 | H04L-009/40","","","","","","4924019004575"
"US","US","P","B2","Concept for authenticating a user of a mobile device","The present disclosure generally relates to authenticating a user of a mobile device based on motion data of said mobile device. Embodiments provide a method, apparatus and computer program for authenticating a user, a mobile device comprising such an apparatus and a system. The method comprises detecting an outfit of the user. The method comprises analyzing a gait of the user using a machine-learning model using motion data of a mobile device as input to the machine-learning model. The analysis is based on the identified outfit of the user. The method comprises authenticating the user based on the analysis of the gait of the user.","1. A method for authenticating a user, the method comprising: identifying an outfit of the user;analyzing a gait of the user using a machine-learning model using motion data of a mobile device as input to the machine-learning model, the analysis being based on the identified outfit of the user; andauthenticating the user based on the analysis of the gait of the user.","20","17/437045","2020-02-26","2022-0179933","2022-06-09","11971970","2024-04-30","SONY GROUP CORPORATION","Hugo Embrechts | Gonzalo Bailador Del Pozo | Dimitri Torfs","2019-163173","EP","2019-03-15","G06F-0021/32","G06F-0021/32 | A61B-0005/112 | A61B-0005/117 | G06N-0020/10 | G06V-0010/82 | G06V-0040/25","G06F-021/32","G06F-021/32 | A61B-005/11 | A61B-005/117 | G06N-020/10 | G06V-010/82 | G06V-040/20","","","","","","4924018004088"
"US","US","P","B2","Emotionally driven software interaction experience","A method for ascertaining an emotional goal includes receiving, via an emotionally responsive computerized system having a user interface communicatively coupled to a networked user device including a processor device, user-input concerning a purpose of a user's interaction with a software interface. It can include registering input indicating a target person to whom the purpose of the user's interaction pertains and prompting the user to provide a root motivator comprising a root emotion or a root reason for the interaction. Some variations include generating a user-perceptible output and a set of user interface elements dependent on the root motivator along with obtaining the user's specific emotional goal with respect to the target person on the basis of user-inputs in response to a presentation of a sequence of user interface elements, to provide the user, via the software interface, a recommendation regarding a fulfillment of the specific emotional goal.","1. A computer implemented method for ascertaining a user'ss emotional goal, comprising: receiving, via an emotionally responsive computerized system having a user interface communicatively coupled to a networked user device having a processor device, a first user-input indicating a purpose of a user'ss interaction with a software interface;registering a second user-input indicative of a target person to whom the purpose of the user'ss interaction pertains after receiving the first user-input;prompting a user to provide at least one root motivator for the interaction after registering the second user-input;generating a visual, audio, or audio-visual output and a set of user interface elements dependent on the at least one root motivator;presenting a sequence of user interface elements that are associated with the root motivator and with the second user-input, after generating the set of user interface elements dependent on the at least one root motivator, wherein a sentiment is indicated or implied by at least one of the fist user-input and the second user input;prompting the user, via a first user element of the sequence, to indicate a main reason specifying an emotional intent that is conceptually narrower relative to the root motivator, wherein the emotional intent is defined relative to how the target person is desired to feel;prompting the user, via a second user element of the sequence, to provide a sub reason in support of the main reason;presenting at least one affirmation responsive to at least one user-input, wherein the at least one affirmation is reflective of the sentiment that is indicated by the at least one user-input;repeating an iterative process of affirming a previous user-input and prompting the user to provide more details specifying the emotional intent until a desired level of precision in determining the user'ss emotional goal is reached, wherein the desired level of precision is determined by a chosen number of times to repeat the iterative process; andresponsive to the desired level of precision being reached, obtaining the user'ss emotional goal with respect to t he target person, based on the second user-input, the at least one root motivator, the main reason, and the sub reason, to provide the user, via the software interface, a recommendation regarding a fulfillment of the user'ss emotional goal.","20","16/874987","2020-05-15","2020-0364068","2020-11-19","11972277","2024-04-30","LOVINGLY, LLC","Joseph Vega | Kenny Garland | Daniela Virginia Marquez | Marque Nolan Staneluis | Dennis Park-Rodriguez | Ryan Wesley A. Lowe | Lakshmi Pillai | James Craig Rosenthal | Matthew Zangen | Kaitlin Heather Schupp | Danielle Sarah Gorton","","","","G06F-0009/453","G06F-0009/453 | G06F-0003/0482 | G06F-0009/542 | G06F-0016/2457 | G06F-0016/436 | G06F-0016/9535 | G06F-0040/35 | A61B-0005/165","G06F-009/451","G06F-009/451 | G06F-003/0482 | G06F-009/54 | G06F-016/2457 | G06F-016/435 | G06F-016/9535 | G06F-040/35 | A61B-005/16","","","","","","4924018004389"
"US","US","P","B2","Systems and methods for brain wave data acquisition and visualization","Systems and methods for providing a computer-generated visualization of EEG data are disclosed. Raw EEG data generated from a multi-channel EEG headset (or other device) may be received. The EEG data may be run through a fast Fourier transform (FFT) to separate out various frequency components in each channel, isolating the brain wave components for each channel. A visual display may be generated based on the isolated components comprising a first display portion and a second display portion. The first display portion may comprise a geometrical mesh with predefined parameters representing the portions of a crystal. The second display portion may comprise a time-varying color visualization based on the variance of the brain waves. A composite computer display in which the first display portion is overlaid over the second display portion may be generated and provided via a display device.","1. A system for providing a computer-generated visualization of electroencephalography (EEG) data, the system comprising: a hardware processor; anda non-transitory machine-readable storage medium encoded with instructions executable by the hardware processor to perform operations comprising:receiving, by a computer system, raw EEG data generated from a multi-channel EEG headset;isolating, by the computer system, brain wave components in at least one EEG channel of the multi-channel EEG headset;generating, by the computer system, a visual display based on the isolated brain wave components, the visual display comprising a first display portion and a second display portion, the first display portion comprising a geometrical mesh based on the isolated brain wave components, the second display portion comprising a time-varying color visualization based on time-varying signals of the isolated brain wave components; andgenerating, by the computer system, a composite computer display that combines the second display portion over the first display portion; andwherein the geometrical mesh has a three-dimensional crystal shape having multiple facets and each facet represents a respective one of the isolated brain wave components.","18","17/411676","2021-08-25","2022-0061735","2022-03-03","11963783","2024-04-23","Dhiraj Jeyanandarajan","Dhiraj Jeyanandarajan","","","","A61B-0005/374","A61B-0005/374 | A61B-0005/384 | G06F-0003/015 | G06F-0017/142 | G06T-0017/20","A61B-005/00","A61B-005/00 | A61B-005/374 | A61B-005/384 | G06F-003/01 | G06F-017/14 | G06T-017/20","","","","","","4924017001315"
"US","US","P","B2","Apparatus and methods for monitoring objects in a surgical field","A system and method for performing a package status check are provided, including a scanner for use in a surgical field and having an antenna for emitting a radio frequency detection field, the scanner including control circuitry configured to determine a predetermined quantity of sponges corresponding to a complete pack of sponges; count a number of sponges associated with the pack with the scanner prior to use of the sponges in a surgical field; and issue an alert when a number of counted sponges does not match the predetermined quantity corresponding to the complete pack. The system and method may include the control circuitry configured to determine whether a sponge has been re-scanned or is unknown by comparing detected information with previously detected information prior.","1. A surgical system for accounting of surgical sponges, the surgical system comprising: a handheld scanner including an antenna capable of emitting a radio frequency detection field, wherein the handheld scanner is operable in a first operating mode to scan in sponges to a surgical procedure and in a second operating mode to scan out sponges from the surgical procedure;a first package including packaging, a first surgical sponge, and a second surgical sponge, with the first surgical sponge and the second surgical sponge being in the packaging, the first surgical sponge including a first RF tag and the second surgical sponge including a second RF tag, the first RF tag including a first memory segment, a second memory segment, and a third memory segment, the first memory segment being programmed with a package identifier, the second memory segment being programmed with a first unique identifier, and the third memory segment being programmed with an object type, wherein the package identifier is different from the first unique identifier; andcontrol circuitry adapted to: receive a response signal from the first RF tag, the response signal comprising data received from the first RF tag, with the response signal including data representing the package identifier and the object type;determine a number of surgical sponges supposed to be in the first package based on the response signal;determine a number of sponges that are in the first package;operate the handheld scanner to simultaneously confirm presence of the first RF tag and the second RF tag;control a visual display to display data that represents an object type based on the object type programmed on the first RF tag; andcontrol the visual display to simultaneously display sponge entry data and sponge exit data.","20","18/103098","2023-01-30","2023-0165657","2023-06-01","11963827","2024-04-23","STRYKER CORPORATION","Steven J. Fleck | David Szakelyhidi | Gautam Gandhi","","","","A61B-0090/08","A61B-0090/08 | A61B-0005/742 | A61B-0005/746 | A61B-0046/00 | A61B-0050/10 | A61B-0050/13 | A61B-0050/30 | A61B-0050/36 | A61B-0050/37 | A61B-0090/37 | A61B-0090/90 | A61B-0090/98 | A61F-0013/36 | A61F-0013/44 | A61G-0013/10 | G06K-0007/01 | G06K-0007/10316 | G06K-0007/10366 | G06K-0007/10386 | G06K-0007/10425 | G06K-0007/10475 | G06Q-0010/087 | G06Q-0010/0875 | H01Q-0001/2216 | A61B-2017/00199 | A61B-2050/0056 | A61B-2050/0065 | A61B-2090/0805 | A61G-0007/0502 | A61G-0013/12 | A61G-2205/10","A61B-090/00","A61B-090/00 | A61B-005/00 | A61B-046/00 | A61B-050/10 | A61B-050/13 | A61B-050/30 | A61B-050/36 | A61B-050/37 | A61B-090/90 | A61B-090/98 | A61F-013/36 | A61F-013/44 | A61G-013/10 | G06K-007/01 | G06K-007/10 | G06Q-010/087 | G06Q-010/0875 | H01Q-001/22 | A61B-017/00 | A61B-050/00 | A61G-007/05 | A61G-013/12","","","","","","4924017001358"
"US","US","P","B2","System with a smart filtration and/or diffusion device","The present disclosure relates to a system comprising a filtration and/or diffusion device and an editable dataset comprising data pertaining to the filtration and/or diffusion device.","1. A process comprising the steps of a) reading an identifier forming part of a hemodialyzer, a hemofilter, or an ultrafilter with a portable communication device;b) accessing an editable dataset assigned to the hemodialyzer, the hemofilter, or the ultrafilter via the identifier with the portable communication device, wherein the editable dataset is either present a) in the memory of a RFID chip or a NFC chip forming part of the hemodialyzer, the hemofilter, or the ultrafilter, or b) in a data cloud accessible by the portable communication device;c) reading data from the editable dataset and/or writing data to the editable dataset with the portable communication device;d) accessing at least one remote computer or computer network with the portable communication device, and either sending data previously read from the editable dataset to at least one remote computer or computer network or writing data previously received from at least one remote computer or computer network to the editable dataset,wherein the editable dataset is present in memory of a RFID chip or an NFC chip forming part of the hemodialyzer, the hemofilter, or the ultrafilter,wherein at least one remote computer or computer network of the manufacturer of the hemodialyzer, the hemofilter, or the ultrafilter is accessed with the portable communication device, anddata previously read from the editable dataset is sent to at least one remote computer or computer network of the manufacturer of the hemodialyzer, the hemofilter, or the ultrafilter, anddata received from at least one remote computer or computer network of the manufacturer of the hemodialyzer, the hemofilter, or the ultrafilter is written to the editable dataset assigned to the hemodialyzer, the hemofilter, or the ultrafilter, and is used to block use of the hemodialyzer, the hemofilter, or the ultrafilter in case the manufacturer of the hemodialyzer, the hemofilter, or the ultrafilter has detected a problem with the hemodialyzer, the hemofilter, or the ultrafilter, or storage conditions of the hemodialyzer, the hemofilter, or the ultrafilter or if environmental parameters have been outside the allowable limitations.","9","17/263309","2019-07-29","2021-0158953","2021-05-27","11967417","2024-04-23","GAMBRO LUNDIA AB","Till Goldammer | Thomas Ertl","2018-186215","EP","2018-07-30","G16H-0040/20","G16H-0040/20 | G06K-0007/10297 | G06K-0019/0723 | G06Q-0010/06395 | G16H-0040/40 | G16H-0040/67 | A61M-0001/14 | A61M-0001/34 | A61M-2205/3553 | A61M-2205/52 | A61M-2205/60","G16H-040/20","G16H-040/20 | G06K-007/10 | G06K-019/07 | G06Q-010/0639 | G16H-040/40 | G16H-040/67 | A61M-001/14 | A61M-001/34","","","","","","4924017004913"
"US","US","P","B2","Biometric information processing device, biometric information processing method and program","A biometric information processing device (1) includes a brain wave detecting unit (10), a control unit (20), and a movement assisting unit (30). The brain wave detecting unit (10) detects biometric information in at least one brain region from among a plurality of brain regions that are selectable in accordance with a body part that is a target of function recovery or function improvement. The control unit (20) determines, based on the detected biometric information, at least one activity state in the brain including the location of the brain region(s) that is (are) activated in a subject while attempting to move the body part, and an activation level of such activated brain region(s). When the control unit (20) determines that the at least one activated state of the brain satisfies a predetermined condition, the movement assisting unit (30) executes a predetermined motion to assist movement of the body part.","1. A method of treating a patient having a hemiplegic shoulder and elbow, comprising: monitoring brain waves in the somatosensory motor cortex of regions of the brain of the patient that are ipsilateral and contralateral to the hemiplegic shoulder and elbow using a biometric information detecting unit attached to the patient'ss scalp while showing the patient imagery designed to cause the patient to imagine moving the hemiplegic shoulder and elbow,calculating event-related desynchronization (ERD) values from the monitored brain waves using an information processing device, andin response to satisfying at least two predetermined conditions, causing a movement assisting device connected to the hemiplegic shoulder and elbow to execute a predetermined motion that assists movement of the hemiplegic shoulder and elbow,wherein the at least two predetermined conditions include (i) a determination that at least one of the calculated ERD values in the region of the brain of the patient that is ipsilateral to the hemiplegic shoulder and elbow has exceeded a first predetermined threshold while the imagery is being shown to the patient and (ii) a determination that at least one of the calculated ERD values in the region of the brain contralateral to the hemiplegic shoulder and elbow does not exceed a second predetermined threshold of activation level.","20","16/300631","2017-05-15","2020-0237250","2020-07-30","11957475","2024-04-16","KEIO UNIVERSITY","Junichi Ushiba | Fumio Liu | Kenichi Takasaki | Atsuko Nishimoto | Miho Hiramoto | Katsuhiro Mizuno | Meigen Liu | Toshiyuki Fujiwara","2016-097345","JP","2016-05-13","A61B-0005/375","A61B-0005/375 | A61B-0005/316 | A61B-0005/377 | A61B-0005/4848 | A61F-0002/72 | G06F-0003/015 | A61B-2505/09","A61B-005/372","A61B-005/372 | A61B-005/00 | A61B-005/316 | A61B-005/375 | A61B-005/377 | A61F-002/72 | G06F-003/01","","","","","","4924016001550"
"US","US","P","B2","Systems, methods, and apparatus for enhanced headsets","In accordance with some embodiments, systems, apparatus, interfaces, methods, and articles of manufacture are provided for ascertaining aspects of a user, such as the users identity, competence, health, and state of mind. In various embodiments, data is captured about a user via a headset worn by the user. Based on the data, a determination may be made about an aspect of the user, and the user may accordingly be granted or denied access to a resource.","1. A headset for authenticating a user based on an on-going, multi-tiered authentication process, the headset comprising: an electronic processing device;a speaker in communication with the electronic processing device;a microphone in communication with the electronic processing device;a camera in communication with the electronic processing device, the camera directed towards the legs of the user when the headset is worn by the user;an accelerometer in communication with the electronic processing device; anda memory storing (i) point allocation instructions, (ii) referential instructions, and (iii) processing instructions that, when executed by the electronic processing device, result in: outputting, by the speaker, a query to a user;receiving, by the microphone and in response to the query, a response from the user;computing, by an execution of the point allocation instructions by the electronic processing device and based on the response from the user, a first number of points;capturing, by the camera, a video of the user'ss legs;identifying, by the electronic processing device, the user'ss legs within the video;identifying, by an execution of the referential instructions by the electronic processing device and based on the video of the user'ss legs, a first movement of the user;retrieving, by an execution of the referential instructions by the electronic processing device, a reference movement of the first user;comparing, by an execution of the referential instructions by the electronic processing device, the first movement of the user to the reference movement of the first user; andcomputing, by an execution of the point allocation instructions by the electronic processing device and based on the comparing, a second number of points;sensing, by the accelerometer, a second movement of the user;identifying, by an execution of the referential instructions by the electronic processing device and based on the second movement of the user, a gesture corresponding to the second movement of the user;computing, by an execution of the point allocation instructions by the electronic processing device and based on the gesture, a third number of points;calculating, based on the first, second, and third numbers of points, an authorization score;identifying that the calculated authorization score meets a threshold criterion for authorization; andauthorizing, in response to the identifying that the calculated authorization score meets the threshold criterion for authorization, the first user to access a resource.","17","18/310040","2023-05-01","2023-0293106","2023-09-21","11957486","2024-04-16","SCIENCE HOUSE LLC","James Jorasch | Christopher Capobianco | Isaac W. Hock | Michael Werner | Geoffrey Gelman | Gennaro Rendino","","","","A61B-0005/6803","A61B-0005/6803 | A61B-0005/291 | A61B-0005/369 | G06F-0001/163 | G06F-0003/012 | G06F-0003/017 | G06F-0018/22 | G06T-0007/70 | G06V-0020/10 | G06V-0020/20 | G07C-0009/37 | G10L-0015/22 | G16H-0040/67 | H04L-0063/0861 | H04L-0063/102 | H04N-0023/64 | G06Q-0010/101 | G06Q-0050/01 | G06Q-0050/205 | G10L-2015/225","H04L-029/06","H04L-029/06 | A61B-005/00 | A61B-005/291 | A61B-005/369 | G06F-001/16 | G06F-003/01 | G06F-018/22 | G06T-007/70 | G06V-020/10 | G06V-020/20 | G07C-009/37 | G10L-015/22 | G16H-040/67 | H04L-009/40 | H04N-023/60 | G06Q-010/101 | G06Q-050/00 | G06Q-050/20","","","","","","4924016001561"
"US","US","P","B2","Portable headset","Arrangements described herein relate to a headset. The headset includes a device. The device includes a transducer configured to interact with a head of a subject. The headset further includes a manually-operated registration system configured to delineate a workspace of the transducer at the head of the subject.","1. A headset, comprising: a housing, containing: a transducer configured to collect data with respect to a subject, wherein the transducer is configured to transmit and receive ultrasound energy waves with respect to the subject; anda registration system configured to register the transducer with respect to the subject, the registration system comprising a registration window having at least one limit and one or more registration markers wherein the one or more registration markers are configured to be moved within the limit, the one or more registration markers configured to define a region of the object within which movement of the transducer is restricted based on an area bounded by the one or more registration markers, the transducer being configured to move along a first axis of movement within the area bounded by the one or more registration markers; anda housing pivot, comprising: a slide, configured to be disposed within a track of a base and configured to allow the housing to translate within the track along a second axis of movement; anda tilt hinge, coupled to the housing on a first end and to the slide on a second end, the tilt hinge configured to rotate the housing with respect to the slide to rotate the first axis relative to the second axis.","20","17/900076","2022-08-31","2023-0063233","2023-03-02","11957510","2024-04-16","NOVASIGNAL CORP. | NEURASIGNAL, INC.","Jan Zwierstra | Trevor Dunlop | Lane Stith","","","","A61B-0008/4227","A61B-0008/4227 | A61B-0003/0083 | A61B-0005/026 | A61B-0005/065 | A61B-0005/6803 | A61B-0006/0421 | A61B-0006/501 | A61B-0008/06 | A61B-0008/0816 | A61B-0008/40 | A61B-0008/4209 | A61B-0008/4218 | A61B-0008/4427 | A61B-0008/4461 | A61B-0008/461 | A61B-0008/488 | A61B-0008/403 | A61B-2090/502 | A61B-2576/026 | A61F-0009/008 | G06F-0003/011","A61B-008/00","A61B-008/00 | A61B-003/00 | A61B-005/00 | A61B-005/026 | A61B-005/06 | A61B-006/00 | A61B-006/04 | A61B-006/50 | A61B-008/06 | A61B-008/08 | A61B-090/50 | A61F-009/008 | G06F-003/01","","","","","","4924016001584"
"US","US","P","B2","Machine-learned movement determination based on intent identification","A mobility augmentation system monitors data representative of a user's motor intent and augments the user's mobility based on the monitored motor intent data. A machine-learned model is trained to identify an intended movement based on the monitored motor intent data. The machine-learned model may be trained based on generalized or specific motor intent data (e.g., user-specific motor intent data). A machine-learned model initially trained on generalized motor intent data may be re-trained on user-specific motor intent data such that the machine-learned model is optimized to the movements of the user. The system uses the machine-learned model to identify a difference between the user's monitored movement and target movement signals. Based on the identified difference, the system determines actuation signals to augment the user's movement. The actuation signals determined can be an adjustment to a currently applied actuation such that the system optimizes the actuation strategy during application.","1. A method comprising: collecting a first set of motor intent data of one or more users from a database;labeling the first set of motor intent data with an intent label representative of intended motion characterized by the first set of motor intent data;creating a first training set based on the labeled first set of motor intent data;training a machine learning model using the first training set, the machine learning model configured to output, based on monitored motor intent data, a movement prediction corresponding to likely motion characterized by the monitored motor intent data;creating a second training set based on the movement prediction and a labeled second set of motor intent data corresponding to movement signals of a target user; andre-training the machine learning model using the second training set such that the machine learning model is customized to motions of the target user.","19","17/113058","2020-12-06","2022-0175555","2022-06-09","11957605","2024-04-16","CIONIC, INC.","Jeremiah Robison | Michael Dean Achelis | Lina Avancini Colucci | Sidney Rafael Primas | Andrew James Weitz","","","","A61F-0002/583","A61F-0002/583 | G06F-0003/015 | G06F-0003/016 | G06F-0003/0346 | G06N-0020/00","A61F-002/58","A61F-002/58 | G06F-003/01 | G06F-003/0346 | G06N-020/00","","","","","","4924016001678"
"US","US","P","B2","Thermal data analysis for determining location, trajectory and behavior","The system is configured to locate, track and/or analyze activities of living beings in an environment. The system does not require the input of personal biometric data. The sensor system detects infrared (IR) energy from a living being moving in an environment, determines a temperature of the living being based on IR energy data of the IR energy, projects the temperature onto a grid having sequential pixels, determines serial changes of the temperature in the sequential pixels and determines a trajectory of the living being based on the serial changes of the temperature in the sequential pixels.","1. The method comprising: determining, by a sensor system, a plurality of temperatures based on infrared (IR) energy data of IR energy from an environment, wherein the sensor system includes a sensor node of a plurality of sensor nodes;projecting, by the sensor system, the plurality of temperatures onto a grid comprising pixels;determining, by the sensor system, differences in the plurality of temperatures across the pixels; anddetermining, by the sensor system, at least one of a presence or position of a plurality of living beings, based on at least one of the differences or variations in the plurality of temperatures across the pixels.","34","18/194880","2023-04-03","2023-0251139","2023-08-10","11959805","2024-04-16","BUTLR TECHNOLOGIES, INC.","Honghao Deng | Jiani Zeng | Ziran Zhang | Yan Zhang","","","","G01J-0005/0025","G01J-0005/0025 | G01J-0005/12 | G16H-0020/60 | G16H-0040/67 | A61B-0005/01 | A61B-0005/1118 | A61B-0005/16 | G06F-0009/541 | G06F-0030/13 | G06Q-0010/087 | G07C-0009/00 | H04W-0084/18","G08B-023/00","G08B-023/00 | G01J-005/00 | G01J-005/12 | G16H-020/60 | G16H-040/67 | A61B-005/01 | A61B-005/11 | A61B-005/16 | G06F-009/54 | G06F-030/13 | G06Q-010/087 | G07C-009/00 | H04W-084/18","","","","","","4924016003859"
"US","US","P","B2","Medical image recognition method, model training method, and computer device","In a medical image recognition method, applied to a computer device, a to-be-recognized medical image set is obtained, where the to-be-recognized medical image set includes at least one to-be-recognized medical image. A to-be-recognized area corresponding to each to-be-recognized medical image in the to-be-recognized medical image set is extracted. The to-be-recognized area is a part of the to-be-recognized medical image. A recognition result of each to-be-recognized area through a medical image recognition model is determined. The medical image recognition model is obtained through training according to a medical image sample set. The medical image sample set includes at least one medical image sample, and each medical image sample carries corresponding annotation information. The annotation information is used for representing a type of the medical image sample, and the recognition result is used for representing the a of the to-be-recognized medical image.","1. A medical image recognition method, applied to a computer device, the method comprising: obtaining a to-be-recognized original medical image set, the to-be-recognized original medical image set comprising at least one to-be-recognized original medical image;for each to-be-recognized original medical image of the to-be-recognized original medical image set, obtaining label information of the respective to-be-recognized original medical image in the to-be-recognized original medical image set, the label information comprising information associated with the respective to-be-recognized original medical image,determining whether the label information of the respective to-be-recognized original medical image satisfies a sample extraction condition,matching the respective to-be-recognized original medical image with a target medical image in a case that the label information of the respective to-be-recognized original medical image is determined to satisfy the sample extraction condition, the target medical image being a preset image template, anddetermining, in a case that the respective to-be-recognized original medical image is successfully matched with the target medical image, that the to-be-recognized original medical image is one of a to-be-recognized medical image set, the to-be-recognized medical image set comprising at least one to-be-recognized medical image;extracting, with circuitry of the computer device, a to-be-recognized area corresponding to each to-be-recognized medical image in the to-be-recognized medical image set, the to-be-recognized area being a part of the to-be-recognized medical image; anddetermining, with the circuitry of the computer device, a recognition result of each to-be-recognized area through a medical image recognition model, the medical image recognition model being obtained through training according to a medical image sample set, the medical image sample set comprising at least one medical image sample, each medical image sample carrying corresponding annotation information, the annotation information being used for representing a type of the medical image sample, the recognition result being used for representing a type of the to-be-recognized medical image.","16","17/078266","2020-10-23","2021-0042564","2021-02-11","11961226","2024-04-16","TENCENT TECHNOLOGY (SHENZHEN) COMPANY LIMITED","Kaiwen Xiao | Zhongqian Sun | Chen Cheng | Wei Yang","2018-11296263","CN","2018-11-01","G06T-0007/0012","G06T-0007/0012 | G06F-0016/5854 | G06F-0018/214 | G06V-0010/32 | G06V-0010/751 | G06V-0010/774 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/70 | G16H-0070/20 | A61B-0005/055 | A61B-0006/5217 | A61B-0008/5223 | G06F-0017/11 | G06V-2201/03","G06T-007/00","G06T-007/00 | G06F-016/583 | G06F-018/214 | G06V-010/32 | G06V-010/75 | G06V-010/774 | G16H-030/20 | G16H-030/40 | G16H-050/20 | G16H-050/70 | G16H-070/20 | A61B-005/055 | A61B-006/00 | A61B-008/08 | G06F-017/11","","","","","","4924016005258"
"US","US","P","B2","Method of powering and communicating with a staple cartridge","A method for establishing signal and power communication between a surgical instrument and a staple cartridge is disclosed.","1. A method, comprising: seating a staple cartridge in a surgical stapling instrument;placing a data emitter coil of the staple cartridge adjacent to a data transfer coil of the surgical stapling instrument during said seating step;placing a power receiving coil of the staple cartridge adjacent to a power transfer coil of the surgical stapling instrument during said seating step;transmitting data from the staple cartridge to the surgical stapling instrument; andtransmitting power from the surgical stapling instrument to the staple cartridge.","17","17/186269","2021-02-26","2022-0273306","2022-09-01","11950779","2024-04-09","CILAG GMBH INTERNATIONAL","Frederick E. Shelton, IV | Patrick L. Creamer | Shane R. Adams | Jason L. Harris | Morgan R. Hunter | Ismail Akram | William S. Honey | Edward G. Colby | Helen S. Clubb | Emily R. Woodhouse","","","","A61B-0017/07207","A61B-0017/07207 | A61B-0005/002 | A61B-0005/0531 | G06F-0021/44 | H02J-0050/12 | A61B-2017/00026 | A61B-2017/00039 | A61B-2017/00075 | A61B-2017/00084 | A61B-2017/00106 | A61B-2017/00221 | A61B-2017/00482 | A61B-2017/07257 | A61B-2017/07271 | A61B-2017/07278 | A61B-2017/07285 | H04B-0005/0025","A61B-017/072","A61B-017/072 | A61B-005/00 | A61B-005/0531 | G06F-021/44 | H02J-050/12 | A61B-017/00 | H04B-005/00","","","","","","4924015001347"
"US","US","P","B2","User interfaces for health monitoring","The present disclosure generally relates to user interfaces for health monitoring. Exemplary user interfaces for initial setup of health monitoring using a first electronic device and a second electronic device is described. Exemplary user interfaces for recording biometric information for use in health monitoring is described. Exemplary user interfaces for using an input device while recording biometric information for health monitoring is described. Exemplary user interfaces for viewing and managing aspects of health monitoring is described.","1. A first electronic device, comprising: a display;one or more input devices including a biometric sensor;one or more processors; andmemory storing one or more programs configured to be executed by the one or more processors, the one or more programs including instructions for: displaying, on the display, a first user interface indicating that the first electronic device is ready to detect biometric information;detecting a first input with the biometric sensor, wherein the first input satisfies a first set of criteria, the first set of criteria including a first criterion that is satisfied when the first input remains in contact with the biometric sensor for a threshold period of time, and a second criterion that is satisfied when an amount of movement is below a predetermined threshold;in response to detecting the first input with the biometric sensor: starting to record biometric information detected by the biometric sensor; anddisplaying, on the display, a second user interface that is different from the first user interface, wherein the second user interface includes an indication of progress in recording the biometric information;after recording at least a portion of the biometric information, detecting, via the one or more input devices, that the first set of criteria are no longer met;in response to detecting that the first criterion is no longer met, replacing display of the second user interface with the first user interface; andin response to detecting that the second criterion is no longer met, resetting the indication of progress in recording the biometric information and maintaining display of the second user interface.","33","17/135710","2020-12-28","2021-0113137","2021-04-22","11950916","2024-04-09","Apple Inc.","Christopher D. Soli | Roxanne B. Brittain | Gary Ian Butcher | Matthew W. Crowley | Bradley W. Griffin | Heather E. Daniel | Stephen O. Lemay","","","","A61B-0005/332","A61B-0005/332 | A61B-0005/02 | A61B-0005/339 | G06F-0001/163 | G06F-0003/017 | G06F-0003/0485 | G06F-0009/453 | G06T-0013/80","A61B-005/332","A61B-005/332 | A61B-005/02 | A61B-005/339 | G06F-001/16 | G06F-003/01 | G06F-003/0485 | G06F-009/451 | G06T-013/80","","","","","","4924015001484"
"US","US","P","B2","Techniques for removing bound target substances during dialysis","Systems, methods, and/or apparatuses may be operative to perform a dialysis process that includes a displacer infusion process. The dialysis machine may include at least one processor and a memory coupled to the at least one processor, the memory comprising instructions that, when executed by the processor, may cause the at least one processor to access dialysis information for a dialysis process performed by a dialysis machine, the dialysis information indicating a target substance to be displaced from a binding compound by a displacer, and determine an infusion profile for infusing the displacer into a patient during a displacer infusion process of the dialysis process, the infusion profile determined based on the dialysis information and an infusion constraint. Other embodiments are described.","1. A method, comprising: accessing dialysis information for a dialysis process performed by a dialysis machine, the dialysis information indicating a target substance to be displaced from a binding compound by a displacer; anddetermining an infusion profile for infusing the displacer into a patient during a displacer infusion process of the dialysis process, the infusion profile determined based on the dialysis information and an infusion constraint.","20","17/943377","2022-09-13","2023-0001064","2023-01-05","11951243","2024-04-09","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Vaibhav Maheshwari | Peter Kotanko","","","","A61M-0001/3406","A61M-0001/3406 | A61M-0001/1601 | A61M-0001/1603 | A61M-0001/1605 | A61M-0001/1613 | A61M-0001/1615 | A61M-0001/1676 | A61M-0001/342 | A61M-0001/3424 | A61M-0001/3441 | A61M-0001/3455 | A61M-0001/3458 | A61M-0005/142 | G06F-0009/30 | G06F-0009/30003 | G06F-0015/16 | G06F-0015/163 | G06F-0015/167 | G06F-0017/11 | A61M-0001/1654 | A61M-0001/287 | A61M-0001/3493 | A61M-2205/52","A61M-001/34","A61M-001/34 | A61M-001/16 | A61M-005/142 | G06F-009/30 | G06F-015/16 | G06F-015/163 | G06F-015/167 | G06F-017/11 | A61M-001/28","","","","","","4924015001810"
"US","US","P","B2","Systems, devices and methods for managing glucose levels","Systems, devices and methods for the management of glucose levels in the body of patient featuring user interface input mechanisms configured to provide haptic feedback to the user are provided.","1. A continuous glucose monitoring system, comprising: (1) a skin-mounted sensor control unit, comprising: a transcutaneous glucose sensor, a portion of which is configured for transcutaneous positioning in a subcutaneous tissue of a user, and wherein the transcutaneous glucose sensor is configured to sense one or more glucose levels in a body of the user;an adhesive patch disposed on a bottom surface of the skin-mounted sensor control unit, the adhesive patch configured to adhere the skin-mounted sensor control unit to skin of the user; anda wireless communication module configured to wirelessly communicate data indicative of the one or more glucose levels to a glucose monitoring device according to a Bluetooth communication protocol; and(2) the glucose monitoring device, comprising: a processor;a transceiver configured for wireless communication with the skin-mounted sensor control unit; anda user interface controllable by the processor, the user interface comprising a touch screen display configured for tactile contact by the user,wherein the touch screen display is configured to visually display a graph of the user'ss one or more glucose levels,wherein the graph comprises a first axis corresponding to time and a second axis corresponding to the one or more glucose levels, andwherein the touch screen display is further configured to provide a haptic feedback response in response to incrementing or decrementing, by the user, a selected glucose level of the one or more glucose levels along the first axis corresponding to time.","19","18/135629","2023-04-17","2023-0251727","2023-08-10","11954273","2024-04-09","ABBOTT DIABETES CARE INC.","Erwin S. Budiman","","","","G06F-0003/0362","G06F-0003/0362 | A61B-0005/14532 | A61M-0005/1723 | G06F-0003/016 | A61B-0005/7455 | A61B-0005/746 | A61B-2560/0276 | A61M-2205/18 | A61M-2205/3584 | A61M-2205/505 | A61M-2205/582 | A61M-2230/201 | H01H-2003/008","G06F-003/0362","G06F-003/0362 | A61B-005/00 | A61B-005/145 | A61M-005/172 | G06F-003/01 | H01H-003/00","","","","","","4924015004808"
"US","US","P","B2","Medical information processing system and medical information processing method","A medical information processing system according to an embodiment includes processing circuitry and a display. On the basis of one of the type of at least one abnormality detection algorithm to which a medical image related to an examined subject is to be input and information relevant to an abnormality detected by inputting the medical image to the abnormality detection algorithm, the processing circuitry judges whether or not urgency is present in a disorder related to the abnormality. When it is determined that the urgency is present, the display displays, in an examination list of examination orders, assessment information related to assessing the abnormality and urgency information indicating the urgency so as to be positioned adjacent to any of the examination orders related to the abnormality.","1. A medical information processing system, comprising: processing circuitry configured to select an abnormality detection algorithm to which a medical image related to an examined subject is to be input,detect information relevant to an abnormality by inputting the medical image to the selected abnormality detection algorithm,determine whether or not urgency is present in a disorder related to the abnormality based on the detected information,cause a display, when it is determined that the urgency is present, to display, in an examination list presenting a list of examination orders, assessment information related to assessing the abnormality, and urgency information indicating the urgency, so as to be positioned adjacent to any of the examination orders related to the abnormality, wherein the displayed assessment information includes a percentage of correct outputs related to a disorder; an index related to a reliability of the detected abnormality; and an index related to a priority of treatment on the abnormality,wherein the processing circuitry is further configured to select the abnormality detection algorithm to which the medical image is to be input, by analyzing the medical image related to the examined subject, andthe displayed assessment information includes a thumbnail image of the medical image including the abnormality, and the display displays an enlarged image obtained by enlarging the thumbnail image, in response to a cursor being moved to an inside of a display region of the thumbnail image.","7","17/173248","2021-02-11","2021-0272278","2021-09-02","11954850","2024-04-09","CANON MEDICAL SYSTEMS CORPORATION","Koichi Terai | Hiroki Saito | Kenichi Usui | Yosuke Okubo | Hirobumi Nonaka","2020-033940 | 2021-003734","JP | JP","2020-02-28 | 2021-01-13","G06T-0007/0012","G06T-0007/0012 | G16H-0010/60 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/30 | A61B-0005/055 | A61B-0006/504 | G06F-0003/04812 | G06F-0003/0482 | G06F-0003/04845 | G06T-2207/30101","G06T-007/00","G06T-007/00 | A61B-005/055 | A61B-006/00 | G06F-003/04812 | G06F-003/0482 | G06F-003/04845 | G16H-010/60 | G16H-030/20 | G16H-030/40 | G16H-050/20 | G16H-050/30 | A61B-006/50","","","","","","4924015005375"
"US","US","P","B2","Method and system for brain activity signal-based treatment and/or control of user devices","A method for characterizing a brain electrical signal comprising forming a temporo-spectral decomposition of the signal to form a plurality of time resolved frequency signal values, associating each instance of the signal value with a predetermined function approximating a neurological signal to form a table of coefficients collectively representative of the brain electrical signal.","1. A system comprising: a. at least one input to receive one or more event-related desynchronization (ERD) signals;b. at least one output to a functional electrical stimulation (FES) device to carry out FES corresponding to an intended activity (IA); andc. a controller to communicate with the at least one input and the at least one output, the controller configured to: i. to record an ERD signal received from the at least one input, the ERD signal corresponding to an uncharacterized IA for each time value of one or more successive time values,ii. for each time value: 1. To access one or more ERD templates of coefficients for one or more characterized IA'ss;2. To update an ERD table for the uncharacterized IA and to compare the updated ERD table with the ERD templates to determine whether the uncharacterized IA is an instance of one of the characterized IA'ss; andiii. to initiate a device action instruction on at the at least one output to the FES device after a minimum number of time values necessary to determine whether the uncharacterized IA is an instance of one of the characterized IA'ss.","15","17/306080","2021-05-03","2021-0257078","2021-08-19","11955217","2024-04-09","UNIVERSITY HEALTH NETWORK","C?sar Marquez Chin | Kathryn Atwell | Milos R. Popovic","","","","G16H-0020/30","G16H-0020/30 | A61B-0005/1468 | A61B-0005/24 | A61B-0005/245 | A61B-0005/291 | A61B-0005/369 | A61B-0005/374 | A61B-0005/4851 | A61B-0005/7246 | A61B-0005/7264 | A61F-0002/72 | A61F-0005/01 | A61N-0001/36003 | G06F-0003/015 | G06F-0003/038 | G16H-0020/70 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | G16Z-0099/00 | A61H-2230/105 | A63B-2230/105","G06F-003/01","G06F-003/01 | A61B-005/00 | A61B-005/1468 | A61B-005/24 | A61B-005/245 | A61B-005/291 | A61B-005/369 | A61B-005/374 | A61F-002/72 | A61F-005/01 | A61N-001/36 | G06F-003/038 | G16H-020/30 | G16H-020/70 | G16H-040/63 | G16H-050/20 | G16H-050/30 | G16Z-099/00","","","","","","4924015005740"
"US","US","P","B2","Medical equipment management","A computer-implemented method for managing medical equipment includes presenting, at a display of a mobile computing device associated with an inspector, an inspection user interface configured to capture one or more user inputs from the inspector, receiving inspection account information via the one or more user inputs, receiving a location of the mobile computing device, providing, at the inspection user interface, identification information for transported medical equipment that is associated with the inspection account information, receiving status information for the transported medical equipment via the one or more user inputs, receiving, at a remote medical equipment database, location information for the transported medical equipment, and updating a maintenance report for the transported medical equipment by storing an inspection information entry in the maintenance report. The inspection information entry includes a time stamp, the location information, the received status information, and the location of the mobile computing device.","1. A computer-implemented method for managing medical equipment comprising: presenting, at a display of a mobile computing device associated with an inspector, an inspection user interface configured to capture one or more user inputs from the inspector;receiving inspection account information for an inspection account associated with the inspector, the inspection account information received via the one or more user inputs;receiving a location of the mobile computing device;identifying a plurality of transported medical equipment items associated with the inspection account;identifying a subset of one or more of the plurality of transported medical equipment items that are within a predetermined distance from the location of the mobile computing device;providing, at the inspection user interface, identification information for the one or more transported medical equipment items in the subset;receiving status information for a particular one of the one or more transported medical equipment items in the subset, the status information received via the one or more user inputs;receiving, at a remote medical equipment database, location information for the particular transported medical equipment item; andupdating a maintenance log for the particular transported medical equipment item by storing an inspection information entry in the maintenance log, wherein the inspection information entry comprises a time stamp, the location information, the received status information, and the location of the mobile computing device.","27","17/812276","2022-07-13","2022-0351849","2022-11-03","11955231","2024-04-09","ZOLL MEDICAL CORPORATION","John P Pierson | Ian B Durrant | Kristopher M Edgell | Joanne R Parrill","","","","G16H-0040/40","G16H-0040/40 | G06F-0008/65 | G06F-0016/24573 | G06Q-0010/087 | G06Q-0050/26 | G16H-0040/20 | G16H-0040/67 | H04L-0067/12 | H04L-0067/34 | H04L-0067/52 | A61B-0005/7475 | A61B-0017/132 | A61F-0017/00 | A61M-0016/024 | A61N-0001/3993 | G01S-0019/42 | G06F-0003/04847 | H04W-0004/40 | H04W-0076/10","G16H-040/40","G16H-040/40 | A61B-005/00 | A61B-017/132 | A61F-017/00 | A61M-016/00 | A61N-001/39 | G01S-019/42 | G06F-003/04847 | G06F-008/65 | G06F-016/2457 | G06Q-010/087 | G06Q-050/26 | G16H-040/20 | G16H-040/67 | H04L-067/00 | H04L-067/12 | H04L-067/52 | H04W-004/40 | H04W-076/10","","","","","","4924015005754"
"US","US","P","B2","Wearable image manipulation and control system with correction for vision defects and augmentation of vision and sensing","A wearable image manipulation system comprising a camera input system, an image projection system, where the image projection system is capable of being worn by a user, and a processor in communication with the camera input system and the image projection system such that the processor is capable of receiving an image from the camera input system, modifying the image to produce a modified image, and displaying the modified image on the image projection system. The camera input system may comprise a contact lens with a camera mounted thereon. Additionally or alternately, the system may be capable of tracking a user's eye movement to accurately capture where the user is looking with the camera input system.","1. A wearable image manipulation system comprising: a camera input system;an image projection system, where the image projection system is capable of being worn by a user, where the image projection system comprises at least one contact lens with a display unit contained thereon or at least partially embedded therein;a database capable of receiving, storing, and transmitting a retinal map; anda processor in communication with the camera input system, the database, and the image projection system such that the processor is capable of receiving an image from the camera input system and modifying the image according to the retinal map to produce a modified image and displaying the modified image on the image projection system where by displaying the modified image comprises correcting eye defects of the user.","44","15/962661","2018-04-25","2018-0249151","2018-08-30","11956414","2024-04-09","RAYTRX, LLC","Michael Hayes Freeman | Richard C. Freeman | Mitchael C. Freeman | Chad Boss | Jordan Boss","","","","H04N-0013/332","H04N-0013/332 | A61B-0003/0025 | A61B-0003/005 | A61B-0003/0058 | A61B-0005/1123 | A61B-0005/6803 | A61B-0005/742 | A61F-0009/00 | A61F-0009/08 | G02B-0027/017 | G02B-0027/0172 | G02C-0011/10 | G06F-0001/163 | G06F-0003/011 | G06F-0003/013 | G06V-0020/20 | G06V-0040/193 | G16H-0020/30 | G16H-0030/40 | G16H-0040/63 | G16H-0050/20 | H04N-0013/189 | H04N-0013/366 | A61B-0003/024 | A61B-0003/113 | A61B-0003/145 | A61B-0005/0059 | A61B-0005/1114 | A61B-0005/746 | A61B-2090/365 | A61B-0090/39 | A61B-2090/502 | A61B-2560/0242 | A61B-2562/0204 | A61B-2562/0219 | A61B-2562/0223 | G02B-2027/0138 | G02B-2027/014 | G02B-2027/0178 | G02C-0005/14 | G06F-0003/012 | G06F-0003/014","H04N-013/332","H04N-013/332 | A61B-003/00 | A61B-005/00 | A61B-005/11 | A61F-009/00 | A61F-009/08 | G02B-027/01 | G02C-011/00 | G06F-001/16 | G06F-003/01 | G06V-020/20 | G06V-040/18 | G16H-020/30 | G16H-030/40 | G16H-040/63 | G16H-050/20 | H04N-013/189 | H04N-013/366 | A61B-003/024 | A61B-003/113 | A61B-003/14 | A61B-090/00 | A61B-090/50 | G02C-005/14","","","","","","4924015006926"
"US","US","P","B2","Stone identification methods and systems","Aspects of stone identification methods and systems are described. According to one aspect, an exemplary method comprises: transmitting to a processing unit, with an imaging element mounted on a distal end of a scope, image data about a stone object inside a body cavity; generating from the image data, with the processing unit, a visual representation of the stone object and the body cavity; establishing from a user input, with the processing unit, a scale for the visual representation; determining from the visual representation, with the processing unit, a size of the stone object on the scale; comparing, with the processing unit, the size of the stone object with a predetermined maximum size to determine a removal status; and augmenting, with the processing unit, the visual representation to include an indicator responsive to the removal status. Associated systems are also described.","1. A method comprising: receiving, at a processor, image data of an object inside a body cavity;generating from the image data, with the processor, a representation of the object and the body cavity by transmitting at least a portion of the image data to an interface device;determining from the representation, with the processor, a size of the object on a scale for the representation, wherein determining the size of the object includes establishing, with the interface device, a first reference point and a second reference point on the representation of the object;comparing, with the processor, the size of the object with a predetermined maximum size to determine a removal status of the object by:determining a first removal status when the size of the object is greater than the predetermined maximum size; anddetermining a second removal status when the size of the object is less than the predetermined maximum size; andaugmenting, with the processor, the representation to include an indicator responsive to the removal status.","20","17/574598","2022-01-13","2022-0133255","2022-05-05","11937768","2024-03-26","BOSTON SCIENTIFIC SCIMED, INC.","Peter J. Pereira | Michael S. H. Chu | Elizabeth Stokley | David Salto | Candace Rhodes","","","","A61B-0001/00009","A61B-0001/00009 | A61B-0001/0005 | A61B-0001/018 | A61B-0001/307 | A61B-0005/1076 | A61B-0005/20 | A61B-0006/5217 | A61B-0008/0841 | A61B-0008/085 | A61B-0008/12 | A61B-0008/4254 | A61B-0017/2256 | A61B-0018/26 | A61B-0034/25 | G06F-0003/0484 | G06F-0003/0488 | G06T-0011/60 | A61B-0001/00048 | A61B-0017/2255 | A61B-2018/00505 | A61B-2018/00517 | A61B-2018/00982 | A61B-2034/2074 | A61B-2090/061 | A61N-2005/061","G06K-009/00","G06K-009/00 | A61B-001/00 | A61B-001/018 | A61B-001/307 | A61B-005/107 | A61B-005/20 | A61B-006/00 | A61B-008/00 | A61B-008/08 | A61B-008/12 | A61B-017/225 | A61B-018/26 | A61B-034/00 | G06F-003/0484 | G06F-003/0488 | G06T-011/60 | A61B-018/00 | A61B-034/20 | A61B-090/00 | A61N-005/06","","","","","","4924013001378"
"US","US","P","B2","Methods and systems for detecting stroke symptoms","A stroke detection system analyzes images of a person's face over time to detect asymmetric changes in the position of certain reference points that are consistent with sagging or drooping that may be symptomatic of a stroke or TIA. On detecting possible symptoms of a stroke or TIA, the system may alert caregivers or others, and log the event in a database. Identifying stroke symptoms automatically may enable more rapid intervention, and identifying TIA symptoms may enable diagnostic and preventative care to reduce the risk of a future stroke.","1. A computerized method for detecting stroke symptoms comprising: capturing image data including two or more images including a face of a person using at least one 3-dimensional (3D) sensor;superimposing at least one axis of a 3-dimensional (3D) axis system at a reference anatomical feature of the face in a portion of the image data including the face of the person; andcommunicating an electronic alert to a remote device in response to detecting an asymmetric change in position of at least one reference point of a plurality of reference points that correspond to anatomical features on the face of the person within the image data relative to the 3D axis system, wherein the detected asymmetric change in position of the at least one reference point represents a change in a degree of asymmetry corresponding to a depth axis in the 3D axis system, and wherein the change in the degree of asymmetry exceeds a threshold change of about 10% or 3 mm in the degree of asymmetry.","19","17/576593","2022-01-14","2022-0133174","2022-05-05","11937915","2024-03-26","CERNER INNOVATION, INC.","Michael Kusens | Neil Kusens","","","","A61B-0005/11","A61B-0005/11 | A61B-0005/0077 | A61B-0005/1176 | A61B-0005/4064 | A61B-0005/746 | G06F-0018/22 | G06T-0007/0012 | G06T-0007/0016 | G06T-0007/20 | G06T-0007/292 | G06T-0011/60 | G06V-0020/52 | G06V-0020/647 | G06V-0040/172 | G06V-0040/20 | G08B-0005/22 | G08B-0013/196 | G08B-0021/182 | G08B-0025/009 | G16H-0010/60 | G16H-0015/00 | G16H-0020/10 | G16H-0030/20 | G16H-0040/20 | G16H-0040/63 | G16H-0040/67 | G16H-0050/30 | G16H-0080/00 | H04N-0007/18 | H04N-0007/181 | H04N-0007/183 | H04N-0013/204 | H04N-0013/207 | H04N-0023/63 | G06F-0003/0482 | G06F-0003/04847 | G06T-2200/04 | G06T-2200/24 | G06T-2207/10012 | G06T-2207/10021 | G06T-2207/10024 | G06T-2207/20221 | G06T-2207/30201 | G06T-2207/30232 | G06V-0040/161 | G08B-0013/19639 | G08B-0021/0476 | H04N-2013/0085","A61B-005/11","A61B-005/11 | A61B-005/00 | A61B-005/1171 | G06F-018/22 | G06T-007/00 | G06T-007/20 | G06T-007/292 | G06T-011/60 | G06V-020/52 | G06V-020/64 | G06V-040/16 | G06V-040/20 | G08B-005/22 | G08B-013/196 | G08B-021/18 | G08B-025/00 | G16H-010/60 | G16H-015/00 | G16H-020/10 | G16H-030/20 | G16H-040/20 | G16H-040/63 | G16H-040/67 | G16H-050/30 | G16H-080/00 | H04N-007/18 | H04N-013/204 | H04N-013/207 | H04N-023/63 | G06F-003/0482 | G06F-003/04847 | G08B-021/04 | H04N-013/00","","","","","","4924013001525"
"US","US","P","B2","Systems and methods for sensor-based operator fatigue management","A system for monitoring a fatigue level of an operator of a vehicle includes a sensor configured to generate a signal indicative of a physiological state of the operator; a display for the operator; and a controller to: receive, from the sensor, the signal; determine the fatigue level of the operator by analyzing the received signal using an algorithm developed using operator fatigue statistics; generate a real-time fatigue report for the operator based on the determined fatigue level of the operator; transmit the generated real-time fatigue report to the display for the operator for display to the operator, and a display for a dispatcher for the vehicle for display to the dispatcher; generate an anonymized version of the real-time fatigue report; and transmit the anonymized version of the real-time fatigue report to a cloud for access by remote users.","1. A system for monitoring a fatigue level of an operator of a vehicle, the system comprising: at least one sensor configured to generate a signal indicative of a physiological state of the operator, the signal being defined at least in part by operator-specific data;a display for the operator; andone or more controllers configured to: receive, from the at least one sensor, the signal indicative of the physiological state of the operator;based at least in part on the operator-specific data, determine the fatigue level of the operator by analyzing the received signal using an algorithm developed using anonymous operator fatigue statistics stored at an operator fatigue statistics database;based at least in part on the operator-specific data, determine a sleep level of the operator by analyzing the received signal using the algorithm developed using anonymous operator fatigue statistics stored at the operator fatigue statistics database;generate a real-time fatigue report for the operator based on the determined fatigue level of the operator, wherein the real-time fatigue report includes the determined fatigue level and the determined sleep level of the operator;transmit the generated real-time fatigue report to (1) the display for the operator for display to the operator, and (2) a display for a dispatcher for the vehicle for display to the dispatcher;based at least in part on the real-time fatigue report, generate an anonymized fatigue report wherein the anonymized fatigue report includes anonymous fatigue level data corresponding to an anonymized version of the determined fatigue level and anonymous sleep level data corresponding to an anonymized version of the determined sleep level, wherein the anonymous fatigue level data is not associated with the operator-specific data of the signal received from the at least one sensor, and wherein an identity of the operator is de-correlated from any associated fatigue data in the anonymized fatigue report;transmit the anonymized fatigue report to a cloud for access by remote users;transmit the anonymized fatigue report to the operator fatigue statistics database; train the machine learning algorithm using the operator fatigue statistics database including the anonymized fatigue report, as the algorithm;optimize a schedule for the operator to operate the vehicle, using the operator fatigue statistics database;wherein the anonymized fatigue report is generated after operation of the vehicle is completed by the operator.","6","17/647096","2022-01-05","2023-0211789","2023-07-06","11938947","2024-03-26","HONEYWELL INTERNATIONAL S.R.O.","St?phane March? | Sander Roosendaal","","","","B60W-0040/08","B60W-0040/08 | A61B-0005/18 | A61B-0005/6802 | B60W-0050/14 | G06N-0020/00 | G06Q-0010/06312 | G07C-0005/008 | G07C-0005/0825 | B60W-2040/0827 | B60W-2040/0872 | B60W-2050/146 | B60W-2540/221 | B60W-2540/229 | B60W-2556/45","B60W-040/08","B60W-040/08 | A61B-005/00 | A61B-005/18 | B60W-050/14 | G06N-020/00 | G06Q-010/0631 | G07C-005/00 | G07C-005/08","","","","","","4924013002542"
"US","US","P","B2","Intelligent closed-loop feedback control for transcranial stimulation","Disclosed within is a closed loop controller having: (a) a signal processing and statistics subsystem sampling an input data stream from at least one sensor, calculating real-time continuous statistics in the input data stream based on a sliding window technique, and outputting one or more classifications based on the real-time statistics; and (b) an intelligent fuzzy logic controller receiving the one or more classifications from the signal processing and statistics subsystem, accessing a heuristic rule set based on expert knowledge, and outputting a noninvasive stimulation pattern based on the one or more classifications and the heuristic rule set.","1. A closed loop controller comprising: a. a signal processing and statistics subsystem sampling an input data stream from at least one sensor, calculating real-time continuous statistics in the input data stream based on a sliding window technique, and outputting one or more classifications based on the real-time continuous statistics; andb. an intelligent fuzzy logic controller receiving the one or more classifications from the signal processing and statistics subsystem, accessing a heuristic rule set based on expert knowledge, and outputting a noninvasive stimulation pattern based on the one or more classifications and the heuristic rule set.","25","16/851051","2020-04-16","2021-0325836","2021-10-21","11940765","2024-03-26","ELECTRO STANDARDS LABORATORIES","Brandon M Sepe | Raymond B Sepe, Jr. | Steven P Bastien","","","","G05B-0013/0295","G05B-0013/0295 | A61B-0005/374 | A61B-0005/7217 | A61B-0005/7264 | A61N-0001/20 | A61N-0001/36031 | G06F-0017/18 | G06N-0007/02 | G16H-0020/30 | G16H-0020/70","A61N-001/00","A61N-001/00 | A61B-005/00 | A61B-005/374 | A61N-001/20 | A61N-001/36 | G05B-013/02 | G06F-017/18 | G06N-007/02 | G16H-020/30 | G16H-020/70","","","","","","4924013004335"
"US","US","P","B2","Imaging system and method for use in surgical and interventional medical procedures","A system and method for displaying images of internal anatomy includes an image processing device configured to provide high resolution images of the surgical field from low resolution scans during the procedure. The image processing device digitally manipulates a previously-obtained high resolution baseline image to produce many representative images based on permutations of movement of the baseline image. During the procedure a representative image is selected having an acceptable degree of correlation to the new low resolution image. The selected representative image and the new image are merged to provide a higher resolution image of the surgical field. The image processing device is also configured to provide interactive movement of the displayed image based on movement of the imaging device, and to permit placement of annotations on the displayed image to facilitate communication between the radiology technician and the surgeon.","1. A method for generating a display of an image of a patient'ss internal anatomy in a surgical field during a medical procedure, the method comprising: with one or more processors:producing an image set using a machine, wherein the image set includes permutations of a first image, the first image being based on a three-dimensional representation of the surgical field including the patient'ss internal anatomy and being with respect to a baseline orientation;applying a first dose of radiation to the patient to obtain the three-dimensional representation;acquiring a new image of the surgical field by applying a second dose of radiation to the patient, wherein the first dose of radiation is greater than the second dose of radiation;selecting a representative image from the image set based on a correlation between the representative image and the new image;identifying, within the new image, a location of a glyph caused by a radiodense object coupled to an imaging device used to obtain the new image;merging the selected representative image with the new image using the location of the glyph as part of the merging; anddisplaying the merged image.","18","17/549059","2021-12-13","2022-0113810","2022-04-14","11941179","2024-03-26","NUVASIVE, INC.","Robert E. Isaacs | Samuel Morris Johnston | David Alexander Skwerer | Randall Graham Campbell","","","","G06F-0003/017","G06F-0003/017 | A61B-0006/06 | A61B-0006/12 | A61B-0006/4405 | A61B-0006/4441 | A61B-0006/486 | A61B-0006/5241 | A61B-0006/547 | G06T-0003/20 | G06T-0003/4053 | G06T-0007/0016 | G06T-0007/33 | G06T-0011/60 | G06T-0015/08 | H04N-0007/18 | G06T-2207/10124 | G06T-2207/20212 | G06T-2207/20221 | G06T-2207/30004 | G06T-2210/41","G06K-009/00","G06K-009/00 | A61B-005/05 | A61B-006/00 | A61B-006/06 | A61B-006/12 | G06F-003/01 | G06T-003/20 | G06T-007/00 | G06T-007/33 | G06T-011/60 | G06T-015/08 | H04N-007/18","","","","","","4924013004742"
"US","US","P","B2","Device, system and method for determining the position of stents in an image of vasculature structure","The present invention relates to a device (10) for determining the position of stents (34, 36) in an image of vasculature structure, the device (10) comprising: an input unit (12); a processing unit (14); and an output unit (16); wherein the input unit (12) is configured to receive a sequence of images (24) of a vasculature structure (38) comprising at least one vessel branch (44, 46); wherein the processing unit (14) is configured to: detect positions of at least two markers (26, 28, 30, 32) for identifying a stent position (50, 52) in at least one of the images (24); detect at least one path indicator (64, 66, 74, 76) for the at least one vessel branch (44, 46) in at least one of the images (24) of the vasculature structure (38) at least for vessel regions in which the positions of the markers (26, 28, 30, 32) are detected; associate the at least two markers (26, 28, 30, 32) to the at least one path indicator (64, 66, 74, 76) based on the detected positions of the markers (26, 28, 30, 32) and the location of the at least one path indicator (64, 66, 74, 76); assign markers (26, 28, 30, 32) which are associated to the same path indicator (64, 66, 74, 76) to a marker group to indicate a position (50, 52) of at least one stent (34, 36) in the vasculature structure (38); and wherein the output unit (16) is configured to provide output data indicative of the positions of the markers (26, 28, 30, 32) of the marker group. The invention provides a device and a method that improve the determination of the position of stents in complicated situations.","1. A device for determining a position of a stent in an image of a vasculature structure, the device comprising: input circuitry configured to receive a sequence of images of the vasculature structure comprising at least one vessel branch;a processor configured to: detect at least two markers associated with the stent in the sequence of images of the vasculature structure and a position of each of the at least two markers;detect a path indicator for a vessel branch of the at least one vessel branch in the sequence of images of the vasculature structure and a location of the path indicator;associate the at least two markers to the path indicator based on the position of each of the at least two markers and the detected location of the detected path indicator; andassign the at least two markers to a marker group based on the association of the at least two markers to the path indicator to indicate position of the stent in the vasculature structure.","20","16/966389","2019-01-28","2020-0372674","2020-11-26","11941842","2024-03-26","KONINKLIJKE PHILIPS N.V.","Peter Maria Johannes Rongen | Markus Johannes Harmen Den Hartog | Javier Olivan Bescos | Thijs Elenbaas | Iris Ter Horst","2018-154357","EP","2018-01-31","G06T-0007/74","G06T-0007/74 | A61B-0006/12 | A61B-0006/504 | A61B-0008/0841 | A61F-0002/82 | G06T-0005/001 | A61F-2250/0098 | G06T-2207/10016 | G06T-2207/10116 | G06T-2207/10132 | G06T-2207/30101 | G06T-2207/30204","G06Q-030/0601","G06Q-030/0601 | A61B-006/12 | A61B-006/50 | A61B-008/08 | A61F-002/82 | G06T-005/00 | G06T-007/73","","","","","","4924013005399"
"US","US","P","B2","Patient-specific surgical methods and instrumentation","A method may be used to correct a condition present in a patient. The method may include obtaining a first bone model of a first bone of one or more bones of the patient's foot, and using at least the first bone model to generate a cutting guide model. The cutting guide model may define a first bone engagement surface shaped to match a first contour on the first bone, and a first guide feature that, with the first bone engagement surface overlying the first contour, is positioned to guide resection of the one or more bones as part of a surgical osteotomy for correcting the condition. The surgical procedure may be selected from a first group consisting of a bunion correction osteotomy, an Evans calcaneal osteotomy, and a medializing calcaneal osteotomy. The first bone may be selected from a second group consisting a metatarsus, a cuneiform, and a calcaneus.","1. A method for correcting a bunion present in a patient'ss foot, the method comprising: obtaining a first bone model of a cuneiform of the patient'ss foot;obtaining a second bone model of a metatarsus of the patient'ss foot;virtually repositioning the second bone model relative to the first bone model to simulate reorientation of the metatarsus relative to the cuneiform to correct the bunion; andusing at least the first bone model relative to the repositioned second bone model to generate a cutting guide model defining: a first bone engagement surface shaped to match a first contour on the cuneiform;a second bone engagement surface shaped to match a second contour of the metatarsus; anda first guide feature that, with the first bone engagement surface overlying the first contour, is positioned to guide resection of at least one of the cuneiform and the metatarsus as part of a bunion correction osteotomy for correcting the bunion.","10","17/020630","2020-09-14","2021-0077192","2021-03-18","11931106","2024-03-19","MIOS MARKETING LLC DBA REDPOINT MEDICAL 3D | TREACE MEDICAL CONCEPTS, INC.","Adam D. Perler | James Q. Spitler","","","","A61B-0034/10","A61B-0034/10 | A61B-0017/152 | A61B-0017/1775 | G06F-0030/10 | G06T-0007/0012 | G06T-0011/008 | G16H-0020/40 | G16H-0030/20 | G16H-0030/40 | G16H-0050/50 | A61B-2017/00526 | A61B-2017/565 | A61B-2034/105 | A61B-2034/108 | A61B-2090/3762 | G06Q-0050/04 | G06T-2207/10081 | G06T-2207/30008","A61B-017/17","A61B-017/17 | A61B-017/15 | A61B-034/10 | G06F-030/10 | G06T-007/00 | G06T-011/00 | G16H-020/40 | G16H-030/20 | G16H-030/40 | G16H-050/50 | A61B-017/00 | A61B-017/56 | A61B-090/00 | G06Q-050/04","","","","","","4924012001491"
"US","US","P","B2","Mobile monitoring and patient management system","A patient management system for generating customized patient reports for users is provided. The system includes communications circuitry configured to receive first physiological data from a plurality of monitoring medical devices and receive second physiological data from a plurality of therapeutic medical devices. The system also includes at least one processor configured to perform instructions configured to cause the at least one processor to receive a selection from an end user regarding a customized report on at least one of the first physiological data from the plurality of monitoring medical devices or the second physiological data from the plurality of therapeutic medical devices, determine at least one information field for the customized report based on an access level of the end user, and compile the customized report including the at least one information field and using the at least one of the first physiological data or the second physiological data.","1. A patient management system for generating customized patient reports for users, comprising: communications circuitry configured to receive first physiological data from a plurality of monitoring medical devices being worn by a first plurality of patients;receive second physiological data from a plurality of therapeutic medical devices being worn by a second plurality of patients; andat least one processor configured to perform instructions configured to cause the at least one processor to receive a selection from an end user regarding a customized report on at least one of the first physiological data from the plurality of monitoring medical devices being worn by the first plurality of patients or the second physiological data from the plurality of therapeutic medical devices being worn by the second plurality of patients;determine at least one information field for the customized report based on an access level of the end user; andcompile the customized report using the at least one of the first physiological data or the second physiological data,wherein the customized report comprises the at least one information field.","20","18/304621","2023-04-21","2023-0329553","2023-10-19","11931126","2024-03-19","ZOLL MEDICAL CORPORATION","Jason T Whiting | Gary A Freeman | Thomas E Kaib","","","","A61B-0005/0015","A61B-0005/0015 | A61B-0005/6805 | A61B-0005/742 | A61N-0001/37247 | A61N-0001/3904 | A61N-0001/39044 | A61N-0001/3925 | A61N-0001/3993 | G06Q-0050/22 | G16H-0010/60 | G16H-0010/65 | G16H-0020/30 | G16H-0040/67 | G16H-0050/20 | A61B-0005/021 | A61B-0005/024 | A61B-0005/11 | A61B-0005/14542 | A61B-0005/14551 | A61B-0005/746 | A61M-0016/0051 | A61M-0016/021 | A61M-2205/3375 | A61M-2205/3553 | A61M-2205/3569 | A61M-2205/3584 | A61M-2205/3592 | A61M-2205/505 | A61M-2205/52 | A61M-2209/088 | A61M-2230/04 | A61M-2230/06 | A61M-2230/201 | A61M-2230/205 | A61M-2230/30 | A61M-2230/42 | A61M-2230/50 | A61M-2230/63 | A61M-2230/65","A61B-005/00","A61B-005/00 | A61N-001/372 | A61N-001/39 | G06Q-050/22 | G16H-010/60 | G16H-010/65 | G16H-020/30 | G16H-040/67 | G16H-050/20 | A61B-005/021 | A61B-005/024 | A61B-005/11 | A61B-005/145 | A61B-005/1455 | A61M-016/00","","","","","","4924012001511"
"US","US","P","B2","Systems and methods for health data visualization and user support tools for continuous glucose monitoring","Disclosed are systems and methods for generating graphical displays of analyte data and/or health information. In some implementations, the graphical displays are generating based on a self-referential dataset that are modifiable based on identified portions of the data. The modified graphical displays can indicate features in the analyte data of a host.","1. A system comprising: a continuous analyte sensor configured to obtain analyte data of a host;a wireless transmitter configured to receive the analyte data from the continuous analyte sensor and transmit the analyte data to a processing module; andthe processing module configured to: receive the analyte data of the host and event data of the host;produce a graphical display on a mobile computing device to display a visual indicating one or more relationships of the analyte data and the event data with each other or time, wherein the visual is scaled to not obscure the display of the analyte data or event data; andupon receiving a user input, automatically modify the graphical display to isolate areas of the analyte data exceeding one or more threshold analyte values.","29","17/448317","2021-09-21","2022-0000432","2022-01-06","11931188","2024-03-19","DEXCOM, INC.","Esteban Cabrera, Jr. | Lauren Danielle Armenta | Scott M. Belliveau | Jennifer Blackwell | Leif N. Bowman | Rian Draeger | Arturo Garcia | Timothy Joseph Goldsmith | John Michael Gray | Andrea Jean Jackson | Apurv Ullas Kamath | Katherine Yerre Koehler | Paul Kramer | Aditya Sagar Mandapaka | Michael Robert Mensinger | Sumitaka Mikami | Gary A Morris | Hemant Mahendra Nirmal | Paul Noble-Campbell | Philip Thomas Pupa | Eli Reihman | Peter C. Simpson | Brian Christopher Smith | Atiim Joseph Wiley","","","","A61B-0005/7275","A61B-0005/7275 | A61B-0005/0022 | A61B-0005/14532 | A61B-0005/743 | A61B-0005/746 | G06F-0001/163 | G06F-0003/0484 | G06F-0003/04847 | G06F-0003/0488 | G06F-0017/18 | G16H-0015/00 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30 | G16H-0070/40 | G06F-0016/00","A61B-005/00","A61B-005/00 | A61B-005/145 | G06F-001/16 | G06F-003/0484 | G06F-003/04847 | G06F-003/0488 | G06F-017/18 | G16H-015/00 | G16H-040/63 | G16H-040/67 | G16H-050/20 | G16H-050/30 | G16H-070/40 | G06F-016/00","","","","","","4924012001573"
"US","US","P","B2","Permission-based control of interfacing components with a medical device","Disclosed are embodiments directed to security methods applied to connections between components in a distributed (networked) system including medical and non-medical devices, providing secure authentication, authorization, patient and device data transfer, and patient data association and privacy for components of the system.","1. A method for communicating with a patient monitoring medical device system, comprising: issuing a certificate signing request (CSR) to a certificate authority for attestation, the certificate signing request including a public key associated with a trusted device;receiving from the certificate authority a signed security certificate, wherein the signed security certificate includes information that identifies a set of permissions that are authorized to the trusted device;initiating a communication session between the trusted device and a secure component of the patient monitoring medical device system;transmitting the signed security certificate to the secure component for verification;authorizing the set of permissions to the trusted device based on the information; andupon confirmation that the signed security certificate has been verified, authorizing communications between the trusted device and the secure component using the communication session.","10","17/738385","2022-05-06","2022-0266043","2022-08-25","11931591","2024-03-19","PHYSIO-CONTROL DEVELOPMENT CO., LLC | WEST AFFUM HOLDINGS DESIGNATED ACTIVITY COMPANY","Steven E. Sjoquist | David P. Finch | Erick M. Roane | Zoie R. Engman | Jonathan P. Niegowski | Dusan Beblavy | Martin Pribula | Peter Curila | Martin Koles?r","","","","A61N-0001/3904","A61N-0001/3904 | A61B-0005/02438 | A61N-0001/046 | A61N-0001/0484 | A61N-0001/3987 | A61N-0001/3993 | G06F-0021/33 | H04L-0009/3268 | H04L-0063/0823 | A61B-0005/361 | A61B-0005/363 | A61B-0005/6805 | A61B-0005/74 | A61N-0001/37258 | A61N-0001/3925","H04L-009/40","H04L-009/40 | A61B-005/024 | A61N-001/04 | A61N-001/39 | G06F-021/33 | H04L-009/32 | A61B-005/00 | A61B-005/361 | A61B-005/363 | A61N-001/372","","","","","","4924012001976"
"US","US","P","B2","Glasses-type electronic device","An object is to provide an electronic device capable of recognizing a user's facial feature accurately. A glasses-type electronic device includes a first optical component, a second optical component, a frame, an imaging device, a feature extraction unit, and an emotion estimation unit. The frame is in contact with a side surface of the first optical component and a side surface of the second optical component. The imaging device is in contact with the frame and has a function of detecting part of a user's face. The feature extraction unit has a function of extracting a feature of the user's face from the detected part of the user's face. The emotion estimation unit has a function of estimating information on the user from the extracted feature.","1. A glasses-type electronic device comprising: a first optical component;a second optical component;a frame;an imaging device;a feature extraction unit; andan emotion estimation unit,wherein the frame is in contact with a side surface of the first optical component and a side surface of the second optical component,wherein the imaging device is in contact with the frame,wherein the imaging device is configured to detect part of a user'ss face,wherein the feature extraction unit is configured to extract a feature of the user'ss face from the detected part of the user'ss face, andwherein the emotion estimation unit is configured to estimate information on the user from the extracted feature.","11","17/429979","2020-02-13","2022-0137409","2022-05-05","11933974","2024-03-19","SEMICONDUCTOR ENERGY LABORATORY CO., LTD.","Shunpei Yamazaki | Takayuki Ikeda | Hidetomo Kobayashi | Hideaki Shishido | Kiyotaka Kimura | Takashi Nakagawa | Kosei Nei | Kentaro Hayashi","2019-030646","JP","2019-02-22","G02B-0027/0093","G02B-0027/0093 | G02B-0027/0101 | G02B-0027/0172 | G02B-0027/0176 | G06T-0007/73 | G06V-0010/454 | G06V-0010/764 | G06V-0010/82 | G06V-0020/20 | G06V-0040/174 | G02B-2027/0138 | G02B-2027/014 | G02B-2027/0178 | G06T-2207/30201","G02B-027/00","G02B-027/00 | A61B-005/00 | A61B-005/11 | A61B-005/16 | G02B-027/01 | G06F-003/147 | G06T-007/73 | G06V-010/44 | G06V-010/764 | G06V-010/82 | G06V-020/20 | G06V-030/19 | G06V-040/16 | G09G-003/3225 | G11C-019/28","","","","","","4924012004327"
"US","US","P","B2","Data playback interface for a medical device","A medical device for review of clinical data in a playback mode is described. The medical device includes at least one output device comprising at least one display screen, at least one memory, and at least one processor coupled to the at least one memory and the at least one output device, the at least one processor configured to receive signals indicative of patient data from one or more patient interface devices communicatively coupled to the medical device, control the at least one display screen to provide a first visual representation of the patient data as an operational interface, and selectively display a playback interface at the at least one display screen wherein the playback interface enables user interactive review of the patient data based on a second visual representation of the patient data.","1. A system for review of clinical data in a playback mode, the system comprising: an external defibrillator comprising: at least one first display screen,a communications module,at least one first memory, andat least one first processor coupled to the at least one first memory, the at least one first display screen and the communications module, the at least one first processor configured to: receive signals indicative of real-time patient data from one or more patient interface devices communicatively coupled to the external defibrillator during ongoing data capture from a patient during a medical encounter with the patient, the one or more patient interface devices comprising electrotherapy electrodes and one or more physiological sensors, and the real-time patient data comprising physiological data comprising electrocardiogram data and one or more of pulse oximetry data and capnography data,record a time at which a medical event occurred based on the real-time patient data,control the at least one first display screen to provide a first visual representation of the real-time patient data at an operational interface that displays the real-time patient data at the external defibrillator in real-time as the signals are received, andtransmit the real-time patient data via the communications module; andat least one auxiliary computing device comprising: at least one second display screen,at least one second memory, andat least one second processor coupled to the at least one second memory and the at least one second display screen, the at least one second processor configured to: receive the real-time patient data from the external defibrillator, andcontrol the at least one second display screen to provide a second visual representation of the real-time patient data at a playback interface that is configured to: display, on an interactive timeline, the real-time patient data comprising the physiological data comprising the electrocardiogram data and the one or more of pulse oximetry data and capnography data as the signals are received and as the real-time patient data comprising the physiological data comprising the electrocardiogram data and the one or more of pulse oximetry data and capnography data is concurrently displayed at the operational interface provided at the external defibrillator,display an event marker at a position on the interactive timeline that corresponds to occurrence of the medical event,provide at least two data display controls on the playback interface that enable a user to select a start time and an end time that define a user-selected playback interval,provide a snap-to-event capability configured to adjust the user-selected playback interval to an automatically-selected playback interval based at least in part on the event marker, wherein the snap-to-event capability enables the playback interface to move at least one of the user-selected start time or the user-selected end time to the position on the interactive timeline that corresponds to occurrence of the medical event, andreplay historic patient data according to the automatically-selected playback interval, during the ongoing data capture from the patient.","35","16/659807","2019-10-22","2020-0121199","2020-04-23","11925439","2024-03-12","ZOLL MEDICAL CORPORATION","Gary A Freeman | Annemarie E Silver | Timothy F Stever","","","","A61B-0005/02055","A61B-0005/02055 | A61B-0005/01 | A61B-0005/026 | A61B-0005/14532 | A61B-0005/14539 | A61B-0005/14551 | A61B-0005/339 | A61B-0005/369 | A61B-0005/4875 | A61B-0007/003 | A61N-0001/39044 | A61N-0001/3968 | A61N-0001/3993 | G06F-0003/0481 | G06F-0003/0482 | G06F-0003/04847 | G06F-0003/0486 | G06F-0003/04883 | G06F-0003/14 | G16H-0010/20 | G16H-0020/30 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0809 | A61B-0005/0816 | A61B-0005/0836 | G06F-2203/04803","A61B-005/0205","A61B-005/0205 | A61B-005/00 | A61B-005/01 | A61B-005/021 | A61B-005/024 | A61B-005/026 | A61B-005/145 | A61B-005/1455 | A61B-005/339 | A61B-005/369 | A61B-007/00 | A61N-001/39 | G06F-003/0481 | G06F-003/0482 | G06F-003/04847 | G06F-003/0486 | G06F-003/04883 | G06F-003/14 | G16H-010/20 | G16H-020/30 | A61B-005/08 | A61B-005/083","","","","","","4924011001112"
"US","US","P","B2","Methods and systems for grouping informed advisor pairings","A system for customizing informed advisor pairings, the system including a computing device. The computing device is configured to identify a user feature wherein the user feature contains a user biological extraction. The computing device is configured to generate using element training data and using a first machine-learning algorithm a first machine-learning model that outputs advisor elements. The computing device receives an informed advisor element relating to an informed advisor. The computing device determines using output advisor elements whether an informed advisor is compatible for a user.","1. A system for grouping informed ad visor pairings, the system comprising: a computing device, the computing device configured to:obtain a user feature of a user, wherein obtaining the feature comprises generating a feature model iteratively trained with physiological training data comprising a plurality of physiological data sets correlated to a plurality of user features, wherein the feature model is configured to receive a biological extraction comprising user physiological data related to at least genomic data of the user, perform a classification algorithm, and output the user feature;receive an informed ad visor element relating an informed ad visor to the user feature, wherein receiving an informed advisor element comprises training, using element training data comprising a plurality of user features and a plurality of correlated informed advisor elements, a machine-learning model configured to receive the user feature as an input and output the informed advisor element, wherein the computing device is configured to iteratively update the element training data to reflect geographical variances among correlations between the plurality of user features and advisor elements;generate an informed advisor grouping element as a function of the informed advisor element, wherein generating further comprises: receiving a grouping training set, the grouping training set relating an informed advisor to an advisor review score; andcalculating the informed advisor grouping element as a function of the grouping training set using an advisor machine-learning process, the advisor machine-learning process trained using the grouping training set;determine, as a function of the informed advisor grouping element, a group compatible element;group, an informed advisor of a plurality of informed advisors as a function of the group compatible element, the group compatible element configured to enhance the user feature; anddisplay the group compatible element on a graphical user interface.","18","16/948102","2020-09-03","2021-0201199","2021-07-01","11928561","2024-03-12","KPN INNOVATIONS, LLC.","Kenneth Neumann","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/7267 | G06N-0005/04 | G06Q-0030/0204 | G06Q-0030/0282 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30","G16H-050/20","G16H-050/20 | A61B-005/00 | G06N-005/04 | G06N-020/00 | G06Q-030/0204 | G06Q-030/0282 | G16H-040/67 | G16H-050/30","","","","","","4924011004210"
"US","US","P","B2","Systems and methods of access validation using distributed ledger identity management","An access control system includes a plurality of sensors and an access controller that includes one or more processors. The sensors detect sensor data regarding a user. The one or more processors receive credential data regarding the user from at least one of an access control device and an electronic wallet, evaluate a medical status using at least one of the sensor data and the credential data, validate access into a space responsive to the medical status being satisfied, and update a record data structure of at least one of the electronic wallet and a distributed ledger using the evaluation of the medical status.","1. An access control system, comprising: a plurality of sensors that detect sensor data regarding a user; andan access controller comprising one or more processors that: receive credential data regarding the user from at least one of an access control device and an electronic wallet;evaluate a condition using the sensor data and the credential data to verify an identify of the user and confirm a medical status of the user;validate access into a space responsive to verification of the identity of the user and confirmation of the medical status of the user; andupdate a record data structure of at least one of the electronic wallet and a distributed ledger using the evaluation of the medical status.","20","17/186719","2021-02-26","2021-0327189","2021-10-21","11928905","2024-03-12","TYCO FIRE & SECURITY GMBH","Graeme Jarvis | Jason Ouellette | Christopher Cianciolo | Karl F. Reichenberger","","","","G07C-0009/29","G07C-0009/29 | A61B-0005/015 | A61B-0005/1176 | G05B-0015/02 | G06F-0016/27 | G06Q-0010/1057 | G06Q-0010/20 | G06Q-0050/163 | G06Q-0050/265 | G06V-0040/166 | G16H-0010/20 | G16H-0010/65 | G16H-0050/30 | G06Q-0010/02 | G06Q-0020/363 | G06Q-2240/00","G07C-009/29","G07C-009/29 | A61B-005/01 | A61B-005/1171 | G05B-015/02 | G06F-016/27 | G06Q-010/1057 | G06Q-010/20 | G06Q-050/16 | G06Q-050/26 | G06V-040/16 | G16H-010/20 | G16H-010/65 | G16H-050/30 | G06Q-010/02 | G06Q-020/36","","","","","","4924011004553"
"US","US","P","B1","Automated device efficacy determination systems for health monitoring devices","A computerized method of automated device efficacy determination for multiple monitor devices includes receiving streaming data including multiple health data values sensed by multiple monitor devices, each value indicative of health status of one or more members, and identifying first and second health data values from first and second target ones of the multiple monitor devices. The method includes determining first and second measured health status values of first and second ones of the members according to the identified first and second health data values, and aggregating the determined first measured health status value of the first member with the determined second measured health status value of the first member or the measured health status value of the second member. The method includes comparing the aggregated measured health status values to a target device efficacy threshold to determine an outcome-based device efficacy of the first and second target monitor devices.","1. A computerized method of automated device efficacy determination for multiple monitor devices, the method comprising: receiving a set of device information corresponding to multiple monitor devices;in response to receiving the set of device information, generating, by a system including a blockchain key ledger, a first set of keys corresponding to the multiple monitor devices;receiving a set of member information associated with the multiple monitor devices;in response to receiving the set of member information, generating, by the system including the blockchain key ledger, a second set of keys corresponding to the set of member information;associating the multiple monitor devices with the first set of keys and the second set of keys;storing the first set of keys and the second set of keys in the blockchain key ledger;receiving a third set of keys from the multiple monitor devices, wherein the third set of keys includes: a first key associated with a first target device of the multiple monitor devices, anda second key associated with a second target device of the multiple monitor devices;determining whether the third set of keys is valid by comparing the third set of keys to: the first set of keys stored in the blockchain key ledger, andthe second set of keys stored in the blockchain key ledger;in response to determining that the third set of keys is valid, authorizing a set of data streams from the multiple monitor devices, wherein: the set of data streams includes multiple health data values sensed by the multiple monitor devices and indicative of health status of one or more members;the multiple health data values represent physical measurements performed on the one or more members, andthe set of data streams includes a first data stream from the first target device and a second data stream from the second target device;identifying first health data values from the first data stream from the first target device;identifying second health data values from the second data stream from the second target device;transforming, by a universal data curation platform, the first health data values and the second health data values into a standardized attribute format according to one or more device profiles associated with the first and second target devices;determining, by a personalized micro-engine, a first measured health status value of a first member of the one or more members according to the identified first health data values, wherein the personalized micro-engine is personalized to a particular member of the one or more members or to a particular device of the multiple monitor devices;determining, by the personalized micro-engine, a second measured health status value of the first member or a measured health status value of a second member according to the identified second health data values;aggregating, by the personalized micro-engine, the determined first measured health status value of the first member with the determined second measured health status value of the first member or the measured health status value of the second member; andcomparing the aggregated measured health status values to a target device efficacy threshold to determine an outcome-based device efficacy of the first and second target devices.","23","17/072693","2020-10-16","","","11929163","2024-03-12","EXPRESS SCRIPTS STRATEGIC DEVELOPMENT, INC.","Ankur Kaneria | Harry S. Gangaikondan-Iyer","","","","G16H-0040/20","G16H-0040/20 | A61B-0005/0022 | G06F-0009/445 | G06Q-0010/06395 | G06Q-0030/0185 | G16H-0040/40 | G16H-0040/67 | G16H-0050/30 | G16H-0050/70 | A61B-0005/021 | A61B-0005/02438 | A61B-0005/14532 | A63B-2024/0065 | G06Q-0010/1057 | G06Q-0040/08 | G06Q-0050/28 | G16H-0020/10 | G16H-0050/20 | G16Y-0020/40","H04L-009/00","H04L-009/00 | A61B-005/00 | G06F-009/445 | G06Q-010/0639 | G06Q-030/018 | G16H-040/20 | G16H-040/40 | G16H-040/67 | G16H-050/30 | G16H-050/70 | A61B-005/021 | A61B-005/024 | A61B-005/145 | A63B-024/00 | G06Q-010/1057 | G06Q-040/08 | G06Q-050/28 | G16H-020/10 | G16H-050/20 | G16Y-020/40","","","","","","4924011004810"
"US","US","P","B2","Orthopedic fixation control and manipulation","A fixation apparatus may be attached to first and second anatomical structure segments. Images of the fixation apparatus and the attached anatomical structure segments may then be captured. In some examples, the images need not necessarily be orthogonal with respect to one another. Configuration information associated with the fixation apparatus may then be received. Additionally, first image information may be received, for example including indications of one or more locations, within the images, of at least part of one or more elements of the fixation apparatus. Additionally, second image information may be received, for example including indications of one or more locations, within the images, of at least part of the first and the second anatomical structure segments. Manipulations to the fixation apparatus for correction of the anatomical structure deformity may then be determined, and indications of the determined manipulations may then be provided to one or more users.","1. A computer-implemented method for controlling manipulation of a fixation apparatus including rings and struts to correct an anatomical structure deformity of first and second anatomical structure segments comprising: receiving, using one or more graphical user interfaces of a computing system, configuration information associated with the fixation apparatus, the configuration information comprising one or more geometric characteristics of the rings and the struts of the fixation apparatus;determining, by the computing system, based at least in part on the configuration information, a position and an orientation of the rings of the fixation apparatus in three-dimensional space;generating, by the computing system, one or more graphical representations showing the position and the orientation of the rings of the fixation apparatus;determining, by the computing system, based at least in part on the configuration information, manipulations to the fixation apparatus for correction of the anatomical structure deformity, the manipulations comprising strut length changes and a strut size change; andproviding, by the computing system, to one or more users, an interface that displays a plurality of areas each corresponding to a respective day of a plurality of days, wherein the interface displays one or more indications of a range of days within the plurality of days on which a strut swap is performable, wherein the one or more indications are included in a group of areas within the plurality of areas corresponding to the range of days, and wherein the strut swap corresponds to the strut size change.","20","17/078629","2020-10-23","2021-0077194","2021-03-18","11918292","2024-03-05","DEPUY SYNTHES PRODUCTS, INC. | SYNTHES GMBH","Michael Wahl | Bernd Gutmann | Kevin Clancy | Dana Heavey | Tina Corey | Todd Kent","","","","A61B-0034/10","A61B-0034/10 | A61B-0034/25 | G06F-0003/04847 | G06T-0007/70 | A61B-0017/62 | A61B-0017/66 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/252 | A61B-2034/254 | A61B-2090/367 | A61B-2090/3966 | G06T-2207/10004 | G06T-2207/30008","A61B-017/66","A61B-017/66 | A61B-017/62 | A61B-034/00 | A61B-034/10 | G06F-003/04847 | G06T-007/70 | A61B-090/00","","","","","","4924010001127"
"US","US","P","B2","Generating final abnormality data for medical scans based on utilizing a set of sub-models","A multi-model medical scan analysis system is operable to generate a plurality of training sets from a plurality of medical scans. Each of a set of sub-models is generated by performing a training step on a corresponding one of the plurality of training sets of the plurality of medical scans. A set of abnormality data is generated by applying a subset of a set of inference functions on a new medical scan. The subset of the set of inference functions utilize the subset of the set of sub-models, and each of the set of abnormality data is generated as output of performing one of the subset of the set of inference functions. The multi-model medical scan analysis system is further operable to generate final abnormality data that includes a global probability indicating a probability that any abnormality is present based on the set of abnormality data.","1. A multi-model medical scan analysis system, comprising: at least one processor; anda memory that stores operational instructions that, when executed by the at least one processor, cause the multi-model medical scan analysis system to: generate a plurality of training sets from a plurality of medical scans;generate each of a set of sub-models by performing a training step on a corresponding one of the plurality of training sets of the plurality of medical scans;receive a new medical scan;generate a set of abnormality data by applying a subset of a set of inference functions on the new medical scan, wherein the subset of the set of inference functions utilize a subset of the set of sub-models, wherein each of the set of abnormality data is generated as output of performing one of the subset of the set of inference functions;generate final abnormality data that includes a global probability indicating a probability that any abnormality is present based on the set of abnormality data; andtransmit the final abnormality data for display via a display device of a client device.","20","17/656925","2022-03-29","2022-0223243","2022-07-14","11922348","2024-03-05","ENLITIC, INC.","Kevin Lyman | Li Yao | Eric C. Poblenz | Jordan Prosky | Ben Covington | Anthony Upton","","","","G06Q-0010/06315","G06Q-0010/06315 | A61B-0005/7264 | G06F-0003/0482 | G06F-0003/0484 | G06F-0009/542 | G06F-0016/245 | G06F-0018/2115 | G06F-0018/214 | G06F-0018/217 | G06F-0018/2415 | G06F-0018/41 | G06F-0021/6254 | G06N-0005/04 | G06N-0005/045 | G06N-0020/00 | G06N-0020/20 | G06Q-0020/14 | G06T-0003/40 | G06T-0005/002 | G06T-0005/008 | G06T-0005/50 | G06T-0007/0012 | G06T-0007/0014 | G06T-0007/10 | G06T-0007/11 | G06T-0007/187 | G06T-0007/44 | G06T-0007/97 | G06T-0011/001 | G06T-0011/006 | G06T-0011/206 | G06V-0010/225 | G06V-0010/25 | G06V-0010/764 | G06V-0010/82 | G06V-0030/19173 | G06V-0040/171 | G16H-0010/20 | G16H-0010/60 | G16H-0015/00 | G16H-0030/20 | G16H-0030/40 | G16H-0040/20 | G16H-0050/20 | H04L-0067/01 | H04L-0067/12 | A61B-0005/055 | A61B-0006/032 | A61B-0006/5217 | A61B-0008/4416 | G06F-0018/2111 | G06F-0018/24 | G06F-0040/295 | G06Q-0050/22 | G06T-0007/70 | G06T-2200/24 | G06T-2207/10048 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/10116 | G06T-2207/10132 | G06T-2207/20076 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30004 | G06T-2207/30008 | G06T-2207/30016 | G06T-2207/30061 | G06V-0030/194 | G06V-2201/03 | G16H-0050/30 | G16H-0050/70","G16H-050/20","G16H-050/20 | A61B-005/00 | G06F-003/0482 | G06F-003/0484 | G06F-009/54 | G06F-016/245 | G06F-018/21 | G06F-018/2115 | G06F-018/214 | G06F-018/2415 | G06F-018/40 | G06F-021/62 | G06N-005/04 | G06N-005/045 | G06N-020/00 | G06N-020/20 | G06Q-010/0631 | G06Q-020/14 | G06T-003/40 | G06T-005/00 | G06T-005/50 | G06T-007/00 | G06T-007/10 | G06T-007/11 | G06T-007/187 | G06T-007/44 | G06T-011/00 | G06T-011/20 | G06V-010/22 | G06V-010/25 | G06V-010/764 | G06V-010/82 | G06V-030/19 | G06V-040/16 | G16H-010/20 | G16H-010/60 | G16H-015/00 | G16H-030/20 | G16H-030/40 | G16H-040/20 | H04L-067/01 | H04L-067/12 | A61B-005/055 | A61B-006/00 | A61B-006/03 | A61B-008/00 | G06F-018/2111 | G06F-018/24 | G06F-040/295 | G06Q-050/22 | G06T-007/70 | G06V-030/194 | G16H-050/30 | G16H-050/70","","","","","","4924010005146"
"US","US","P","B2","Systems and methods for surgical and interventional planning, support, post-operative follow-up, and functional recovery tracking","Various systems and methods are provided for surgical and interventional planning, support, post-operative follow-up, and functional recovery tracking. In general, a patient can be tracked throughout medical treatment including through initial onset of symptoms, diagnosis, non-surgical treatment, surgical treatment, and recovery from the surgical treatment. In one embodiment, a patient and one or more medical professionals involved with treating the patient can electronically access a comprehensive treatment planning, support, and review system. The system can provide recommendations regarding diagnosis, non-surgical treatment, surgical treatment, and recovery from the surgical treatment based on data gathered from the patient and the medical professional(s). The system can manage the tracking of multiple patients, thereby allowing for data comparison between similar aspects of medical treatments and for learning over time through continual data gathering, analysis, and assimilation to decision-making algorithms.","1. A medical method, comprising: a processor receiving plan data regarding a virtual performance of a particular surgical procedure on a patient, the virtual performance being a simulation of the particular surgical procedure on a three-dimensional simulated model of the patient using virtual surgical instruments;the processor receiving performance data regarding an actual performance of the particular surgical procedure on the patient, the performance data being received in real time during the actual performance of the surgical procedure, and the virtual performance of the surgical procedure having been performed and completed prior to a start of the actual performance of the surgical procedure on the patient;the processor determining deviation between the plan data and the performance data;if the plan data is determined to deviate from the performance data outside a predetermined degree of tolerance, the processor providing a warning to a user that includes an indication of the determined variance, and the processor receiving subsequent performance data in real time during the actual performance of the surgical procedure and determining deviation between the plan data and the subsequently received performance data; andif the plan data is determined to deviate from the performance data within the predetermined degree of tolerance, the processor receiving subsequent performance data in real time during the actual performance of the surgical procedure and determining deviation between the plan data and the subsequently received performance data;wherein the patient is on a bed Burin the actual performance of the surgical procedure;the surgical procedure Is a spinal surgical procedure;the method further comprises, upon determining that the plan data deviates from the performance data outside a predetermined degree of tolerance, the processor causing the bed to move so as to position the patient in the desired position;the desired position aligns a vertebra of patient for a trajectory of approach;the method further comprises the processor receiving, from a head-mountable stereoscopic viewing device worn by the surgeon, an input indicative of the surgeon desiring the patient to be in a second desired position; andthe method further comprises the processor causing the bed to move so as to position the patient in the second desired position; andthe second desired position aligns a second vertebra of the patient for a trajectory of posterior approach.","16","16/214947","2018-12-10","2019-0110784","2019-04-18","11923068","2024-03-05","DePuy Synthes Products, Inc.","Namal Nawana | Michael Gorhan | William J. Frasier | William C. Horton | Mark T. Hall | Matthew Parsons | Jennifer DiPietro | Christopher Nordstrom","","","","G16H-0030/20","G16H-0030/20 | A61B-0005/0022 | A61B-0005/1118 | A61B-0005/4509 | A61B-0005/4566 | A61B-0005/4571 | A61B-0005/4824 | A61B-0005/4833 | A61B-0005/4848 | A61B-0005/7282 | A61B-0005/742 | A61B-0005/746 | A61B-0005/7475 | A61B-0006/485 | A61B-0008/08 | A61B-0017/02 | A61B-0017/025 | A61B-0017/50 | A61B-0034/10 | A61B-0034/20 | A61B-0034/25 | G01L-0005/00 | G06Q-0010/00 | G06Q-0010/10 | G09B-0023/28 | G16H-0010/20 | G16H-0020/40 | G16H-0040/20 | G16H-0040/40 | G16H-0040/63 | G16H-0050/20 | G16H-0050/50 | G16H-0050/70 | G16H-0070/20 | G16Z-0099/00 | A61B-0006/4494 | A61B-0006/506 | A61B-2010/009 | A61B-2010/0093 | A61B-2017/00115 | A61B-2017/00119 | A61B-2017/0262 | A61B-2034/101 | A61B-2034/105 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2057 | A61B-2034/252 | A61B-2034/254 | A61B-2034/256 | A61B-2090/365 | A61B-2090/502 | A61B-0090/98","G16H-030/20","G16H-030/20 | A61B-005/00 | A61B-005/11 | A61B-006/00 | A61B-008/08 | A61B-017/02 | A61B-017/50 | A61B-034/00 | A61B-034/10 | A61B-034/20 | G01L-005/00 | G06Q-010/00 | G06Q-010/10 | G09B-023/28 | G16H-010/20 | G16H-020/40 | G16H-040/20 | G16H-040/40 | G16H-040/63 | G16H-050/20 | G16H-050/50 | G16H-050/70 | G16H-070/20 | G16Z-099/00 | A61B-010/00 | A61B-017/00 | A61B-090/00 | A61B-090/50 | A61B-090/98","","","","","","4924010005861"
"US","US","P","B2","Adaptable asymmetric medicament cost component in a control system for medicament delivery","The exemplary embodiments provide medicament delivery devices that use cost functions in their control systems to determine medicament dosages. The cost function may have a medicament cost component and a performance cost component. The exemplary embodiments may use cost functions having medicament cost components that scale asymmetrically for different ranges of inputs (i.e., different candidate medicament dosages). The variance in scaling for different input ranges provides added flexibility to tailor the medicament cost component to the user and thus provide better management of medicament delivery to the user and better conformance to a performance target. The exemplary embodiments may use a cost function that has a medicament cost component (such as an insulin cost component) of zero for candidate dosages for a range of candidate dosages (e.g., below a reference dosage).","1. A medicament delivery device, comprising: a memory for storing data and computer programming instructions;a pump for delivering a medicament to a user;a processor for executing the computer programming instructions to: determine values of a cost function for candidate dosages to the user, the cost function has a performance cost component and a medicament cost component,wherein the medicament cost component is configured to be asymmetrical about a threshold amount; andchoose a dosage to be delivered to the user by the pump from among the candidate dosages based on values of the cost function for the candidate dosages.","23","17/330115","2021-05-25","2022-0288300","2022-09-15","11904140","2024-02-20","INSULET CORPORATION","Joon Bok Lee | Yibin Zheng | Jason O'Connor | Trang Ly","","","","A61M-0005/1723","A61M-0005/1723 | A61B-0005/14532 | A61B-0005/4839 | A61M-0005/142 | G06Q-0030/0283 | G16H-0020/17 | A61M-2005/14208 | A61M-2205/52 | A61M-2230/201","A61M-005/172","A61M-005/172 | G16H-020/17 | A61B-005/145 | A61B-005/00 | A61M-005/142 | G06Q-030/0283","","","","","","4924008001588"
"US","US","P","B2","Systems and methods to process images for skin analysis and to visualize skin analysis","Systems and methods process images to determine a skin condition severity analysis and to visualize a skin analysis such as using a deep neural network (e.g. a convolutional neural network) where a problem was formulated as a regression task with integer-only labels. Auxiliary classification tasks (for example, comprising gender and ethnicity predictions) are introduced to improve performance. Scoring and other image processing techniques may be used (e.g. in assoc. with the model) to visualize results such as highlighting the analyzed image. It is demonstrated that the visualization of results, which highlight skin condition affected areas, can also provide perspicuous explanations for the model. A plurality (k) of data augmentations may be made to a source image to yield k augmented images for processing. Activation masks (e.g. heatmaps) produced from processing the k augmented images are used to define a final map to visualize the skin analysis.","1. A skin diagnostic device comprising circuitry providing a processing unit coupled to a storage unit to configure the skin diagnostic device to provide: a skin analysis unit to classify pixels of an image, that is a selfie face image from a user mobile device, using a deep neural network, which is already trained using a dataset of facial image data comprising selfie face images from user mobile devices, comprising a regressor and a classifier for image classification to generate the skin diagnosis for a skin condition;wherein the deep neural network determines the skin diagnosis as an integer value on a scale classifying a severity of the skin condition over the image, and the deep neural network further provides an activation map to visualize the skin diagnosis in association with the image; andwherein the skin diagnostic device is configured to: perform a plurality (k) of data augmentations to the image to yield k augmented images for processing by the skin analysis unit;process the k augmented images using the skin analysis unit to obtain k activation maps; anddefine a final activation map for the image from the k activation maps to visualize the skin analysis.","22","16/996087","2020-08-18","2021-0012493","2021-01-14","11908128","2024-02-20","L'OREAL","Ruowei Jiang | Irina Kezele | Zhi Yu | Sophie Seite | Frederic Antoinin Raymond Serge Flament | Parham Aarabi | Mathieu Perrot | Julien Despois","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0005/0077 | A61B-0005/441 | A61B-0005/7267 | G06Q-0030/0631 | G06Q-0030/0633 | G06Q-0030/0641 | G16H-0050/20 | G06T-2207/20076 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30088 | G06T-2207/30201","G06T-007/00","G06T-007/00 | A61B-005/00 | G16H-050/20 | G06Q-030/0601","","","","","","4924008005550"
"US","US","P","B2","Method of supporting interpretation of genetic information by medical specialist, information management system, and integrated data management device","A method of supporting an expert meeting of medical specialists to interpret genetic information, may include: accepting a test request for genetic information on a patient; extracting test progress information that is stored in association with the accepted test request and indicates progress of a test for the test request, and schedule information on a schedule of the expert meeting to interpret the genetic information obtained in the test for the test request; and displaying the extracted test progress information and the extracted schedule information on terminal devices of the medical specialists.","1. A method of supporting an expert meeting of medical specialists to interpret genetic information, comprising: accepting a test request requesting a test for genetic information on a sample extracted from a patient;extracting a plurality of test status information that are stored in association with the accepted test request and indicates status of the test for the test request, and schedule information on a schedule of the expert meeting to interpret the genetic information obtained in the test for the test request; anddisplaying the extracted plurality of test status information in a first display region and the extracted schedule information in a second display region, the first and the second display regions displayed side-by-side on one of the terminal devices of the medical specialists such that the test status information and the extracted schedule information for the patient appears on a same line, whereinthe test status information comprises information on whether the test for the genetic information on the sample extracted from the patient is completed.","23","16/810973","2020-03-06","2020-0286634","2020-09-10","11908589","2024-02-20","SYSMEX CORPORATION","Takayuki Takahata | Tatsuru Wakimoto | Yusaku Matsuo","2019-041671","JP","2019-03-07","G16H-0080/00","G16H-0080/00 | A61B-0005/742 | G06F-0009/542 | G06Q-0010/1095 | G16H-0010/40 | G16H-0010/60","G16H-010/40","G16H-010/40 | G16H-010/60 | A61B-005/00 | G06F-009/54 | G06Q-010/1093 | G16H-080/00","","","","","","4924008006005"
"US","US","P","B2","Ophthalmic system, image signal output method, image signal output device, program, and three-dimensional fundus image generation method","An image signal output method including outputting a first image signal to display an eyeball model selection screen for selecting one eyeball model from out of plural eyeball models of different types, converting a two-dimensional fundus image of the subject eye so as to generate a three-dimensional fundus image based on a selected eyeball model, and outputting a second image signal to display a fundus image display screen including the three-dimensional fundus image.","1. An image processing method comprising: displaying an eyeball model selection screen for selecting one eyeball model from out of a plurality of eyeball models of different forms;selecting one eyeball model from out of the plurality of eyeball models;converting a two-dimensional fundus image of a subject eye so as to generate a three-dimensional fundus image based on a selected eyeball model and a position of imaging said two-dimensional fundus image; anddisplaying a fundus image display screen including the three-dimensional fundus image,wherein the selecting one eyeball model out of the plurality of eyeball models comprises selecting one eyeball model from out of the plurality of eyeball models based on: a distance, passing through a center of a pupil of the subject eye, from a first predetermined point on a cornea of the subject eye to a first point on a retina of the subject eye;a distance, passing through the center of the pupil of the subject eye, from a second predetermined point on the cornea of the subject eye to a second point on the retina;a distance, passing through the center of the pupil of the subject eye, from a third predetermined point on the cornea of the subject eye to a third point on the retina;a first angle formed by a line connecting the center of the pupil of the subject eye and the first point, and a reference line, which connects an eyeball center of the subject eye and the center of the pupil;a second angle formed by a line connecting the center of the pupil and the second point, and the reference line; anda third angle formed by a line connecting the center of the pupil and the third point, and the reference line.","8","16/753149","2018-10-11","2020-0258295","2020-08-13","11900533","2024-02-13","NIKON CORPORATION","Mariko Hirokawa","2017-199485","JP","2017-10-13","G06T-0017/00","G06T-0017/00 | A61B-0003/0025 | A61B-0003/0041 | A61B-0003/1005 | A61B-0003/12 | A61B-0005/055 | G06F-0003/0482 | G06T-0007/33 | G06T-0019/20 | G06F-2203/04803 | G06T-2200/24 | G06T-2207/10088 | G06T-2207/30041 | G06T-2210/41 | G06T-2219/2004","A61B-003/00","A61B-003/00 | G06T-017/00 | G06T-007/33 | A61B-003/10 | A61B-003/12 | A61B-005/055 | G06F-003/0482 | G06T-019/20","","","","","","4924007005472"
"US","US","P","B2","System for displaying medical monitoring data","A patient monitoring hub can communicate bidirectionally with external devices such as a board-in-cable or a dongle. Medical data can be communicated from the patient monitoring hub to the external devices to cause the external devices to initiate actions. For example, an external device can perform calculations based on data received from the patient monitoring hub, or take other actions (for example, creating a new patient profile, resetting baseline values for algorithms, calibrating algorithms, etc.). The external device can also communicate display characteristics associated with its data to the monitoring hub. The monitoring hub can calculate a set of options for combined layouts corresponding to different external devices or parameters. A display option may be selected for arranging a display screen estate on the monitoring hub.","1. A patient monitoring hub connectable to a plurality of physiological monitors, each physiological monitor configured to be in communication with one or more physiological sensors, the patient monitoring hub comprising: a plurality of ports operable to be in communication with the plurality of physiological monitors;a display; anda hardware processor configured to: identify a plurality of parameters to be displayed by the patient monitoring hub based at least in part on the plurality of physiological monitors connected to the patient monitoring hub, wherein each of the plurality of physiological monitors is configured to measure at least one parameter of the identified plurality of parameters and includes at least one preprogrammed display layout, wherein the plurality of physiological monitors comprise different types of independent medical devices each including their own displays that are configured to display the at least one parameter on the respective displays, the preprogrammed display layouts of the different types of independent medical devices being different from one another;generate a set of self-configurable display layout options based on layout restrictions, wherein the layout restrictions are based on the plurality of physiological monitors connected to the patient monitoring hub and the at least one preprogrammed display layout of each of the plurality of physiological monitors connected to the patient monitoring hub;automatically populate a display layout manager on the display with the set of self-configurable display layout options;output the plurality of parameters on the display according to a user selection of one of the self-configurable display layout options or a default selection by the hardware processor of one of the self-configurable display layout options in an absence of the user selection;detect a change to the plurality of physiological monitors connected to the patient monitoring hub; andautomatically update the set of self-configurable display layout options and the display layout manager based at least in part on the change.","16","17/811994","2022-07-12","2023-0033122","2023-02-02","11901070","2024-02-13","MASIMO CORPORATION","Bilal Muhsin | Massi Joe E. Kiani | Peter Scott Housel | Ammar Al-Ali","","","","G16H-0040/63","G16H-0040/63 | A61B-0005/002 | A61B-0005/0205 | A61B-0005/743 | A61B-0005/744 | A61B-0005/7435 | A61B-0005/7445 | G06F-0021/84 | G08B-0021/02 | G16H-0040/60 | G16H-0050/30 | A61B-0005/746 | A61B-2560/045 | A61B-2562/222 | A61B-2562/225 | A61M-0005/14 | A61M-2205/18 | A61M-2205/3561 | A61M-2205/505 | A61M-2205/52 | G08B-0029/06 | G16H-0010/60","A61B-005/00","A61B-005/00 | G16H-040/63 | G16H-040/60 | A61B-005/0205 | G06F-021/84 | G16H-050/30 | G08B-021/02 | G16H-010/60 | A61M-005/14 | G08B-029/06","","","","","","4924007006006"
"US","US","P","B2","Adapting a device to a user based on user emotional state","A change in the emotional state of a user of a device is detected, and in response to this change a reason for the change in user emotional state is determined. This determination is made based on both the current user emotional state and context data for the device or user. The device then adapts to the user based on the current emotional state and the reason for the change in user emotional state. This adaptation of the computing device refers to an alteration of the operation of the computing device with a goal of increasing the likelihood of the user being in a good emotional state (e.g., happy, relaxed) and reducing the likelihood of the user being in a bad emotional state (e.g., sad, angry).","1. A method comprising: monitoring a user emotional state while a user is engaged with a computing device;detecting that the user emotional state has changed from a first detected emotional state to a current detected emotional state;identifying a context of the computing device based at least in part on environment data that describes an environment in which the computing device is physically present;determining, based on the context of the computing device, a reason for the change from the first detected emotional state to the current detected emotional state; andadapting the computing device to the user by altering operation of the computing device based on the current detected emotional state and the reason for the change from the first detected emotional state to the current detected emotional state.","20","16/861873","2020-04-29","2021-0344560","2021-11-04","11902091","2024-02-13","MOTOROLA MOBILITY LLC","Rachid M. Alameh | Zhengping Ji | Alvin Von Ruff","","","","H04L-0041/0816","H04L-0041/0816 | A61B-0005/165 | G06F-0003/011 | G06N-0005/04 | G06N-0020/00 | A61B-0005/024 | A61B-0005/1172 | G06F-2203/011","H04L-012/24","H04L-012/24 | G06F-003/01 | G06N-020/00 | H04L-041/0816 | A61B-005/16 | G06N-005/04 | A61B-005/1172 | A61B-005/024","","","","","","4924007007007"
"US","US","P","B2","Using an intraoral mirror with an integrated camera to record dental status, and applications thereof","Disclosed embodiments integrate a camera into an intraoral mirror. Integrating a camera into an intraoral mirror provides an efficient way to record and display what is visible to the healthcare provider in the mirror.","1. A method for retrieving data from a dental instrument using a remote computational device, comprising: (a) connecting the remote computational device to a network published by the dental instrument such that the dental instrument is acting as a router and the remote computational device connects to the router, the dental instrument comprising an intraoral mirror;(b) sending an HTTP request to the dental instrument from an application stored in a memory of the remote computational device; and(c) receiving a response to the HTTP request from an HTTP server implemented in the dental instrument, the response comprising data from the dental instrument.","20","17/889078","2022-08-16","2022-0409036","2022-12-29","11889991","2024-02-06","DENTAL SMARTMIRROR, INC.","Gidon Oded Elazar | Dan Zidkiahu Harkabi | Joshua Israel Wachspress | Yael Miriam Harkabi","","","","A61B-0001/247","A61B-0001/247 | A61B-0001/0004 | A61B-0001/00006 | A61B-0001/00009 | A61B-0001/00016 | A61B-0001/00032 | A61B-0001/00034 | A61B-0001/00045 | A61B-0001/00087 | A61B-0001/000095 | A61B-0001/00105 | A61B-0001/00195 | A61B-0001/045 | A61B-0001/05 | A61B-0001/06 | A61B-0001/063 | A61B-0001/0607 | A61B-0001/0655 | A61B-0001/0676 | A61B-0001/24 | A61B-0005/002 | A61B-0005/0022 | A61B-0005/0077 | A61B-0005/0079 | A61B-0005/0086 | A61B-0005/0088 | A61B-0005/061 | A61B-0005/065 | A61B-0005/067 | A61B-0005/1176 | A61B-0006/14 | A61C-0003/00 | A61C-0009/0073 | A61C-0013/0004 | A61C-0013/34 | A61C-0019/004 | A61C-0019/04 | F21V-0033/0068 | G02B-0027/144 | G06F-0003/023 | G06F-0003/0334 | G06T-0003/40 | G06T-0003/4038 | G06T-0003/60 | G06T-0005/009 | G06T-0007/0012 | G06T-0007/0016 | G06T-0007/11 | G06T-0017/20 | G06V-0040/166 | G16H-0020/40 | G16H-0030/20 | H04L-0067/01 | H04L-0067/02 | H04L-0067/025 | H04L-0067/12 | H04L-0067/125 | H04N-0007/183 | H04N-0023/51 | H04N-0023/54 | H04N-0023/55 | H04N-0023/56 | H04N-0023/57 | H04N-0023/60 | H04N-0023/65 | H04N-0023/698 | H04N-0023/74 | H04W-0012/73 | H05B-0045/00 | H05B-0045/22 | H05B-0047/105 | A61B-0001/0684 | A61B-2560/0456 | A61B-2576/02 | A61C-2204/005 | F21W-2131/202 | G06T-0007/337 | G06T-2200/32 | G06T-2207/10048 | G06T-2207/10116 | G06T-2207/20208 | G06T-2207/20212 | G06T-2207/30036 | G06T-2210/22 | G06T-2210/41 | G10L-0015/26 | G16H-0030/40 | G16H-0040/63 | H04N-0023/555","H04L-067/12","H04L-067/12 | A61B-001/247 | A61B-001/00 | A61B-001/05 | A61B-001/06 | A61B-005/06 | A61B-005/00 | A61B-001/045 | A61C-003/00 | A61B-001/24 | G16H-030/20 | G16H-020/40 | A61B-005/1171 | H05B-045/22 | H05B-045/00 | H05B-047/105 | G06V-040/16 | H04N-023/51 | H04N-023/54 | H04N-023/55 | H04N-023/56 | H04N-023/57 | H04N-023/60 | H04N-023/65 | H04N-023/74 | H04N-023/698 | A61B-006/14 | H04L-067/01 | H04L-067/02 | H04L-067/025 | H04L-067/125 | A61C-019/04 | H04W-012/73 | G06F-003/023 | G06F-003/033 | G06T-003/40 | G06T-003/60 | H04N-007/18 | F21V-033/00 | G02B-027/14 | G06T-007/00 | G06T-017/20 | A61C-013/15 | G06T-005/00 | G06T-007/11 | A61C-009/00 | A61C-013/00 | A61C-013/34 | G16H-040/63 | G10L-015/26 | G16H-030/40 | H04N-023/50 | F21W-131/202 | G06T-007/33","","","","","","4924006000917"
"US","US","P","B2","Method of creating a personal bioelectric ID (password) with hand movements of person and identification of behavioral biometric based person with EMG signals","A method?for biometric based person recognition systems is provided. The method provides an identification of a personalized bioelectric code and a personal ID code by identifying persons and gestures of a person with a benefit of behavioral biometric data of Electromyography (EMG) signals. The method includes the steps of: making the person wishing to create a password to wear a wristband, simultaneously recording of hand movements in eight bioelectric signals from eight EMG sensors in recordings of up to 10 seconds, repeating each selected movement type by the person at least ten times, clearing a recorded raw signal group from noise signals with a bandpass filter, separating a signal cleaned from the noise signals into to windows, creating a customized behavioral biometric data set with generated attributes for each transaction, obtaining the personalized bioelectrical code and the personal ID code.","1. A method of identification of a behavioral biometric based on a person by Electromyography (EMG) signals and creation of a personalized bioelectrical code and a personal ID code by hand movements of the person, comprising the steps of: simultaneously sensing, by eight EMG sensors, bioelectrical signals transmitted from a brain of the person to lower arm muscle groups of the person to provide the hand movements, wherein the bioelectrical signals are recorded in eight bioelectric signals from the eight EMG sensors in a wristband worn by the person with windows of up to 10 seconds, wherein the eight EMG sensors in the wristband have a signal sampling frequency of 200 Hz,repeating sensing and recording bioelectrical signals for each type of the hand movements selected by the person for at least ten times,clearing, from the recorded bioelectrical signals, a recorded raw signal group from noise signals with a bandpass filter,separating a signal cleared from the noise signals within time windows, wherein each time window of length R is intersected in steps of length r, with R=256 ms and r=32 ms,creating a customized behavioral biometric data set with generated attributes for each transaction, andobtaining the personalized bioelectrical code and the personal ID code in response to the creating the customized behavioral biometric data set, wherein said personalized bioelectrical code and the personal ID code are obtained using the customized behavioral biometric data set.","1","17/286831","2019-11-01","2021-0334567","2021-10-28","11893820","2024-02-06","FIRAT UNIVERSITESI REKTORLUGU","Beyda Tasar | Arif Gulten | Oguz Yakut","2018/16762","TR","2018-11-07","G06V-0040/10","G06V-0040/10 | A61B-0005/389 | G06F-0003/015 | G06F-0021/32 | G06V-0040/28 | G06V-0040/15","G06K-009/00","G06K-009/00 | G06F-003/01 | G06F-021/32 | G06V-040/10 | G06V-040/20 | A61B-005/389","","","","","","4924006004719"
"US","US","P","B2","Fault-tolerant grid frequency measurement algorithm during transients","A system determines the frequency of grid signals corresponding to an electrical grid in real time. The system includes a transient detector that monitors a grid signal from a voltage meter or a current meter connected to the electrical grid. The system produces, in real time and at a sampling rate, a deviation signal indicative of a periodicity of the monitored grid signal. The system determines, over one or more cycles of the monitored grid signal, a measurement signal corresponding to the deviation signal. The system determines a frequency signal that corresponds a frequency estimation of the monitored signal by applying a frequency estimation when values of the measurement signal are less than a deviation threshold and maintaining the frequency signal at a constant value when values of the measured signal equal or exceeds the deviation threshold.","1. An apparatus that determines in real time frequency of grid signals x(t) corresponding to an electrical grid comprising: a transient detector configured to: monitor a signal indicative of a grid signal x(k) from a voltage meter or current meter connected to the electrical grid, where the grid signal is expected to include a periodic signal with a nominal frequency fo and a sampling rate fs;produce, in real time and at the sampling rate of the monitored signal, a deviation signal Xdev indicative of whether or not the monitored signal includes the periodic signal; anddetermine, over one or more cycles, a measurement signal corresponding to the deviation signal; anda frequency estimator communicatively linked with the transient detector, the frequency estimator configured to:determine a frequency signal fe that corresponds to a frequency estimation of the monitored signal while values of the measurement signal are less than a deviation threshold; andmaintain the frequency signal at a constant value while values of the measured signal equal or exceed the deviation threshold;where the transient detector is configured to transmit a flag to the frequency estimator indicating that the monitored signal:comprises a periodic signal when values of the measurement signal are less than a deviation threshold expressed as γ<thr, where γ comprises a measurement of the deviation signal and thr comprises the deviation threshold; andexhibits a transient when values of the measurement signal equal or exceed the deviation threshold expressed as γ≥thr.","10","17/204494","2021-03-17","2021-0328431","2021-10-21","11882889","2024-01-30","UNIVERSITY OF TENNESSEE RESEARCH FOUNDATION | UT-BATTELLE, LLC","Lingwei Zhan | Thomas J. King, Jr. | Fuhua Li | Yilu Liu | Wenxuan Yao | He Yin | Bailu Xiao","","","","A41D-0013/1184","A41D-0013/1184 | A61B-0005/01 | A61F-0009/045 | H02J-0003/003 | H02J-0003/004 | H02J-0003/144 | H02J-0003/46 | A42B-0003/225 | G06F-0017/14","A41D-013/11","A41D-013/11 | A61B-005/01 | A61F-009/04 | H02J-003/00 | H02J-003/14 | H02J-003/46 | A42B-003/22 | G06F-017/14","","","","","","4924005001053"
"US","US","P","B2","Motion capturing garments and system and method for motion capture using jeans and other garments","A real-time motion capture system and garment includes a wearable activity monitor that may be a pair of denim jeans. The wearable activity monitor includes multiple sensors such as accelerometers, gyrometers, magnetometers disposed within the seams of the garment. A microprocessor and wireless transmitter) communicate the motion data to an external device. The microprocessor and wireless transmitter may be included within one of the seams. An elastically stretchable ribbon or a flexible ribbon such as a kapton ribbon or a ribbon formed of textile, electrically couples the sensors and microprocessor and is also disposed inside the seams and the components within the seam are coated with a waterproof coating. The external device can store the data or display and analyze the data real-time, and may communicate the data to a further electronic device.","1. A motion capture circuit comprising a ribbon of a flexible material, a plurality of electrical connections forming a communication bus, a plurality of inertial measurement units (IMU'ss) electrically coupled to one another by said electrical connections along said ribbon, a microcontroller electrically connected with said plurality of IMU'ss, and a wireless transmitter adapted to wirelessly transmit data from said IMU'ss to at least one external data receiving and processing device, said wireless transmitter further comprises a wireless receiver capable of receiving data from said at least one external data receiving and processing device, said at least one external data receiving and processing device adapted to control at least one further electronic device based upon said data, said further electronic device being chosen from a gaming device, a home or automotive appliance, a computer game, a video game console or any other applications of an electronic device capable of receiving a signal, wherein said ribbin is elastically stretchable by at least 10%, said plurality of electrical connections being arranged along said ribbon in a crooked path, wherein said ribbon is formed of a textile comprising stretchable yarns coupled to the electrical connections by weaving, wherein the motion capture circuit is configured to be contained within a seam of a pair of pants, said sems including for each pant leg of said pair of pants an outer lateral seam extending in a longitudinal direction and/or an inner medial seam extending in said longitudinal direction, each of said outer lateral seams and/or each of said inner medial seams including at least one of said plurality of IMU'ss disposed along a pant leg portion corresponding to the human wearer'ss femur and at least one of said plurality of IMU'ss disposeed along a pant leg portion corespondint to the human wearer'ss tibia and biula, and parallel to the wear'ss tibia and fibula, wherein said processor is configured for analyzing motion data regarding angle, yaw, pitch, location and acceleration of the human wearer'ss femurs and said human wearer'ss tibias and fibulas.","6","15/654789","2017-07-20","2018-0024622","2018-01-25","11886627","2024-01-30","SANKO TEKSTIL ISLETMELERI SAN. VE TIC. A.S.","Ozgur Cobanoglu | Jitka Eryilmaz | Serkan Mert | Fehim Caglar","2016-180659","EP","2016-07-21","G06F-0003/011","G06F-0003/011 | A41D-0001/005 | A41D-0001/06 | A61B-0005/112 | A61B-0005/1118 | A61B-0005/1121 | A61B-0005/6804 | H04W-0004/025 | A61B-2562/028 | A61B-2562/0219 | A61B-2562/043 | G06F-0003/0346 | G06T-0007/20","G06F-003/01","G06F-003/01 | A61B-005/11 | A41D-001/06 | A41D-001/00 | A61B-005/00 | G06F-003/0346 | H04W-004/02 | G06T-007/20","","","","","","4924005004753"
"US","US","P","B2","Eye movement detecting device, electronic device and system","According to one embodiment, an eye movement detecting device comprises first, second, third, fourth and fifth electrodes. A line connecting the first and the third electrodes passes through the right eye and a line connecting the second and the fourth electrodes passes through the left eye on at least one of a front view, a plan view or a side view. A distance between the fifth and the first electrodes is equal to a distance between the fifth and the second electrodes. A distance between the fifth and the third electrodes is equal to a distance between the fifth and the fourth electrodes. The detector respectively detects a horizontal movement of the right eye and a horizontal movement of the left eye.","1. A wearable device comprising: a detector configured to detect a right eye movement of a right eye of a user wearing the wearable device and a left eye movement of a left eye of the user, the detector further detecting a convergence angle at which a line of sight of the right eye and a line of sight of the left eye of the user intersect based on the right eye movement and the left eye movement; anda display configured to display an augmented reality image based on the convergence angle detected by the detector, whereinthe detector is configured to detect a first convergence angle when the user looks at a first object at a first distance,the detector is configured to detect a second convergence angle when the uses looks at a second object at a second distance,the first distance is shorter than the second distance,the first convergence angle is wider than the second convergence angle,the augmented reality image comprises first, second, and third pages,the second page succeeds the first page,the third page is previous to the first page,the display displays the first page when the detector detects the second convergence angle,the display displays one of the second page and the third page when the detector detects that the line of sight of the right eye and the line of sight of the left eye move to right,the display displays another one of the second page and the third page when the detector detects that the line of sight of the right eye and the line of sight of the left eye move to left, andthe display turns off the augmented reality image when the detector detects the first convergence angle.","4","17/940058","2022-09-08","2023-0004221","2023-01-05","11886635","2024-01-30","Kabushiki Kaisha Toshiba","Hiroaki Komaki","2018-051471","JP","2018-03-19","G06F-0003/013","G06F-0003/013 | A61B-0005/398 | H04N-0013/344 | A61B-2562/0209 | G02B-0027/0172 | G02B-2027/0134 | G02B-2027/0178 | G06Q-0010/087","G06F-003/01","G06F-003/01 | H04N-013/344 | A61B-005/398 | G06Q-010/087 | G02B-027/01","","","","","","4924005004761"
"US","US","P","B2","Generating augmented reality images using sensor and location data","Embodiments relate to using sensor data and location data from a device to generate augmented reality images. A mobile device pose can be determined (a geographic position, direction and a three dimensional orientation of the device) within a location. A type of destination in the location can be identified and multiple destinations can be identified, with the mobile device receiving queue information about the identified destinations from a server. A first image can be captured. Based on the queue information, one of the identified destinations can be selected. The geographic position of each identified destination can be identified, and these positions can be combined with the mobile device pose to generate a second image. Finally, an augmented reality image can be generated by combining the first image and the second image, the augmented reality image identifying the selected one destination.","1. A computer-implemented method for displaying images received from a plurality of mobile devices in an event, comprising: estimating location of each of the plurality of mobile devices present inside a bounded geographical area, wherein a bounded geographical area is associated with a venue of the event;selecting an activity associated with the event;estimating location of the activity associated with the event;determining ambient characteristics associated with the location of the activity;analyzing the ambient characteristics associated with the location of the activity with respect to the location of each of the plurality of mobile devices and type of each of the plurality of mobile devices;generating a respective capture setting for each of the plurality of mobile devices based on a result of the analyzing of the ambient characteristics associated with the location of the activity;controlling each of the plurality of mobile devices to capture an image based on the respective capture settings to produce multiple images;receiving the multiple images from the plurality of mobile devices;combining the multiple images from the plurality of mobile devices to generate a mass image; andtransmitting the mass image to an external interface.","21","18/083426","2022-12-16","2023-0211090","2023-07-06","11887264","2024-01-30","Live Nation Entertainment, Inc.","James Paul Callaghan","","","","G06T-0019/006","G06T-0019/006 | A61M-0005/20 | A61M-0005/31595 | G06F-0003/0346 | G06Q-0010/02 | G06Q-0030/06 | G06Q-0030/0643 | G06T-0007/70 | A61M-2005/004 | G06T-0001/00","G06T-019/00","G06T-019/00 | G06F-003/0346 | G06T-007/70 | G06Q-010/02 | G06Q-030/06 | G06Q-030/0601 | A61M-005/20 | A61M-005/315 | A61M-005/00 | G06T-001/00","","","","","","4924005005386"
"US","US","P","B2","System for analyzing vascular refill during short-pulse ultrafiltration in hemodialysis","A method includes: receiving measurements of a blood-related parameter corresponding to a patient undergoing hemodialysis; estimating a value of one or more hemodialysis treatment-related parameters by applying a vascular refill model based on the received measurements of the blood-related parameter, wherein the one or more hemodialysis treatment-related parameters are indicative of an effect of vascular refill on the patient caused by the hemodialysis; determining, based on the one or more estimated values of the one or more hemodialysis treatment-related parameters, a hemodialysis treatment-related operation; and facilitating performance of the treatment-related operation. The vascular refill model is a two-compartment model based on a first compartment corresponding to blood plasma in the patient's body, a second compartment based on interstitial fluid in the patient's body, and a semi-permeable membrane separating the first compartment and the second compartment.","1. A system, comprising: a dialysis machine configured to provide hemodialysis treatment to a patient;a monitoring device configured to obtain hematocrit measurements corresponding to blood of the patient during the hemodialysis treatment; anda processing system configured to: receive the hematocrit measurements;determine an initial set of parameter values to be used with a two-compartment vascular refill model;estimate values of one or more parameters for the patient based on the hematocrit measurements, the initial set of parameter values, and the two-compartment vascular refill model; andoutput the estimated values for the patient;wherein estimating the values of the one or more parameters for the patient based on the hematocrit measurements, the initial set of parameter values, and the two-compartment vascular refill model further comprises: performing a parameter identification;solving model equations using the initial set of parameter values to determine whether the two-compartment vascular refill model fits the hematocrit measurements; andin response to determining that the two-compartment vascular refill model does not fit the hematocrit measurements, modifying the initial set of parameter values.","21","16/737563","2020-01-08","2020-0139034","2020-05-07","11878098","2024-01-23","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Aurelio A. de los Reyes, V | Doris H. Fuertinger | Franz Kappel | Anna Meyring-Wosten | Stephan Thijssen | Peter Kotanko","","","","A61M-0001/1613","A61M-0001/1613 | A61B-0005/026 | A61B-0005/02028 | A61B-0005/14535 | A61B-0005/4836 | A61B-0005/4875 | A61M-0001/3609 | G06F-0017/13 | G16H-0050/50 | A61M-2205/18 | A61M-2205/3306 | A61M-2205/3334 | A61M-2205/3344 | A61M-2205/3379 | A61M-2205/3553 | A61M-2205/3584 | A61M-2205/50 | A61M-2205/502 | A61M-2230/005 | A61M-2230/20 | A61M-2230/207 | A61M-2230/30","A61M-001/16","A61M-001/16 | A61B-005/00 | A61B-005/026 | A61B-005/02 | A61M-001/36 | A61B-005/145 | G16H-050/50 | G06F-017/13","","","","","","4924004001313"
"US","US","P","B2","User experience computing system for gathering and processing user experience information","Enterprise organizations may use observational data to gather information about user experiences with their products and tools. For instance, patients with kidney failure may undergo dialysis treatment in order to remove toxins and excess fluids from their blood. The dialysis treatment may be performed at a hospital or clinic, or in a user's home, and the enterprise organization may use gathered information to gain understanding of user experiences with their dialysis machines and services. A user experience computing system gathers and processes user experience information from across the enterprise organization. Using stored observation data (e.g., surveys, studies etc.) in its smallest common form, the computing system may use this data as building blocks for creating more complex data objects (e.g. journey matrices and/or empathy gardens) using inputs from multiple different sources, and to facilitate presenting this information in an effective and empathetic way to product developers.","1. A method, comprising: receiving, by a user experience (UX) computing system, observation data in a plurality of non-standardized data formats, wherein the observation data comprises one or more surveys describing user experiences associated with a plurality of actors interacting with a product or service associated with an enterprise organization;converting, by the UX computing system, the observation data from the plurality of non-standardized data formats into one or more standardized data formats using a plurality of classification identifiers, wherein the plurality of classification identifiers comprises a section classification identifier associated with a plurality of section data elements, a story classification identifier associated with a plurality of story data elements, and a moment classification identifier associated with a plurality of moment data elements, wherein converting the observation data into the one or more standardized data formats comprises: breaking down the observation data into the plurality of section data elements, wherein the plurality of section data elements comprises a plurality of structured and unstructured section data elements;breaking down the plurality of unstructured section data elements into a plurality of first story data elements using artificial intelligence (AI) algorithms;breaking down the plurality of structured section data elements into a plurality of second story data elements without using AI algorithms, wherein the plurality of story data elements comprise the plurality of first story data elements and the plurality of second story data elements, and wherein each of the plurality of story data elements is associated with a user experience of an actor interacting with the product or service at a plurality of different instances in time; andbreaking down each of the plurality of story data elements into the plurality of moment data elements, wherein each of the plurality of moment data elements is associated with a plurality of variables at a particular instance in time from the plurality of different instances in time;receiving, by the UX computing system, a request indicating a particular actor model, wherein the particular actor model represents a subset of the plurality of actors;generating, by the UX computing system, the particular actor model based on the request and the converted observation data, wherein generating the particular actor model is based on: determining a plurality of binary values from the converted observation data;calculating an n-dimensional distance between the plurality of actors identified in the observation data based on the plurality of binary values; andclustering a subset of the plurality of actors together based on the request and using a clustering algorithm;generating, by the UX computing system, a journey matrix for the particular actor model based on the converted observation data, wherein the journey matrix indicates a plurality of sequential moments describing a nephrologist throughout a dialysis treatment for a patient;causing display of the journey matrix;receiving, by the UX computing system, operator input indicating a request for original data associated with one or more moments from the journey matrix; andbased on the operator input, providing, by the UX computing system, at least a portion of the one or more surveys from the observation data, wherein the portion of the one or more surveys comprises quotes or recording samples associated with the one or more moments from the journey matrix.","14","17/078989","2020-10-23","2022-0129917","2022-04-28","11875365","2024-01-16","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Daniel L. Sloat | Beth Kun | Brittany R. Aube | Benjamin N. Davies | Derek Merrikin","","","","G06Q-0030/0201","G06Q-0030/0201 | G06F-0016/258 | G06F-0016/285 | G06N-0020/00 | G06Q-0010/067 | A61M-0001/14 | G06F-0040/20 | G16H-0010/20 | G16H-0020/40 | G16H-0040/20","G06Q-030/0201","G06Q-030/0201 | G06F-016/28 | G06F-016/25 | G06Q-010/067 | G06N-020/00 | G16H-020/40 | G16H-040/20 | G16H-010/20 | G06F-040/20 | A61M-001/14","","","","","","4924003004401"
"US","US","P","B2","Reusable respiratory device monitoring system","A respiratory system and method comprise a tracker module adaptable to be secured to a variety of inhalers, the tracker module sensing activation of the medication canister of the inhaler for delivery of medication to a user. The tracker module also senses the rate of inhalation air flow of the user when inhaling medication for determination of proper inhaler use. Upstream and downstream sensors provide flow information to determine quality of the inhalation. Other sensors are provided that monitor user presence at the inhaler, user technique in using the inhaler, and the attitude of the inhaler when it was used. Low power devices are used to conserve battery power.","1. A respiratory device monitoring system for monitoring the use of an inhaler, the inhaler having a hollow inhaler body that is L-shaped and which includes a mouthpiece section at a first end of the inhaler body and an opening at a second end of the inhaler body with an opening diameter that is larger than an outer diameter of a canister thereby accepting a canister in the inhaler body, the canister containing an inhaler medication that is actuated by pressing a top of the canister to move the canister inwards into the inhaler body to provide a medication dose, wherein the length of the inhaler body is selected such that the canister top and a length of the canister adjoining the canister top protrude from the inhaler body opening, the inhaler body further including an internal inhaled-air passage located from the inhaler body opening and extending through the mouthpiece, wherein the inhaled-air passage is located in a space between the inhaler body and a canister mounted in the inhaler body, wherein the inhaler is configured so that both the inhaler medication and the inhaled-air passage are connected to the mouthpiece at a point of convergence whereby a user who inhales through the mouthpiece will inhale both the medication dose from a canister and air through the inhaled-air passage, the monitoring system comprising: a tracking module comprising a shell that has a shell body mounted around the inhaler body between the mouthpiece and the inhaler body opening, the shell not covering the inhaler body opening, the shell having a tracking module processor to which are connected a tracking module non-transient memory, and a tracking module communications component, the shell also including a tracking module battery, wherein the battery is connected to provide electrical power to the processor, the memory, and the communications component;wherein the shell further includes a dose sensor connected to the shell and extending to the top of the canister to sense pressure applied to the top of the canister to actuate the canister to provide a medication dose through the mouthpiece of the inhaler, the dose sensor providing a dose signal when it has sensed said actuation pressure, wherein the dose sensor does not cover the opening of the inhaler body;wherein the tracking module processor is in communication with the dose sensor and is programmed to receive a dose signal from the dose sensor, and to store the received dose signal in the tracking module memory;wherein the shell further comprises an extension portion to which is mounted an air flow sensor wherein the extension portion has a length so that the air flow sensor is located within the inhaled-air passage of the inhaler body, wherein the air flow sensor detects a flow of air in the inhaled-air passage when a user inhales through the inhaler for a medication dose, the air flow sensor providing air flow data in response to detecting air flow drawn through the inhaled-air passage when a user of the inhaler inhales;wherein the tracking module processor is in communication with the air flow sensor and is programmed to receive the inhaled-air data from said sensor and to store the inhaled-air data in the non-transient memory; andan application program stored in a local device that is in communication with the tracking module communications component, the application program configured to program the local device to communicate with the tracking module processor to request stored dose data and inhaled-air data to be transmitted to the local device, wherein the application program further programs the local device to receive the transmitted dose data and inhaled-air data.","30","17/821289","2022-08-22","2023-0005585","2023-01-05","11875886","2024-01-16","APTARGROUP, INC.","Melissa P. Manice | Joseph A. Condurso, III | Houston A. Brown | Francis T. Rodriguez | Daniel Z. Glazerman","","","","G16H-0020/13","G16H-0020/13 | A61M-0015/008 | A61M-0015/0083 | G09B-0019/00 | G16H-0040/63 | A61B-0005/087 | A61M-0015/009 | A61M-0015/0051 | A61M-2205/18 | A61M-2205/332 | A61M-2205/3334 | A61M-2205/3553 | A61M-2205/3561 | A61M-2205/3569 | A61M-2205/3584 | A61M-2205/3592 | A61M-2205/505 | A61M-2205/52 | A61M-2205/581 | A61M-2205/582 | A61M-2205/60 | A61M-2205/8212 | A61M-2230/40 | G06Q-0010/087 | G06Q-0050/22","G16H-020/13","G16H-020/13 | A61M-015/00 | G16H-040/63 | A61B-005/087 | G09B-019/00 | G06Q-010/087 | G06Q-050/22","","","","","","4924003004917"
"US","US","P","B2","Health testing and diagnostics platform","Systems and methods for providing a universal platform for at-home health testing and diagnostics are provided herein. In particular, a health testing and diagnostic platform is provided to connect medical providers with patients and to generate a unique, private testing environment. In some embodiments, the testing environment may facilitate administration of a medical test to a patient with the guidance of a proctor. In some embodiments, the patient may be provided with step-by-step instructions for test administration by the proctor within a testing environment. The platform may display unique, dynamic testing interfaces to the patient and proctor to ensure proper testing protocols and accurate test result verification.","1. A computer-implemented system for a proctored examination platform for a medical diagnostic test, the computer-implemented system comprising an electronic storage comprising computer-executable instructions and one or more processors in electronic communication with the electronic storage medium and configured to execute the computer-executable instructions in order to: receive, by the computing system, a request for a proctored examination session for a medical diagnostic test, wherein the request is received from a user device of a user and includes information associated with one or more past interactions of the user with the proctored examination platform;determine, by the computer system, that the user is eligible for an expedited proctored examination session based on: the information associated with the one or more past interactions of the user with the proctored examination platform; anda determination of a high probability of the user performing the medical diagnostic test accurately with reduced supervision, wherein the expedited proctored examination session comprises less proctor supervision than a non-expedited proctored examination session;based on the determination that the user is eligible for an expedited proctored examination session, automatically select, by the computing system, a proctor from among a plurality of available proctors, wherein the proctor is automatically selected based on a type of the medical diagnostic test and the information associated with one or more past interactions of the user; andestablish, by the computing system, the expedited proctored examination session between the user device of the user and a proctor device of the proctor, wherein the expedited proctored examination session comprises at least a one-way video conference session.","16","17/808365","2022-06-23","2023-0049589","2023-02-16","11875896","2024-01-16","EMED LABS, LLC","Michael W. Ferro, Jr. | Sam Miller | Marco Magistri | Colman Thomas Bryant | Adam Charles Carlson | Zachary Carl Nienstedt | Chris Ensey","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/7465 | G06F-0003/0482 | G06Q-0050/205 | G06T-0007/70 | G06V-0020/41 | G09B-0019/003 | G16H-0010/40 | G16H-0030/40 | G16H-0040/20 | G16H-0050/20 | G16H-0080/00 | H04L-0065/1069 | H04L-0065/403 | H04N-0007/15 | G06T-2207/10016","G16H-040/67","G16H-040/67 | G16H-040/20 | G06Q-050/20 | G16H-010/40 | G16H-080/00 | H04L-065/1069 | H04L-065/403 | G06F-003/0482 | H04N-007/15 | G06T-007/70 | G09B-019/00 | A61B-005/00 | G16H-050/20 | G06V-020/40 | G16H-030/40","","","","","","4924003004927"
"US","US","P","B2","Transmitting treatment information","A system includes a first computing device comprising a processor coupled to a memory. The processor and the memory are configured to receive at least one of (i) information indicative of treatment of a victim by a first caregiver using the first computing device and (ii) information indicative of a health status of the victim; determine that treatment of the victim by the first caregiver using the first computing device is completed; and transmit the received information to a second computing device.","1. A defibrillating system for providing defibrillation treatment to a patient at a rescue scene, the defibrillating system comprising: electrode pads;a first computing device accessible by a first caregiver located at the rescue scene and configured to transmit data to a second computing device accessible by a second caregiver located at the rescue scene;at least one processor configured to control operation of the electrode pads; anda memory storing instructions that, when executed by the at least one processor, cause the at least one processor to perform operations comprising: receiving cardiac data indicative of a cardiac rhythm in a patient, the cardiac data representing an electrocardiogram (ECG) signal;determining, based on the cardiac rhythm indicated in the cardiac data, whether the ECG signal is indicative of a shockable rhythm or a non-shockable rhythm;identifying a portion of the ECG signal that is associated with a defibrillation treatment for being delivered to the patient having the shockable rhythm;causing the defibrillation treatment to be delivered to the patient via the electrode pads;receiving chest compression data, and health status data of the patient, the health status data comprising at least a representation of the identified portion of the ECG signal associated with the defibrillation treatment; andcausing the first computing device accessible by the first caregiver located at the rescue scene to transmit to the second computing device accessible by the second caregiver located at the rescue scene, either during treatment of the patient or when treatment of the patient is completed, treatment data comprising at least one of: the chest compression data, or the health status data.","26","17/072327","2020-10-16","2021-0204813","2021-07-08","11864859","2024-01-09","ZOLL MEDICAL CORPORATION","John Amann | Gary A. Freeman","","","","A61B-0005/0006","A61B-0005/0006 | A61B-0005/316 | A61B-0005/332 | A61N-0001/025 | A61N-0001/3904 | A61N-0001/3931 | A61N-0001/3937 | A61N-0001/39044 | A61N-0001/3968 | A61N-0001/3987 | A61N-0001/3993 | G16Z-0099/00 | A61B-0005/053 | A61B-0005/4848 | G06Q-0050/22 | G16H-0020/40 | G16H-0040/67","A61N-001/39","A61N-001/39 | A61B-005/00 | A61B-005/316 | A61B-005/332 | G16Z-099/00 | A61N-001/02 | A61B-005/053 | G16H-020/40 | G16H-040/67 | G06Q-050/22","","","","","","4924002001093"
"US","US","P","B2","Devices, systems, methods and assemblies for medical electrodes","A device including at least one medical electrode; a radio-frequency (RF) tag with an identifier (ID) that identifies the at least one medical electrode; and at least one substrate that attaches the at least one medical electrode to the RF tag.","1. A system including: at least one medical electrode;a radio-frequency (RF) tag with an identifier (ID) that identifies the at least one medical electrode; andat least one substrate that attaches the at least one medical electrode to the RF tags;a medical device having a device identifier (ID) that identifies the medical device; anda radio-frequency (RF) reader that operates with the medical device, the RF tag being readable by the RF reader,wherein use of the medical electrode with the medical device is controlled based on a control signal determined from a combination of the ID and the device ID.","19","17/424129","2020-01-24","2022-0096825","2022-03-31","11865335","2024-01-09","BAYMATOB PTY LTD","Sarah Catherine McDonald | Rishi Ramakrishnan","2019-900236 | 2019-900237","AU | AU","2019-01-25 | 2019-01-25","A61N-0001/08","A61N-0001/08 | A61B-0005/24 | A61B-0005/25 | G06K-0007/10297 | G06K-0019/0723 | A61B-2562/08","A61N-001/08","A61N-001/08 | A61B-005/25 | G06K-007/10 | G06K-019/07 | A61B-005/24","","","","","","4924002001567"
"US","US","P","B2","Runtime adaptive risk assessment and automated mitigation","A security framework for life-critical and safety-critical devices, specifically medical devices, using: a) runtime, adaptive methods that dynamically assess the risk of newly discovered vulnerabilities and threats, and b) automatic mitigation methods that reduce system risk by seamlessly reconfiguring the device to operate within different execution modes. This technology automatically isolates threats by disabling affected system components. A multi-modal software design uses adaptive software in which operational modes have monotonically decreasing cumulative risk. Formal risk models are used to model the individual risk of accessing or controlling system components and to automatically calculate the cumulative risk of software modes. The automated detection of potential threats by the system or reporting of known vulnerabilities will dynamically change the system risk. To support an accurate and fine grained adaptive risk model, novel statistical methods non-intrusively detect potential threats, isolate the threat to a specific component, and estimate the threat probability.","1. A system for detecting malware in a device, said system comprising: said device having a computer processor, wherein the device is able to be connected to a network or external computer system; anda module implemented on the computer processor able to model normal system behavior of the device, compare current system operation to the modeled normal system behavior, and estimate a probability of the current system operation being affected by malware based on performance deviation between the current system operation and the modeled normal system behavior,wherein the compared current system operation comprises execution times of one or more operations performed by the device, and wherein estimating the probability of the current system operation being affected by malware comprises determining a number of execution times that fall outside predefined upper and lower timing boundaries in the modeled normal system behavior for the performed operations.","24","17/290627","2019-11-01","2022-0035927","2022-02-03","11868479","2024-01-09","JOHANNES KEPLER UNIVERSITY LINZ | ARIZONA BOARD OF REGENTS ON BEHALF OF THE UNIVERSITY OF ARIZONA","Roman Lysecky | Jerzy Rozenblit | Johannes Sametinger | Aakarsh Rao | Nadir Carreon","","","","G06F-0021/577","G06F-0021/577 | A61B-0005/0022 | A61B-0005/0031 | A61N-0001/362 | A61N-0001/37223 | G06F-0021/566 | G16H-0020/17 | A61M-0005/14276 | G06F-2221/034","G06F-021/57","G06F-021/57 | G16H-020/17 | A61B-005/00 | A61N-001/362 | A61N-001/372 | G06F-021/56 | A61M-005/142","","","","","","4924002004699"
"US","US","P","B1","System, method and apparatus for wearable computing","A system and method for analyzing wearable sensor data in reconciliation with calendar entry data. A computer processor is coupled to a user configurable calendar system and a user wearable device. The processor detects one or more user activity events from one or more electronic wearable devices associated with the user and detects one or more user calendar events from a user configurable calendar system. The processor is configured to determine a course of action to be taken by the user based upon the one or more calendar events and the detected one or more user events.","1. A system for analyzing wearable sensor data, comprising: a computer device coupled to one or more electronic wearable devices associated with a user wherein the computer device includes: a memory;a processor disposed in communication with the memory and configured to issue a plurality of instructions stored in the memory such that the processor:detects a user duress health condition from the one or more electronic wearable devices associated with the user;detects a user location from GPS information received from the one or more electronic wearable devices associated with the user and a location of an Automated Teller Machine (ATM) responsive to detection of user duress; andtransmit via a communication network, from the computer device, responsive to the detected user duress health condition from the one or more electronic wearable devices, a radio frequency electronic signal to the detected ATM the user is interacting with to change performance of the ATM triggered by detection of the user duress health condition from the one or more electronic wearable devices associated with the user.","9","14/941234","2015-11-13","","","11868968","2024-01-09","UNITED SERVICES AUTOMOBILE ASSOCIATION","Bharat Prasad | Wayne M. Hartman | Jonathan W. Barlow | Bradly J. Billman | Charles L. Oakes, III | Joshua D. Maldonado","","","","G06Q-0010/1095","G06Q-0010/1095 | A61B-0005/02438 | A61B-0005/1112 | A61B-0005/14532 | A61B-0005/16 | G09B-0019/0092","G06Q-020/10","G06Q-020/10 | G06Q-010/1093 | A61B-005/024 | A61B-005/16 | A61B-005/145 | A61B-005/11 | G09B-019/00","","","","","","4924002005185"
"US","US","P","B2","System for supply chain management","A system for tracking a product from origin to destination is disclosed. The system includes a probe that comprises two plates, a power source and a processor. The power source is controlled by the processor to produce an oscillating output at the plates. Using the oscillating voltage, the probe interrogates a device through capacitive coupling. The device includes a control unit, a memory unit, and first and second materials physically associated with the device for communication using capacitive coupling. Information associated with the device is transferred from the device to the probe through capacitive coupling between the first and second materials and the first and second plates, respectively.","1. A method for testing an ingestible device using an external probing apparatus, the ingestible device comprising a substrate having a control unit and a memory, and first and second materials physically associate with the substrate, the external probing apparatus comprising first and second probing plates, the method comprising: powering the first and second probing plates;capacitively coupling the first probing plate to the first material of the ingestible device;capacitively coupling the second probing plate to the second material of the ingestible device;energizing the ingestible device by transferring energy from the first and second probing plates to the corresponding first and second materials;receiving information stored in the memory of the ingestible device encoded by the control unit, through the first and second probing plates; andvalidating a functionality of the ingestible device.","20","17/577869","2022-01-18","2022-0311474","2022-09-29","11870508","2024-01-09","OTSUKA PHARMACEUTICAL CO., LTD.","Mark J. Zdeblick","","","","H04B-0005/0012","H04B-0005/0012 | A61B-0005/061 | G06K-0007/01 | G06Q-0010/06","H04B-005/00","H04B-005/00 | G06Q-010/06 | G06K-007/01 | A61B-005/06","","","","","","4924002006716"
"US","US","P","B2","System and method for authenticating wireless programming system and method for authenticating wireless programming devices in programmable medical systems","A medical device of a medical system is configured for communicating with an external programmer over a wireless communications link. The medical device comprises a wireless communications module configured for receiving a first unencrypted version of a random number and a first encrypted version of the random number from the external programmer over the wireless communications link. The medical device further comprises control circuitry configured for performing an authentication procedure on the external programmer based on the first unencrypted version of the random number and the first encrypted version of the random number, and preventing the external programmer from commanding the medical device to perform an action unless the authentication procedure is successful.","1. A method of communicating between a first device and a second device of a system over a wireless communications link, the method comprising: receiving a trigger signal in response to a user action;establishing a wireless communications link between the first device and the second device in response to the trigger signal;initiating an authentication procedure on the second device over the established wireless communication link;receiving a first unencrypted version of a random number and a first encrypted version of the random number from the second device over the wireless communications link; andperforming an authentication procedure on the second device based on the first unencrypted version of the random number and the first encrypted version of the random number; andpreventing the second device from commanding the first device to perform an action if the authentication procedure is not completed within a predetermined period of time after the wireless communication link has been established between the first device and the second device or if the authentication procedure fails within the predetermined period of time.","22","17/664411","2022-05-21","2022-0286849","2022-09-08","11871224","2024-01-09","THE ALFRED E. MANN FOUNDATION FOR SCIENTIFIC RESEARCH","Saul Rodriguez | Dianna (Dan) Han | Emil Istoc","","","","H04W-0012/06","H04W-0012/06 | A61B-0005/0024 | A61B-0005/0031 | A61F-0002/72 | A61N-0001/37252 | G16H-0040/40 | H04L-0009/3271 | H04L-0063/061 | H04L-0063/0869 | H04L-0067/12 | H04L-0067/141 | H04L-0067/53 | H04L-0069/40 | H04Q-0009/00 | H04W-0012/04 | H04W-0012/50 | A61B-0005/0004 | G06F-0021/6245 | G08C-2201/60 | H04L-0063/0428 | H04L-0063/06 | H04L-2209/80 | H04L-2209/88 | H04Q-2209/43","H04L-029/06","H04L-029/06 | H04W-012/06 | H04L-009/32 | H04L-067/12 | H04L-009/40 | H04L-069/40 | A61N-001/372 | H04L-067/141 | H04W-012/50 | H04L-067/53 | G16H-040/40 | A61B-005/00 | A61F-002/72 | H04Q-009/00 | H04W-012/04 | G06F-021/62","","","","","","4924002007425"
"US","US","P","B2","Roboticized surgery system with improved control","A robotized surgery system comprises at least one robot arm which acts under the control of a control console intended for the surgeon. The console comprises an eye tracking system for detecting the direction of the surgeon's gaze and for entering commands depending on the directions of the gaze detected. The console comprises advantageously a screen with at least one zone for viewing the operating field and, among the commands which can be performed depending on the gaze directions.","1. A method of using a surgical robotic system, comprising: providing a surgical system comprisinga first robotic arm carrying a surgical instrument;a second robotic arm carrying an endoscopic camera,a surgeon console includinga display displaying an image from the endoscopic camera;an eye tracking system, anda manipulator moveable by a user'ss hands, the manipulator further including a manually-actuatable auxiliary input;causing movement of the first robotic arm in response to movement of the manipulator,causing the system to enter an eye tracking mode in response to actuation of the auxiliary input on the manipulator, wherein, in the eye tracking mode, movement of the second robotic arm is commanded based on signals from the eye tracking system corresponding to an estimated direction of the user'ss gaze towards the image.","12","17/566596","2021-12-30","2022-0192765","2022-06-23","11857278","2024-01-02","ASENSUS SURGICAL ITALIA S.R.L.","Damien Brasset | Paolo Invernizzi | Emilio Ruiz Morales","MI2010-000579","IT","2010-04-07","A61B-0034/30","A61B-0034/30 | A61B-0017/00 | G06F-0003/013 | G06F-0003/0482 | G06F-0003/04842 | G06F-0003/04847 | G06V-0040/19 | A61B-0003/113 | A61B-0005/11 | A61B-0034/37 | A61B-0090/361 | A61B-2017/00216","A61B-034/30","A61B-034/30 | A61B-034/37 | G06F-003/01 | A61B-017/00 | G06V-040/19 | G06F-003/0482 | G06F-003/04842 | G06F-003/04847 | A61B-003/113 | A61B-005/11 | A61B-090/00","","","","","","4924001001289"
"US","US","P","B1","Authorized remote control","Some embodiments provide a vehicle navigation system which can navigate a vehicle through an environment based on driving commands received from a remote control system based on manual operator interaction with an interface of the remote control system. Remote driving control can be engaged based on determination, via processing vehicle sensor data, of a health emergency associated with one or more occupants of the vehicle, and the remote control system can generate remote driving commands which cause the vehicle to be navigated to a particular location without requiring the occupant associated with the health emergency to manually navigate the vehicle. The remote control system can monitor the occupant via communicated vehicle sensor data and can control remote control devices included in the vehicle to provide external indication that the vehicle is being navigated according to remote driving control.","1. An apparatus, comprising: a remote control system, remotely located from a vehicle, which is configured to selectively engage in remote driving control of the vehicle, wherein the remote control system is configured to: receive a remote control request message sent based on a determination that an occupant of an interior of the vehicle is associated with a health state which meets at least one threshold;receive an authorization message from a device of an authorized entity or the vehicle, wherein the authorization message comprises authorization information indicating the vehicle is to be remotely controlled;determine that remote driving control of the vehicle by the remote control system is authorized by the authorized entity based on the authorization information included in the authorization message; andgenerate a set of remote driving commands which, when executed at a vehicle navigation system of the vehicle, cause the vehicle to be navigated through an environment, based at least in part upon a determination that remote driving control of the vehicle is authorized by the authorized entity.","18","16/416076","2019-05-17","","","11858459","2024-01-02","Apple Inc.","Bartholomeus C. Nabbe | Tie-Qi Chen | Benjamin B. Lyon","","","","B60R-0025/2018","B60R-0025/2018 | G01C-0021/26 | G05D-0001/0011 | G06F-0021/305 | A61B-0005/18 | B60R-0099/00 | G05D-0001/0022 | G06Q-0010/06 | G07C-0005/00 | G07C-0005/008","B60R-025/20","B60R-025/20 | G01C-021/26 | G06F-021/30 | G05D-001/00 | A61B-005/18 | B60R-099/00 | G07C-005/00 | G06Q-010/06","","","","","","4924001002461"
"US","US","P","E1","Medical device adjusting operation when used with non-authenticated patient parameter collecting accessory","Embodiments are directed to a medical device, such as a defibrillator, for use with an accessory capable of collecting a parameter of a patient. The medical device is capable of at least performing a basic functionality, an advanced functionality, and of defibrillating the patient. The medical device includes an energy storage module within a housing for storing an electrical charge that is to be delivered to the patient for the defibrillating. The medical device includes a processor structured to determine whether a data set received from the accessory confirms or not a preset authentication criterion about the accessory. Although when the accessory is coupled to the housing the medical device is capable of the defibrillating and the basic functionality, the medical device is capable of the advanced functionality only when the accessory is coupled to the housing and it is determined that the preset authentication criterion is confirmed. Embodiments also include methods of operation and a programmed solution.","10. A method in a medical device for use an accessory capable of collecting a parameter of a patient, comprising: receiving a data set from a remote accessory configured to collect a parameter of a patient ;determining whether the data set confirms a preset authentication criterion about the remote accessory;disabling at least one a function of the a medical device in response to not confirming determining that the data set received against does not confirm the preset authentication criterion , wherein the function comprises reporting an aspect of the parameter ; andcollecting at least one patient the parameter while the at least one function is disabled.","20","16/457681","2019-06-28","","","RE49764","2023-12-26","PHYSIO-CONTROL, INC.","Richard C. Nova","","","","A61N-0001/3931","A61N-0001/3931 | A61B-0005/01 | A61B-0005/021 | A61B-0005/0215 | A61B-0005/082 | A61B-0005/14552 | A61B-0005/25 | A61B-0005/282 | A61N-0001/3975 | G06F-0016/245 | G06F-0021/602 | H01M-0010/425 | A61B-2560/0214 | H01M-0006/5033 | H01M-0010/4221 | H01M-0016/00 | H01M-0050/572 | H02J-0007/0063 | H02J-0007/345","A61N-001/39","A61N-001/39 | A61B-005/01 | G06F-016/245 | A61B-005/282 | A61B-005/25 | A61B-005/021 | A61B-005/0215 | A61B-005/08 | A61B-005/1455 | G06F-021/60 | H01M-010/42 | H01M-050/572 | H01M-006/50 | H01M-016/00 | H02J-007/00 | H02J-007/34","","","","","","4923052000807"
"US","US","P","B2","Automatic cardiac therapy advisor with hidden Markov model processing","Apparatuses and methods are provided for automatically determining which type of resuscitation treatment is most appropriate for a patient. Methods are provided that include the following. One or more time domain signal measurements are transformed into frequency domain data representative of a frequency content of the one or more time domain signal measurements. The frequency domain data is processed to identify peaks. For each of the peaks, for each of multiple points in time, multiple parameters of the peak are determined. Based on the multiple parameters of the peaks for each of the multiple points in time, a trajectory is determined. The determined trajectory is analyzed in determining a recommended type of resuscitation treatment. An output indication is provided of the recommended type of resuscitation treatment.","1. An apparatus for automatically determining which type of resuscitation treatment is most appropriate for a patient, the apparatus comprising: one or more processors; andcircuitry for delivering one or more time domain signal measurements from the patient to the one or more processors, wherein the one or more processors are configured to: using at least one mathematical transformation, transform the time domain signal measurements into frequency domain data representative of a frequency content of the time domain signal measurements;process the frequency domain data to identify a plurality of peaks in a frequency spectrum;for each of the identified peaks, for each of a plurality of points in time, determine a plurality of parameters of the peak, comprising mathematical curve-fitting to estimate a shape of each of the identified peaks;based on the determined plurality of parameters of the plurality of peaks for each of the plurality of points in time, calculate a trajectory comprising calculating a spectral shape matrix and a direction in a multidimensional space;analyze the calculated trajectory to algorithmically estimate a probability of success of a particular type of resuscitation treatment; andbased at least in part on the estimated probability of success of the particular type of resuscitation treatment, determine and display on a display screen an output indication of a recommended type of resuscitation treatment.","32","16/949398","2020-10-28","2021-0228158","2021-07-29","11850076","2023-12-26","ZOLL MEDICAL CORPORATION","Gary A Freeman | James E Brewer","","","","A61B-0005/7264","A61B-0005/7264 | A61B-0005/363 | A61B-0005/726 | A61B-0005/7207 | A61B-0005/7257 | A61H-0031/005 | A61N-0001/3925 | G09B-0023/288 | G16H-0050/20 | A61H-2201/5015 | A61H-2230/04 | Y10S-0128/92 | Y10S-0128/923 | Y10S-0128/924 | Y10S-0706/924","A61N-001/39","A61N-001/39 | A61B-005/00 | G09B-023/28 | G16H-050/20 | A61B-005/363 | A61H-031/00","","","","","","4923052001230"
"US","US","P","B2","Machine-implemented acne grading","An image is accepted by one or more processing circuits from a user depicting the user's skin. Machine learning models stored in one or more memory circuits are applied to the image to classify acne characteristics. An acne severity grade is provided by the one or more processing circuits and a user interface displays the acne severity grade.","1. A system comprising: one or more memory circuits configured to store machine learning models;one or more processing circuits configured to:accept at least one image from a user depicting the user'ss skin;apply the machine learning models to the image to classify acne characteristics; andprovide an indication of the classified acne characteristics; anda user interface configured to display the provided indication of the classified acne characteristics,wherein the one or more processing circuits is further configured to generate a regimen recommendation based on the classified acne characteristics and the user interface is further configured to display the generated regimen recommendation,wherein the one or more processing circuits is further configured to:accept another image from the user depicting the user'ss skin;apply the machine learning models to the other image to classify the acne characteristics; andupdate the regimen recommendation to the user based on the classified acne characteristics of the other image.","13","17/083175","2020-10-28","2021-0133968","2021-05-06","11854188","2023-12-26","L'OREAL","Christine Elfakhri | Guive Balooch | Florent Valceschini | Dominique Moyal | Hemant Joshi | Yuanjie Li | Zhiyuan Song","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0005/445 | A61B-0005/486 | A61B-0005/7267 | A61B-0005/742 | G06N-0020/00 | G16H-0015/00 | G16H-0020/10 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/30 | G16H-0050/50 | G16H-0070/60 | A61B-0005/0077 | A61B-2576/02 | G06Q-0030/0631 | G06T-2207/20081 | G06T-2207/30088","G06K-009/00","G06K-009/00 | G06T-007/00 | G16H-015/00 | G16H-050/50 | G16H-050/30 | G16H-070/60 | G16H-030/40 | G16H-030/20 | G16H-020/10 | G06N-020/00 | A61B-005/00 | G16H-050/20 | G06Q-030/0601","","","","","","4923052005306"
"US","US","P","B2","Bedside interface for percutaneous coronary intervention planning","Devices, systems, and methods configured to assess the severity of a blockage in a vessel and, in particular, a stenosis in a blood vessel, provide measurements of a vessel that allow assessment of the vessel and, in particular, any stenosis or lesion of the vessel, simulate diagnostic visualizations a first visualization device and a second visualization device. For example, the methods can include displaying, on a first visualization device, an image of the vessel with treatment diagnostic visualizations based on obtained pressure measurements and displaying, on a second visualization device, a portion of the image of the vessel with diagnostic visualizations based on the obtained pressure measurements, wherein the portion of the image of the vessel displayed on the second visualization device is a close up of a region of interest of the vessel.","1. A system of planning treatment of a vessel of a patient, comprising: a processor configured for communication with an intravascular pressure-sensing instrument configured to be positioned within the vessel, wherein the processor is configured to: obtain intravascular pressure measurements from the vessel using the intravascular pressure-sensing instrument while the intravascular pressure-sensing instrument is moved through the vessel;display, on a first visualization device, a two-dimensional x-ray image of the vessel obtained by an external imaging device;receive a first user input directly on the two-dimensional x-ray image, wherein the first user input comprises a selection of a region of interest from the two-dimensional x-ray image;display a portion of two-dimensional x-ray image comprising the region of interest on a second visualization device different from the first visualization device, at the same time as the two-dimensional x-ray image is displayed on the first visualization device, wherein the region of interest is magnified on the second visualization device;receive a second user input representative of a proposed treatment to position a treatment device in the region of interest; andupdate, in response to the second user input, the second visualization device to include a treatment visualization overlaid on the region of interest, the treatment visualization comprising: a first graphical representation of a length of the proposed treatment in the region of interest; anda second graphical representation proximate to the first graphical representation, the second graphical representation indicating a simulated change in an intravascular pressure ratio resulting from the proposed treatment, wherein the intravascular pressure ratio is determined based on the obtained intravascular pressure measurements from the region of interest.","12","16/881049","2020-05-22","2020-0281748","2020-09-10","11854687","2023-12-26","PHILIPS IMAGE GUIDED THERAPY CORPORATION","Jacqueline Keller","","","","G16H-0030/40","G16H-0030/40 | A61B-0005/0035 | A61B-0005/0066 | A61B-0005/02007 | A61B-0005/0215 | A61B-0005/6876 | A61B-0005/7264 | A61B-0005/743 | A61B-0006/032 | A61B-0006/12 | A61B-0006/461 | A61B-0006/469 | A61B-0006/504 | A61B-0008/06 | A61B-0008/0891 | A61B-0008/12 | A61B-0008/5223 | A61B-0034/10 | A61F-0002/82 | G06F-0003/0488 | G06F-0003/04815 | G16H-0030/20 | G16H-0040/63 | G16H-0050/50 | A61B-0005/004 | A61B-0005/0084 | A61B-0005/02158 | A61B-0005/055 | A61B-0005/6852 | A61B-0005/7445 | A61B-2034/101 | A61B-2034/107","A61B-005/02","A61B-005/02 | G16H-030/40 | A61B-005/0215 | A61B-005/00 | A61B-006/00 | A61B-006/03 | A61B-008/12 | A61B-008/08 | A61F-002/82 | A61B-006/12 | A61B-008/06 | A61B-034/10 | G06F-003/0488 | G16H-030/20 | G16H-040/63 | G16H-050/50 | G06F-003/04815 | A61B-005/055","","","","","","4923052005799"
"US","US","P","B2","Medical devices with circuitry for capturing and processing physiological signals","A medical device comprises a control system, processing modules, and a wire bundle connecting the control system to the processing modules, the wire bundle comprising control lines and data lines. Each processing module is coupled to a respective set of sensors arranged to interface with a biological tissue site, the sensors being configured to capture analog physiological signals generated from the biological tissue site. The control system is configured to generate a control signal on the control lines to initiate a data collection cycle by the processing modules. In response to the control signal, each processing module is configured to perform a respective data collection process which comprises (i) capturing and processing an analog physiological signal on each enabled sensor to generate a data sample for each analog physiological signal captured on each enabled sensor, and (ii) outputting data samples to the control system on the data lines.","1. A medical device, comprising: a control system;a plurality of processing modules; anda wire bundle connecting the control system to the plurality of processing modules, wherein the wire bundle comprises control lines and data lines;wherein each processing module is coupled to a respective set of sensors arranged to interface with a biological tissue site, wherein the sensors are configured to capture analog physiological signals generated from the biological tissue site;wherein the control lines comprise a first control line that is coupled to a first processing module of the plurality of processing modules;wherein the control system is configured to generate a control signal on the first control line of the control lines to initiate a data collection cycle by the plurality of processing modules starting with initiating a data collection process by the first processing module and then sequentially initiating a respective data collection process by each processing module by the control signal being passed in sequence to each processing module;wherein in response to receiving the control signal, each processing module of the plurality of processing modules is configured to perform the respective data collection process which comprises (i) capturing and processing an analog physiological signal on each enabled sensor in the respective set of sensors coupled to the processing module to generate a data sample for each analog physiological signal captured on each enabled sensor, and (ii) outputting data samples to the control system on the data lines.","20","17/977625","2022-10-31","2023-0049000","2023-02-16","11848078","2023-12-19","AUTONOMIX MEDICAL, INC.","Landy Toth | Siu Bor Lau","","","","G16H-0040/63","G16H-0040/63 | A61B-0005/01 | A61B-0005/14503 | A61B-0005/30 | A61B-0005/6852 | A61M-0025/00 | G06F-0003/00 | G06F-0013/14 | G06F-0013/37 | G06F-0013/38 | H04L-0012/56 | H04L-0063/0823 | H04Q-0011/04 | A61B-0005/026 | A61B-2090/064 | A61B-2562/0219 | A61B-2562/0223 | A61B-2562/0261 | A61B-2562/222 | G06F-2213/40","G16H-040/63","G16H-040/63 | A61B-005/01 | A61B-005/145 | A61B-005/00 | G06F-013/38 | A61M-025/00 | H04L-009/40 | G06F-013/14 | H04L-012/54 | H04Q-011/04 | G06F-013/37 | G06F-003/00 | A61B-005/30 | A61B-090/00 | A61B-005/026","","","","","","4923051004566"
"US","US","P","B2","Machine learning models for automated request processing","A computerized method of automatically processing a medical imaging record using a machine learning model includes training a machine learning model prediction engine with historical medical imaging records, receiving a first medical imaging record from a first system, applying a set of specified approval criteria to the first medical imaging record to determine a provisional outcome, and in response to the provisional outcome being negative, selectively identifying an exception to the provisional outcome in response to input received by a user interface. In response to the exception not being identified, the method includes inputting a feature vector based on the first medical imaging record to the machine learning model prediction engine to generate a likelihood estimate, comparing the generated likelihood estimate to a target threshold, and in response to the generated likelihood estimate being greater than the target threshold, transmitting a signal indicating approval to the first system.","1. A computerized method of automatically processing a medical imaging record using a machine learning model, the method comprising: training a machine learning model prediction engine with historical medical imaging records to generate likelihood estimates, according to historical feature vector inputs generated from the historical medical imaging records;receiving a first medical imaging record from a first system, wherein the first medical imaging record is specific to a first entity;applying a set of specified approval criteria to the first medical imaging record to determine a provisional outcome;in response to the provisional outcome being positive, transmitting a signal indicating approval to the first system;in response to the provisional outcome being negative: providing data related to the first medical imaging record to a user interface; andselectively identifying an exception to the provisional outcome in response to input received by the user interface;in response to the exception being identified, transmitting the signal indicating approval to the first system;in response to the exception not being identified: generating a feature vector based on the first medical imaging record;inputting the feature vector to the machine learning model prediction engine;generating, by the machine learning model prediction engine, a likelihood estimate based on the feature vector;comparing the generated likelihood estimate to a target threshold; andin response to the generated likelihood estimate being greater than the target threshold, transmitting the signal indicating approval to the first system;receiving an appeal notification from the first entity at the first system; andin response to receiving, at the first system, the appeal notification and the signal indicating approval, responding to the appeal notification according to the signal indicating approval.","18","17/124712","2020-12-17","2023-0360778","2023-11-09","11848097","2023-12-19","EVICORE HEALTHCARE MSI, LLC","William K. Cochran | Bijuna S. Pramila | Han Van Vo | Hari C. Narayanan | Navin Rai","","","","G16H-0040/20","G16H-0040/20 | G06N-0020/00 | G06Q-0010/10 | G06Q-0040/08 | G06T-0007/0012 | G16H-0010/20 | G16H-0015/00 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/70 | A61B-0005/055 | A61B-0006/032 | G06T-2207/20076 | G06T-2207/20081","G16H-040/20","G16H-040/20 | G06N-020/00 | G16H-030/20 | G16H-030/40 | G16H-050/20 | G16H-015/00 | G16H-050/70 | G06Q-010/10 | G06Q-040/08 | G16H-010/20 | G06T-007/00 | A61B-006/03 | A61B-005/055","","","","","","4923051004585"
"US","US","P","B2","Extended reality headset tool tracking and control","A surgical tool tracking array can include a first marker holder, a second marker holder, and a tool holder. The first marker holder is configured to couple a first marker to the surgical tracking array in a first plane. The second marker holder is configured to couple a second marker to the surgical tool tracking array in a second plane that is independent and substantially parallel to the first plane. The tool holder is configured to couple a portion of a surgical tool to the surgical tool tracking array in a third plane that is independent from the first plane and the second plane.","1. A method of operating a surgical system, the method comprising: providing a surgical tool tracking array comprising:a first marker holder configured to couple a first marker to the surgical tool tracking array in a first plane;a second marker holder configured to couple a second marker to the surgical tool tracking array in a second plane that is at least substantially parallel to the first plane and is spaced apart from the first plane in a direction orthogonal to one of the first and second planes; anda tool holder configured to couple a portion of a surgical tool to the surgical tool tracking array in a third plane that is spaced part from the first plane and the second plane in a direction orthogonal to one of the first and second planesdetecting a tracking array in a field of view of a user of an XR headset; anddetermining that the user is selecting a surgical tool associated with the tracking array based on the tracking array being in the field of view of the user of the XR headset.","20","17/824080","2022-05-25","2022-0280249","2022-09-08","11839435","2023-12-12","GLOBUS MEDICAL, INC.","Thomas Calloway | Dana Wisniewski | Amaya Raphaelson | Michael Robinson","","","","A61B-0034/20","A61B-0034/20 | G06F-0003/011 | G06T-0007/73 | A61B-2017/00207 | A61B-2034/2057 | A61B-2034/2072 | A61B-2090/365 | A61B-2090/373 | A61B-2090/3937 | A61B-2090/502 | G06T-2207/30204","G09G-005/00","G09G-005/00 | A61B-034/20 | G06F-003/01 | G06T-007/73 | A61B-090/50 | A61B-090/00 | A61B-017/00","","","","","","4923050001000"
"US","US","P","B1","System for tailoring dialysis treatment based on sensed potassium concentration, patient data, and population data","A dialysis system is provided that includes a dialysis machine and a potassium sensing device that is configured to measure the concentration of potassium in the patient's blood, in spent dialysate resulting from treating the patient, or in both. The potassium sensing device can be configured to generate a sensed value of the concentration of potassium. A control and computing unit, including a processor and a memory, is configured to receive the sensed value, compare the value with one or more values stored in the memory, and generate a control signal based on the comparison. A potassium infusion circuit uses the control signal to infuse supplemental potassium solution into the treatment dialysate, a replacement fluid, or both. The memory can include stored patient-historical and population data.","1. A dialysis system comprising: a dialysis machine configured to perform a dialysis treatment on a patient;a potassium sensing device configured to sense a concentration of potassium in at least one of (a) the patient'ss blood serum, and (b) spent dialysate resulting from treating the patient with the dialysis machine, the potassium sensing device further being configured to generate a sensed value of the concentration of blood serum potassium;a control and computing unit comprising a processor, a memory and an input device, the input device being configured for inputting a patient time-since-last-treatment value, the processor being configured to receive the sensed value, compare the sensed value with one or more values stored in the memory, to form a comparison, and to generate a control signal based on the comparison and the patient time-since-last-treatment value; anda potassium infusion circuit configured to infuse potassium solution into treatment dialysate, replacement fluid, or both, that is to be used by the dialysis machine, whereinthe control and computing unit is in data transfer communication with the potassium infusion circuit,the control and computing unit is configured to send the control signal to the potassium infusion circuit, andthe potassium infusion circuit is configured to receive the control signal and infuse potassium solution into the treatment dialysate, replacement fluid, or both, based on the control signal.","18","17/974023","2022-10-26","","","11839709","2023-12-12","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Stephen Merchant | Roland Levin | Chris Chau | Shakil Aslam","","","","A61M-0001/1603","A61M-0001/1603 | A61B-0005/0075 | A61B-0005/02055 | A61B-0005/02427 | A61M-0001/1609 | A61M-0001/1613 | A61M-0001/1635 | A61M-0001/1672 | A61M-0001/1694 | A61M-0001/1696 | A61M-0001/342 | A61M-0001/3424 | A61M-0001/3455 | A61M-0001/3607 | A61M-0001/3609 | G01J-0001/00 | G01N-0021/31 | G01N-0021/33 | G01N-0021/62 | G01N-0021/63 | G01N-0021/631 | G01N-0021/64 | G01N-0021/6402 | G01N-0021/7703 | G01N-0027/333 | G01N-0027/3335 | G01N-0033/0067 | G01Q-0030/04 | G06F-0003/14 | A61M-2205/18 | A61M-2205/33 | A61M-2205/502 | A61M-2230/04 | A61M-2230/20","A61M-001/16","A61M-001/16 | A61M-001/34 | A61M-001/36 | A61B-005/00 | A61B-005/02 | G01J-001/00 | G01N-021/31 | G01N-021/63 | G01N-021/64 | G01N-021/77 | G01N-027/333 | G01N-033/00 | G01Q-030/04 | G06F-003/14 | G01N-021/33 | G01N-021/62 | A61B-005/024 | A61B-005/0205","","","","","","4923050001272"
"US","US","P","B2","Apparatus and method for registering images in real-time","The invention relates to a device for superimposing known patterns, characteristic of a region, on (real) images of said region. The device comprises, a memory in which patterns are stored, which are representative of a selected region, of known position and orientation with relation to a common reference and processing means, for determining a pattern representative of the selected portion in the memory, on receipt of the designation of at least one portion of an observed image of the selected region, taken at a selected angle and at least one representative attribute of said region, taking account of the attribute selected, then superimposing the determined pattern on the selected portion of the image taking account of the selected angle.","1. An apparatus comprising: a surgical robot;an image capturing device configured to capture an image of an observed region;a display device configured to generate a displayed image corresponding to the captured image;a registration device configured to, on receipt of a designation by a user of a selected portion of the displayed image, determine position data representing a position in the observed region corresponding to the selected portion; anda controller coupled to the registration device, the controller configured to move the surgical robot to the position in the observed region represented by the determined position data;the registration device further configured to, based on the designation of the selected portion and further designation of an identity attribute of said selected portion by a user, determine the position data by registering the selected portion to a three-dimensional (3D) pattern stored in a memory coupled to the registration device, the 3D pattern being associated with an identity matched with the identity attribute; the apparatus further comprising:a processor configured to superimpose a graphical representation of said 3D pattern on the displayed image.","16","17/226966","2021-04-09","2021-0219866","2021-07-22","11842503","2023-12-12","INTUITIVE SURGICAL OPERATIONS, INC.","Eve Coste-Maniere | Thierry Vieville | Fabien Mourgues","2003-006176","FR","2003-05-22","G06T-0007/30","G06T-0007/30 | A61B-0001/000094 | A61B-0005/066 | A61B-0005/749 | A61B-0034/10 | A61B-0034/30 | G06T-0007/60 | A61B-0001/04 | A61B-2034/107 | A61B-2034/301 | A61B-2090/365 | A61B-2090/373 | A61B-2090/374 | G06F-0003/0484 | G06T-2200/24 | G06T-2207/10068 | G06T-2207/10072 | G06T-2207/20092 | G06T-2207/30048 | G06T-2207/30101 | Y10S-0128/922","G06T-007/30","G06T-007/30 | A61B-034/30 | A61B-001/00 | A61B-005/06 | A61B-034/10 | A61B-005/00 | G06T-007/60 | A61B-001/04 | A61B-090/00 | G06F-003/0484","","","","","","4923050004048"
"US","US","P","B2","Systems and methods for impairment baseline learning","Various embodiments provide systems and methods for identifying impairment using measurement devices.","1. A system for determining individual impairment, the system comprising: a first individual characteristic sensor;a first impairment detection module configured to: determine a first impairment value for the monitored individual based at least in part on a first sensed information received from a first individual characteristic sensor;compare the first impairment value to a baseline impairment threshold; andgenerate a first impairment output based upon the comparison of the first impairment value to the baseline impairment threshold;a second impairment detection module configured to determine a second impairment output based at least in part upon a second sensed information from a second individual characteristic sensor; andan impairment threshold learning module configured to modify the baseline impairment threshold based at least in part on the second impairment output.","22","17/969520","2022-10-19","2023-0048282","2023-02-16","11832945","2023-12-05","BI INCORPORATED","Duke Hanson | Dustin Pettit | Joseph P. Newell","","","","A61B-0005/165","A61B-0005/165 | A61B-0003/113 | A61B-0003/14 | A61B-0005/0077 | A61B-0005/0205 | A61B-0005/02055 | A61B-0005/1112 | A61B-0005/1113 | A61B-0005/1118 | A61B-0005/1176 | A61B-0005/163 | A61B-0005/4023 | A61B-0005/4266 | A61B-0005/4845 | A61B-0005/681 | A61B-0005/6831 | A61B-0005/6898 | A61B-0005/7246 | A61B-0005/7282 | A61B-0005/742 | A61B-0005/748 | B60W-0040/08 | G06F-0016/54 | G06Q-0050/265 | G06T-0007/0012 | G06T-0007/30 | G06V-0020/597 | G06V-0040/19 | G09B-0019/00 | G16H-0010/60 | G16H-0020/70 | G16H-0040/67 | G16H-0050/30 | G16H-0050/70 | H04Q-0009/00 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0816 | A61B-2503/12 | A61B-2562/0219 | B60W-2040/0836 | B60W-2420/42 | B60W-2540/223 | B60W-2540/225 | G01S-0019/17 | G06T-2207/30041","G09B-019/00","G09B-019/00 | A61B-005/16 | A61B-005/0205 | A61B-005/11 | G16H-040/67 | G16H-010/60 | G16H-050/30 | G06Q-050/26 | G16H-050/70 | H04Q-009/00 | A61B-005/00 | B60W-040/08 | G06T-007/30 | G16H-020/70 | G06F-016/54 | A61B-003/113 | A61B-003/14 | A61B-005/1171 | G06T-007/00 | G06V-020/59 | G06V-040/19 | A61B-005/08 | G01S-019/17 | A61B-005/024 | A61B-005/021","","","","","","4923049001134"
"US","US","P","B2","Breath analysis system","A portable system is provided for measuring an analyte, such as acetone, in the breath or other bodily fluid of a user. The system includes a portable measurement device that analyzes fluid samples and generates corresponding measurements. The portable measurement device communicates with an application which runs on a smartphone or other mobile device of the user, and the application reports measurement data to a remote system.","1. A method for measuring a ketone in a fluid of a user of portable measurement device, the method comprising: in a first phase, measuring, by the portable measurement device, the ketone at a first location;generating, by the portable measurement device, a measurement signal indicative of a concentration of the ketone in the fluid; andtransmitting, by the portable measurement device, the measurement signal to a communications device located at the first location, wherein the communications device transmits the measurement signal to a remote system disposed at a second location remote from the first location, wherein the communications device does not display the measurement signal to the user during the first phase;wherein, during a second phase performed after completion of the first phase, the communications device receives a response signal from the remote system and responds to the response signal by displaying a representation of the measurement signal to the user.","14","18/183830","2023-03-14","2023-0210454","2023-07-06","11832963","2023-12-05","Invoy Holdings Inc.","Lubna M. Ahmad | Salman A. Ahmad | Zachary Smith","","","","A61B-0005/4833","A61B-0005/4833 | A61B-0005/0015 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/082 | A61B-0005/083 | A61B-0005/097 | A61B-0005/486 | A61B-0005/4866 | A61B-0005/6898 | A61B-0005/7264 | A61B-0005/7267 | A61B-0005/742 | A61B-0005/743 | A61B-0005/7405 | A61B-0005/7475 | G01N-0033/497 | G01N-0033/64 | G06F-0003/0482 | G06F-0003/04842 | G06F-0009/453 | G08B-0021/18 | G08B-0021/182 | H04L-0043/045 | H04W-0004/80 | A61B-2010/0087 | A61B-2560/045 | A61B-2560/0431 | G01N-2033/4975","A61B-005/00","A61B-005/00 | G08B-021/18 | A61B-005/08 | G06F-003/0482 | G06F-003/04842 | G06F-009/451 | H04W-004/80 | G01N-033/64 | G01N-033/497 | H04L-043/045 | A61B-005/083 | A61B-005/097 | A61B-010/00","","","","","","4923049001152"
"US","US","P","B2","Electrical isolation of neurostimulation circuitry from neurorecording circuitry","Various embodiments of an interface control subsystem may be used between an electrode terminal and a recording terminal of a neurostimulation and neurorecording system. The interface control subsystem may operate in three modes. In a disable mode, a first transistor and a second transistor disposed between the electrode terminal and the recording terminal may operate in a cutoff region and generate a high impedance. In an active mode, the first transistor and the second transistor may operate in a saturation region and generate a low impedance. In a stimulation mode, the first transistor and the second transistor operate in a triode region and generate an impedance between the high impedance of the disable mode and the low impedance of the active mode. The interface control subsystem may further limit voltage at the recording terminal in response to a detected overvoltage condition.","1. A neurostimulation and neurorecording interface control circuit, comprising: a stimulation terminal;a recording terminal; andan interface control subsystem in electrical communication with the stimulation terminal and the recording terminal and disposed between the stimulation terminal and the recording terminal, the interface control subsystem configured to operate in: a disable mode in which the neurostimulation and neurorecording interface control circuit is configured to provide a high impedance between the stimulation terminal and the recording terminal;an active mode in which the neurostimulation and neurorecording interface control circuit is configured to provide a low impedance between the stimulation terminal and the recording terminal; anda stimulation mode in which the neurostimulation and neurorecording interface control circuit is configured to provide an impedance between the high impedance of the disable mode and the low impedance of the active mode.","23","17/315160","2021-05-07","2021-0346700","2021-11-11","11833353","2023-12-05","VERITAS IP, LLC","Andrew Miller Wilder | Scott Darold Hiatt | Steven John Barrus | Cliff C. Nixon","","","","A61N-0001/36128","A61N-0001/36128 | A61B-0005/7217 | A61N-0001/025 | G06F-0003/015 | A61N-2001/083","A61N-001/02","A61N-001/02 | A61N-001/36 | A61B-005/00 | G06F-003/01 | A61N-001/08","","","","","","4923049001537"
"US","US","P","B2","Augmented reality triggering of devices","An augmented reality trigger system (10) comprising a primary augmented reality device (30) and a trigger action controller (40) for implementing an augmented reality trigger method based on a medical tool (20) and/or a tool identifier (21) 5 associated with the medical tool (20). In operation, the primary augmented reality device (30) generates a camera image of the real world, which may or may not at any time include the medical tool (20) and/or the tool identifier (21). The trigger action controller (40) recognizes a generation by the primary augmented reality device (30) of the camera image of the real world including the medical tool (20) and/or the tool 10 identifier (21) and in response to such recognition, triggers a medical procedure action by the primary augmented reality device (30) and/or a medical device (50) in support of a medical procedure involving the medical tool (20).","1. An augmented reality trigger system comprising: a primary augmented reality device configured to generate a camera image of a real world; anda trigger action controller configured to: receive, from the primary augmented reality device, the camera image of the real world including a medical tool,identify the medical tool within the camera image as a particular medical tool, andtrigger a medical procedure action, by at least one of the primary augmented reality device and a medical device configured to image a medical procedure, in response to the identification of the medical tool as the particular medical tool,wherein the triggered medical procedure action includes triggering the medical device to set an operational mode related to the medical tool for imaging the medical tool.","19","16/762155","2018-11-03","2020-0302694","2020-09-24","11836863","2023-12-05","KONINKLIJKE PHILIPS N.V.","Molly Lara Flexman | Atul Gupta | Ashish Panse","","","","G06T-0019/006","G06T-0019/006 | A61B-0090/36 | A61B-0090/90 | G06F-0003/011 | G06T-0007/60 | G06V-0020/20 | A61B-2090/365 | A61F-0002/2427 | A61F-0002/95 | A61M-0025/0113 | A61M-0025/09041 | G06T-2200/24 | G06V-2201/034","G06T-019/00","G06T-019/00 | A61B-090/00 | A61B-090/90 | G06F-003/01 | G06T-007/60 | G06V-020/20 | A61F-002/24 | A61F-002/95 | A61M-025/01 | A61M-025/09","","","","","","4923049005033"
"US","US","P","B2","Acute care treatment systems dashboard","A system for clinical decision support for a caregiver during a clinical encounter involving respiratory distress includes a defibrillator-patient monitor including sensors configured to gather patient physiological data, a user interface device, a memory, and a processor communicatively coupled to the defibrillator-patient monitor, the memory, and the user interface device and configured to control the user interface device to display diagnosis and treatment pathways (DTPs) including a respiratory distress DTP, receive a user selection of the respiratory distress DTP, control the user interface device to provide at least a portion of the physiological data in response to the user selection of the respiratory distress DTP, provide a suggested respiratory distress diagnosis at the user interface device based on the physiological data and the user selection of the respiratory distress DTP, and provide a respiratory distress treatment protocol at the user interface device based on the suggested respiratory distress diagnosis.","1. A system for clinical decision support for a caregiver during a clinical encounter involving respiratory distress, the system comprising: a defibrillator-patient monitor comprising one or more sensors configured to gather physiological data from a patient;at least one user interface device;a memory; anda processor communicatively coupled to the defibrillator-patient monitor, the memory, and the at least one user interface device, the processor configured to: control the at least one user interface device to display a plurality of diagnosis and treatment pathways (DTPs) comprising a respiratory distress DTP,receive a user selection of respiratory distress DTP;control the at least one user interface device to provide at least a portion of the physiological data in response to the user selection of the respiratory distress DTP, wherein the at least a portion of the physiological data comprises capnography data,determine one or more capnography waveform measurements from the capnography data,control the at least one user interface device to provide the one or more capnography waveform measurements for the caregiver, wherein the one or more capnography waveform measurements comprise one of more of: dead space volume, a phase II slope, and a phase III slope,provide at least one suggested respiratory distress diagnosis at the at least one user interface device based on the physiological data and the user selection of the respiratory distress DTP, andprovide a respiratory distress treatment protocol at the at least one user interface device based on the at least one suggested respiratory distress diagnosis.","27","16/947554","2020-08-06","2020-0359972","2020-11-19","11826181","2023-11-28","ZOLL MEDICAL CORPORATION","Gary A Freeman | Guy R Johnson","","","","A61B-0005/7475","A61B-0005/7475 | A61B-0005/0002 | A61B-0005/0205 | A61B-0005/318 | A61B-0005/742 | G06F-0003/017 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | A61B-0005/02405 | A61B-0005/0816 | A61B-0005/1135 | A61B-0005/14532 | A61B-0005/14539 | A61B-0005/14542 | A61B-0005/4824 | A61B-0007/00 | A61B-2505/01 | A61B-2560/0456 | A61N-0001/3904 | A61N-0001/3925 | A61N-0001/3993 | G06F-0001/1632","A61B-005/00","A61B-005/00 | G16H-040/63 | A61B-005/0205 | G06F-003/01 | G16H-050/20 | G16H-040/67 | A61B-005/318 | A61B-005/024 | A61B-005/08 | A61B-005/113 | A61B-005/145 | A61B-007/00 | A61N-001/39 | G06F-001/16","","","","","","4923048001166"
"US","US","P","B2","Channel-specific engagement machine learning architecture","A method includes generating an intervention model by determining principal components for features of a training set, associating each feature of the training set with a principal component, selecting features of the training set most highly correlated with principal components, training a machine learning model with at least some of the selected features, and saving the verified trained machine learning model as the intervention model. The method includes determining multiple channel-specific intervention expectations. Each channel-specific intervention expectation indicates a likelihood that the user will take action in response to an intervention being executed using the engagement channel corresponding to the channel-specific intervention expectation. The method includes selecting an intervention and scheduling the selected intervention for execution.","1. A computer-implemented method comprising: generating an intervention model by: determining principal components for features of a training set,associating each feature of the training set with a principal component,selecting features of the training set most highly correlated with principal components,performing a regression analysis on the selected features to determine a subset of the selected features that are most highly correlated with a model target,training a machine learning model based on the subset of the selected features, andsaving the trained machine learning model as the intervention model;obtaining data related to a user, wherein: the data includes engagement data indicating successfulness of prior interventions with the user andeach prior intervention with the user is associated with one of multiple engagement channels;supplying the obtained data as input to the intervention model to determine multiple channel-specific intervention expectations, wherein each channel-specific intervention expectation: corresponds to one of the multiple engagement channels andindicates a likelihood that the user will take action in response to an intervention being executed using the corresponding engagement channel;determining a likelihood of a gap in care for the user; andin response to the care gap likelihood being outside of a threshold: identifying a highest determined value of the channel-specific intervention expectations,selecting an intervention corresponding to the highest determined value of the channel-specific intervention expectation, andscheduling the selected intervention for execution.","20","18/092260","2022-12-31","2023-0139811","2023-05-04","11830610","2023-11-28","EXPRESS SCRIPTS STRATEGIC DEVELOPMENT, INC.","Amit K. Bothra | Pritesh J. Shah | Christopher G. Lehmuth | Bradley D. Flynn | Varun Tandra","","","","G16H-0040/20","G16H-0040/20 | A61B-0005/4833 | G06N-0003/08 | G06Q-0010/107 | G06Q-0030/0201 | G06Q-0050/01 | G16H-0020/10 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | G16H-0080/00 | H04L-0065/1066","G16H-040/20","G16H-040/20 | G06N-003/08 | G06Q-050/00 | G06Q-030/0201 | G06Q-010/107 | G16H-020/10 | G16H-050/30 | G16H-050/20 | G16H-050/70 | A61B-005/00 | G16H-080/00 | H04L-065/1066","","","","","","4923048005493"
"US","US","P","B2","Arterial imaging and assessment systems and methods and related user interface based-workflows","In part, the disclosure relates to method of displaying a representation of an artery. The method may include storing an intravascular image dataset in a memory device of a diagnostic imaging system, the intravascular image dataset generated in response to intravascular imaging of a segment of an artery; automatically detecting lumen boundary of the segment on a per frame basis; automatically detecting EEL and displaying a stent sizing workflow. In part, the disclosure also relates to automatically detecting one or more regions of calcium relative to lumen boundary of the segment; calculating an angular or circumferential measurement of detected calcium for one or more frames; calculating a calcium thickness of detected calcium for one or more frames; and displaying the calcium thickness and the angular or circumferential measurement of detected calcium for a first frame of the one or more frames.","1. A method of displaying a representation of an artery comprising: storing an intravascular image data set in a memory device of a diagnostic imaging system, the intravascular image data set generated in response to intravascular imaging of a segment of an artery;automatically detecting lumen boundary of the segment on a per frame basis;automatically detecting external elastic lamina (EEL) of the segment on a per frame basis; anddisplaying a workflow operable for stent sizing comprising a graphical user interface,the graphical user interface comprising: a first representation of the artery at a first frame; anda second representation of the artery at a second frame;wherein a first EEL diameter and a first lumen diameter are displayed relative to the first representation;wherein a second EEL diameter that is distinct from the first EEL diameter, and a second lumen diameter are displayed relative to the second representation.","23","16/821877","2020-03-17","2020-0294659","2020-09-17","11819309","2023-11-21","LIGHTLAB IMAGING, INC.","Ajay Gopinath | Mark Hoeveler","","","","A61B-0005/0066","A61B-0005/0066 | A61B-0034/10 | A61B-0034/25 | G06F-0003/0484 | G06T-0007/0012 | G06T-0007/13 | G06T-0007/62 | G06V-0010/764 | G06V-0010/82 | G16H-0030/20 | G16H-0030/40 | G16H-0040/40 | G16H-0040/63 | A61B-2034/107 | A61B-2034/108 | A61F-0002/958 | G06T-2200/24 | G06T-2207/30052 | G06T-2207/30101 | G06V-2201/03","G06F-003/048","G06F-003/048 | A61B-005/00 | A61B-034/10 | A61B-034/00 | G06T-007/62 | G06T-007/13 | G16H-030/20 | G16H-030/40 | G16H-040/40 | G16H-040/63 | G06F-003/0484 | G06T-007/00 | G06V-010/764 | G06V-010/82 | A61F-002/958","","","","","","4923047001066"
"US","US","P","B2","System and method for an improved graphical user interface that provides independent control of multiple radiofrequency probes during an ablation procedure","A system for delivering energy to a patient's body includes a plurality of probes, a touch-sensitive display screen, and a controller communicatively coupled to each of the probes and the display screen. The controller is configured to perform operations including displaying a plurality of dynamically sized channel control regions within a user interface of the touch-sensitive display screen. Each of the plurality of channel control regions corresponds with at least one of the plurality of probes and is sized based at least in part on a number of the plurality of probes. The operations can include detecting a user touch action directed to a user-selected channel control region of the plurality of dynamically sized channel control regions. The operations can include performing a control action associated with the probe(s) that correspond with the user-selected channel control region when the user touch action is detected.","1. An ablation system comprising: at least two probes for delivering radiofrequency (RF) energy to a patient;a touch-sensitive display screen; anda controller communicatively coupled to each of the at least one probes and the touch-sensitive display screen, the controller comprising a processor and memory having instructions stored thereon that, when executed by the processor, cause the ablation system to: display, on the touch-sensitive display screen, a user interface that includes a first channel control region associated with a first probe of the at least two probes and a second channel control region associated with a second probe of the at least two probes, wherein the first channel control region includes a first set of graphical elements for controlling an operation of the first probe and the second channel control region includes a second set of graphical elements for controlling an operation of the second probe, and wherein each of the first channel control region and the second channel control region indicate real-time operating parameters associated with the operation of a corresponding one of the first probe and the second probe; andmodify the user interface responsive to detecting that the second probe has been electrically disconnected from the ablation system by: i) removing the second channel control region from the user interface, and ii) dynamically resizing and repositioning the first channel control region to fill a portion of the user interface that previously contained the second channel control region.","20","18/177212","2023-03-02","2023-0200919","2023-06-29","11813031","2023-11-14","AVENT, INC.","Joseph A. Cesa | Lisa M. McGregor | Jennifer J. Barrett | Tyler W. Crone | Lee W. Rhein | Christopher W. Thurrott | Morgan Rudolph | Scott Woodruff","","","","A61B-0034/25","A61B-0034/25 | A61B-0005/743 | A61B-0005/7435 | A61B-0018/14 | A61B-0018/1482 | G06F-0003/04845 | G06F-0003/04847 | G06F-0003/04886 | A61B-2017/00199 | A61B-2018/00077 | A61B-2018/00083 | A61B-2018/00577 | G06F-2203/04803","G06F-003/0488","G06F-003/0488 | A61B-034/00 | A61B-005/00 | A61B-018/14 | G06F-003/04845 | G06F-003/04847 | G06F-003/04886 | A61B-017/00 | A61B-018/00","","","","","","4923046001033"
"US","US","P","B1","Methods, systems, and computer readable media for conducting an automatic assessment of postural control of a subject","The subject matter described herein includes methods, systems, and computer readable media for conducting an automatic assessment of postural control of a subject. According to one aspect, a method occurs at a computing platform including a processor and memory. The method includes displaying a stimulus to which a subject responds, capturing facial image data of the subject, analyzing the facial image data to determine a frequency of head displacement information associated with the subject, using the head displacement information to derive postural control assessment data, and determining that the postural control assessment data is indicative of a neurodevelopmental or neuropsychiatric disorder associated with the subject.","1. A method for conducting an automatic assessment of postural control of a subject, the method comprising: at a computing platform including a processor and memory: displaying a stimulus to which a subject responds;capturing facial image data of the subject;analyzing the facial image data to determine a frequency of head displacement information associated with the subject, wherein analyzing the facial image data to determine the frequency of head displacement information includes measuring head displacement of the subject during a series of frames where the subject is exhibiting a head yaw pose with a magnitude less than a predetermined value;using the frequency of head displacement information to derive postural control assessment data; anddetermining that the postural control assessment data is indicative of a neurodevelopmental or neuropsychiatric disorder associated with the subject.","20","16/678828","2019-11-08","","","11813054","2023-11-14","DUKE UNIVERSITY","Geraldine Dawson | Guillermo Sapiro | Jordan Hashemi","","","","A61B-0005/1116","A61B-0005/1116 | G06F-0003/012 | G06F-0003/017 | G06T-0007/0012 | G06T-0007/70 | G16H-0050/20 | G16H-0050/30 | G06T-2207/30201","G06T-007/70","G06T-007/70 | A61B-005/11 | G06F-003/01 | G16H-050/20 | G16H-050/30 | G06T-007/00","","","","","","4923046001056"
"US","US","P","B2","Foot measuring and sizing application","Systems and processes for measuring and sizing a foot are provided. In one exemplary process, at least one image of a foot and a horizontal reference object from a first point of view is captured. A portion of the foot may be disposed against a vertical reference object. The at least one image may be displayed, where one or more camera guides are overlaid on the at least one displayed image. In response to aligning the one or more camera guides with one or more of the horizontal reference object and the vertical reference object, a measurement of the foot based on the at least one captured image from the first point of view is determined.","1. A method for measuring an object, comprising: displaying, by a mobile computing device, at least one image of a portion of a user body part, and a plurality of reference dots from a point of view, wherein one or more camera guides are overlaid on the display of the at least one image;aligning, by the mobile computing device, the one or more camera guides with the portion of the user body part; anddetermining, by the mobile computing device, a measurement of the portion of the user body part based on the at least one image and the plurality of reference dots from the point of view.","20","17/574275","2022-01-12","2022-0202138","2022-06-30","11805861","2023-11-07","NIKE, INC.","Joseph Hei | Vivian Chiang | Jason Warren","","","","A43D-0001/025","A43D-0001/025 | A61B-0005/0022 | A61B-0005/0077 | A61B-0005/1074 | A61B-0005/1079 | A61B-0005/6898 | G06Q-0030/06 | G06Q-0030/0627 | G06T-0007/521 | G06T-0007/62","A43D-001/02","A43D-001/02 | A61B-005/00 | G06Q-030/0601 | G06Q-030/06 | G06T-007/521 | A61B-005/107 | G06T-007/62","","","","","","4923045000847"
"US","US","P","B2","Systems and methods for selecting, activating, or selecting and activating transducers","Transducer-based systems can be configured to display a graphical representation of a transducer-based device, the graphical representation including graphical elements corresponding to transducers of the transducer-based device, and also including between graphical elements respectively associated with a set of the transducers and respectively associated with a region of space between the transducers of the transducer-based device. Selection of graphical elements and/or between graphical elements can cause activation of the set of transducers associated with the selected elements. Selection of a plurality of graphical elements and/or between graphical elements can cause visual display of a corresponding activation path in the graphical representation. Visual characteristics of graphical elements and between graphical elements can change based on an activation-status of the corresponding transducers. Activation requests for a set of transducers can be denied if it is determined that a transducer in the set of transducers is unacceptable for activation.","1. A medical system comprising: a data processing device system;an input-output device system communicatively connected to the data processing device system; anda memory device system communicatively connected to the data processing device system and storing a program executable by the data processing device system, the program comprising:reception instructions configured to cause reception, from the input-output device system, of transducer data from at least some of a plurality of transducers of a transducer-based device at least during a particular state in which the plurality of transducers is located in a bodily cavity;first identification instructions configured to cause, based at least on an analysis of the received transducer data, identification of a first set of transducers from the plurality of transducers as a not-activation-ready transducer set overlying a port in the bodily cavity,second identification instructions configured to cause, based at least on the analysis of the received transducer data, identification of a second set of transducers from the plurality of transducers as an activation-ready transducer set overlying non-fluidic tissue; andgraphical representation instructions configured to cause display of a plurality of graphical elements associated with the plurality of transducers, with the graphical elements of the plurality of graphical elements corresponding to the activation-ready transducer set displayed with a set of visual characteristics that distinguishes the graphical elements of the plurality of graphical elements corresponding to the activation-ready transducer set from the graphical elements of the plurality of graphical elements corresponding to the not-activation-ready transducer set.","12","17/382498","2021-07-22","2021-0346104","2021-11-11","11805974","2023-11-07","KARDIUM INC.","Jeffery Charles Brewster | Daniel Martin Reinders | Daniel Robert Weinkam","","","","A61B-0034/25","A61B-0034/25 | A61B-0005/026 | A61B-0005/053 | A61B-0005/0538 | A61B-0005/283 | A61B-0005/287 | A61B-0005/6858 | A61B-0005/743 | A61B-0005/7435 | A61B-0018/1206 | A61B-0018/1233 | A61B-0018/14 | A61B-0018/1492 | A61N-0001/37264 | G06F-0003/0482 | G06F-0003/04842 | A61B-0005/01 | A61B-0005/6869 | A61B-2017/00199 | A61B-2018/00267 | A61B-2018/00351 | A61B-2018/00357 | A61B-2018/00363 | A61B-2018/00577 | A61B-2018/00642 | A61B-2018/00648 | A61B-2018/00708 | A61B-2018/00797 | A61B-2018/00839 | A61B-2018/00863 | A61B-2018/00875 | A61B-2018/00892 | A61B-2018/00904 | A61B-2018/00988 | A61B-2018/124 | A61B-2034/254","A61B-034/00","A61B-034/00 | A61B-018/12 | A61B-018/14 | A61B-018/00 | A61B-017/00 | A61B-005/053 | A61B-005/00 | A61B-005/0538 | A61B-005/287 | A61B-005/283 | A61N-001/372 | G06F-003/0482 | G06F-003/04842 | A61B-005/026 | A61B-005/01","","","","","","4923045000960"
"US","US","P","B2","Dosimetric features-driven machine learning model for DVHs/dose prediction","A treatment planning prediction method to predict a Dose-Volume Histogram (DVH) or Dose Distribution (DD) for patient data using a machine-learning computer framework is provided with the key inclusion of a Planning Target Volume (PTV) only treatment plan in the framework. A dosimetric parameter is used as an additional parameter to the framework and which is obtained from a prediction of the PTV-only treatment plan. The method outputs a Dose-Volume Histogram and/or a Dose Distribution for the patient including the prediction of the PTV-only treatment plan. The method alleviates the complicated process of quantifying anatomical features and harnesses directly the inherent correlation between the PTV-only plan and the clinical plan in the dose domain. The method provides a more robust and efficient solution to the important DVHs prediction problem in treatment planning and plan quality assurance.","1. A treatment planning prediction method, comprising: (a) training a machine deep learning algorithm operable by a computer wherein the machine deep learning algorithm is trained to correlate (i) Dose-Volume Histograms (DVHs) or Dose Distributions (DDs) from Planning Target Volume (PTV) only treatment plans with (ii) DVHs and DDs of treatment plans for patients, wherein the PTV only treatment plans are defined as treatment plans where dose constraints to Organs-At-Risk (OARs) have been removed and have PTV constrains defined for dose homogeneity and dose conformity;(b) the computer receiving dosimetric parameters, defined as DVHs or DDs from a PTV only treatment plan for a new patient, and the computer inputting the dosimetric parameters into the machine deep learning algorithm;(c) the computer processing the inputted dosimetric parameters by the machine deep learning algorithm; and(d) the computer generating an output from the machine deep learning algorithm, wherein the output are DVHs or DDs for the new patient, wherein the output is characterized by containing OARs constraints for the new patient as well as PTV constrains defined for dose homogeneity and dose conformity for the PTV only treatment plan for the new patient.","1","16/697725","2019-11-27","2020-0171325","2020-06-04","11806551","2023-11-07","THE BOARD OF TRUSTEES OF THE LELAND STANFORD JUNIOR UNIVERSITY","Yong Yang | Lei Xing | Ming Ma","","","","A61N-0005/1031","A61N-0005/1031 | G06F-0017/18 | G06N-0020/10","G06N-020/10","G06N-020/10 | A61N-005/10 | G06F-017/18","","","","","","4923045001531"
"US","US","P","B2","Method for controlling a limb of a virtual avatar by means of the myoelectric activities of a limb of an individual and system thereof","A method for controlling a limb of a virtual avatar by the myoelectric activities of a limb of an individual. The method includes a first step of calibrating and second step of moving the limb of the virtual avatar. Also, a system suitable for implementing the method for controlling a limb of a virtual avatar by the myoelectric activities of a limb of an individual.","1. A method for controlling a limb of a virtual avatar by the myoelectric activities of a limb of a subject, comprising a first calibration step comprising: acquiring first and second raw calibration myoelectric activity signals by a device for measuring myoelectric activity signals, the first and second raw calibration myoelectric activity signals resulting from contractions of variable intensities of first and second antagonistic muscles of the limb of the subject respectively, during a given period, and then:determining the envelope of each of the first and second signals by a processing device,determining the minimum and maximum myoelectric activities from the envelope of each of the first and second signals by the processing device, then, for each of the two antagonistic musclesdetermining a myoelectric activity threshold by the processing device,normalizing the maximum myoelectric activity and the myoelectric activity threshold by the processing device,determining a coefficient of conversion of the normalized myoelectric activity into a component of speed of movement of a limb of the virtual avatar by the processing device, andthe method for controlling comprising a second step of moving the limb of the virtual avatar comprising:acquiring first and second raw myoelectric activity signal by the device for measuring myoelectric activity signals, the first and second raw myoelectric activity signals resulting from contractions of the two antagonistic muscles of the limb of the subject, during a given period, and then, for each of the first and second raw myoelectric activity signals:normalizing the myoelectric activity resulting from the signal by the processing device,converting the normalized myoelectric activity into a component of speed of movement of a limb of a virtual avatar by applying the coefficient of conversion by the processing device,determining a speed and a direction of movement of the limb of the virtual avatar by subtracting the movement speed components obtained for each of the muscles by the processing device, andmoving the limb of the virtual avatar with the speed and direction of movement determined by the processing device.","11","17/910963","2021-03-12","2023-0147243","2023-05-11","11809625","2023-11-07","UNIVERSIT? DE BORDEAUX | CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE","Christophe Halgand | Matthieu Guemann | Aymar Goullet De Rugy | Daniel Cattaert","2020-002472","FR","2020-03-12","G06F-0003/015","G06F-0003/015 | A61B-0005/296 | A61B-0005/397 | A61B-0005/6824 | A61B-0005/744 | G06F-0003/016 | A61B-2560/0223 | A61F-0002/72","G06F-003/01","G06F-003/01 | A61B-005/296 | A61B-005/397 | A61B-005/00 | A61F-002/72","","","","","","4923045004585"
"US","US","P","B2","Non-contacting monitor for bridges and civil structures","A system for monitoring the movement of objects, structures, models of structures, cables and the like provides for the acquisition of images with an optical sensing device such as a video camera fixedly mounted at a selected distance from the item studied, in which the images are arranged into frames divided into pixels which are characterized by an intensity reflected or emitted over a selected time interval, and a data processing system to calculate a physical displacement as function of time of the item being studied or a portion of the item being studied based on an output from the video camera, and in some embodiments the system visually distinguishes one or more locations in the frame to indicate a difference in the phase of motion for multiple objects appearing in the frame.","1. A system for monitoring a condition of a structure, the structure having at least one surface that reflects light from an environment surrounding the structure and having a capacity to store and emit heat from said environment, comprising: a video camera that obtains sampling data as a plurality of video images of the structure, with the video images being divisible into individual video image frames comprising at least a first video image frame and a second video image frame, and with each video image frame being divisible into a plurality of pixels;a data processing system configured to monitor said structure using images of the structure obtained from the video camera, wherein the video image frames comprise images of the structure which include at least a first moving part and a second moving part;wherein the data processing system monitors said structure by: sensing a change in a measurement of intensity of at least one of the plurality of pixels in the first video image frame compared to the second video image frame, the intensity resulting from one or both of visible light energy resulting from light reflected from the at least one surface of the structure or infra-red energy emitted from thermal heating of the structure over a selected time interval;calculating a physical parameter related to movement of said selected portion of said structure as a function of time;establishing a continuum of operation for said structure during a first period that is referred to as a learning period which is characterized as a condition of baseline operation of the structure, and a second period referred to as a monitoring period when one or more deviations a learned baseline operation condition are sensed and reported; anddisplaying at least one image from at least one of the video image frames containing the first moving part of the structure and the second moving part of the structure wherein the at least one image has been modified to indicate a difference in phase of motion between the first moving part and the second moving part.","1","17/693795","2022-03-14","2022-0197492","2022-06-23","11803297","2023-10-31","RDI TECHNOLOGIES, INC.","Jeffrey R. Hay","","","","G06F-0003/04847","G06F-0003/04847 | A61B-0005/0077 | A61B-0005/01 | A61B-0005/024 | A61B-0005/0816 | A61B-0005/165 | A61B-0005/4815 | A61B-0005/7435 | G01N-0029/44 | G06F-0016/7335 | G06T-0007/0016 | G06T-0007/11 | G06T-0007/248 | G06T-0007/262 | G06V-0020/52 | G06V-0040/20 | G01N-2291/028 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/20056 | G06T-2207/20216 | G06T-2207/30004 | G06T-2207/30164 | H04N-0007/18","G06K-009/00","G06K-009/00 | G06F-003/04847 | G06F-016/732 | G06V-020/52 | G06V-040/20 | A61B-005/00 | A61B-005/01 | A61B-005/024 | A61B-005/08 | A61B-005/16 | G06T-007/00 | G01N-029/44 | G06T-007/11 | G06T-007/246 | G06T-007/262 | H04N-007/18","","","","","","4923044003038"
"US","US","P","B2","Interactive virtual assistant system","A method, computer program product, and computer system for defining, at a computing device, psychometric data for a user. An interactive virtual assistant, selected from a plurality of interactive virtual assistants, may be provided on the computing device based upon, at least in part, the psychometric data defined for the user. The user may be prompted, via the interactive virtual assistant, with one or more options.","1. A computer-implemented method comprising: defining, at a computing device, psychometric data for a user;selecting an interactive virtual assistant from a plurality of interactive virtual assistants based upon, at least in part, the psychometric data defined for the user;providing the selected interactive virtual assistant to the user through the computing device;receiving financial transaction data associated with the user;prompting the user, via the selected interactive virtual assistant, with an option to rate the financial transaction data by using an emotional annotation; andgenerating an aggregated insight based on emotional annotations associated with members of a combination of cluster groups associated with the user, wherein a cluster group is based on a minimum absolute value distance between the user score and a population median score.","18","16/888831","2020-05-31","2021-0374863","2021-12-02","11803399","2023-10-31","HAPPY MONEY, INC.","Adam Zarlengo | Chris Courtney | Michael Tepper | Josh Hemsley | Ryan Howes | Daniel Sinner | Scott Saunders","","","","G06F-0009/453","G06F-0009/453 | A61B-0005/165 | G06F-0003/04847 | G06Q-0040/02 | G06Q-0040/06","G06F-009/451","G06F-009/451 | G06Q-040/06 | G06Q-040/02 | G06F-003/04847 | A61B-005/16","","","","","","4923044003140"
"US","US","P","B2","Kinetic assessment and alignment of the muscular-skeletal system and method therefor","A system is disclosed herein for providing a kinetic assessment and preparation of a prosthetic joint comprising one or more prosthetic components. The system comprises a prosthetic component including sensors and circuitry configured to measure load, position of load, and joint alignment. The system further includes a remote system for receiving, processing, and displaying quantitative measurements from the sensors. The kinetic assessment measures joint alignment under loading that will be similar to that of a final joint installation. The kinetic assessment can use trial or permanent prosthetic components. Furthermore, adjustments can be made to the applied load magnitude, position of load, and joint alignment by various means to fine-tune an installation. The kinetic assessment increases both performance and reliability of the installed joint by reducing error that is introduced by elements that load or modify the joint dynamics not taken into account by prior assessment methods.","1. A method of kinetic assessment of a joint of a musculoskeletal system comprising: positioning a device configured to couple to the joint, wherein the device comprises a sensor configured to measure one or more parameters;measuring the one or more parameters, wherein the one or more parameters includes a load applied by the musculoskeletal system to the joint;transmitting data including the one or more parameters to a remote system;adjusting the load measured by the device until the load is within a predetermined range;measuring alignment of the joint,displaying on an electronic display the measured load,determining a contact point location of the musculoskeletal system on a surface of the device;repositioning the device relative to a bone of the musculoskeletal system to adjust the contact point; andfixing a position of the device when the position of the load is within a predetermined area range of the surface.","19","17/394017","2021-08-04","2022-0022774","2022-01-27","11793424","2023-10-24","ORTHOSENSOR INC.","Marc Stein | Martin Roche","","","","A61B-0005/103","A61B-0005/103 | A61B-0005/1036 | A61B-0005/1072 | A61B-0005/1121 | A61B-0005/45 | A61B-0005/4528 | A61B-0005/4571 | A61B-0005/4585 | A61B-0005/4851 | A61B-0005/686 | A61B-0017/154 | A61B-0017/155 | A61B-0017/157 | A61B-0017/1764 | A61B-0034/10 | A61B-0034/20 | A61B-0034/25 | A61B-0090/37 | A61F-0002/3836 | A61F-0002/461 | A61F-0002/4657 | A61F-0002/4684 | G06F-0003/0481 | A61B-0005/4887 | A61B-2034/102 | A61B-2034/104 | A61B-2034/105 | A61F-0002/38 | A61F-2002/4658 | A61F-2002/4668","A61B-005/103","A61B-005/103 | A61B-005/00 | A61B-017/15 | A61B-034/20 | A61B-034/00 | A61B-090/00 | A61F-002/46 | A61B-005/107 | A61B-005/11 | A61B-017/17 | A61F-002/38 | A61B-034/10 | G06F-003/0481","","","","","","4923043001192"
"US","US","P","B2","Indicator and analytics for sensor insertion in a continuous analyte monitoring system and related methods","The present embodiments provide systems and methods for, among others, tracking sensor insertion locations in a continuous analyte monitoring system. Data gathered from sensor sessions can be used in different ways, such as providing a user with a suggested rotation of insertion locations, correlating data from a given sensor session with sensor accuracy and/or sensor session length, and providing a user with a suggested next insertion location based upon past sensor accuracy and/or sensor session length at that location.","1. A method of continuous analyte monitoring, the method comprising: displaying, on a display device, a diagram of a body of a host;initiating a first sensor session between a first transmitter, in communication with a first sensor, and the display device;receiving as a first input, via the display device, a first location on the body where the first sensor was inserted into a skin of the host;receiving compression artifact data from the first transmitter corresponding to the first location on the body where the first sensor was inserted;determining an optimized location to position a second sensor on the body based on the compression artifact data received from the first transmitter; andoutputting, on the display device, the optimized location of the second sensor for a second sensor session based on the compression artifact data received from the first transmitter.","20","18/046282","2022-10-13","2023-0055750","2023-02-23","11793428","2023-10-24","DEXCOM, INC.","Katherine Yerre Koehler | Leif N. Bowman | Rian Draeger | Laura Dunn | Eli Reihman","","","","A61B-0005/14532","A61B-0005/14532 | A61B-0005/0022 | A61B-0005/684 | A61B-0005/743 | A61B-0005/748 | G06F-0003/0481 | G06T-0007/70 | G16H-0010/60 | G16H-0040/63 | G16H-0050/70 | H04L-0067/125 | A61B-2560/0487 | G06F-0003/0488 | G06F-0003/04842 | G06F-0003/04883 | G06T-2207/30196 | Y02A-0090/10","A61B-005/145","A61B-005/145 | A61B-005/00 | G16H-040/63 | G16H-010/60 | G16H-050/70 | G06F-003/0481 | H04L-067/125 | G06T-007/70 | G06F-003/04842 | G06F-003/04883 | G06F-003/0488","","","","","","4923043001196"
"US","US","P","B2","Calculation of an ablation plan","Systems and methods are described for planning of catheter ablation procedures, and in particular for planning of the placement of lesions and/or parameters used in ablation. In some embodiments, planning is based on thermal and/or dielectric simulation of lesions, individualized to the anatomy of the particular patient. Optionally, a plan comprises planning of a path along which an ablation lesion is to be formed, the ablation lesion optionally comprising one or more sub-lesions. The plan is optionally optimized for one or more criteria including, for example: minimization of path length, minimization of sub-lesion number, simplification of catheter maneuvering, avoidance of collateral damage to non-target tissue, access to the target dependent on anatomy shape and/or catheter mechanics, and/or features of the target anatomy such as tissue wall thickness and/or fiber direction.","1. A method for planning an ablation plan of a target tissue in a patient, the method comprising: receiving data characterizing patient-specific anatomy comprising at least the target tissue, wherein the data include data on dielectric properties associated with the target tissue;simulating, by computer, one or more operations to lesion the target tissue, based on the received data, to create simulated results;evaluating, by a computer, one or more criteria on said simulated results, said one or more criteria including a criterion to block abnormal cardiac tissue conduction;producing, by computer, a planned target form of the lesion, wherein the planned target form is produced based on said evaluating;producing, by computer, an ablation plan for producing the planned target form, the ablation plan comprising parameters of ablation;automatically adjusting the planned target form to avoid lesioning of non-target tissue; andproviding an indication of the planned target form.","26","17/348810","2021-06-16","2021-0307836","2021-10-07","11793576","2023-10-24","NAVIX INTERNATIONAL LIMITED","Yitzhack Schwartz | Zalman Ibragimov | Yehonatan Ben David | Eli Dichterman","","","","A61B-0034/10","A61B-0034/10 | A61B-0005/0538 | A61B-0005/063 | A61B-0018/1492 | A61B-0034/20 | A61B-0090/37 | G06F-0030/00 | G06F-0030/20 | G16H-0010/60 | G16H-0030/20 | G16H-0050/50 | A61B-0090/39 | A61B-2017/00026 | A61B-2018/00357 | A61B-2018/00577 | A61B-2018/00642 | A61B-2018/00702 | A61B-2018/00738 | A61B-2018/00761 | A61B-2018/00791 | A61B-2018/00875 | A61B-2018/00904 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2046 | A61B-2034/2053 | A61B-2090/065 | A61B-2090/365 | A61B-2090/374 | A61B-2090/3762 | A61B-2090/3983 | A61M-2025/0166 | G01H-0017/00 | G06F-0017/18 | H02J-2203/20 | H02K-0007/1823","G06F-030/00","G06F-030/00 | A61B-034/10 | A61B-005/0538 | A61B-005/06 | A61B-018/14 | A61B-090/00 | G06F-030/20 | A61B-034/20 | G16H-050/50 | G16H-010/60 | G16H-030/20 | A61B-018/00 | A61B-017/00 | H02K-007/18 | G06F-017/18 | G01H-017/00 | A61M-025/01","","","","","","4923043001343"
"US","US","P","B2","Body shape indicator","A body shape indicator device (1) comprises a scanner (2) arranged to obtain a three-dimensional model of a person, a body volume calculator (11) for calculating the volume of at least first, second and third parts of the person's body from the three-dimensional model and a body shape calculator (19) for calculating an indication of the person's body shape based on part volume ratios calculated from the output from at least the body volume calculator. A method for determining the body shape of a person and a method of automated clothing selection based on the body shape as determined are also disclosed.","1. A body shape indicator device comprising: a body volume calculator for calculating amounts of space taken up by respective body parts, the body volume calculator being arranged to: receive a three-dimensional (3D) model of a person generated using a scanner; andcalculate the volume of at least a first, a second, and a third par of the person'ss body from the 3D model, the first, second and third parts being chest, pelvis and abdomen, each volume being a measure of the amount of space taken up by the respective part of the person'ss body in 3 dimensions; anda body shape calculator arranged to calculate an indication of the person'ss body shape based on one or more ratios between volumes of the first, second, and third parts, wherein the body shape indication is an indication of a body type selected from a predefined list of body shapes.","17","17/042660","2019-03-27","2021-0065394","2021-03-04","11798186","2023-10-24","SELECT RESEARCH LIMITED","Richard Barnes","2018005233","GB","2018-03-29","G06T-0007/62","G06T-0007/62 | A61B-0005/0064 | A61B-0005/1077 | G06F-0030/20 | G06K-0019/06112 | G06Q-0030/0643 | G06T-0007/50 | G06T-0017/00 | A61B-0005/1079 | A61B-2503/12 | G06T-2207/30196","G06T-017/00","G06T-017/00 | G06T-007/62 | G06T-007/50 | G06F-030/20 | A61B-005/00 | A61B-005/107 | G06K-019/06 | G06Q-030/0601","","","","","","4923043005925"
"US","US","P","B2","Comparison and identification of attribute similarity based on genetic markers","A method, software, database, and system in which a query attribute is used as the basis for accessing stored attribute combinations and their frequencies of occurrence for individuals; and tabulating, based on frequencies of occurrence, those attribute combinations that are most likely to co-occur with the query attribute.","1. A computer-implemented method comprising: determining, based on representations of genetic sequences from a first user and a second user, overlap of genetic markers in the genetic sequences, wherein the genetic markers are respectively associated with physical or behavioral attributes, and wherein presence or absence of the physical or behavioral attributes are self-reported by the first user and the second user;receiving, from a client device associated with the first user, a selection indicating the second user; andin response to the selection and based on the overlap of genetic markers, providing, for display on a user interface of the client device, an indication of an overall degree of genetic similarity between the first user and the second user and indications of degrees of genetic similarity between the first user and the second user for one or more of the physical or behavioral attributes.","20","18/099478","2023-01-20","2023-0154629","2023-05-18","11791054","2023-10-17","23ANDME, INC.","Andrew Alexander Kenedy | Charles Anthony Eldering","","","","G16H-0070/20","G16H-0070/20 | G06F-0016/00 | G06F-0016/2282 | G06F-0016/24575 | G06F-0016/24578 | G06F-0016/285 | G06F-0016/951 | G06F-0016/955 | G06F-0016/9535 | G06F-0016/9538 | G06N-0003/08 | G06N-0005/04 | G06N-0007/01 | G06Q-0040/08 | G16B-0020/00 | G16B-0020/20 | G16B-0020/40 | G16H-0020/30 | G16H-0040/63 | G16H-0050/30 | G16H-0050/70","A61N-001/00","A61N-001/00 | G16H-070/20 | G06F-016/00 | G06F-016/28 | G06F-016/951 | G06F-016/955 | G06F-016/22 | G06F-016/9535 | G06F-016/2457 | G16H-020/30 | G16H-040/63 | G06Q-040/08 | G16H-050/30 | G16B-020/00 | G06N-003/08 | G06N-005/04 | G16H-050/70 | G16B-020/40 | G16B-020/20 | G06N-007/01 | G06F-016/9538","","","","","","4923042005896"
"US","US","P","B2","Method of using reinforced flexible circuits with multiple sensors to optimize performance of radio frequency devices","A method implemented by a surgical instrument is disclosed. The surgical instrument includes first and second jaws and a flexible circuit including multiple sensors to optimize performance of a radio frequency (RF) device. The flexible circuit includes at least one therapeutic electrode couplable to a source of RF energy, at least two sensing electrodes, and at least one insulative layer. The insulative layer is positioned between the at least one therapeutic electrode and the at least two sensing electrodes. The method includes contacting tissue positioned between the first and second jaws of the surgical instrument with the at least one therapeutic electrode and at the least two sensing electrodes; sensing signals from the at least two sensing electrodes; and controlling RF energy delivered to the at least one therapeutic electrode based on the sensed signals.","1. A surgical instrument for use in a surgical procedure, the surgical instrument comprising: a first jaw; anda second jaw moveable from an open position to a closed position;a flexible electrode positioned on either the first jaw or the second jaw, the flexible electrode comprising: a therapeutic electrode couplable to a source of radio frequency (RF) energy; anda sensing electrode configured to measure data indicative of a parameter of a tissue in contact with the flexible electrode; anda control circuit configured to: receive the data from the sensing electrode during a tissue treatment; andcontrol an amount of RF energy delivered to the therapeutic electrode based on the data received during the tissue treatment.","11","17/825754","2022-05-26","2022-0395276","2022-12-15","11779337","2023-10-10","CILAG GMBH INTERNATIONAL","David C. Yates | Frederick E. Shelton, IV | Jason L. Harris","","","","A61B-0017/07207","A61B-0017/07207 | A61B-0001/00009 | A61B-0001/00045 | A61B-0001/000096 | A61B-0001/051 | A61B-0001/0661 | A61B-0005/0066 | A61B-0005/0075 | A61B-0005/0261 | A61B-0006/5247 | A61B-0017/0682 | A61B-0017/072 | A61B-0017/1114 | A61B-0017/1155 | A61B-0017/1285 | A61B-0017/320092 | A61B-0018/1442 | A61B-0018/1445 | A61B-0034/20 | A61B-0034/32 | A61B-0034/71 | A61B-0090/35 | A61B-0090/361 | A61M-0001/73 | A61M-0001/79 | B25J-0009/1697 | B25J-0013/006 | G06K-0007/10316 | G06K-0019/07749 | G16H-0010/60 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | G16H-0070/20 | H01Q-0001/22 | H04L-0063/1416 | H04L-0067/10 | H04L-0067/12 | H04N-0005/272 | H04N-0007/183 | H05K-0001/028 | H05K-0001/189 | A61B-0034/30 | A61B-2017/0003 | A61B-2017/0011 | A61B-2017/00022 | A61B-2017/00026 | A61B-2017/00039 | A61B-2017/00044 | A61B-2017/00057 | A61B-2017/00061 | A61B-2017/00075 | A61B-2017/00084 | A61B-2017/00097 | A61B-2017/00106 | A61B-2017/00115 | A61B-2017/00119 | A61B-2017/00199 | A61B-2017/00203 | A61B-2017/00221 | A61B-2017/00398 | A61B-2017/00402 | A61B-2017/00734 | A61B-2017/00809 | A61B-2017/00818 | A61B-2017/07257 | A61B-2017/07271 | A61B-2017/07278 | A61B-2017/07285 | A61B-2017/1132 | A61B-2017/32007 | A61B-2017/320074 | A61B-2017/320084 | A61B-2017/320095 | A61B-2017/320097 | A61B-2018/0063 | A61B-2018/00541 | A61B-2018/00589 | A61B-2018/00595 | A61B-2018/00601 | A61B-2018/00607 | A61B-2018/00642 | A61B-2018/00684 | A61B-2018/00791 | A61B-2018/00827 | A61B-2018/00875 | A61B-2018/00892 | A61B-2018/00988 | A61B-2018/00994 | A61B-2034/2055 | A61B-2034/2057 | A61B-2034/301 | A61B-2034/305 | A61B-2090/309 | A61B-2217/005 | A61B-2217/007 | A61B-2218/002 | A61B-2218/007 | A61B-2218/008 | A61M-0001/80 | A61M-0013/003 | A61M-2205/3306 | A61M-2205/3327 | A61M-2205/3331 | A61M-2205/3365 | A61M-2205/3368 | G05B-2219/40174 | G05B-2219/45119","A61B-017/072","A61B-017/072 | A61M-001/00 | A61B-001/00 | A61B-034/20 | A61B-034/32 | A61B-034/00 | A61B-090/35 | A61B-090/00 | G16H-040/67 | G16H-010/60 | G16H-050/20 | G16H-040/63 | G16H-070/20 | A61B-001/05 | A61B-001/06 | A61B-005/00 | A61B-005/026 | A61B-006/00 | A61B-017/068 | A61B-017/11 | A61B-017/115 | A61B-017/128 | A61B-017/32 | A61B-018/14 | B25J-009/16 | B25J-013/00 | G06K-007/10 | G06K-019/077 | H01Q-001/22 | H04L-009/40 | H04L-067/10 | H04L-067/12 | H04N-005/272 | H04N-007/18 | H05K-001/02 | H05K-001/18 | A61B-034/30 | A61B-090/30 | A61B-017/00 | A61B-018/00 | A61M-013/00","","","","","","4923041001078"
"US","US","P","B2","Surface sensing probe and methods of use","Disclosed is a surface sensing apparatus, one embodiment having a source of coherent radiation capable of outputting wavelength emissions to create a first illumination state to illuminate a surface and create a first speckle pattern, an emission deviation facility capable of influencing the emission to illuminate the surface and create a second illumination state and a second speckle pattern, and a sensor capable of sensing a representation of the first and a second speckle intensity from the first and second speckle pattern. Also disclosed are methods of sensing properties of the surface, one embodiment comprising the steps of illuminating the surface having a first surface state with the source of coherent radiation emission, sensing a first speckle intensity from the surface, influencing a relationship of the surface to the emission to create a second surface state and sensing a second speckle intensity from the surface at the second surface state.","1. A method for determining a translation of a location on an object having a surface, the method comprising: illuminating a neighborhood of the location on the object with a source of coherent radiation emission having a beam centerline, a beam direction, and a beam waist thus creating an illumination state and a first speckle pattern;sensing the first speckle pattern;sensing a second speckle pattern produced by a translation of a position of the location on the object;determining the translation of the position of the location on the object in at least two dimensions from the first and second speckle patterns wherein two of the at least two dimensions lie substantially in a plane of the surface of the object;wherein the source of coherent radiation emission defines an illumination beam waist;wherein the step of determining the translation of the position of the location on the object in at least two dimensions from the first and second speckle patterns depends on a position of the illumination beam waist with respect to the surface of the object;wherein the position of the illumination beam waist with respect to the surface of the object defines a scaling factor for converting from a speckle shift to a surface translation; andwherein a sign of the scaling factor depends on whether the illumination beam waist is in front of the surface or behind the surface.","11","17/186860","2021-02-26","2021-0356250","2021-11-18","11781855","2023-10-10","Lyle G. Shirley","Lyle G. Shirley","","","","G01B-0009/02004","G01B-0009/02004 | A61B-0005/117 | G01B-0009/02005 | G01B-0009/02069 | G01B-0009/02083 | G01B-0009/02094 | G01B-0009/02096 | G01B-0011/14 | G01B-0011/2441 | G06F-0018/253 | G06F-0021/32 | G06T-0007/521 | G06T-0007/55 | G06V-0010/143 | G06V-0010/806 | A61B-0005/0077 | A61B-2576/00 | G06T-2207/10152","G01B-011/02","G01B-011/02 | G01B-009/02004 | A61B-005/117 | G06F-021/32 | G06T-007/55 | G01B-009/02055 | G01B-009/02 | G01B-011/24 | G01B-009/02002 | G06T-007/521 | G01B-011/14 | G06V-010/143 | G06F-018/25 | G06V-010/80 | A61B-005/00","","","","","","4923041003577"
"US","US","P","B2","System and method for human operator and machine integration","Aspects of the present disclosure are directed to devices, systems, and methods for optimized integration of a human operator with a machine for safe and efficient operation. Accordingly, aspects of the present disclosure are directed to systems, methods, and devices which evaluate and determine a cognitive state of an operator, and allocate tasks to either the machine and/or operator based on the cognitive state of the operator, among other factors.","1. A computer-implemented method of allocating a task to a human operator and/or a computer within an automated system having a human operator-computer interface, the method comprising: receiving, via controller circuitry, a measured set of multimodal signals from a set of psycho-physiological sensors communicatively coupled to an operator and the controller circuitry, wherein the measured set of multimodal signals is indicative of psychophysiological responses of the operator over a first time period;sampling the measured set of multimodal signals via a sampling circuit to thereby generate a sampled set of multimodal signals, including converting the measured set of multimodal signals to a digital time series over the first time period;associating the sampled set of multimodal signals over the time period with one or more cognitive states of the operator as an associated cognitive state;dynamically allocating a task to at least one of the operator and the computer via a task allocation circuit, the allocating based at least in part on the associated cognitive state of the operator; andtriggering an electric signal configured to provide instructional feedback to the human operator for operating the computer, the triggering being based, at least in part, on the one or more cognitive states of the operator during at least one of the first time period and one or more cognitive states of the user during a second time period.","9","17/307639","2021-05-04","2021-0256425","2021-08-19","11783228","2023-10-10","UNITED STATES OF AMERICA AS REPRESENTED BY THE ADMINISTRATOR OF NASA","Angela R. Harrivel | Chad L. Stephens | Kellie D. Kennedy | Alan T. Pope","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/18 | A61B-0005/7264 | A63F-0013/212 | A63F-0013/42 | A63F-0013/67 | G06F-0003/011 | G06F-0003/015 | G06N-0005/04 | G09B-0005/00 | A61B-0005/0022 | A61B-0005/0075 | A61B-0005/021 | A61B-0005/0816 | A61B-0005/318 | A61B-0005/369 | A61B-0005/389 | A63F-0013/21 | A63F-0013/428 | G06F-2203/011","G09B-005/00","G09B-005/00 | G06N-020/00 | G06N-005/04 | A61B-005/18 | G06F-003/01 | A61B-005/00 | A63F-013/212 | A63F-013/67 | A63F-013/42 | A63F-013/21 | A63F-013/428 | A61B-005/08 | A61B-005/021 | A61B-005/318 | A61B-005/369 | A61B-005/389","","","","","","4923041004937"
"US","US","P","B2","Automatic detection and generation of medical imaging data analytics","An information system and user interface to enable the analysis of clinical data for operational improvement and issue identification in medical imaging procedures is disclosed. In an example, opportunity and usage analytics are generated from relevant imaging procedure data (e.g., radiology procedure data) through operations including: obtaining clinical data that indicates usage of imaging resources to perform medical imaging procedures; analyzing the usage of the imaging resources from the clinical data, to identify values of opportunities for predicted changes to the usage of the imaging resources; and generating a visualization of the values of the opportunities for output in a graphical user interface, the visualization indicating values of opportunities relative to past usage and predicted changes to the usage of the imaging resources. Further examples also enable a detailed visualization and interaction with data for a particular opportunity in relation to medical facilities, organizations, modalities, and staffing resources.","1. A method for generating a graphical user interface of medical imaging resource usage analytics in a computing system, performed by electronic operations executed by processing circuitry of the computing system, with the electronic operations comprising: obtaining a set of clinical data from an electronic data source, the set of clinical data relating to a medical imaging procedure type, wherein the clinical data indicates usage of respective imaging resources to perform a plurality of medical imaging procedures of the medical imaging procedure type;analyzing the usage of the respective imaging resources from the clinical data, using an opportunity analysis model, the opportunity analysis model comprising at least one algorithm that is trained to generate resource usage values that optimize the usage of the respective imaging resources for at least idle time and schedules of the respective imaging resources;generating a visualization of the resource usage values for output in a graphical user interface, the visualization indicating positions of the resource usage values generated for each of the respective imaging resources and the optimized usage of the respective imaging resources;analyzing the usage of the respective imaging resources and the optimized usage of the respective imaging resources, using the opportunity analysis model, the at least one algorithm of the opportunity analysis model further trained to generate monetary values associated with the resource usage values generated for each of the respective imaging resources; andgenerating a representation of the monetary values associated with the resource usage values for output in the graphical user interface, the representation of the monetary values being a circle having a magnitude according to a respective monetary value displayed as a bubble chart.","22","16/762412","2018-11-20","2020-0356935","2020-11-12","11783262","2023-10-10","CANON MEDICAL SYSTEMS CORPORATION","Benjamin Roy Slater | James E. Rosenthal | Joseph John Fromm","","","","G06Q-0010/06375","G06Q-0010/06375 | G06F-0003/0482 | G06N-0005/04 | G06N-0020/00 | G06Q-0010/06312 | G16H-0015/00 | G16H-0030/20 | G16H-0040/20 | G16H-0050/20 | G16H-0050/70 | G16H-0070/20 | G16H-0070/60 | A61B-0005/0035 | A61B-0005/055 | A61B-0006/032 | A61B-0008/00","G06Q-010/06","G06Q-010/06 | G06Q-010/0637 | G16H-070/60 | G16H-040/20 | G16H-015/00 | G16H-030/20 | G16H-050/70 | G16H-050/20 | G16H-070/20 | G06N-020/00 | G06F-003/0482 | G06N-005/04 | G06Q-010/0631 | A61B-005/00 | A61B-005/055 | A61B-006/03 | A61B-008/00","","","","","","4923041004971"
"US","US","P","B2","Human body portion tracking method and human body portion tracking system","A human body portion tracking method is provided and including obtaining a first image from an image capturing apparatus; identifying a first reference point and a second reference point from the first image; determining a position relationship between a first section and a second section of the human body portion according to three-dimensional coordinates of the first reference point and the second reference point; obtaining a second image from the image capturing apparatus; identifying a third reference point from the second image.","1. A human body portion tracking method, comprising: obtaining a first image from an image capturing apparatus, wherein the first image captures a first section and a second section of a human body portion at a first time point, and the first section of the human body portion connects to the second section of the human body portion;identifying a first reference point and a second reference point from the first image, wherein the first reference point indicates a location of the first section of the human body portion at the first time point, and the second reference point indicates a location of the second section of the human body portion at the first time point;determining a position relationship between the first section and the second section of the human body portion according to three-dimensional coordinates of first reference point and the second reference point;obtaining a second image from the image capturing apparatus, wherein the second image captures the first section but not the second section of the human body portion at a second time point;identifying a third reference point from the second image, wherein the third reference point indicates a location of the first section of the human body portion at the second time point; andpredicting a three-dimensional coordinate of a fourth reference point by using a three-dimensional coordinate of the third reference point and the position relationship, wherein the fourth reference point indicates a location of the second section of the human body portion at the second time point.","12","17/008667","2020-09-01","2022-0061700","2022-03-03","11783492","2023-10-10","XRSPACE CO., LTD.","Sheng-Hsiu Kuo","","","","G06T-0007/246","G06T-0007/246 | A61B-0005/0077 | A61B-0005/7425 | G06F-0003/011 | G06T-0007/215 | G06V-0040/28 | G06T-2207/30196 | G06T-2207/30241 | G06V-2201/07","G06T-007/246","G06T-007/246 | A61B-005/00 | G06F-003/01 | G06T-007/215 | G06V-040/20","","","","","","4923041005200"
"US","US","P","B1","Method and system for music and dance recommendations","A method and system that includes at least one processor, the at least one computer in communication with at least one data storage unit, the at least one computer programmed and/or configured to: generate a plurality of dance sequences; associate or cause the association of each of the plurality of dance sequences with at least one dance corpus classification; generate, for each of the plurality of users, a user profile; receive, from each of the plurality of users, at least one preference data; associate or causing the association of each of the user's profile with at least one preference data; and generate a recommendation of a dance sequence for each of the plurality of users.","1. A system for implementing a method for improving physical condition or mood of a subject through personalized dance activity, and for building a subject data base, and for building a subject data set of the system, said system comprising: at least one computer including at least one processor,at least one data storage unit coupled to the at least one processor, said data storage unit including:the subject data base containing subject data base information and tracking information from the subject, wherein said subject data base information comprises unique user identifier information, demographic data, preference data, historical data, lifestyle data, real-time situational data, and real-time mood data, andwherein said tracking information comprises facial recognition data from the subject and physiological measurements of the subject from tracking information collection devices and comprising skeletal tracking data, or thermal tracking data, or oxygenation circulatory tracking data,the subject data set comprising subject data base information and tracking information from a plurality of users of the system, anda repository of live video classes or recorded dance videos, andthe tracking information collection devices associated with the subject comprising an input device, a camera, and biosensor peripheral device(s) worn by the subject that communicate with the processor, wherein:the system initiates and initially builds the subject data base by initially collecting subject data base information and tracking information via the tracking information collection devices from the subject initiating use of the system,the system analyzes and interprets the subject data base information and tracking information and makes determinations as to the subject'ss physiological state and mood and generates from data comprising the subject data base information and the tracking information, the determinations as to the subject'ss physiological state and mood, the repository of live video classes or recorded dance videos, and the subject data set, a selection of dance routines for the subject,the system presents to the subject the selection of dance routines,the system accepts from the subject the subject'ss selected dance routine,the system presents to the subject a recorded dance video or live video classes corresponding to the subject'ss selected dance routine,the system collects subject data base information and tracking information via the tracking information collection devices when the subject executes the subject'ss selected dance routine and engages in the personalized dance activity, and at the conclusion of the subject executing the subject'ss selected dance routine and engaging in the personalized dance activity,the system analyzes and interprets the subject data base information and tracking information from when the subject executes the subject'ss selected dance routine and engages in the personalized dance activity, and at the conclusion of the subject executing the subject'ss selected dance routine and engaging in the personalized dance activity, and makes determinations as to the subject'ss physiological state and mood, whereby the system tracks improvement in the subject'ss physiological state and mood and refines its ability to generate and present dance routine selections for the subject in each subsequent use by the subject,the system builds the subject data set by adding the subject data base information and tracking information to the subject data set, andthe system continues to build the subject data base and the subject data set with each subsequent use of the system by the subject by collecting subject data base information and tracking information via the tracking information collection devices from each subsequent use of the system by the subject, and adding that subsequent use subject data base information and tracking information to the subject data set.","20","16/900883","2020-06-13","","","11783723","2023-10-10","DANCE4HEALING INC.","Amy Chunmei Li","","","","G09B-0019/0015","G09B-0019/0015 | A61B-0005/165 | A61B-0005/167 | G06N-0005/04 | G06N-0020/00 | G06Q-0050/01 | G09B-0005/065 | G16H-0020/30 | G16H-0050/20","G09B-019/00","G09B-019/00 | G06Q-050/00 | G16H-020/30 | G16H-050/20 | A61B-005/16 | G06N-020/00 | G09B-005/06 | G06N-005/04","","","","","","4923041005430"
"US","US","P","B2","Reduced pressure therapy device operation and authorization monitoring","Embodiments of a negative pressure wound therapy systems and methods for operating the systems are disclosed. In some embodiments, a system includes a pump assembly, canister, and a wound dressing configured to be positioned over a wound. The pump assembly, canister, and the wound dressing can be fluidically connected to facilitate delivery of negative pressure to a wound. The pump assembly can be configured to communicate data to a remote computer. The data can include location information, usage information, therapy information, and the like. Remote management and tracking of the pump assembly can be performed.","1. A method for processing registration and usage information for a plurality of negative pressure wound therapy devices, the method comprising: storing, in a memory device, account information for a plurality of negative pressure wound therapy devices; andby a computer system comprising computer hardware: receiving a registration request to register a negative pressure wound therapy device of the plurality of negative pressure wound therapy devices to an account in the account information, the account associated with a user;determining, based at least on the registration request, that the negative pressure wound therapy device is authorized to be registered to the account;in response to determining that the negative pressure wound therapy device is authorized to be registered to the account, registering the negative pressure wound therapy device to the account so that data gathered by the computer system about operations of the negative pressure wound therapy device is accessible via the account;receiving a first usage notification for the negative pressure wound therapy device, the first usage notification indicating a first location of the negative pressure wound therapy device and a first operation performed by the negative pressure wound therapy device;determining, based at least on a comparison of the first location to a first location identifier corresponding to an authorized location and a comparison of the first operation performed by the negative pressure wound therapy device to a first operation identifier corresponding to an authorized operation, that the negative pressure wound therapy device is being operated in a different location from the authorized location or that the first operation performed by the negative pressure wound therapy device is not the authorized operation;in response to determining that the negative pressure wound therapy device is being operated in the different location from the authorized location or that the first operation performed by the negative pressure wound therapy device is not the authorized operation, performing an exception action;receiving a second usage notification for the negative pressure wound therapy device, the second usage notification indicating a second location of the negative pressure wound therapy device and a second operation performed by the negative pressure wound therapy device;determining, based at least on a comparison of the second location to the first location identifier corresponding to the authorized location and a comparison of the second operation performed by the negative pressure wound therapy device to the first operation identifier corresponding to the authorized operation, that the negative pressure wound therapy device is being operated in the authorized location and that the second operation performed by the negative pressure wound therapy device is the authorized operation; andin response to determining that being operated in the authorized location and that the second operation performed by the negative pressure wound therapy device is the authorized operation, storing in the memory device an indication of performance of the second operation by the negative pressure wound therapy device to the account.","15","17/703145","2022-03-24","2022-0215946","2022-07-07","11783943","2023-10-10","Smith & Nephew, Inc.","Edward Armstrong | Carrie Lee Childress | Tim Warren Dana | William W. Gregory | William Joseph Jaecklein | Michael Mosholder | Felix C. Quintanar","","","","G16H-0040/63","G16H-0040/63 | A61M-0001/96 | G16H-0040/67 | A61M-0001/962 | A61M-0001/982 | A61M-2205/3561 | A61M-2205/3584 | A61M-2205/505 | A61M-2205/52 | G16H-0020/40","G06Q-040/06","G06Q-040/06 | G06Q-040/04 | G06N-005/04 | G06N-020/00 | A61M-001/00 | G16H-040/63 | G16H-040/67 | G16H-020/40","","","","","","4923041005649"
"US","US","P","B2","Method and apparatus for automatic disease state diagnosis","A method of automatically diagnosing pneumonia in a patient includes using an input/output interface device to obtain values of two or more diagnostic parameters of the patient from a caregiver for the patient. The method includes using a processor coupled to the input/output interface to apply the two or more diagnostic parameters to an electronic memory storing precompiled pneumonia diagnostic models to identify an optimal diagnostic model for making a diagnosis. The values of the two or more diagnostic signs are applied to the identified optimal diagnostic model to generate a diagnosis output. The input/output interface device is operated in accordance with the diagnosis output to indicate the presence or absence of pneumonia in the patient to the caregiver. The caregiver may use the diagnosis to provide appropriate care to the patient. The pneumonia diagnostic models are derived from investigation of a population of pneumonia positive and non-pneumonia subjects.","1. A method for automatically providing a caregiver of a patient with a disease state diagnosis of the patient, the method comprising: providing a diagnostic application software product including a multiplicity of diagnostic models derived from investigation of a population containing disease state positive and non-disease state subjects, wherein the diagnostic application software product when executed performs operations comprising: responsive to a first predefined set of general danger signs not being positive and a second predefined set of screening criteria being positive: prompting the caregiver, via an input/output interface of an electronic device including a memory storing the software product, for further patient data for the patient including prompting the caregiver to identify a plurality of diagnostic parameters;automatically selecting an optimal diagnostic model from the diagnostic models from the memory, based at least in part on the further patient data;applying at least the further patient data to the automatically selected optimal diagnostic model; andpresenting a diagnosis to the caregiver on the input/output interface based at least in part on results of the application of at least the further patient data to the automatically selected optimal diagnostic model for the caregiver to use in providing therapy to the patient.","21","16/336269","2017-09-26","2020-0027558","2020-01-23","11783947","2023-10-10","UNIVERSITY OF QUEENSLAND","Udantha Abeyratne | Keegan Kosasih","2016-903894 | 2016-903896","AU | AU","2016-09-26 | 2016-09-26","G16H-0050/20","G16H-0050/20 | A61B-0005/0823 | G06F-0003/16 | G06F-0009/542","G16H-050/20","G16H-050/20 | A61B-005/08 | G06F-003/16 | G06F-009/54","","","","","","4923041005653"
"US","US","P","B2","In-ear nonverbal audio events classification system and method","A system and method for training a classification module of nonverbal audio events and a classification module for use in a variety of nonverbal audio event monitoring, detection and command systems. The method comprises capturing an in-ear audio signal from an occluded ear and defining at least one nonverbal audio event associated to the captured in-ear audio signal. Then sampling and extracting features from the in-ear audio signal. Once the extracted features are validated, associating the extracted features to the at least one nonverbal audio event and updating the classification module with the association. The nonverbal audio event comprises one or a combination of user-induced or externally-induced nonverbal audio events such as teeth clicking, tongue clicking, blinking, eye closing, teeth grinding, throat clearing, saliva noise, swallowing, coughing, talking, yawning with inspiration, yawning with expiration, respiration, heartbeat and head or body movement, wind, earpiece insertion or removal, degrading parts, etc.","1. A method for real time estimation of a noise exposure value in an ear of a user net of user-induced artefacts, the method comprising: using a contactless in-ear microphone to capture an in-ear sound pressure present in an inner portion of an occluded ear canal of the user as an in-ear audio signal;using an outer-ear microphone to capture an outer-ear sound pressure present at the outer entry of the occluded ear canal as an outer-ear audio signal wherein the outer-ear sound pressure is captured simultaneously to the captured in-ear sound pressure;denoising the captured in-ear audio signal using the captured outer-ear audio signal;sampling the in-ear audio signal;extracting, in real time, user-induced non-verbal audio artefacts of each sample of the in-ear audio signal;finding a match between each of the extracted user-induced non-verbal audio artefacts with one of a plurality of pre-recorded samples of user-induced artifacts:subtracting the matching user-induced non-verbal artefact from the simultaneous in-ear audio signal:estimating in-ear audio signal without user-induced artefacts using the remainder of the simultaneous in-ear audio signal:calculating, in real-time, the noise-exposure value using the estimated signal.","20","16/759502","2018-10-29","2020-0312321","2020-10-01","11771372","2023-10-03","ECOLE DE TECHNOLOGIE SUPERIEURE","Jeremie Voix | Hami Montsarrat-Chanon | Rachel Bou Serhal | Patrick Cardinal | Philippe Chabot","","","","A61B-0005/6817","A61B-0005/6817 | G06F-0003/16 | G06F-0018/213 | G06F-0018/217 | G06F-0018/241 | G06N-0020/00 | G10L-0015/22 | G10L-0025/51 | G10L-2015/227","A61B-005/00","A61B-005/00 | G06N-020/00 | G06F-003/16 | G10L-015/22 | G10L-025/51 | G06F-018/213 | G06F-018/21 | G06F-018/241","","","","","","4923040001116"
"US","US","P","B2","System and method for virtual reality data integration and visualization for 3D imaging and instrument position data","Systems and methods for virtual reality or augmented reality (VR/AR) visualization of 3D medical images using a VR/AR visualization system are disclosed that includes a computing device operatively coupled to a VR/AR device, which includes a holographic display and at least one sensor. The holographic display is configured to display a holographic image to an operator. The computing device is configured to receive at least one stored 3D image of a subject's anatomy and at least one real-time 3D position of at least one surgical instrument, to register the at least one real-time 3D position of the at least one surgical instrument to correspond to the at least one 3D image of the subject's anatomy, and to generate the holographic image comprising the at least one real-time position of the at least one surgical instrument overlaid on the at least one 3D image of the subject's anatomy.","1. A VR/AR visualization system, comprising: a VR/AR device comprising a holographic display configured to display a holographic image to an operator; anda computing device operatively coupled to the VR/AR device, the computing device comprising a non-volatile memory and a processor, wherein the computing device is configured to:receive at least one stored 3D image of a subject'ss anatomy, the at least one 3D image including at least one anatomical landmark of the subject'ss anatomy;receive mapping data from at least one surgical instrument, the mapping data including at least one real-time 3D position of the at least one surgical instrument, spatial data of the subject'ss anatomy, and electrophysiological data of the subject'ss anatomy;generate a 3D model using the mapping data;register the 3D model and the at least one stored 3D image of the subject'ss anatomy to a coordinate system by utilizing the spatial data and matching the at least one anatomical landmark of the subject'ss anatomy with the 3D model;generate, using the coordinate system, the holographic image comprising the at least one real-time 3D position of the at least one surgical instrument overlaid on the at least one 3D image of the subject'ss anatomy; andmodify the holographic image by adding or removing a display of the electrophysiological data.","20","16/386156","2019-04-16","2020-0107904","2020-04-09","11771520","2023-10-03","WASHINGTON UNIVERSITY","Jonathan Silva | Jennifer Silva","","","","A61B-0090/37","A61B-0090/37 | A61B-0005/066 | A61B-0005/283 | A61B-0005/743 | A61B-0005/745 | A61B-0034/20 | A61B-0090/361 | G06F-0003/017 | G06T-0015/205 | G06T-0017/20 | G06T-0019/00 | G06T-0019/006 | A61B-0018/02 | A61B-0018/1492 | A61B-2017/00207 | A61B-2018/00351 | A61B-2018/00577 | A61B-2018/00642 | A61B-2018/00839 | A61B-2018/0212 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2063 | A61B-2034/2065 | A61B-2034/252 | A61B-2090/365 | A61B-2090/367 | A61B-2090/368 | A61B-2090/376 | A61B-2090/3782 | A61B-2090/502 | G06T-2210/41 | G06T-2219/004 | G06T-2219/024","A61B-090/00","A61B-090/00 | A61B-005/00 | A61B-005/06 | G06T-019/00 | A61B-005/283 | A61B-034/20 | G06F-003/01 | G06T-015/20 | G06T-017/20 | A61B-034/10 | A61B-034/00 | A61B-017/00 | A61B-090/50 | A61B-018/02 | A61B-018/14 | A61B-018/00","","","","","","4923040001261"
"US","US","P","B1","Using physiological cues to measure data sensitivity and implement security on a user device","Using physiological cues to measure data sensitivity and implement security on a user device. The method may include obtaining data associated with a first physiological state of a user engaged in a first activity on a user device, obtaining data associated with a second physiological state of the user engaged in a second activity on the user device, where the second activity is determined to be more sensitive to the user than the first activity, and where the second physiological state indicates the user's emotional response to the second activity, and implementing a security action on the user device based on the second physiological state of the user engaged in the second activity.","1. A computer-implemented method for using physiological cues to measure data sensitivity and implement security on a user device, at least a portion of the method being performed by a computer device comprising one or more processors, the method comprising: obtaining data associated with a first physiological state of a user while the user is engaged in a first activity on a user device, wherein the first physiological state of the user is based, at least in part, on the first activity on the user device and the first activity on the user device is a non-sensitive activity that the user deems safe;obtaining data associated with a second physiological state of the user while the user is engaged in a second activity on the user device;comparing the first physiological state of the user with the second physiological state of the user to identify a change in the user'ss physiological state while the user is engaged in the second activity;determining, based on the change in the user'ss physiological state while the user is engaged in the second activity, that the second activity is more sensitive to the user than the first activity; andimplementing a security action on the user device based on the determination that the second activity is more sensitive to the user than the first activity.","20","16/293446","2019-03-05","","","11775673","2023-10-03","GEN DIGITAL INC.","Saurabh Shintre | Darren Shou","","","","G06F-0021/6245","G06F-0021/6245 | A61B-0005/0205 | G06F-0021/00 | G06N-0020/00 | H04L-0009/00 | A61B-0003/112 | A61B-0005/01 | A61B-0005/021 | A61B-0005/024 | A61B-0005/112 | A61B-0005/1176 | A61B-0005/443 | A61B-0005/4803 | A61B-2503/12 | G06Q-2220/00","G06F-007/04","G06F-007/04 | H04N-007/16 | G06F-021/62 | G06N-020/00 | A61B-005/0205 | G06F-021/00 | H04L-009/00 | A61B-003/11 | A61B-005/01 | A61B-005/024 | A61B-005/11 | A61B-005/1171 | A61B-005/00 | A61B-005/021","","","","","","4923040005376"
"US","US","P","B2","Risk analysis system and risk analysis method","The risk analysis system according to the present invention includes: a storage apparatus which stores subject data including information related to health of a subject; an analyzer which analyzes a risk related to the health of the subject based on the subject data acquired from the storage apparatus; and an output apparatus which outputs an analysis result by the analyzer. The analyzer has: a risk estimating unit which estimates an event onset risk of the subject based on the subject data; and a medical expense predicting unit which predicts future medical expenses, which are medical expenses to be incurred in the future by the subject, based on the event onset risk estimated by the risk estimating unit and the subject data.","1. A risk analysis system, comprising: an analyzer which acquires blood pressure data of a subject for a plurality of times which are measured during a predetermined period of time and analyzes the acquired blood pressure data for the plurality of times; andan output apparatus which outputs an analysis result by the analyzer, whereinthe analyzer calculates an average value and a variation of systolic blood pressure of the subject based on the acquired blood pressure data for the plurality of times, andthe output apparatus displays a graph in which a circular figure representing a statistical value of systolic blood pressure of the subject is plotted, whereinthe graph is a graph that includes a numeric axis of office blood pressure and a numeric axis of home blood pressure, anda position of a center of the circular figure represents the average value of systolic blood pressure of the subject, and a diameter of the circular figure represents a standard deviation or dispersibility of systolic blood pressure of the subject.","9","17/361372","2021-06-29","2021-0326997","2021-10-21","11776063","2023-10-03","OMRON HEALTHCARE CO., LTD. | OMRON CORPORATION","Hironori Sato | Mitsuharu Konishi | Seisuke Fujiwara","2016-134248","JP","2016-07-06","G06Q-0040/08","G06Q-0040/08 | A61B-0005/021 | G06Q-0010/0635 | G16H-0050/30 | G06Q-0050/22","G06Q-010/10","G06Q-010/10 | H04W-004/029 | G01C-021/20 | G06Q-040/08 | G06Q-010/0635 | G16H-050/30 | A61B-005/021 | G06Q-050/22","","","","","","4923040005764"
"US","US","P","B2","Capturing person-specific self-reported subjective experiences as behavioral predictors","Disclosed methodologies provide improved predictors of patient treatment adherence by using person-specific subjective experience and social-environmental factors. Methodologies combine emotion and data sciences. Advanced tools capture, measure, store, and analyze self-report of subjective experiences using digital applications and platforms. Patient-specific data is obtained regarding emotional or affective determinants and social determinants for generating a calculated composite score of the patient's probability of adherence or achievement relative to target outcomes, e.g. adherence to treatment plans, wellness activities, etc. for a subject individual. Internal/subjective factors are judged by self-report measures designed to validly judge tested factors based on a patient adjusting continuously-variable graphical interfaces to capture and measure subjective experiences. Emotional characteristics may include perception and intensity in each category of sickness versus wellness, stress, depression, anxiety, pain, and feelings about most recent health provider/staff interaction (with determined intensity for choices of Delighted, Satisfied, Meh, Disappointed, Frustrated). Emotional characteristics may be considered among health, and social characteristics in measuring potential obstacles to adherence.","1. A methodology for predictively determining a patient'ss likelihood to adhere to a healthcare treatment plan for such patient, comprising: creating a survey comprising a plurality of survey items related to selected determined obstacles to adherence;interactively conducting the survey for a given patient by having the patient use a movable feature of a graphical interface on a display to respectively capture and definitively measure through non-verbal communication the patient'ss subjective experiences for each of the plurality of survey items, to form a set of data for the given patient for the corresponding plurality of survey items, wherein the graphical interface comprises digital graphic shape-shifting icons for such patient to view on said display, with the patient using the movable feature to manipulate the appearance of each respective icon through a range of appearances thereof so that it reflects the self-reported intensity of how such patient subjectively feels in response to each item of the patient survey; andassessing the patient'ss set of data to determine a relative score for such patient for likelihood to adhere to a healthcare treatment plan.","59","17/011240","2020-09-03","2021-0065854","2021-03-04","11776668","2023-10-03","ADOH SCIENTIFIC, LLC","Gregg T. Hanold | Brian Sullivan | Corley Sullivan","","","","G16H-0010/20","G16H-0010/20 | A61B-0005/14532 | A61B-0005/165 | A61B-0005/4833 | A61B-0005/7275 | A61B-0005/7405 | A61B-0005/748 | G06F-0003/0482 | G06N-0020/00 | G06Q-0030/0203 | G06Q-0030/0282 | G16H-0010/60 | G16H-0015/00 | G16H-0020/10 | G16H-0020/30 | G16H-0040/20 | G16H-0050/30 | G16H-0050/70 | G06F-0003/04847","G16H-010/20","G16H-010/20 | G16H-050/70 | G16H-015/00 | G16H-050/30 | G16H-020/10 | G16H-020/30 | G06Q-030/0203 | G06Q-030/0282 | G16H-040/20 | G06N-020/00 | G06F-003/0482 | A61B-005/16 | A61B-005/145 | A61B-005/00 | G16H-010/60 | G06F-003/04847","","","","","","4923040006364"
"US","US","P","B2","Methods and devices for determining signal quality for a swallowing impairment classification model","A device can screen swallowing safety and swallowing efficiency. The device includes a processor configured to receive accelerometry data, determine a an A-P and S-I signal summed spectrogram from the accelerometry data, and perform one or more of identifying a missing swallow, identifying that the data was clipped from the start, identifying that the data was clipped from the end, or identifying that the data contains noise. The device can have a user interface configured to provide one or more outputs including at least one of audio or graphics based on these identifications. If the data does not contain these signal quality issues, the device can compare the data against preset classification criteria defined for each of swallowing safety and swallowing efficiency and thus classify each of the swallowing events with a swallowing safety classification and a swallowing efficiency classification.","1. A device for screening swallowing safety and swallowing efficiency, the device comprising: a processor configured to receive accelerometry data of throat vibrations, determine vibration signals of the accelerometry data along an anterior-posterior axis and a superior-inferior axis of the throat to be used in determining a spectrogram, and perform at least one method comprising (i) determining a signal variance of the accelerometry data as a function of time, as summed power over a specific frequency range of the spectrogram, and comparing the signal variance to a first threshold value,the at least one method further comprising at least one additional method selected from the group consisting of (ii) determining a normalized variance signal from the spectrogram by setting a span between 0 and 1, selecting a beginning portion of the normalized variance signal, and comparing values of the beginning portion of the normalized variance signal to a second threshold value, (iii) determining a normalized variance signal from the spectrogram by setting a span between 0 and 1, selecting an end portion of the normalized variance signal, and comparing values of the end portion of the normalized variance signal to a third threshold value, and (iv) determining a summed power spectral density of both the vibration signals along the anterior-posterior axis and the superior-inferior axis of the throat as an average of the spectrogram over a full duration or signal length relating to consumption of a single bolus and applying spectral entropy for the summed power spectral density (PSD) of the vibration signals along both the anterior-posterior axis and the superior-inferior axis of the throat for comparison to a fourth threshold value, wherein the at least one method is performed by the processor in real-time relative to receipt of the accelerometry data; anda user interface configured to provide one or more outputs comprising at least one of audio or graphics, and the one or more outputs indicate a missing swallow for the accelerometry data based on one or more results of the at least one method.","17","16/636449","2018-07-31","2020-0170562","2020-06-04","11766210","2023-09-26","SOCIETE DES PRODUITS NESTLE S.A.","Michael Reuben Jedwab | Juha M. Kortelainen | Rajat Mukherjee | Harri Polonen","","","","A61B-0005/4205","A61B-0005/4205 | A61B-0005/7203 | A61B-0005/7264 | A61B-0005/7435 | G06F-0003/011 | G06F-0003/017 | G06F-0017/18 | A61B-2562/0219","A61B-005/00","A61B-005/00 | G06F-003/01 | G06F-017/18","","","","","","4923039000801"
"US","US","P","B2","Intravascular data visualization and interface systems and methods","In part, the disclosure relates to intravascular data collection systems and the software-based visualization and display of intravascular data relating to detected side branches and side branch obstruction. An estimate of side branch diameter can be made based on a vessel profile or a maximum diameter of a vessel at a distal and proximal location relative to the side branch. A amount of side branch obstruction may be determined by comparing an observed side branch diameter with in the image data with the estimated side branch diameter. In addition, an amount of blood flow obstruction may also be determined.","1. A method of identifying vascular structure, the method comprising the steps of: receiving, by one or more processors, image data for a blood vessel;identifying a side branch from the blood vessel within the image data;identifying an observed diameter of the side branch based on the image data;obtaining, by the one or more processors, a representation of vessel diameters at a distal segment and at a proximal segment of the blood vessel relative to the side branch;determining, by the one or more processors, an estimated diameter of the side branch based on the representation of vessel diameters; anddetermining an amount of branch obstruction based on a comparison of the estimated diameter with the observed diameter.","18","17/674424","2022-02-17","2022-0244841","2022-08-04","11768593","2023-09-26","LIGHTLAB IMAGING, INC.","Ajay Gopinath | Denis Dion | Christopher E. Griffin | Desmond Adler","","","","G06F-0003/04847","G06F-0003/04847 | A61B-0005/0066 | A61B-0005/0084 | A61B-0005/02007 | A61B-0005/6876 | A61B-0005/743 | A61B-0005/7435 | A61B-0008/12 | A61B-0034/10 | A61B-0034/20 | G06F-0003/04815 | G06T-0007/0012 | G06T-0015/08 | G06T-0019/003 | G16H-0020/40 | G16H-0030/40 | G16H-0050/20 | A61B-0005/6852 | A61B-2034/105 | A61B-2034/2055 | A61B-2034/2063 | G06T-2200/24","G06F-003/048","G06F-003/048 | G06F-003/04847 | A61B-034/10 | A61B-005/00 | A61B-034/20 | A61B-008/12 | G06T-015/08 | G06T-007/00 | G06T-019/00 | G06F-003/04815 | A61B-005/02 | G16H-030/40 | G16H-050/20 | G16H-020/40","","","","","","4923039003170"
"US","US","P","B2","Video used to automatically populate a postoperative report","Systems and methods for automatically populating a post-operative report of a surgical procedure are disclosed. A system may include at least one processor configured to implement a method including receiving an identifier of a patient, an identifier of a healthcare provider, and surgical footage of a surgical procedure performed on the patient. The method may include analyzing frames of the surgical footage to identify phases of the surgical procedure based on interactions between medical instruments and biological structures and, based on the interactions, associate a name with each phase. The method may include determining a beginning of each phase and associating a time marker with the beginning of each phase. The method may include populating a post-operative report with the patient identifier, the names of the phases, and time markers associated with the phases in a manner that enables the health care provider to alter the post-operative report.","1. A non-transitory computer readable medium containing instructions that, when executed by at least one processor, cause the at least one processor to execute operations enabling automatically populating a post-operative report of a surgical procedure, the operations comprising: receiving an input of an identifier of a patient;receiving an input of an identifier of a health care provider;receiving an input of surgical footage of a surgical procedure performed on the patient by the health care provider;analyzing a plurality of frames of the surgical footage to identify phases of the surgical procedure based on detected interactions between medical instruments and biological structures and, based on the interactions, associate a name with each identified phase;determining at least a beginning of each identified phase;associating a time marker with the beginning of each identified phase;transmitting data to the health care provider, the transmitted data including the patient identifier, the names of the identified phases of the surgical procedure, and the time marker of the identified phase; andpopulating a post-operative report with the transmitted data in a manner that enables the health care provider to alter the phase names in the post-operative report.","15","17/347629","2021-06-15","2021-0298869","2021-09-30","11769207","2023-09-26","THEATOR INC.","Tamir Wolf | Dotan Asselmann","","","","G06Q-0040/08","G06Q-0040/08 | A61B-0001/000094 | A61B-0001/000096 | A61B-0005/02042 | A61B-0005/1032 | A61B-0005/7267 | A61B-0005/7275 | A61B-0005/746 | A61B-0017/00 | A61B-0034/10 | A61B-0034/25 | A61B-0034/30 | A61B-0034/37 | A61B-0034/70 | A61B-0090/06 | A61B-0090/361 | A61B-0090/37 | A61B-0090/92 | G06F-0016/71 | G06F-0016/735 | G06Q-0010/06312 | G06V-0020/20 | G06V-0020/40 | G06V-0020/41 | G06V-0020/49 | G06V-0020/52 | G11B-0027/10 | G16H-0015/00 | G16H-0020/40 | G16H-0030/20 | G16H-0030/40 | G16H-0040/20 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | G16H-0070/20 | A61B-0001/04 | A61B-2017/0011 | A61B-2017/00119 | A61B-2034/104 | A61B-2034/252 | A61B-2034/254 | A61B-2034/256 | A61B-2034/302 | A61B-2090/065 | A61B-2090/0807 | A61B-2505/05 | G06V-0020/44 | G06V-0040/10 | G06V-2201/034","G06K-009/00","G06K-009/00 | G06Q-040/08 | A61B-090/00 | A61B-034/00 | A61B-034/37 | G16H-020/40 | G16H-050/70 | G16H-050/20 | G16H-030/40 | G16H-070/20 | G16H-015/00 | A61B-034/30 | G16H-040/63 | A61B-017/00 | G16H-050/30 | G06F-016/71 | A61B-034/10 | G16H-040/20 | G16H-030/20 | A61B-005/02 | A61B-005/103 | A61B-005/00 | G06Q-010/0631 | G06V-020/20 | G06V-020/40 | G06V-020/52 | A61B-090/92 | G06F-016/735 | G11B-027/10 | A61B-001/00 | G06V-040/10 | A61B-001/04","","","","","","4923039003776"
"US","US","P","B2","Treatment procedure planning system and method","A system and method for planning surgical procedure including a treatment zone setting view presenting at least one slice of a 3D reconstruction generated from CT image data including a target. The treatment zone setting view presenting a treatment zone marker defining a location and a size of a treatment zone and configured to adjust the treatment zone marker in response to a received user input. The system and method further including a volumetric view presenting a 3D volume derived from the 3D reconstruction and a 3D representation of the treatment zone marker relative to structures depicted in the 3D volume.","1. A user interface for planning a treatment procedure, the user interface comprising: a treatment zone setting view including a display of:at least one of a 3D rendering generated from CT image data including a target or a slice of a 3D reconstruction generated from CT image data including the target;at least one of a user-selectable power level setting having a predetermined size limit corresponding to the user-selectable power level setting or a user-selectable treatment device having a predetermined size limit corresponding to the user-selectable treatment device, wherein the predetermined size limit corresponding to the user-selectable power level setting and the predetermined size limit corresponding to the user-selectable treatment device are associated with a threshold of a size of a treatment zone;a treatment zone marker defining a location and the size of the treatment zone displayed relative to the at least one of the 3D rendering or the slice of the 3D reconstruction, the treatment zone marker being adjustable by received treatment zone marker adjustment inputs to adjust the size of the treatment zone defined by the treatment zone marker; andan alert generated when the size of the treatment zone defined by the treatment zone marker is adjusted to exceed at least one of the predetermined size limit corresponding to the user-selectable power level setting or the predetermined size limit corresponding to the user-selectable treatment device.","20","17/567940","2022-01-04","2022-0130102","2022-04-28","11769292","2023-09-26","COVIDIEN LP","Jeetendra Bharadwaj | Kevin J. Frank | Darren G. Girotto | Benjamin M. Corum","","","","G06T-0015/08","G06T-0015/08 | A61B-0006/032 | A61B-0034/10 | A61B-0090/10 | G06F-0003/0481 | G06F-0003/04842 | G06F-0003/04847 | G06T-0007/0012 | G06T-0019/00 | A61B-0006/12 | A61B-0006/463 | A61B-0006/465 | A61B-0006/469 | A61B-0006/487 | A61B-0006/5235 | A61B-0034/25 | A61B-2018/00577 | A61B-2034/101 | A61B-2034/102 | A61B-2034/105 | A61B-2034/107 | A61B-2560/0475 | A61B-2576/00 | A61N-0005/1001 | A61N-0005/103 | A61N-2005/1074 | G06T-2200/24 | G06T-2207/10081 | G06T-2207/20108 | G06T-2210/41 | G06T-2219/028","G06T-015/08","G06T-015/08 | A61B-090/10 | G06T-019/00 | A61B-034/10 | A61B-006/03 | G06F-003/0481 | G06F-003/04842 | G06F-003/04847 | G06T-007/00 | A61B-006/00 | A61B-006/12 | A61N-005/10 | A61B-034/00 | A61B-018/00","","","","","","4923039003860"
"US","US","P","B2","Skin detection method and electronic device","A skin detection and evaluation method includes: obtaining a face image of a user; detecting a skin problem that appears in the face image; prompting in a first interface, the user that the skin problem appears on a face, wherein the first interface comprises the face image; and displaying a second interface in response to a first operation performed by the user in the first interface, wherein the second interface comprises a first facial simulated image obtained after the skin problem is aged; or displaying a third interface in response to a second operation performed by the user in the first interface, wherein the third interface comprises a second facial simulated image obtained after the skin problem is de-aged.","1. A skin detection method, comprises: obtaining, by an electronic device, a face image of a user;detecting, by the electronic device, a skin problem that appears in the face image;prompting, by the electronic device in a first interface, the user that the skin problem appears on a face, wherein the first interface comprises the face image; anddisplaying, by the electronic device, a second interface in response to a selection performed by the user in the first interface, wherein the selection is a selection of a first aging operation or a second aging operation, wherein the second interface comprises a first facial simulated image obtained after the skin problem is aged; ordisplaying, by the electronic device, a third interface in response to the selection performed by the user in the first interface, wherein the selection is a selection of a first de-aging operation or a second de-aging operation, wherein the third interface comprises a second facial simulated image obtained after the skin problem is de-aged;wherein the method further comprises:(1) when the skin problem comprises a color spot problem and the selection is a selection of the first aging operation: detecting, by the electronic device in the face image, a color spot region in which the color spot problem appears;obtaining, by the electronic device, a change coefficient K1 of an L pixel channel, a change coefficient K2 of a pixel channel, and a change coefficient K3 of a b pixel channel in the color spot region;performing, by the electronic device, aging processing on the L pixel channel in the color spot region, to obtain a pixel value L′ of an aged L pixel channel, wherein L′=L+K1×C1×L, L is a pixel value of the L pixel channel before the aging processing, and C1 is a constant;performing, by the electronic device, aging processing on the a pixel channel in the color spot region, to obtain a pixel value a′ of an aged a pixel channel, wherein a′=a+K2×C2× a, a is a pixel value of the a pixel channel before the aging processing, and C2 is a constant; andperforming, by the electronic device, aging processing on the b pixel channel in the color spot region, to obtain a pixel value b′ of an aged b pixel channel, wherein b′=b+K3×C3×b, b is a pixel value of the b pixel channel before the aging processing, and C3 is a constant; or(2) when the skin problem comprises a color spot problem and the selection is a selection of the first de-aging operation: detecting, by the electronic device in the face image, a color spot region in which the color spot problem appears;obtaining, by the electronic device, a change coefficient K1 of an L pixel channel, a change coefficient K2 of a pixel channel, and a change coefficient K3 of a b pixel channel in the color spot region;performing, by the electronic device, de-aging processing on the L pixel channel in the color spot region, to obtain a pixel value L′ of de-aged L pixel channel, wherein L′=L?K1×C1×L, L is a pixel value of the L pixel channel before the de-aging processing, and C1 is a constant;performing, by the electronic device, de-aging processing on the a pixel channel in the color spot region, to obtain a pixel value a′ of a de-aged a pixel channel, wherein a′=a?K2×C2× a, a is a pixel value of the pixel channel before the de-aging processing, and C2 is a constant; andperforming, by the electronic device, de-aging processing on the b pixel channel in the color spot region, to obtain a pixel value b′ of a de-aged b pixel channel, wherein b′=b?K3×C3×b, b is a pixel value of the b pixel channel before the de-aging processing, and C3 is a constant; or(3) the skin problem comprises a fine line problem and the selection is a selection of the second aging operation: detecting, by the electronic device in the face image, a fine line region in which the fine line problem appears;obtaining, by the electronic device, a change coefficient D of the fine line region;performing, by the electronic device, aging processing on an R pixel channel in the fine line region, to obtain a pixel value R′ of an aged R pixel channel, wherein R′=R+C5×D, R is a pixel value of the R pixel channel before the aging processing, and C5 is a constant;performing, by the electronic device, aging processing on a G pixel channel in the fine line region, to obtain a pixel value G′ of an aged G pixel channel, wherein G′=G+C6×D, G is a pixel value of the G pixel channel before the aging processing, and C6 is a constant; andperforming, by the electronic device, aging processing on a B pixel channel in the fine line region, to obtain a pixel value B′ of an aged B pixel channel, wherein B′=B+C7×D, R is a pixel value of the B pixel channel before the aging processing, and C7 is a constant; or(4) the skin problem comprises a fine line problem and the selection is a selection of the second de-aging operation: obtaining, by the electronic device, a change coefficient D of the fine line region in the face image;performing, by the electronic device, de-aging processing on an R pixel channel in the fine line region, to obtain a pixel value R′ of a de-aged R pixel channel, wherein R′=R?C5×D, R is a pixel value of the R pixel channel before the de-aging processing, and C5 is a constant;performing, by the electronic device, de-aging processing on a G pixel channel in the fine line region, to obtain a pixel value G′ of a de-aged G pixel channel, wherein G′=G?C6×D, R is a pixel value of the G pixel channel before the de-aging processing, and C6 is a constant andperforming, by the electronic device, de-aging processing on a B pixel channel in the fine line region, to obtain a pixel value B′ of a de-aged B pixel channel, wherein B′=B?C7×D, R is a pixel value of the B pixel channel before the de-aging processing, and C7 is a constant.","10","17/418368","2019-12-02","2022-0148161","2022-05-12","11759143","2023-09-19","HONOR DEVICE CO., LTD. | HUAWEI TECHNOLOGIES CO., LTD.","Zhizhi Guo | Hongwei Hu | Wenmei Gao | Chen Dong","2018-11603196","CN","2018-12-26","A61B-0005/441","A61B-0005/441 | A61B-0005/1032 | A61B-0005/444 | A61B-0005/7435 | G06F-0003/04847 | G06T-0007/0012 | G06T-0007/90 | G06T-0011/001 | G06T-0011/60 | G06V-0010/507 | G06V-0010/56 | G06V-0040/169 | G06V-0040/171 | G06T-2200/24 | G06T-2207/30088 | G06T-2207/30201","G06T-007/90","G06T-007/90 | G06T-011/00 | G06T-007/00 | G06T-011/60 | G06F-003/048 | H04N-001/60 | A61B-005/00 | A61B-005/103 | G06F-003/04847 | G06V-010/56 | G06V-010/50 | G06V-040/16","","","","","","4923038000868"
"US","US","P","B2","Medical report labeling system and method for use therewith","A medical scan report labeling system is operable to transmit a medical report that includes natural language text to a first client device for display. Identified medical condition term data is received from the first client device in response. An alias mapping pair in a medical label alias database is identified by determining that a medical condition term of the alias mapping pair compares favorably to the identified medical condition term data. A medical code that corresponds to the alias mapping pair and a medical scan that corresponds to the medical report are transmitted to a second client device of an expert user for display, and accuracy data is received from the second client device in response. The medical code is mapped to the first medical scan in the medical scan database when the accuracy data indicates that the medical code compares favorably to the medical scan.","1. A medical scan report labeling system, comprising: a medical label alias database that includes a plurality of alias mapping pairs, wherein each alias mapping pair includes a one of a plurality of medical condition terms and a corresponding one of a plurality of medical codes, wherein each of the plurality of medical condition terms in the plurality of alias mapping pairs are unique, and wherein each of the plurality of medical condition terms includes at least one word;a processing system that includes a processor; anda memory that stores executable instructions that, when executed by the processing system, facilitate performance of operations comprising: facilitating retrieval, at a first client device associated with a first user in a plurality of users of the medical scan report labeling system, of a first medical report from a medical scan database that includes a plurality of medical scans and a corresponding plurality of medical reports, wherein each of the plurality of medical reports includes natural language text data describing the corresponding one of the plurality of medical scans, wherein the first user is distinct from a medical professional that wrote the first medical report, wherein the natural language text data of the first medical report is displayed to the first user via a first interactive interface displayed by a first display device corresponding to the first client device;facilitating retrieval of a first identified medical condition term data for the first medical report indicating one or more words selected from the natural language text data of the first medical report via user input by the first user, wherein the first identified medical condition term data is generated by the first client device in response to a first prompt via the first interactive interface displayed by the first display device to identify a medical condition term based on the natural language text data of the first medical report;identifying a first alias mapping pair of the plurality of alias mapping pairs in the medical label alias database based on determining a first medical condition term of the first alias mapping pair includes at least one word that matches at least one of the one or more words included in the first identified medical condition term data;retrieving a first medical code of the plurality of medical codes that corresponds to the first alias mapping pair from the medical label alias database;transmitting the first medical code and a first medical scan in the plurality of medical scans that corresponds to the first medical report to a second client device associated with a second user that is different from the first user, wherein the first medical code and the first medical scan are displayed to the second user via a second interactive interface displayed by a second display device corresponding to the second client device;receiving first accuracy data from the second client device indicating whether the first medical code correctly corresponds to the first medical scan, wherein the first accuracy data is generated by the second client device based on user input via the second user in response to a second prompt displayed by the second display device via the second interactive interface to provide first accuracy data based on the first medical code and the first medical scan; andmapping the first medical code to the first medical scan in the medical scan database based on the first accuracy data indicating that the first medical code correctly corresponds to the first medical scan.","20","17/135067","2020-12-28","2021-0118552","2021-04-22","11763933","2023-09-19","ENLITIC, INC.","Devon Bernard | Kevin Lyman | Li Yao | Brian Basham | Rewon Child","","","","G16H-0030/40","G16H-0030/40 | A61B-0005/002 | A61B-0005/0022 | A61B-0006/032 | A61B-0006/4233 | A61B-0006/463 | A61B-0006/468 | A61B-0006/50 | A61B-0006/503 | A61B-0006/5217 | A61B-0006/5288 | A61B-0006/5294 | A61B-0006/563 | A61B-0008/468 | A61B-0008/5223 | A61B-0008/565 | G01T-0001/247 | G06F-0003/048 | G06F-0003/167 | G06F-0018/22 | G06F-0018/24 | G06F-0040/169 | G06F-0040/197 | G06F-0040/247 | G06F-0040/279 | G06F-0040/30 | G06F-0040/56 | G06N-0003/04 | G06N-0003/045 | G06N-0003/084 | G06Q-0010/10 | G06Q-0010/103 | G06Q-0050/22 | G06T-0007/0012 | G06T-0007/0016 | G06T-0007/11 | G06V-0010/761 | G06V-0010/764 | G06V-0010/98 | G16H-0010/60 | G16H-0015/00 | G16H-0030/20 | G16H-0040/20 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | H04N-0005/32 | A61B-0006/505 | G06F-0003/0485 | G06F-0003/04842 | G06F-0018/214 | G06N-0007/01 | G06N-0020/10 | G06T-0011/003 | G06T-0011/60 | G06T-2200/24 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/10104 | G06T-2207/10116 | G06T-2207/10132 | G06T-2207/20081 | G06T-2207/30004 | G06T-2207/30061 | G06T-2207/30068 | G06V-0010/751 | G06V-2201/03 | G16H-0040/67 | G16H-0050/50 | H04L-0067/01 | H04L-0067/12","G16H-030/40","G16H-030/40 | G16H-015/00 | G06F-003/048 | G06T-007/00 | G06Q-050/22 | G16H-050/20 | A61B-006/00 | A61B-008/00 | A61B-008/08 | G16H-050/70 | G06F-040/30 | G06F-040/56 | G06F-040/169 | G06F-040/197 | G06F-040/247 | G06F-040/279 | G16H-030/20 | G06T-007/11 | G16H-040/20 | G01T-001/24 | G06V-010/98 | G06F-018/22 | G06F-018/24 | G06N-003/045 | G06V-010/74 | G06V-010/764 | G16H-010/60 | G16H-050/30 | A61B-006/03 | G06F-003/16 | A61B-005/00 | G06N-003/04 | G06N-003/084 | G06Q-010/10 | H04N-005/32 | G16H-040/63 | G06T-011/60 | G06N-020/10 | G16H-040/67 | G06V-010/75 | H04L-067/01 | G06F-018/214 | G06N-007/01 | G16H-050/50 | H04L-067/12 | G06F-003/04842 | G06F-003/0485 | G06T-011/00","","","","","","4923038005612"
"US","US","P","B2","Apparatus and method for operating a personal grooming appliance or household cleaning appliance","A system and method for operating a personal grooming/household appliance, including: providing a personal grooming/household appliance including at least one physical sensor taken from a group consisting of: an orientation sensor, an acceleration sensor, an inertial sensor, a global positioning sensor, a pressure sensor, and a load sensor, audio sensor, humidity sensor, and a temperature sensor; providing a camera associated with the personal grooming/household appliance; classifying data received from the physical sensor and from the camera using at least one trained machine learning classifier to generate an augmented classification; and providing user feedback information based upon the augmented classification or modifying operation of the grooming/household appliance based upon the augmented classification.","1. A method for operating a personal grooming appliance, comprising: providing a personal grooming appliance including, at least one physical sensor taken from a group consisting of: an orientation sensor, an acceleration sensor, an inertial sensor, a global positioning sensor, a pressure sensor, a load sensor, audio sensor, humidity sensor, and a temperature sensor;providing a camera associated with the personal grooming appliance;deriving an augmented classification using one or more classifiers classifying the physical sensor data and the image data; andmodifying operation of a powered and electronically controlled grooming implement based upon the augmented classification.","14","17/897289","2022-08-29","2022-0414879","2022-12-29","11752650","2023-09-12","The Procter & Gamble Company","Jonathan Livingston Joyce | Faiz Feisal Sherman | Xiaole Mao | Reiner Engelmohr | Christian Peter Mandl | Moritz Poetzsch | Nasir Saeed Khan","","","","B26B-0021/4087","B26B-0021/4087 | A46B-0013/02 | A46B-0015/0004 | A47L-0009/2826 | A61B-0005/0077 | A61B-0005/0088 | G05B-0013/0265 | G05B-0015/02 | G06F-0018/214 | G06F-0018/2431 | G06N-0003/08 | G06N-0005/04 | G06N-0020/00 | G06Q-0030/0631 | G06T-0007/0012 | G09B-0019/0084 | A47L-2201/04 | A47L-2201/06 | A61C-0017/221 | G06F-0018/217 | G06T-2207/20081 | G06T-2207/30036 | G06T-2207/30088 | G06T-2207/30201","B26B-021/40","B26B-021/40 | A61B-005/00 | G06N-003/08 | G06Q-030/0601 | G06T-007/00 | G06N-020/00 | A46B-013/02 | A46B-015/00 | A47L-009/28 | G05B-015/02 | G06N-005/04 | G09B-019/00 | G05B-013/02 | G06F-018/214 | G06F-018/2431 | A61C-017/22 | G06F-018/21","","","","","","4923037001824"
"US","US","P","B2","Biometric authentication device, biometric authentication method, and computer readable medium","A measurement unit (110) performs a measurement process for measuring a biological signal (21) from a target person (20). The biological signal (21) contains a plurality of components and can be measured in a manner to be unnoticeable by the target person (20). A component extraction unit (120) extracts an authentication component, which is to be used for authentication, from the plurality of components. A feature amount extraction unit (130) extracts a current feature amount indicating a present feature amount of the authentication component, from the authentication component. A registration unit (140) registers an identifier, which is used for identifying the target person (20), and a template feature amount, which is a feature amount extracted from the target person (20) in the past, in a storage unit (160), as template information (161). A comparison unit (150) compares the current feature amount to the template feature amount. When a difference between the current feature amount and the template feature amount is within a tolerance value (162), the comparison unit (150) returns processing to the measurement process and the authentication is repeated. When the difference between the current feature amount and the template feature amount is larger than the tolerance value (162), the processing is ended.","1. A biometric authentication device to execute authentication to a target person by biometric authentication, the biometric authentication device comprising: processing circuitry to:perform a measurement process for measuring a biological signal from the target person, the biological signal being able to be measured in a manner to be unnoticeable by the target person,separate a respiration component derived from respiratory movement and a pulse component derived from a pulse, from the biological signal and extract the respiration component and the pulse component as a plurality of authentication components being to be used for authentication,extract a current feature amount indicating a present feature amount of each of the plurality of authentication components, from each of the plurality of authentication components,register an identifier and a template feature amount in a memory, as template information, the identifier being used for identifying the target person, the template feature amount being a feature amount extracted from the target person in a past, andcompare the current feature amount to the template feature amount registered in the template information so as to return processing to the measurement process and repeat authentication when a difference between the current feature amount and the template feature amount is within a tolerance value, and so as to end the processing when the difference between the current feature amount and the template feature amount is larger than the tolerance value, whereinthe processing circuitry executes authentication by using a current feature amount of each of the respiration component and the pulse component in initial authentication, and executes authentication by using a current feature amount of only the respiration component in second and subsequent authentication.","6","17/742156","2022-05-11","2022-0269765","2022-08-25","11755710","2023-09-12","MITSUBISHI ELECTRIC CORPORATION","Shun Hinatsu | Daisuke Suzuki","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/0205 | A61B-0005/02433 | A61B-0005/0816 | A61B-0005/1172 | G06V-0040/1306 | G06V-0040/1365 | G06V-0040/15 | G06V-0040/50","G06K-009/62","G06K-009/62 | G06F-021/32 | G06V-040/12 | G06V-040/13 | G06V-040/10 | G06V-040/50 | A61B-005/0205 | A61B-005/024 | A61B-005/08 | A61B-005/1172","","","","","","4923037004865"
"US","US","P","B2","ML-based methods for pseudo-CT and HR MR image estimation","The present disclosure describes a computer-implemented method of transforming a low-resolution MR image to a high-resolution MR image using a deep CNN-based MRI SR network and a computer-implemented method of transforming an MR image to a pseudo-CT (sCT) image using a deep CNN-based sCT network. The present disclosure further describes a MR image-guided radiation treatment system that includes a computing device to implement the MRI SR and CT networks and to produce a radiation plan based in the resulting high resolution MR images and sCT images.","1. A computer-implemented method of transforming a low-resolution MR image into a super-resolution MR image using an MRI SR deep CNN system comprising a deep CNN-based de-noising auto-encoder (DAE) network and a deep CNN-based super-resolution generative network (SRG), the method comprising: receiving, using a computing device, a low-resolution MR image;transforming, using the computing device, the low-resolution MR image into a de-noised MR image using the DAE network, the DAE network comprising six convolutional encoder layers with 4×4 filters and six de-convolutional decoder layers with 4×4 filters, wherein each convolutional encoder layer comprises a single convolutional filter with stride 2, each de-convolution decoder layer comprises a single deconvolutional filter with stride 2, and each convolutional encoder layer and each de-convolution decoder layer ends with a leaky and standard rectified linear unit (ReLU); and,transforming, using the computing device, the de-noised MR image into the super-resolution MR image using the SRG network.","6","16/525562","2019-07-29","2020-0034948","2020-01-30","11756160","2023-09-12","WASHINGTON UNIVERSITY","Chunjoo (Justin) Park | Sasa Mutic | Hao Zhang | Olga Green","","","","G06T-0003/4053","G06T-0003/4053 | A61B-0006/5258 | A61N-0005/1039 | G06F-0017/18 | G06N-0003/045 | G06N-0003/08 | G06N-0020/20 | G06T-0005/002 | G06T-2207/10088","G06T-003/40","G06T-003/40 | G06T-005/00 | G06N-003/08 | G06N-003/04 | G06F-017/18 | A61B-006/00 | A61N-005/10 | G06N-020/20 | G06N-003/045","","","","","","4923037005310"
"US","US","P","B2","Method for objectively tracking and analyzing the social and emotional activity of a patient","A method and system for objectively tracking and analyzing the social and emotional activity of a patient using an augmented reality computing device is provided. A patient is permitted to manually manipulate a target object in the physical world while viewing an augmented version showing a unique animated character representing either an abstract language, emotions, or social skills, depending on the module. The present system tracks and records the active face and the time spent on the active face, where the active face is the face upon which the patient's focus is automatically estimated, through calculation, to be trained upon. An observer views the session, the data recorded, and an automatically generated graphical representation of the data, which permits the observer to speak to patient regarding the character or scene rendered on the face which is determined to be the active face, helping the student engage in the session.","1. A method for objectively tracking and analyzing the social and emotional activity of a patient using a computing device, the method comprising the steps of: implementing an augmented reality tracking application in memory on the computing device;detecting by at least one camera a multi-faced target object manipulated by the patient, the multi-faced target object comprising at least one face, each of the at least one face comprising a computer-readable fiducial marker;displaying on a screen of the computing device at least a first set of augmented images or videos overlaid on the multi-faced target object, each of the first set of augmented images or videos being associated with one of the at least one face of the multi-faced target object, and wherein each of the at least first set of augmented images or videos is associating with a therapeutic subject;detecting a position and a rotation data relative to the camera of the multi-faced target object for each point in time; wherein the position and rotation data comprises one or more parameters indicative of the patient focusing attention on each of the at least first set of augmented images or videos, wherein the one or more parameters comprises the time each of the at least one face is in an upright orientation, the time each of the at least one face has a larger screen area, the amount of patient interaction associated with each of the at least one face, and the therapeutic subject associated with each of the at least first set of augmented images or videos;calculating a coordinate data set of the multi-faced target object based on the position and rotation data detected;determining an active face for each point in time using the coordinate data set, wherein the active face can be dynamically assigned to each of the at least one face at each point in time and; andstoring as a data set the position and rotation data, the coordinate data set, the active face, and a time for each point in time.","17","17/651795","2022-02-19","2022-0167896","2022-06-02","11744495","2023-09-05","FROM ZERO, LLC","Kevin Chaja | Natasha Chaja","","","","A61B-0005/165","A61B-0005/165 | G06F-0003/012 | G06F-0003/04883 | G06F-0009/542 | G06T-0007/0014 | G06T-0019/006 | G06F-2203/011 | G06T-2207/30201","A61B-005/16","A61B-005/16 | G06F-003/01 | G06F-003/04883 | G06T-019/00 | G06F-009/54 | G06T-007/00","","","","","","4923036001140"
"US","US","P","B2","Machine implemented virtual health and beauty system","An apparatus is provided that includes comprising: a processing circuit to accept data indicative at least one health of beauty state of a user; a communication circuit to convey the accepted data to machine learning models and to receive a regimen recommendation from the machine learning models; and a user interface circuit to present the regimen recommendation to the user.","1. An apparatus comprising: circuitry configured to accept data indicative of at least one health or beauty state of a user from multiple types of diagnostic devices, including a hairbrush equipped with sensors and a detector that measures sleep patterns; anda microphone configured to receive a vocal query from the user regarding the least one health or beauty state of a user;wherein the circuitry is further configured to collect the accepted data from at least one of the multiple types of diagnostic devices based on associating a type of data sensed or measured from the at least one of the multiple types of diagnostic devices and a health or beauty issue included in words extracted from the vocal query from the user;convey the collected accepted data to machine learning models and to receive a regimen recommendation from the machine learning models; anda user interface circuit to audibly present the regimen recommendation to the user via a speaker,wherein when the vocal query from the user regards an appearance of the user'ss eyes the circuitry is configured to associate the vocal query from the user with the detector that measures sleep patterns, and (i) when the accepted data from the detector that measures sleep patterns indicates that the user is getting an amount of sleep that is predetermined as insufficient, the regimen recommendation includes a recommendation of obtaining more sleep for treating the appearance of the user'ss eyes,(ii) when the accepted data from the detector that measures sleep patterns indicates that the user is getting a sufficient amount of sleep, the regimen recommendation includes a recommendation of applying a particular cosmetic product for treating the appearance of the user'ss eyes.","6","16/241417","2019-01-07","2019-0213227","2019-07-11","11748421","2023-09-05","L'OREAL","Celia Ludwinski | Guive Balooch | Rafal Pielak | Adam Jones","","","","G06F-0016/9535","G06F-0016/9535 | A45D-0044/005 | A61B-0005/0077 | G06N-0020/00 | A45D-2044/007 | G06F-0003/0482 | G06N-0005/04 | G06Q-0030/02 | G06Q-0030/0631","G06F-016/9535","G06F-016/9535 | A45D-044/00 | A61B-005/00 | G06N-020/00 | G06F-003/0482 | G06Q-030/02 | G06N-005/04 | G06Q-030/0601","","","","","","4923036005038"
"US","US","P","B1","Generating skin care recommendations for a user based on skin product attributes and user location and demographic data","Systems and methods for generating user-specific recommendations of products or services by collaborative filtering executed and/or performed by one or more trained or untrained predictive models configured to ingest product attribute(s), product purpose(s), user location data, and/or user demographics. The predictive model(s) are leveraged to detect and determine user-specific preferences for, and preferences against, particular attributes, features, ingredients, aesthetic styles, and so on.","1. A host server comprising: a memory storing executable instructions; anda processor configured to load the executable instructions from the memory to instantiate a service to communicably couple to a client application instantiated by a client device operated by a user, the service configured to: receive from the client device, a user dataset comprising an input provided by the user to the client device, the input comprising demographic information about the user;generate a query based on the demographic information to a database service storing location-specific information;cause the query to be executed;receive, a result from the database service, the result comprising historical geographic information about the local area of the user, the historical geographic information comprising water hardness information, seasonal temperature information, and at least one of: local area pollution information;local area humidity information; andlocal area seasonal ultraviolet light exposure information;for each respective skin concern identifier of a set of skin concern identifiers stored in the memory: query the database service to retrieve a set of diagnostics comprising data signaling a respective skin concern identified by the respective skin concern identifier, the skin concern being an aesthetic skin concern or a medical skin concern;determine, for each respective diagnostic of the set of diagnostics, a credibility metric representing a first statistical likelihood based on the user dataset that the user exhibits the respective diagnostic; anddetermine, based on each credibility metric, a confidence metric representing a second statistical likelihood that the user does exhibit the respective skin concern;ingest by a predictive model service comprising a trained predictive model, the user dataset, each respective confidence metric, and the historical geographic information, the trained predictive model trained against a dataset derived from customer review sentiment data correlating review author demographics and geographies against known ingredients of products reviewed by those review authors;receive from the predictive model service, a diagnosis comprising a set of co-occurring skin concerns statistically likely to be exhibited by the user;for each respective skin concern of the set of co-occurring skin concerns, query the database service to retrieve: a respective first list of ingredients that each exhibit a therapeutic effect for users exhibiting the respective skin concern; anda respective second list of ingredients that each exacerbate the respective skin concern;generate a formulation for a custom skincare product by selecting a base from a set of skincare bases and at least one additive from a set of skincare base additives, at least one of the selected based and the one or more selected additives containing the first list of ingredients and none of the respective second list of ingredients;causing a custom product to be manufactured by combining the selected skincare base and the selected one or more additives;providing the custom product to the end user; andcommunicating a regimen recommendation in respect of the custom product to the client device to cause the client device to display the regimen recommendation to the user.","11","17/016342","2020-09-09","","","11748800","2023-09-05","LIFE SPECTACULAR, INC., D/B/A PROVEN SKINCARE","Zaoshi Amy Yuan | Ming S. Zhao","","","","G06Q-0030/0631","G06Q-0030/0631 | A61B-0005/441 | A61K-0008/00 | A61N-0005/0616 | A61Q-0019/005 | A61Q-0019/007 | A61Q-0019/008 | A61Q-0019/08 | A61Q-0019/10 | G01N-0033/6881 | G06N-0020/00 | G06Q-0030/0282 | G16H-0020/00 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | G16H-0050/80 | A61B-0005/411 | A61B-2505/05 | A61F-2007/0052 | A61N-2007/0034 | G01N-2800/00 | G06T-2207/30088","G16H-020/00","G16H-020/00 | G16H-050/70 | G06Q-030/0601 | G06N-020/00 | G06Q-030/0282 | A61K-008/00 | A61Q-019/00 | A61Q-019/08 | A61Q-019/10 | G01N-033/68 | A61N-005/06 | G16H-050/20 | G16H-050/80 | G16H-050/30 | A61B-005/00 | A61F-007/00 | A61N-007/00","","","","","","4923036005415"
"US","US","P","B2","Sensor systems and methods for evaluating activity","Systems and methods are discussed for providing sensor enhanced safety, recovery, and activity evaluation systems. Sensors that monitor user activity and behavior are worn by a user and/or placed in the user environment. Data from the sensors are processed to obtain a safety, recovery, and/or activity evaluation. Based on the evaluation, recommendations or adjustments to the terms of an insurance policy covering the user, the user's employer, or a facility providing health care to the user, are generated, to accurately reflect the risks associated with the user, employer, and/or facility. In embodiments, an alert may be generated when a failure to conform with activity guidelines is detected.","1. An activity evaluation system, comprising: an activity evaluation module configured to: collect sensor data from at least one sensor configured to monitor activity of an individual associated with and distinct from a covered entity;analyze the collected sensor data to determine activity characteristics of the individual; andoutput an activity evaluation based on the activity characteristics and at least one stored activity guideline associated with the individual;a computer hardware server, in communication with the activity evaluation module via a communications device, and operated by a risk management entity remote from the at least one sensor, configured to: receive one or both of the collected sensor data and the activity evaluation;adjust data corresponding to a parameter of a risk management policy covering the covered entity based on the one or both of the collected sensor data and the activity evaluation, wherein the computer hardware server is configured to adjust the data corresponding to the parameter so as to provide an adjustment favorable to the covered entity responsive to the one or both of collected sensor data and the activity evaluation being indicative of safe activity by the individual; andresponsive to the one or both of collected sensor data and the activity evaluation being indicative of unsafe activity by the individual, transmit a notification to one of a computing device of the covered entity or a device of the individual.","18","17/744490","2022-05-13","2022-0270182","2022-08-25","11748819","2023-09-05","HARTFORD FIRE INSURANCE COMPANY","Andrew J. Amigo | Michael Gingrave","","","","G06Q-0040/08","G06Q-0040/08 | A61B-0005/1038 | A61B-0005/112 | A61B-0005/1112 | A61B-0005/1115 | A61B-0005/1118 | A61B-0005/1121 | A61B-0005/1123 | A61B-0005/6804 | G06Q-0010/105 | G16H-0050/30 | A61B-0005/6807 | A61B-2503/20","G06Q-040/08","G06Q-040/08 | A61B-005/103 | A61B-005/11 | A61B-005/00 | G16H-050/30 | G06Q-010/105","","","","","","4923036005434"
"US","US","P","B2","Dynamic display of glucose information","Method and system including displaying a first representation of a medication treatment parameter profile, displaying a first representation of a physiological profile associated with the medication treatment parameter profile, detecting a modification to a segment of the medication treatment parameter profile, displaying a modified representation of the medication treatment parameter profile and the physiological profile based on the detected modification to the segment of the medication treatment parameter profile, modifying an attribute of the first representation of the medication treatment parameter profile, and modifying an attribute of the first representation of the physiological profile are provided.","1. A non-transitory computer readable medium, containing instructions for causing a processor to perform a method of determining a customized estimated insulin bolus amount for a user, the method comprising: receiving current user meal data about a meal consumed during a meal time period;receiving current user insulin data about insulin provided to the user during the meal time period;receiving current user glucose concentration data from a continuous glucose monitoring system;querying at least stored past user meal data during a past meal time period, wherein the current user meal data and the stored past user meal data are comparable in at least one of a meal type or a meal size;customizing an estimated insulin bolus amount based on at least the current user meal data, the current user insulin data, and the current user glucose concentration data compared to at least the stored past user meal data; andtransmitting a command to administer the customized estimated insulin bolus amount to an insulin delivery device.","11","15/468156","2017-03-24","2017-0193184","2017-07-06","11749410","2023-09-05","ABBOTT DIABETES CARE INC.","Gary Alan Hayter | Timothy Christian Dunn","","","","G16H-0050/50","G16H-0050/50 | A61M-0005/142 | A61M-0005/1723 | G06F-0003/04847 | G16H-0020/17 | G16H-0020/60 | G16H-0040/63 | H04W-0004/80 | A61M-2005/14208 | A61M-2205/18 | A61M-2205/3584 | A61M-2205/50 | A61M-2205/502 | A61M-2205/52 | A61M-2205/582 | A61M-2230/005 | A61M-2230/201 | G06F-2218/16","G16H-050/50","G16H-050/50 | G06K-009/00 | G16H-020/17 | A61M-005/142 | A61M-005/172 | G16H-040/63 | H04W-004/80 | G06F-003/04847 | G16H-020/60","","","","","","4923036006018"
"US","US","P","B2","Perfusion device for treating an injured blood vessel","The present disclosure concerns embodiments of an implantable perfusion device that can be implanted in an injured blood vessel to control bleeding without occluding the vessel. In one specific implementation, the perfusion device can be implanted percutaneously into a patient's descending aorta to control bleeding at the site of a ruptured portion of the aorta (known as torso hemorrhage) while still allowing for the antegrade flow of blood from a location upstream of the ruptured portion of the aorta to a location downstream of the ruptured portion of the aorta. The perfusion device can be left inside the patient as the patient is transported to a medical facility where the injury can be repaired. Following repair of the vessel, the perfusion device can be withdrawn from the patient.","1. A perfusion device for treating a blood vessel of a patient, comprising: an elongated shaft extending from a proximal end portion to a distal end portion, the shaft having a distal opening and a lumen in fluid communication with the distal opening;an inflatable balloon mounted on the distal end portion of the shaft, the balloon having a distal end and a proximal end, wherein the shaft and the balloon are configured to be positioned in the aorta of the patient;a plurality of blood conduits, each having a distal end portion, a proximal end portion, and a lumen extending therebetween, the distal end portion of each conduit being connected to the shaft at a location between the proximal end of the balloon and the proximal end portion of the shaft and being in fluid communication with the lumen of the shaft, the proximal end portion of each blood conduit being configured to be positioned within a respective branch artery of the aorta, wherein the locations where the distal end portions of the conduits are connected to the shaft are spaced substantially the same distance from the balloon.","24","16/533555","2019-08-06","2019-0374226","2019-12-12","11737760","2023-08-29","UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION","Bryan W. Tillman | William W. Clark | Sung Kwon Cho | Youngjae Chun","","","","A61B-0017/12109","A61B-0017/12109 | A61B-0005/0215 | A61B-0005/02141 | A61B-0017/1204 | A61B-0017/12031 | A61B-0017/12036 | A61B-0017/12136 | A61B-0090/98 | A61F-0002/966 | G06K-0007/10366 | A61B-2017/00115 | A61B-2090/397 | A61B-2090/3966 | A61F-0002/07 | A61F-2002/9511","A61B-017/12","A61B-017/12 | A61B-090/98 | A61B-005/021 | A61B-005/0215 | A61F-002/966 | G06K-007/10 | A61B-090/00 | A61B-017/00 | A61F-002/07 | A61F-002/95","","","","","","4923035001121"
"US","US","P","B2","Medical device apparatus, system, and method","Disclosed are a medical device apparatus, system, and method. A method includes receiving biometric information, by an external device external to a body of a user, of the user from an internal device within the body of the user, and wirelessly transmitting stimulus information configured to specify a stimulus based on the biometric information, and power to the internal device configured to drive the internal device and to apply the stimulus in response to the transmitted stimulus information. A method also includes wirelessly transmitting, from an internal device in a body of a user, biometric information of the user to an external device located outside the body of the user, and wirelessly receiving from the external device stimulus information configured to specify a stimulus, and power configured to drive the internal device and to apply the stimulus to the user in response to the received stimulus information.","1. A medical device method, the method comprising: receiving, by an electronic device, biometric information of a user sensed by an internal device in a body of the user from a user terminal controlled by the user;determining control information set by a medical specialist controlling the electronic device, the control information being configured to cause an external device that is located outside a body of the user to wirelessly transmit stimulus information and power to the internal device inserted in the body of the user in response to the external device receiving the control information; andtransmitting the control information to the user terminal,wherein the control information includes information used to select a stimulus pattern from at least one stimulus pattern determined in association with a feedback loop including the internal device, the external device, the user terminal, and the electronic device.","4","16/189177","2018-11-13","2019-0175902","2019-06-13","11738191","2023-08-29","SAMSUNG ELECTRONICS CO., LTD. | SAMSUNG LIFE PUBLIC WELFARE FOUNDATION","Hyungwoo Lee | Duk Lyul Na | Dae Won Seo | Young Min Shon | Jin San Lee | Woo Ram Jung | Sang Joon Kim | Joonseong Kang | Wonseok Lee","10-2017-0170762","KR","2017-12-12","A61N-0001/0529","A61N-0001/0529 | A61B-0005/0031 | A61B-0005/4064 | A61B-0005/4836 | A61N-0001/36135 | G06F-0003/015 | A61B-0005/0006 | A61B-0005/0008 | A61B-0005/01 | A61B-0005/053 | A61B-0005/291 | A61B-0005/30 | A61B-0005/316 | A61B-0005/375 | A61B-2560/0219 | A61N-0001/36064 | A61N-0001/36103 | A61N-0001/3787 | A61N-0001/37247 | A61N-0001/37264 | H02J-0050/10","A61N-001/05","A61N-001/05 | G06F-003/01 | A61B-005/00 | A61N-001/36 | A61N-001/378 | H02J-050/10 | A61N-001/372 | A61B-005/053 | A61B-005/01 | A61B-005/30 | A61B-005/291 | A61B-005/316 | A61B-005/375","","","","","","4923035001550"
"US","US","P","B2","Intelligent real time resource instrument activation using electroencephalogram signals","Embodiments of the present invention provide a system for secure communication of information that may be used to authorize communications or transfer of resources by use of an intelligent resource instrument with nano display. The provided systems, methods, and computer program products are designed to select and display viewable information, simultaneously record EEG readings for a user, and use this information to verify user identity. Upon verification of user identity, the intelligent resource instrument may be activated for use in a resource transfer.","1. A system for intelligent resource instrument activation, the system comprising: a memory device; anda processing device operatively coupled to the memory device, wherein the processing device is configured to execute computer-readable program code to:receive a request from a user to initiate a resource action;transmit instructions to an intelligent resource instrument to emit an activation stimulus from the intelligent resource instrument, wherein the intelligent resource instrument comprises a physical card, and wherein the activation stimulus emitted from the intelligent resource instrument comprises a digital video or image displayed on a nano display of the intelligent resource instrument;record electroencephalogram (EEG) readings via an EEG wearable device that is separated from the intelligent resource instrument for the user during a duration of the activation stimulus emitted from the intelligent resource instrument, wherein the EEG wearable device comprises a portable auxiliary component of a mobile wearable device, and wherein the EEG readings are transmitted to the intelligent resource instrument and combined with data for the activation stimulus using a cryptographic hash algorithm;extract one or more patterns from the EEG readings and compare the one or more patterns to stored user data;determine a match between the one or more patterns and the stored user data;validate an identity of the user; andtransmit an authorization signal to the intelligent resource instrument activating the intelligent resource instrument for use in the resource action.","12","16/921132","2020-07-06","2022-0004608","2022-01-06","11741204","2023-08-29","BANK OF AMERICA CORPORATION","Shailendra Singh","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/117 | A61B-0005/378 | A61B-0005/38 | G06N-0020/00 | G06V-0040/10 | H04L-0009/3236 | H04L-0063/102 | G06V-0040/15","G06F-021/32","G06F-021/32 | H04L-009/40 | H04L-009/32 | G06N-020/00 | A61B-005/117 | A61B-005/38 | A61B-005/378 | G06V-040/10","","","","","","4923035004536"
"US","US","P","B2","Methods and systems for compliance confirmation and incentives","Example methods, apparatus, and articles of manufacture for monitoring use by a user of a portable research device in accordance with at least one predetermined use criterion are disclosed. The disclosed examples include passively gathering data for assessing an identity of a user of the portable research device, processing the passively gathered data to produce assessment data indicating a possibility that the user is not a predetermined correct user of the portable research device, based on the assessment data, displaying a message to the user requesting a response from which the user's identity may be determined, and processing a response to the message to produce data indicating whether the user is the predetermined correct user.","1. A method for monitoring use by a user of a portable research device in accordance with at least one predetermined use criterion comprising: passively gathering gait data for assessing an identity of the user of the portable research device, the portable research device including an accelerometer, the passively gathered gait data obtained from the accelerometer;determining, via a processer communicatively coupled to the portable research device, gait assessment data based on the passively gathered gait data, the gait assessment data indicating a possibility that the user is not a predetermined user of the portable research device, the processor remote from the portable research device;based on the gait assessment data, displaying a first message to the user requesting a response from which the user'ss identity is determined; anddetermining, via the processor, whether the user is the predetermined user based on a response to the first message.","20","16/544879","2019-08-19","2019-0371462","2019-12-05","11741431","2023-08-29","CITIBANK, N.A","Alan R. Neuhauser | Jack C. Crystal | Jack K. Zhang | Eugene L. Flanagan","","","","G06Q-0010/101","G06Q-0010/101 | A61B-0003/1216 | A61B-0003/14 | A61B-0005/0077 | A61B-0005/0205 | A61B-0005/1172 | A61B-0005/1176 | A61B-0005/489 | A61B-0005/4833 | A61B-0005/6898 | A61B-0007/003 | A61B-0007/008 | A61B-0007/02 | G06Q-0010/00 | G06Q-0030/018 | G06Q-0030/0201 | G16H-0010/60 | G16H-0030/20 | G16H-0040/20 | G16H-0040/63 | G16H-0040/67 | A61B-0005/02438 | A61B-0005/0816 | A61B-2503/12 | G06Q-0030/0203","G06Q-030/0201","G06Q-030/0201 | G06Q-030/018 | G06Q-010/101 | G16H-040/67 | G16H-040/20 | G16H-010/60 | G16H-030/20 | G16H-040/63 | G06Q-010/00 | A61B-003/12 | A61B-003/14 | A61B-005/00 | A61B-005/0205 | A61B-005/1172 | A61B-005/1171 | A61B-007/00 | A61B-007/02 | G06Q-030/0203 | A61B-005/024 | A61B-005/08","","","","","","4923035004759"
"US","US","P","B2","Method for treating arterial stenosis","Disclosed herein is a method of treating a subject having arterial stenosis. The method comprises: (a) providing a plurality of image frames of an artery of the subject taken in sequence; (b) in a plurality of cross-sections of the artery, determining a maximum diameter and a minimum diameter of each of the plurality of cross-sections of the artery among the plurality of image frames of the step (a); (c) calculating an average vasodilation ratio of the artery base on the maximum diameter and the minimum diameter determined in the step (b); and (d) treating the subject based on the average vasodilation ratio calculated in the step (c), by implanting a stent to the subject when the average vasodilation ratio is equal to or greater than 0.2; or administering to the subject an effective amount of a vasodilator when the average vasodilation ratio is less than 0.2.","1. A method of treating a subject having arterial stenosis, comprising, (a) providing a plurality of image frames of an artery of the subject taken in sequence;(b) in a plurality of cross-sections of the artery, determining a maximum diameter and a minimum diameter of each of the plurality of cross-sections of the artery among the plurality of image frames of the step (a);(c) calculating an average vasodilation ratio of the artery based on the maximum diameter and the minimum diameter of each of the plurality of cross-sections of the artery determined in the step (b) by using equations (1) and (2): wherein i represents any of the plurality of cross-sections of the artery, n represents the total number of the plurality of cross-sections, Dmax,i is the maximum diameter of the artery in the cross-section i, Dmin,i is the minimum diameter of the artery in the cross-section i, Vi is a vasodilation ratio corresponding to the cross-section i, and Vavg represents the average vasodilation ratio of the artery; and(d) treating the subject based on the average vasodilation ratio calculated in the step (c) by implanting a stent to the subject when the average vasodilation ratio is equal to or greater than 0.2; andwherein the method is characterized in not having a step of measuring blood flow in the artery.","7","16/988716","2020-08-10","2022-0044393","2022-02-10","11741599","2023-08-29","MACKAY MEMORIAL HOSPITAL | NATIONAL CHIAO TUNG UNIVERSITY","Shen Chi | Po-Lin Lin | Ying-Hsiang Lee | Yu-Min Liu | Long Hsu | Ruo-Jing Ho | Chang Francis Hsu | Han-Ping Huang","","","","G06T-0007/0012","G06T-0007/0012 | A61F-0002/82 | G06F-0017/18 | A61F-2002/823 | G06T-2207/30104","G06T-007/00","G06T-007/00 | A61F-002/82 | G06F-017/18","","","","","","4923035004927"
"US","US","P","B2","System and method for assuring patient medication and fluid delivery at the clinical point of use","A system for confirmation of fluid delivery to a patient at the clinical point of use is provided. The system includes a wearable electronic device. The wearable electronic device has a housing; at least one imaging sensor associated with the housing; a data transmission interface; a data reporting accessory for providing data to the user; a microprocessor for managing the at least one imaging sensor, the data transmission interface, and the data reporting accessory; and a program for acquiring and processing images from the at least one imaging sensor. The system further includes a fluid delivery apparatus; and one or more identification tags attached to or integrally formed with the fluid delivery apparatus. The program processes an image captured by the at least one imaging sensor to identify the one or more identification tags and acquires fluid delivery apparatus information from the one or more identification tags.","1. A system comprising: a wearable electronic device configured to be worn by a user comprising: a housing,at least one sensor associated with the housing, wherein the at least one sensor comprises at least one imaging sensor,a data transmission interface for sending data to or receiving data from an external electronic device,a data reporting accessory for providing information to the user,a microprocessor for managing the at least one sensor, the data transmission interface, and the data reporting accessory, anda program for acquiring and processing data acquired by the at least one sensor;a fluid delivery apparatus for passively or actively delivering a therapeutic agent to a patient;one or more identification tags attached to or integrally formed with the fluid delivery apparatus; anda patient identification device including or associated with identifying information about the patient and readable by the at least one sensor,wherein the program manages acquiring information from the one or more identification tags and the patient identification device by processing a series of images captured by the at least one imaging sensor, wherein the processing the series of images comprises: identifying and reading the one or more identification tags attached to or integrally formed with the fluid delivery apparatus as the one or more identification tags enter a field of view of the user, and checking, based on the information acquired from the one or more identification tags and the patient identification device, the fluid delivery apparatus to ensure that only items necessary for a fluid delivery procedure for the patient are obtained and to ensure that no additional items are needed for the fluid delivery procedure.","6","17/209360","2021-03-23","2021-0233657","2021-07-29","11742082","2023-08-29","BECTON, DICKINSON AND COMPANY","Jonathan Karl Burkholz | Jeff O'Bryan","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/15003 | A61B-0005/154 | A61B-0005/157 | A61B-0005/150748 | A61B-0005/150786 | A61B-0005/150992 | A61B-0010/0096 | A61B-0010/02 | A61M-0005/14 | A61M-0005/142 | A61M-0005/16831 | A61M-0005/172 | A61M-0005/315 | A61M-0005/31525 | A61M-0005/427 | G06V-0020/52 | G16H-0010/40 | G16H-0010/60 | G16H-0020/17 | G16H-0030/20 | G16H-0040/63 | G16Z-0099/00 | H04N-0005/44 | H04N-0007/18 | H04N-0023/51 | A61B-0005/150732 | A61B-0005/150847 | A61B-0090/90 | A61B-0090/96 | A61M-0005/1452 | A61M-0005/20 | A61M-0005/28 | A61M-2005/1588 | A61M-2205/3306 | A61M-2205/3389 | A61M-2205/3561 | A61M-2205/3569 | A61M-2205/3576 | A61M-2205/52 | A61M-2205/583 | A61M-2205/6009 | A61M-2205/6054 | A61M-2205/6063 | A61M-2205/6072 | G06Q-0050/22 | G06V-2201/03 | G16H-0010/65 | G16H-0030/40","G16H-040/67","G16H-040/67 | G16H-020/17 | A61M-005/14 | A61M-005/315 | A61M-005/42 | A61B-005/15 | A61B-005/154 | G16H-010/60 | G16H-010/40 | G16H-040/63 | G16H-030/20 | G16Z-099/00 | G06V-020/52 | H04N-023/51 | A61M-005/142 | A61M-005/168 | A61M-005/172 | A61B-005/157 | A61B-010/00 | A61B-010/02 | H04N-005/44 | H04N-007/18 | A61B-090/96 | A61B-090/90 | A61M-005/20 | A61M-005/28 | A61M-005/145 | A61M-005/158 | G16H-030/40 | G16H-010/65 | G06Q-050/22","","","","","","4923035005405"
"US","US","P","B2","Method and system for supporting medical interventions","A method for supporting medical interventions using an augmented reality system is disclosed, which comprises determining, using a position marker and an electronic tracking system, positions of a set of points; calculating a geometric shape using the determined positions of the set of points; and displaying, on an optical head mounted display the calculated geometric shape in the field of view of a bearer of the display. Furthermore, a system and a position marker adapted to be used for this method are disclosed.","1. A method for supporting medical interventions using an augmented reality system, comprising: determining, using a position marker and an electronic tracking system of the augmented reality system, positions of a set of points, wherein at least one point of the set of points is an attachment point of a pedicle screw;calculating a template of a shape of a rod for stabilizing bones of a vertebrae using the determined positions of the set of points wherein the template of a shape of a rod comprises the determined positions of the set of points; anddisplaying, on an optical head mounted display of the augmented reality system, the calculated template of a shape of a rod in the field of view of a bearer of the optical head mounted display,wherein the calculated template is displayed in a size that corresponds to a size of the rod at a distance between 1 cm and 100 cm from the optical head mounted display.","16","16/259768","2019-01-28","2020-0237256","2020-07-30","11730389","2023-08-22","INCREMED AG","Mazda Farshad | Till Bay | Simon Roner | Florentin Liebmann | Florian Wanivenhaus","","","","A61B-0005/064","A61B-0005/064 | A61B-0005/107 | A61B-0090/37 | A61B-0090/39 | G02B-0027/0172 | G06T-0007/60 | G06T-0007/73 | A61B-2090/372 | A61B-2090/3916 | A61B-2090/3975 | G02B-2027/0141 | G06F-0003/013 | G06F-0003/017 | G06F-0003/167 | G06T-2207/10081 | G06T-2207/10088 | G06T-2207/30012 | G06T-2207/30052 | G06T-2207/30204","A61B-005/00","A61B-005/00 | A61B-005/06 | G06T-007/60 | G02B-027/01 | A61B-005/107 | A61B-090/00 | G06T-007/73 | G06F-003/01 | G06F-003/16","","","","","","4923034001140"
"US","US","P","B2","Apparatus and method for button-free control of a wearable transcutaneous electrical nerve stimulator using interactive gestures and other means","Apparatus for transcutaneous electrical nerve stimulation in a user, the apparatus comprising: a stimulator for electrically stimulating at least one nerve, a stimulator housing, a monitor for monitoring transient motion of the stimulator housing, an analyzer for analyzing transient motion monitored by the monitor for determining whether transient motion of the stimulator housing has occurred, and a controller for automatically transitioning at least one of the stimulator, the monitor, and the analyzer between a standby mode and a power save mode, wherein the power save mode supports a subset of the functionality of the stimulator and the monitor which is available in the standby mode so as to conserve battery power in the power save mode.","1. Apparatus for delivering transcutaneous electrical stimulation to a user, said apparatus comprising: a housing;a stimulator for electrically stimulating the user;a monitor for monitoring motion of said housing; anda controller for automatically transitioning at least one of said stimulator and said monitor between a standby mode and a power save mode;wherein said power save mode supports a subset of the functionality of said stimulator and said monitor which is available in said standby mode so as to conserve battery power in said power save mode.","34","17/196176","2021-03-09","2021-0260374","2021-08-26","11730959","2023-08-22","Neurometrix, Inc.","Shai N. Gozani | Xuan Kong | Thomas C. Ferree","","","","A61N-0001/36021","A61N-0001/36021 | A61B-0005/1118 | A61B-0005/1123 | A61B-0005/7475 | A61N-0001/0456 | A61N-0001/0492 | A61N-0001/08 | A61N-0001/36031 | G06F-0001/163 | G06F-0001/3231 | G06F-0003/015 | G06F-0003/017 | G06F-0003/0346 | A61B-0005/11 | A61B-0005/681 | A61B-0005/6828 | A61B-0005/7455 | A61B-2560/0276 | A61B-2562/0219 | A61N-2001/083","A61N-001/36","A61N-001/36 | A61B-005/11 | G06F-001/3231 | G06F-003/0346 | G06F-001/16 | A61B-005/00 | A61N-001/04 | A61N-001/08 | G06F-003/01","","","","","","4923034001708"
"US","US","P","B2","Device, system and method for emotion detection","An emotion detecting device includes a memory, a processor and an output/input device. An emotion template with a plurality of emotional statuses is stored in the memory. The processor is configured to receive characteristic values transformed from brain waves of a pet and determine whether the brain waves correspond to a stable state based on variation of the characteristic values during a period. When it is determined that the brain waves correspond to the stable state, the processor determines whether the brain waves match some emotional status among the emotional statuses. When the brain waves match the emotional status, the output/input device outputs information regarding the pet being in the emotional status. When the brain waves do not match any of the emotional statuses in the emotion template, the processor updates the emotion template based on a confirm operation and the characteristic values.","1. An emotion detecting device, comprising: a memory storing an emotion template with a plurality of emotional statuses;a processor electrically coupled to the memory, the processor receiving a plurality of characteristic values transformed from a plurality of brain waves of a pet during a period, and determining whether the brain waves correspond to a stable state based on variation of the characteristic values during the period, wherein the brain waves are obtained by detection via a brainwave detecting device; andan output/input device electrically coupled to the processor;wherein, when the brain waves correspond to the stable state, the processor determines whether the brain waves match a first emotional status of the plurality of emotional statuses, and when the brain waves match the first emotional status, information regarding the pet being in the first emotional status is output at the output/input device;wherein, when the brain waves do not match at least one of the plurality of emotional statuses, the processor updates the emotion template in accordance with a confirmation operation and the plurality of characteristic values,wherein the processor transforms the brain waves during the period to a number K of characteristic sets,wherein each of the brain waves corresponding to a number N of frequencies based on a transform algorithm, wherein frequency data items for the number N of frequencies are the characteristic values, and each of the number K of characteristic sets includes the frequency data items for the number N of frequencies;the processor, based on the number K of frequency data items for each frequency among the number K of characteristic sets during the period, computing a median of the number K of frequency data items corresponding to the number N of frequencies, and generating a reference characteristic set of the period, wherein the reference characteristic set includes the number N of the medians calculated from the number K of characteristic sets, and each of the number N of the medians corresponds to one of the number N of frequencies;wherein the processor calculates a frequency data difference between the frequency data items for the number N of frequencies of each of the number K of characteristic sets and the number N of the medians of the reference characteristic set,wherein when the frequency data difference is less than a predetermined threshold, a count is incremented, and the brain waves are determined as in the stable state when the count in the period is larger than a predetermined parameter.","10","16/732339","2020-01-01","2020-0229748","2020-07-23","11723567","2023-08-15","AMTRAN TECHNOLOGY CO., LTD.","Kao-Min Lin | Yu-Hsaing Lin","2019102440","TW","2019-01-22","A61B-0005/165","A61B-0005/165 | A61B-0005/369 | G06F-0003/015 | G06F-0017/142 | A61B-0005/7257 | G06F-2203/011","A61B-005/16","A61B-005/16 | G06F-003/01 | G06F-017/14 | A61B-005/369 | A61B-005/00","","","","","","4923033001239"
"US","US","P","B2","Electromyography sensor","An electromyography (EMG) sensor for a wearable device, such as a prosthetic device attachable to a residual limb, includes a flexible substrate comprising an elongated portion and an electrode portion. At least two electrodes are disposed at a surface of the electrode portion of the flexible substrate, and leads from the at least two electrodes extend through the elongated portion of the flexible substrate.","1. An electromyography sensor for a wearable device comprising: a flexible substrate comprising an elongated portion and an electrode portion;at least two electromyography electrodes disposed at a surface of the electrode portion of the flexible substrate, a thickness of the electrode portion of the flexible substrate and the at least two electromyography electrodes being in the range of about 50 μm to about 500 μm;leads extending from the at least two electromyography electrodes through the elongated portion of the flexible substrate; andconnectors at the end of the leads;the electromyography sensor configured to be positioned within a prosthetic liner to be fit over a residual limb with the two electromyography electrodes facing inwardly to contact skin of the residual limb, the elongated portion extending along the prosthetic liner to position the connectors to be engaged with and decoupled from electromyography electronics associated with a prosthetic socket positioned over the liner and residual limb.","22","16/661283","2019-10-23","2020-0121210","2020-04-23","11723581","2023-08-15","MASSACHUSETTS INSTITUTE OF TECHNOLOGY","Hugh M. Herr | Seong Ho Yeon","","","","A61B-0005/389","A61B-0005/389 | A61B-0005/688 | A61F-0002/64 | A61F-0002/6607 | A61F-0002/72 | G06F-0003/015 | A61F-0002/741 | A61F-2002/6664","A61B-005/389","A61B-005/389 | G06F-003/01 | A61B-005/00 | A61F-002/72 | A61F-002/64 | A61F-002/66 | A61F-002/74","","","","","","4923033001253"
"US","US","P","B2","Program, information processor, and information processing method","There is provided a program, an information processor, and an information processing method that make it possible to obtain position information for sites of a body with higher accuracy. The program causes a computer to implement a correction function of referencing a first output obtained by performing a first process on sensor data acquired by two or more motion sensors attached to two or more sites of a body and a second output obtained by performing a second process on the sensor data, and correcting position information for attachment sites to which the motion sensors are attached.","1. A non-transitory computer-readable medium having embodied thereon a program, which when executed by a computer causes the computer to execute an information processing method, the method comprising: implementing a correction function of referencing a first position information obtained by performing a first process on sensor data acquired by two or more motion sensors attached to two or more sites of a body and a second position information estimated by using a learning model on the sensor data;correcting the first position information for attachment sites to which the motion sensors are attached on a basis of the second position information; andimplementing an interpolation function of estimating, on a basis of the first position information for the attachment sites corrected by the correction function, position information for a non-attachment site of the body to which none of the motion sensors are attached, the position information for the non-attachment site not including the first position information for the attachment sites,wherein the first position information and the second position information include coordinates of the attachment sites in a coordinate system and the position information for the non-attachment site of the body include coordinates of the non-attachment site in the coordinate system.","19","17/045024","2019-04-15","2021-0141443","2021-05-13","11726549","2023-08-15","SONY CORPORATION","Yasutaka Fukumoto | Keita Mochizuki","2018-079335","JP","2018-04-17","G06F-0003/011","G06F-0003/011 | A61B-0005/1114 | A61B-0005/1122 | A61B-0005/1126 | A61B-0005/744 | A63F-0013/428 | G01B-0021/00 | G06F-0003/04815 | G06T-0007/251 | G06T-0013/40 | G06T-0019/003 | A61B-0005/0205 | A61B-0005/7267 | A61B-2562/0219","G06F-003/01","G06F-003/01 | G06T-007/246 | G06F-003/04815 | G06T-019/00 | A61B-005/11 | A61B-005/00 | A63F-013/428 | G06T-013/40 | G01B-021/00 | A61B-005/0205","","","","","","4923033004200"
"US","US","P","B2","Delivery of somatosensation for medical diagnostics or guiding motor action","A device is provided for delivering somatosensation. A garment is wearable on a body part, and includes an array of electrodes in electrical contact with skin of the body part when the garment is worn on the body part. An electronics module is configured to use the array of electrodes of the garment to apply a somatosensation pattern providing guidance in performing a motor action, or providing a pain sensation to the wearer.","1. A training device comprising: a learner wearable device including an array of electrodes, a garment wearable on a body part with the array of electrodes in electrical contact with skin of the body part, and an electronics module configured to use the array of electrodes of the learner wearable device to measure electromyography (EMG) signals generated by the wearer of the learner wearable device and to apply a somatosensation pattern providing guidance to the wearer of the learner wearable device in performing a motor action; andan expert wearable device including an array of electrodes, a garment wearable on a body part with the array of electrodes in electrical contact with skin of the body part, and an electronics module configured to use the array of electrodes of the expert wearable device to measure EMG signals from a wearer of the expert wearable device while the wearer of the expert wearable device is performing the motor action;wherein the electronics module of the learner wearable device is further configured to: compare the measured EMG signals generated by the wearer of the learner wearable device and the EMG signals from the wearer of the expert wearable device measured while the wearer of the expert wearable device is performing the motor action; andgenerate the somatosensation pattern based on the comparison.","9","17/714225","2022-04-06","2022-0229493","2022-07-21","11726567","2023-08-15","BATTELLE MEMORIAL INSTITUTE","Samuel Colachis | Eric Meyers | Justin Sanchez | David Friedenberg","","","","G06F-0003/015","G06F-0003/015 | A61N-0001/0484 | A61N-0001/36031 | A61N-0001/36034 | G06N-0020/00 | G16H-0040/67 | G16H-0080/00","G06F-003/01","G06F-003/01 | A61N-001/36 | G16H-040/67 | G16H-080/00 | G06N-020/00 | A61N-001/04","","","","","","4923033004218"
"US","US","P","B2","Systems and methods for privacy-aware motion tracking with notification feedback","Systems and methods to perform privacy-aware computer-vision-based human activity monitoring with real-time haptic feedback. The system and method described in this disclosure employing a registration process for consenting human subjects before their activities are monitored. The registration process involves the corroboration of human motion captured in different modalities, i.e., computer-vision-based via the programmable computer-vision-based motion sensor and accelerometer-based via the wearable device. The tracked human activities are assessed in real-time and upon detection of activities that violated predefined rules, haptic feedback is delivered in real-time to the tracked human subject via the wearable device worn by the caregiver.","1. A method for identifying a consenting human subject, comprising: recognizing, via one or more sensors, a predefined gesture performed by a consenting human subject, the consenting human subject wearing a detectable wearable device;providing a registration request, the registration request including an indication that the consenting human subject has interest in being monitored;receiving vision-based motion data from one or more motion sensors;recognizing a gesture by the consenting human subject from the vision-based motion data; andproviding a confirmation message to the consenting human subject via haptic feedback and a short message displayed on the wearable device,wherein the gesture is a tapping motion on the wearable device by one or more fingers of the opposite hand,wherein the gesture is detected by one or more accelerometers in the wearable device, andwherein, once the gesture is detected, recognizing the gesture includes comparing the wrist-to-wrist distance against a predefined configurable threshold.","8","16/278452","2019-02-18","2019-0267786","2019-08-29","11727780","2023-08-15","CLEVELAND STATE UNIVERSITY","Wenbing Zhao","","","","G08B-0021/0446","G08B-0021/0446 | A61B-0005/11 | A61B-0005/486 | G06F-0001/1694 | G06F-0003/016 | G06F-0003/017 | G06T-0007/246 | G06T-0007/73 | G06V-0040/23 | G06V-0040/28 | H02G-0007/06 | A61B-0005/681 | A61B-0005/7455 | G06T-2207/10016 | G06T-2207/30196 | G06T-2207/30232 | G06T-2207/30241","H02G-007/06","H02G-007/06 | G08B-021/04 | G06F-003/01 | G06T-007/246 | G06T-007/73 | G06F-001/16 | A61B-005/00 | A61B-005/11 | G06V-040/20","","","","","","4923033005424"
"US","US","P","B2","Methods and systems for predicting sensitivity of blood flow calculations to changes in anatomical geometry","Embodiments include methods and systems for determining a sensitivity of a patient's blood flow characteristic to anatomical or geometrical uncertainty. For each of one or more of individuals, a sensitivity of a blood flow characteristic may be obtained for one or more uncertain parameters. An algorithm may be trained based on the sensitivities of the blood flow characteristic and one or more of the uncertain parameters for each of the plurality of individuals. A geometric model, a blood flow characteristic, and one or more of the uncertain parameters of at least part of the patient's vascular system may be obtained for a patient. The sensitivity of the patient's blood flow characteristic to one or more of the uncertain parameters may be calculated by executing the algorithm on the blood flow characteristic of at least part of the patient's vascular system, and one or more of the uncertain parameters.","1. A computer-implemented method of determining a sensitivity of a patient'ss blood flow characteristic to uncertainty in a geometric model of a patient'ss vascular system, the method comprising: obtaining, for each of a plurality of individuals, a geometric model of at least a portion of a vascular system of each individual and at least one sensitivity of a blood flow characteristic to at least one uncertainty in geometry in the geometric model;mapping, in a machine learning database, a plurality of features of each geometric model to the obtained sensitivities;obtaining a geometric model of at least part of a patient'ss vascular system;determining, for the patient, at least one value of uncertainty in geometry in the geometric model of at least part of the patient'ss vascular system; anddetermining a sensitivity of a blood flow characteristic of the patient to at least one value of uncertainty in geometry in the geometric model of at least part of the patient'ss vascular system, using (i) the machine learning database in which the plurality of features are mapped to the obtained sensitivities and/or (ii) the mapped plurality of features.","20","16/695489","2019-11-26","2020-0098481","2020-03-26","11728039","2023-08-15","HEARTFLOW, INC.","Sethuraman Sankaran | Leo Grady | Charles A. Taylor","","","","G16H-0050/50","G16H-0050/50 | A61B-0005/7267 | A61B-0006/00 | A61B-0006/507 | A61B-0006/5217 | G06F-0017/18 | G06N-0003/00 | G06T-0007/0012 | A61B-0005/026 | A61B-2576/02 | G06T-2207/30104","G16H-050/50","G16H-050/50 | G06T-007/00 | G06N-003/00 | A61B-006/00 | A61B-005/00 | G06F-017/18 | A61B-005/026","","","","","","4923033005683"
"US","US","P","B2","Systems and methods for evaluating body motion","The present disclosure is directed towards computer-based systems and methods for evaluating a user's body motion based on motion data obtained via a series of wireless sensors attached to a user's body. In one embodiment, a computer-implemented method is disclosed herein. A server system receives motion data from one or more input devices. The motion data corresponds to movement of a user. The server system generates a motion profile based on at least the motion data received from the one or more input devices. The server system retrieves a pre-defined target motion profile from a database structure. The server system objectively evaluating the extracted motion profile by comparing one or more parameters of the generated motion profile with one or more parameters of the retrieved pre-defined target motion profile.","1. A computer-implemented method comprising: receiving, by a computing system over a wireless network, motion data directly from one or more sensors located on a body of a user, the motion data corresponding to movement of the user;converting, by the computing system, the motion data to positional data for each joint of the user based on an orientation of each sensor with respect to each joint and limb of the user, the converting comprising: transforming roll, pitch, and yaw coordinates of the motion data to three dimensional positional data that illustrates one or more positions of at least one of one or more joints, limbs, and other reference points on the body of the user;generating, by the computing system, joint data for each joint of the user based on the positional data;generating, by the computing system, a motion profile based on at least the joint data; andevaluating the motion profile, by the computing system, by comparing one or more parameters of the motion profile with one or more pre-defined parameters of a pre-defined target motion profile.","20","17/587750","2022-01-28","2022-0151513","2022-05-19","11717190","2023-08-08","Latella Sports Technologies, LLC","Frank A. Latella, Jr.","","","","A61B-0005/1128","A61B-0005/1128 | A61B-0005/1121 | A61B-0005/7425 | A63B-0024/0062 | G06F-0003/011 | G06F-0003/0346 | G06T-0007/248 | G06T-0007/74 | A61B-0005/0077 | A61B-0005/1114 | A61B-2505/09 | A61B-2576/00 | A63B-2024/0068 | A63B-2071/065 | A63B-2220/806 | G06T-2207/20072 | G06T-2207/30196","G06K-009/00","G06K-009/00 | A61B-005/11 | G06T-007/246 | G06T-007/73 | A63B-024/00 | A61B-005/00 | G06F-003/01 | G06F-003/0346 | A63B-071/06","","","","","","4923032001061"
"US","US","P","B2","System and method of use of augmented reality in measuring body circumference for the use of apparel production","The present invention discloses a system and method for improvement in process of measurement of body circumference using Augmented Reality (AR) and 4-point mathematical calculations approach and mobile device camera. The method includes the steps of receiving two or more individual parameters from an individual device; receiving at least one set of 4 points capture through AR technology; measurement through AR technology from the individual device, at least one dimension including user's inputs on height, weight, age, and size range; performing body segmentation on at least one dimensions to identify one or more body features associated with the human from the background; performing the distance calculation between four points; compare the calculation results with standard sizing database and displaying the final output to the individual. The application utilizes Augmented Reality for estimating the circumference body measurements of an individual from specific point to point capture in the individual's environment using the individual's device.","1. A system for capturing body measurements comprising: a handheld device having a sensor module for locating the handheld device in space; anda user interface for utilizing Augmented Reality (AR) to capture a user'ss body measurements, the user interface configured for instructing the user in locating the handheld device relative to the user'ss body or an environment;wherein the handheld device is configured to identify a measurement of the user to be captured, and wherein, upon identification of the measurement to be captured, the system indicates a plurality of locations relative to the user'ss body to be recorded, and wherein the handheld device is physically moved to each of the plurality of locations adjacent the user'ss body and pointed towards the user'ss skin at each of the plurality of locations, and wherein the sensor module records a location in space associated with each of the plurality of locations,wherein the system determines a value for the measurement of the user based on relative locations of the recorded locations in space.","20","17/339953","2021-06-05","2021-0390727","2021-12-16","11721035","2023-08-08","TAILORU LLC","Thu Minh Do","","","","G06T-0007/60","G06T-0007/60 | G06F-0003/0488 | G06F-0003/14 | G06Q-0030/0282 | G06T-0007/55 | G06T-0019/006 | G16H-0040/67 | G16H-0050/30 | A61B-0005/1079 | G06F-0017/17 | G06Q-0030/0643 | G06T-2200/24","G06T-007/60","G06T-007/60 | G06T-019/00 | G06F-003/0488 | G06T-007/55 | G06F-003/14 | G16H-040/67 | G06Q-030/0282 | G16H-050/30 | A61B-005/107 | G06F-017/17 | G06Q-030/0601","","","","","","4923032004878"
"US","US","P","B2","Range of motion tracking system","A method for range of motion (ROM) tracking, that determines with a ROM tracking system, an exercise identified by a caregiver to be performed by a subject by positioning a sensor of the ROM tracking system to allow the sensor to detect at least one movement by the subject during a performance of the exercise, and then detecting, through the sensor, at least one movement of the subject. The system further analyzes the movement by the subject to determine a range of motion of the at least one movement; recording through a user interface an indication by the subject of an experiential narrative; and finally, providing a report to the caregiver, where the report contains the results of at least one movement in conjunction with at least a portion of the experiential narrative.","1. A method for range of motion (ROM) tracking comprising: providing to a subject, with a ROM tracking system, an exercise identified by a caregiver to be performed by the subject;positioning a sensor of the ROM tracking system to allow the sensor to detect a set of movements by the subject during a performance of the exercise;detecting, by the ROM tracking system, through the sensor, the set of movements performed by the subject during the performance of the exercise;after completion of the exercise by the subject, providing a 3-dimensional range of motion map of the set of movements performed by the subject during the performance of the exercise;creating with the ROM tracking system an educational tool and an evaluation system easily employed and used by a single person at home without aid of another person.","2","17/236326","2021-04-21","2021-0352066","2021-11-11","11722486","2023-08-08","Chris Outwater | William Gibbens Redmann","Chris Outwater | William Gibbens Redmann","","","","H04L-0063/0861","H04L-0063/0861 | A61B-0005/1114 | A61B-0005/1121 | A61B-0005/18 | A61B-0005/4824 | A61B-0005/6898 | G06V-0020/597 | G06V-0040/10 | G06V-0040/19 | G06V-0040/23 | H04L-0063/083 | H04M-0001/724 | H04W-0004/029 | H04W-0012/06 | A61B-0005/1124 | A61B-2505/09 | H04L-0063/0853 | H04M-2250/22","H04L-029/06","H04L-029/06 | G06K-009/00 | A61B-005/00 | H04W-012/06 | H04W-004/029 | A61B-005/11 | H04M-001/724 | H04L-009/40 | A61B-005/18 | G06V-020/59 | G06V-040/10 | G06V-040/19 | G06V-040/20","","","","","","4923032006320"
"US","US","P","B2","Systems and methods for using a transcutaneous electrical stimulation device to deliver titrated therapy","The disclosed electrical stimulation system generates interventions to assist patients in complying with a diet. The wearable device includes a microprocessor, electrical stimulator and at least one electrode configured to deliver electrical stimulation to the epidermis, through a range of 0.1 mm to 10 mm or a range of 0.1 mm to 20 mm of the dermis, of a T2 dermatome to a T12 dermatome or meridian of the patient, a C5 to a T1 dermatome across the hand and/or arm, and/or the upper chest regions. The device is adapted to provide electrical stimulation as per stimulation protocols and to communicate wirelessly with a companion control device configured to monitor and record appetite patterns of the patient and deliver titrated therapy. The control device is also configured to monitor, record, and modify stimulation parameters of the stimulation protocols.","1. A computer program product configured to generate real-time interventions in response to a user'ss degree of appetite or hunger, comprising: a first plurality of programmatic instructions stored in a non-transient memory in a client device, wherein, when executed, said first plurality of programmatic instructions is adapted to cause the client device to generate at least one visual or auditory prompt to the user, wherein the at least one visual or auditory is adapted to prompt the user to input data indicative of the user'ss degree of appetite or hunger via a microphone or a display of the client device;a second plurality of programmatic instructions stored in a non-transient memory in the client device or in another computing device, wherein, when executed, the second plurality of programmatic instructions acquires the inputted data indicative of the user'ss degree of appetite or hunger and determines an appetite pattern or hunger pattern of the user based upon the inputted data;a third plurality of programmatic instructions stored in a non-transient memory in the client device or in another computing device, wherein, when executed, the third plurality of programmatic instructions determines a timing of a future appetite event or hunger event specific to the user based on the appetite pattern or hunger pattern; anda fourth plurality of programmatic instructions stored in a non-transient memory in the client device or in another computing device, wherein, when executed, the fourth plurality of programmatic instructions determines an intervention and causes the intervention to be generated based on the determined timing of the future appetite or hunger event.","39","16/694990","2019-11-25","2020-0164209","2020-05-28","11712562","2023-08-01","Elira, Inc.","Bevil Hogg | Raul E. Perez","","","","A61N-0001/36031","A61N-0001/36031 | A61N-0001/0456 | A61N-0001/0484 | A61N-0001/0492 | A61N-0001/0496 | A61N-0001/36007 | A61N-0001/36014 | A61N-0001/37247 | G09B-0019/0092 | A61B-0005/1118 | A61B-0005/14532 | A61B-0005/4866 | A61B-0005/681 | A61N-0001/0502 | G06Q-0010/1093","A61N-001/36","A61N-001/36 | A61N-001/372 | A61B-005/11 | A61N-001/04 | G09B-019/00 | A61B-005/00 | A61B-005/145 | G06Q-010/1093 | A61N-001/05","","","","","","4923031001274"
"US","US","P","B2","Bio-sensing based monitoring of health","In one embodiment, a health-monitoring system may access a waist-hip measurement of a user. The system may determine one or more stress-related parameters of the user using one or more computing devices. The system may determine one or more correlations between the waist-hip measurement and the one or more stress-related parameters of the user. The system may provide feedback to the user based on one or more of the one or more stress-related parameters or the determined correlations between the waist-hip measurement and the one or more stress-related parameters.","1. A method comprising, by one or more computing devices: accessing, by the one or more computing devices, a waist-hip measurement of a user;detecting, by the one or more computing devices, a start of a stress episode during one or more of heart-rate monitoring or activity monitoring of the user;determining, by the one or more computing devices, one or more stress-related stages of the user based on the one or more monitored heart rate or activity;determining, by the one or more computing devices, one or more correlations between the waist-hip measurement and one or more stress-related parameters of the user, wherein one of the stress-related parameters comprises an LHPA (limbic-hypothalamic-pituitary-adrenal) axis activation threshold time period;providing, by the one or more computing devices within the LHPA axis activation threshold time period, instructions for presenting feedback to the user, wherein the feedback is based on the determined correlations between the waist-hip measurement and the one or more stress-related parameters, and wherein a type of content in the feedback is determined based on the one or more stress-related parameters; andmonitoring, by the one or more computing devices, changes in a waist size measurement of the user in response to presenting the feedback to the user.","44","15/965542","2018-04-27","2019-0328316","2019-10-31","11707225","2023-07-25","SAMSUNG ELECTRONICS COMPANY, LTD.","Fannie Fontanel | Jawahar Jain | Sajid Sadi","","","","A61B-0005/486","A61B-0005/486 | A61B-0005/0077 | A61B-0005/0245 | A61B-0005/02405 | A61B-0005/02416 | A61B-0005/02438 | A61B-0005/1072 | A61B-0005/1075 | A61B-0005/1079 | A61B-0005/1118 | A61B-0005/165 | A61B-0005/4884 | A61B-0005/7246 | A61B-0005/7275 | G06T-0007/13 | G06T-0007/60 | G06T-0007/70 | G16H-0010/60 | G16H-0050/20 | A61B-2562/0219 | G06F-0003/04842 | G06T-2207/30196","A61B-005/00","A61B-005/00 | G06T-007/70 | G06T-007/13 | A61B-005/107 | A61B-005/0245 | A61B-005/024 | A61B-005/11 | A61B-005/16 | G16H-010/60 | G16H-050/20 | G06T-007/60 | G06F-003/04842","","","","","","4923030000953"
"US","US","P","B2","Soft tissue balancing in articular surgery","Systems and methods may be used to perform robot-aided surgery. A system may include a display device and a computing device including a memory device with instructions. The instructions can cause the system to access surgical data, calculate medial and lateral gap data, calculate a recommended component set, and generate a graphical user interface. Accessing surgical data can include accessing soft tissue data indicative of at least tension in soft tissues surrounding a surgical location. The graphical user interface can include an interactive trapezoidal graphic overlaid onto a graphical representation of a distal femur and a proximal tibia. The interactive trapezoidal graphic can include a graphical representation of a medial total gap, a lateral total gap, and a recommended spacer size. The interactive trapezoidal graphic can update in response to adjustments in implant parameters to assist in surgical planning.","1. A method comprising: on a computing device performing operations including:accessing surgical data including soft tissue data indicative of at least tension in soft tissues surrounding a surgical location;calculating, based at least in part on the soft tissue data, medial total gap and a lateral total gap;determining, based at least in part on the medial total gap and lateral total gap, a recommended component set, the recommended component set including a femoral component, a tibial component, and a spacer;generating, for display on a display device, a graphical user interface including an interactive trapezoidal graphic overlaid onto a graphical representation of a distal femur and a proximal tibia, wherein the interactive trapezoidal graphic includes a graphical representation of the medial total gap, the lateral total gap, the recommended component set, a medial overlap indicator and a lateral overlap indicator, wherein the medial overlap indicator and the lateral overlap indicator include a bar-graph type graphic or a triangular type graphic overlaid on the interactive trapezoidal graphic; andoutputting the graphical user interface to the display device.","19","17/559971","2021-12-22","2022-0183774","2022-06-16","11707333","2023-07-25","Zimmer, Inc.","Jean-Sebastien Merette | Harlan Levine | Pierre Couture | Olivier Boisvert | Emily Gogarty | Marc-Antoine Dufour | Vincent Masse","","","","A61B-0034/25","A61B-0034/25 | A61B-0005/1075 | A61B-0005/4533 | A61B-0017/025 | A61B-0017/1764 | A61B-0034/10 | G06F-0003/04845 | G06F-0003/04847 | G06T-0011/206 | G06T-0011/60 | A61B-0017/154 | A61B-2017/0268 | A61B-2017/568 | A61B-2034/104 | A61B-2034/107 | A61B-2034/108 | G06T-2200/24 | G06T-2210/41","A61B-034/00","A61B-034/00 | A61B-005/00 | A61B-034/10 | G06F-003/04847 | G06F-003/04845 | A61B-005/107 | G06T-011/20 | A61B-017/15 | A61B-017/56 | A61B-017/02 | A61B-017/17 | G06T-011/60","","","","","","4923030001061"
"US","US","P","B2","Non-invasive nerve stimulation with mobile device","Devices, systems and methods for treating various disorders and medical conditions through noninvasive stimulation of a nerve. A system comprises a stimulator including an electrode configured to contact an outer skin surface of a patient and an energy source coupled to the housing, The energy source generates an electrical impulse and the stimulator transmits the electrical impulse from the electrode transcutaneously through an outer skin surface of the patient to a selected nerve within the patient. The system further includes an application on a mobile device that receives data from a remote source. The mobile device couples to the stimulator and the application causes the mobile device to transmit the data to the stimulator.","1. A system comprising: a stimulator including an electrode configured to contact an outer skin surface of a patient and a user interface for turning the stimulator ON to enable the stimulator to operate;an energy source coupled to the stimulator, wherein the energy source generates an electrical impulse, wherein the stimulator transmits the electrical impulse from the electrode transcutaneously through an outer skin surface of the patient to a selected nerve within the patient; anda mobile device that receives data from a remote source, wherein the mobile device couples to the stimulator and transmits the data to the stimulator, wherein the data includes authorization for the stimulator to transmit the electrical impulse; andwherein the energy source only generates the electrical impulse when the stimulator has been turned ON and said authorization has been provided by the mobile device.","29","17/694655","2022-03-14","2022-0266015","2022-08-25","11701515","2023-07-18","ELECTROCORE, INC","Bruce J. Simon | Joseph P. Errico | John T. Raffle","","","","A61N-0001/36021","A61N-0001/36021 | A61N-0001/36034 | G06Q-0010/10 | G16H-0020/30 | G16H-0020/40 | G16H-0040/20 | A61B-0005/4836 | A61B-2560/0456 | A61N-0001/0456 | A61N-0001/37235 | A61N-0001/40 | A61N-0002/006 | A61N-0002/008 | A61N-0002/02","A61N-001/36","A61N-001/36 | G16H-020/30 | G06Q-010/10 | G16H-020/40 | G16H-040/20 | A61N-001/372 | A61N-001/04 | A61N-001/40 | A61N-002/00 | A61N-002/02 | A61B-005/00","","","","","","4923029001368"
"US","US","P","B2","Neurological profiles for market matching and stimulus presentation","A neurological profile associated with introversion/extroversion levels, simultaneous visual element processing capability, and/or dynamism processing capability, etc., is determined to select market categories and stimulus material targeted to the particular neurological profile. The neurological profile is determined using information such as user input, user activity, social and environmental factors, genetic and developmental factors, and/or neuro-response data. The neurological profile can be matched with corresponding neurological profile templates to select market categories and stimulus material.","1. A system for creating media based on a visual processing capability of a user, the system comprising: a sensor to obtain first neuro-response data from the user during exposure of the user to first media and to obtain second neuro-response data during exposure of the user to second media, the first media including a first number of simultaneous visual elements and the second media including a second number of simultaneous visual elements, the first number different than the second number;memory including instructions; anda processor to execute the instructions to: determine a maximum number of simultaneously presented visual elements in a stimulus that invokes an increase in a user response in the user based on the first neuro-response data and the second neuro-response data;assign a simultaneous visual element processing capability to the user based on the maximum number of simultaneously presented visual elements;tailor source media to have the maximum number of simultaneously presented visual elements corresponding to the simultaneous visual element processing capability of the user to generate first tailored media; andoutput the first tailored media for exposure to the user.","20","16/561980","2019-09-05","2020-0211033","2020-07-02","11704681","2023-07-18","CITIBANK, N.A | NIELSEN CONSUMER LLC","Anantha Pradeep | Robert T. Knight | Ramachandran Gurumoorthy","","","","G06Q-0030/02","G06Q-0030/02 | G06Q-0010/08 | G06Q-0030/0202 | A61B-0005/378 | A61B-0005/7207 | A61B-0005/7278","G06Q-030/02","G06Q-030/02 | G06Q-030/0202 | G06Q-010/08 | A61B-005/378 | A61B-005/00","","","","","","4923029004507"
"US","US","P","B2","Mobile device for viewing of dental treatment outcomes","A mobile computing device comprises an AR display, an image capture device that generates image data of a face of a viewer of the AR display, and a processing device. The processing device receives the image data; processes the image data to identify a position of a dental arch in the image data; determines a treatment outcome for the dental arch; generates a post-treatment image of the dental arch that shows the treatment outcome; generates updated image data comprising a superimposition of the post-treatment image of the dental arch over the received image data depicting the face of the viewer; and outputs the updated image data to the AR display, wherein the post-treatment image of the dental arch is superimposed over the dental arch in the received image data such that the post-treatment image is visible in the AR display rather than a true depiction of the dental arch.","1. A mobile computing device comprising: an augmented reality (AR) display;an image capture device, the image capture device to generate image data of a face of a viewer of the AR display; anda processing device, the processing device to: process the image data to identify a position of a dental arch in the image data;determine a treatment outcome for the dental arch;generate a post-treatment image of the dental arch that shows the treatment outcome;generate updated image data comprising a superimposition of the post-treatment image of the dental arch over the image data depicting the face of the viewer, wherein the post-treatment image of the dental arch is positioned over the image data using the identified position of the dental arch; andoutput the updated image data to the AR display, wherein the post-treatment image of the dental arch is superimposed over the dental arch in the image data such that the post-treatment image is visible in the AR display rather than a true depiction of the dental arch, wherein a remainder of the face from the image data that is not covered by the post-treatment image of the dental arch is visible in the AR display.","20","17/244837","2021-04-29","2021-0248832","2021-08-12","11704876","2023-07-18","ALIGN TECHNOLOGY, INC.","Pavel Pokotilov | Anton Lapshin | Evgeniy Malashkin | Sergei Ozerov | Yury Slynko | Andrey Sergeevich Nekrasov | Leonid Vyacheslavovich Grechishnikov | Anna Orlova | Yingjie Li | Phillip Thomas Harris | Maurice K. Carrier","","","","G06T-0019/006","G06T-0019/006 | A61B-0034/10 | A61B-0090/36 | A61C-0007/002 | A61C-0009/008 | G02B-0027/0093 | G02B-0027/017 | G06F-0003/011 | G06V-0040/168 | A61B-2017/00216 | A61B-2034/105 | A61B-2034/2048 | A61B-2034/2065 | A61B-2090/365 | A61B-2090/3612 | A61B-2090/371 | A61B-2090/372 | A61B-2090/502 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0141 | G02B-2027/0178 | G02B-2027/0181","G06T-019/00","G06T-019/00 | A61C-007/00 | G06K-009/00 | A61C-009/00 | A61B-034/10 | G06F-003/01 | A61B-090/00 | G02B-027/01 | G02B-027/00 | G06V-040/16 | A61B-017/00 | A61B-034/20 | A61B-090/50","","","","","","4923029004700"
"US","US","P","B2","Resolving entities from multiple data sources for assistant systems","In one embodiment, a method includes receiving a request to access a first record in a plurality of records, where the first record describes a first set of attributes of a first entity, determining the first record is linked to a globally unique entity identifier, identifying one or more second records linked to the unique entity identifier, where the one or more second records describe one or more second sets of attributes of the first entity, generating a fused record comprising descriptions of attributes of the first entity from the first set and second sets of attributes, where the fused record is generated by deduping the plurality of records to associated the first record and the one or more second record with the unique entity identifier and compiling the first set and one or more second sets of attributes, and sending, in response responsive to the request to access the first record, instructions for presenting the fused record.","1. A method comprising, by one or more computing systems: receiving, from a client system, a request to access a first record in a plurality of records, wherein the first record describes a first set of attributes of a first entity;determining the first record is linked to a globally unique entity identifier;identifying one or more second records linked to the unique entity identifier, wherein the one or more second records describe one or more second sets of attributes of the first entity;generating a fused record comprising descriptions of attributes of the first entity from the first set and second sets of attributes, wherein the fused record is generated by deduping the plurality of records to associate the first record and the one or more second record with the unique entity identifier and compiling the first set and one or more second sets of attributes; andsending, to the client system, responsive to the request to access the first record, instructions for presenting the fused record.","20","17/018764","2020-09-11","2020-0409936","2020-12-31","11704899","2023-07-18","META PLATFORMS, INC.","Markku Salkola","","","","G06F-0016/3329","G06F-0016/3329 | G06F-0003/011 | G06F-0003/013 | G06F-0003/017 | G06F-0003/167 | G06F-0007/14 | G06F-0009/453 | G06F-0016/176 | G06F-0016/2255 | G06F-0016/2365 | G06F-0016/243 | G06F-0016/248 | G06F-0016/24552 | G06F-0016/24575 | G06F-0016/24578 | G06F-0016/338 | G06F-0016/3323 | G06F-0016/3344 | G06F-0016/904 | G06F-0016/9038 | G06F-0016/90332 | G06F-0016/90335 | G06F-0016/951 | G06F-0016/9535 | G06F-0040/205 | G06F-0040/295 | G06F-0040/30 | G06F-0040/40 | G06K-0009/6269 | G06N-0003/006 | G06N-0003/08 | G06N-0007/005 | G06N-0020/00 | G06Q-0050/01 | G06V-0020/10 | G06V-0040/28 | G10L-0015/02 | G10L-0015/063 | G10L-0015/07 | G10L-0015/16 | G10L-0015/183 | G10L-0015/187 | G10L-0015/1815 | G10L-0015/1822 | G10L-0015/22 | G10L-0015/26 | G10L-0017/06 | G10L-0017/22 | H04L-0012/2816 | H04L-0041/20 | H04L-0041/22 | H04L-0043/0882 | H04L-0043/0894 | H04L-0051/02 | H04L-0051/18 | H04L-0051/216 | H04L-0051/52 | H04L-0067/306 | H04L-0067/535 | H04L-0067/5651 | H04L-0067/75 | H04W-0012/08 | G06F-2216/13 | G10L-0013/00 | G10L-0013/04 | G10L-2015/223 | G10L-2015/225 | H04L-0051/046 | H04L-0067/10 | H04L-0067/53","A61N-001/00","A61N-001/00 | G06F-016/332 | G06F-009/451 | G10L-015/18 | G10L-015/183 | G10L-015/22 | G06F-016/338 | G06F-016/33 | G06N-020/00 | G06F-016/9535 | G06Q-050/00 | H04L-067/306 | G06F-016/176 | G10L-015/06 | G10L-015/16 | G06F-003/01 | G06F-016/9032 | G06F-016/2457 | H04L-051/02 | G06F-003/16 | G06K-009/62 | G06N-003/08 | G10L-015/26 | G06F-016/9038 | G06F-016/904 | G06F-040/30 | G06F-040/40 | G06F-016/22 | G06F-016/23 | G06F-007/14 | H04L-043/0882 | H04L-043/0894 | H04L-012/28 | H04L-041/00 | H04L-041/22 | H04W-012/08 | G10L-015/07 | G10L-017/22 | G06N-003/006 | G10L-017/06 | G06F-016/248 | G06F-016/951 | G06F-016/242 | G06F-016/2455 | G10L-015/02 | G10L-015/187 | G06V-020/10 | G06V-040/20 | G06F-040/295 | H04L-051/52 | H04L-051/216 | H04L-067/50 | H04L-067/5651 | G06N-007/00 | H04L-051/18 | H04L-067/75 | G06F-016/903 | G06F-040/205 | H04L-067/10 | H04L-051/046 | G10L-013/00 | G10L-013/04 | H04L-067/53","","","","","","4923029004723"
"US","US","P","B2","Discernment of comfort/discomfort","The computer implemented method makes it possible to discern, for a variety of sensations, whether a sensation is a pleasant (comfortable) sensation or a sensation of discomfort. A classifier is generated for discerning the stress or comfort/discomfort of a subject. The method comprising: a) imparting, to a subject, different stimuli under the same environment, and obtaining brain wave data or analysis data thereof for the environment; b) correlating a reaction of the subject relating to the stimulation and the difference of the brain wave data or analysis data thereof obtained under the environment; c) generating a classifier for discerning the stress or comfort/discomfort of the subject, on the basis of the correlation; and d) performing comfort/discomfort discernment using a basic step for amplifying a sample from a small stimulation.","1. A method of determining whether a pain corresponds to stress or unpleasantness, comprising: a) providing the computer implementing an unpleasantness determination classifier for determining whether a pain corresponds to stress or unpleasantness of an object, wherein the computer includes a processor and the unpleasantness determination classifier is generated by a machine learning process; andb) obtaining, by the processor, brainwave data or analysis data thereof from the object and applying the data to the unpleasantness determination classifier to determine unpleasantness of the object, wherein the machine learning process includes,c) applying the different stimuli to the object under the same environment to obtain each brainwave data or analysis data thereof by using the brainwave recording sensor, wherein the brainwave recording sensor has a plurality of electrodes including brainwave measurement electrodes configured to be placed on a subject'ss scalp,d) associating, by the processor, a difference in the brainwave data or analysis data thereof obtained under the same environment with a reaction of the object to the stimulation, ande) generating, by the processor, the unpleasantness determination classifier based on the association, wherein the generating the unpleasantness determination classifier includes, i) preprocessing the brainwave data by filtering to remove noises of eye blinking and myogenic potential and by normalizing the filtered brainwave data based on the brainwave data during a first predetermined period before applying the stimulation, andii) generating the unpleasantness determination classifier by machine learning with feature selection based on first and second brainwave data, wherein the first brainwave data are the preprocessed brainwave data during a second predetermined period after applying a first level of the stimulation corresponding to unpleasantness, and the second brainwave data are the preprocessed brainwave data during the second predetermined period after applying a second level of the stimulation corresponding to pleasantness, wherein the associating the difference includes,extracting features by converting the brainwave data into the mean value of amplitude and the frequency power during the second predetermined period; andgenerating, using Support Vector Machine Recursive Feature Elimination method, the unpleasantness determination classifier by ranking the features and eliminating the feature with the lowest contribution.","10","16/634310","2018-07-27","2020-0178888","2020-06-11","11690547","2023-07-04","PAMELA, INC. | OSAKA UNIVERSITY","Aya Nakae | Takahiro Soshi","2017-146553","JP","2017-07-28","A61B-0005/165","A61B-0005/165 | A61B-0005/316 | A61B-0005/377 | A61B-0005/4824 | G06F-0003/015 | G06N-0020/10 | G06F-2218/14","A61B-005/16","A61B-005/16 | G06N-020/10 | A61B-005/00 | G06F-003/01 | A61B-005/316 | A61B-005/377","","","","","","4923027001076"
"US","US","P","B2","Activity monitoring device with assessment of exercise intensity","Aspects relate to a portable device that may be used to identify a critical intensity and an anaerobic work capacity of an individual. The device may utilize muscle oxygen sensor data, speed data, or power data. The device may utilize data from multiple exercise sessions, or may utilize data from a single exercise session. The device may additionally estimate a critical intensity from a previous race time input from a user.","1. An apparatus, comprising: a processor;an interface;an oxygenation sensor, configured to be positioned proximate an area of skin of a user, the oxygenation sensor further configured to output data indicative of a tissue oxygenation of a body tissue of the user;a non-transitory computer-readable medium comprising computer-executable instructions that when executed by the processor are configured to perform at least: receive tissue oxygenation data from the oxygenation sensor as the user is performing an exercise session, wherein at least a first portion of the exercise session comprises an intermittent activity;calculate a change in tissue oxygenation as a difference between a current tissue oxygenation value and a previous tissue oxygenation value obtained from the tissue oxygenation data, wherein the current tissue oxygenation value and the previous tissue oxygenation value are rolling averages of tissue oxygenation data points received from the oxygenation sensor during a rolling average duration;calculate a rate of change for the change in tissue oxygenation;based on the rate of change, calculate an estimated time to exhaustion for the user; andbased upon the estimated time to exhaustion for the user and while the user is performing the exercise session, transmit an output configured to display a notification on the interface, wherein the notification includes a user message to alter behavior so as to result in a different time to actual exhaustion.","20","17/477126","2021-09-16","2022-0000419","2022-01-06","11690565","2023-07-04","NIKE, INC.","Brett S. Kirby | Bradley W. Wilkins | David Clark | Eric Bradley | Elizabeth Besemer","","","","A61B-0005/486","A61B-0005/486 | A43B-0003/34 | A43B-0007/141 | A43B-0013/125 | A43B-0013/181 | A61B-0005/0015 | A61B-0005/1123 | A61B-0005/14542 | A61B-0005/222 | A61B-0005/224 | A61B-0005/4866 | A61B-0005/6807 | A63B-0071/06 | G06Q-0010/10 | G06Q-0050/01 | G16H-0020/30 | G16H-0040/63 | A61B-2503/10","A61B-005/00","A61B-005/00 | A61B-005/145 | A61B-005/22 | G06Q-050/00 | G16H-020/30 | A43B-013/12 | A43B-013/18 | G16H-040/63 | G06Q-010/10 | A43B-007/1405 | A61B-005/11 | A43B-003/34 | A63B-071/06","","","","","","4923027001094"
"US","US","P","B2","Medical voice command integration","System and methods for controlling healthcare devices and systems using voice commands are presented. In some aspects a listening device may receive voice command from a person. The voice command may be translated into human readable or machine readable text via a speech-to-text service. A control component may receive the text and send device-specific instructions to a medical device associated with a patient based on the translated voice command. In response to the instructions, the medical device may take an action on a patient. Some examples of actions taken may include setting an alarm limit on a monitor actively monitoring a patient and adjusting the amount of medication delivered by an infusion pump. Because these devices may be controlled using a voice command, in some cases, no physical or manual interaction is needed with the device. As such, multiple devices may be hands-free controlled from any location.","1. A system for controlling medical devices, the system comprising: a microphone configured to receive voice commands from a user;a medical device associated with a patient;a processor in communication with the microphone, the medical device associated with the patient, and a medical record system comprising an electronic health record associated with the patient; andone or more computer-readable media storing computer-readable instructions that, when executed by the processor, cause the processor to: receive, via the microphone, a voice command, the voice command having instructions for the medical device to administer an amount of medication to the patient;translate the instructions for use by the medical device;authorize the voice command based on a determination that the amount of medication is within a medication administration threshold of a medical order in the electronic health record associated with the patient; andsubsequent to the authorization, send the translated instructions to the medical device, wherein the medical device administers the amount of medication to the patient in accordance with the translated instructions.","20","17/165683","2021-02-02","2021-0153819","2021-05-27","11690578","2023-07-04","CERNER INNOVATION, INC.","Chad Hays | Randy Lantz","","","","A61B-0005/749","A61B-0005/749 | A61B-0005/0022 | A61M-0005/142 | A61M-0016/022 | G06F-0003/167 | G10L-0015/22 | G16H-0040/63 | G16H-0040/67 | A61B-0005/0006 | A61B-0005/318 | A61B-0005/6898 | A61B-0005/746 | A61B-2017/00017 | A61B-2017/00119 | A61B-2017/00203 | A61M-2205/3375 | A61M-2205/3553 | A61M-2205/3561 | A61M-2205/3584 | A61M-2205/3592 | A61M-2205/502 | A61M-2205/52 | A61M-2205/60 | A61M-2205/609 | A61M-2205/80 | G10L-2015/223 | G16H-0010/60 | G16H-0050/20","G10L-015/00","G10L-015/00 | A61B-005/00 | G16H-040/63 | G10L-015/22 | G06F-003/16 | G16H-040/67 | A61M-016/00 | A61M-005/142 | A61B-017/00 | G16H-010/60 | G16H-050/20 | A61B-005/318","","","","","","4923027001107"
"US","US","P","B2","Dynamic sauna","Systems and methods are provided for controlling infrared radiation (IR) sources of a sauna including tuning IR wavelength-ranges and radiated power-levels of IR sources, and directing IR to locations on a user's body. In one illustrative embodiment, a sauna may be provided having adjustable IR emitters to emit IR at any wavelength resulting in a desirable radiation treatment for the sauna user. In another illustrative embodiment, a method is provided for tuning IR emitters in a sauna.","1. A sauna comprising: an enclosure assembly for accommodating a user;a first heating element coupled with the enclosure assembly;anda computing device comprising a control module and a diagnostic module, wherein the control module is operable to control the first heating element and wherein the diagnostic module is operable to receive diagnostic information associated with an electrical system circuit of the sauna and wherein the diagnostic module is further operable to transmit diagnostic information to the remote server.","14","17/820126","2022-08-16","2022-0395428","2022-12-15","11690782","2023-07-04","SUNLIGHTEN, INC.","James T. O'Keeffe | Aaron Michael Zack | Martin C. Ku | Ian Richard Kuklenski | Steven J. Murray","","","","A61H-0033/063","A61H-0033/063 | A61H-0033/06 | A61H-0033/066 | A61N-0005/0625 | G06Q-0010/0639 | G06Q-0050/22 | G16H-0020/30 | G16H-0020/40 | G16H-0040/63 | H05B-0001/0275 | H05B-0003/008 | H05B-0003/009 | H05B-0003/267 | A61H-0033/067 | A61H-2033/061 | A61H-2201/0188 | A61H-2201/0228 | A61H-2201/10 | A61H-2201/5012 | A61H-2201/5015 | A61H-2201/5035 | A61H-2201/5038 | A61H-2201/5043 | A61H-2201/5046 | A61H-2201/5048 | A61H-2201/5087 | A61H-2230/04 | A61H-2230/06 | A61H-2230/207 | A61H-2230/30 | A61H-2230/40 | A61H-2230/42 | A61H-2230/50 | A61H-2230/80 | A61N-2005/0636 | A61N-2005/0652 | A61N-2005/0659 | H05B-2203/032","A61H-033/06","A61H-033/06 | G16H-040/63 | G06Q-010/0639 | H05B-001/02 | H05B-003/26 | A61N-005/06 | G16H-020/30 | G06Q-050/22 | H05B-003/00 | G16H-020/40","","","","","","4923027001311"
"US","US","P","B2","Method and system for outputting augmented reality information","A method and system are disclosed for outputting augmented reality information to a first user. In an embodiment, the method includes acquiring first information, including image information, depth information, coordinate information and combinations thereof, the first information relating to at least one of a medical device and a medical examination of a patient; creating the augmented reality information, relating to the medical device and/or the medical examination of the patient, based on the first information; and outputting the augmented reality information such that the augmented reality information is perceivable in a field of view of the first user.","1. A method for outputting augmented reality information to a user, the method comprising: acquiring a medical imaging dataset relating to an anatomical structure of a patient;acquiring first information, the first information including one or more of image information, depth information, and coordinate information;creating the augmented reality information by evaluating, via a machine learning algorithm, the first information and the medical imaging dataset, the augmented reality information including a structure marking, including a 3-dimensional (3D) image marking the anatomical structure of the patient; andoutputting the augmented reality information in response to the user placing at least one of a hologram of the anatomical structure of the patient and a hologram-type representation of the anatomical structure of the patient in a field of view of the user, the augmented reality information being combined, in an anatomically correct manner, with the at least one of the hologram of the anatomical structure of the patient and the hologram-type representation of the anatomical structure of the patient, and being perceivable in the field of view of the user.","37","17/072514","2020-10-16","2021-0042919","2021-02-11","11694328","2023-07-04","SIEMENS HEALTHCARE GMBH","Thomas Boettger | Christophe Della Monta | Thilo Hannemann | Philipp Hoelzer | Gerhard Kraemer | Stefan Reichelt | Grzegorz Soza","10-2015-226669","DE","2015-12-23","G06T-0007/0012","G06T-0007/0012 | A61B-0005/0013 | A61B-0005/055 | A61B-0005/743 | A61B-0005/745 | A61B-0005/748 | A61B-0090/30 | G06F-0003/011 | G06F-0003/012 | G06F-0003/013 | G06F-0003/0304 | G06F-0003/0346 | G06F-0003/04815 | G06T-0007/50 | G06T-0007/70 | G06T-0019/006 | A61B-0005/744 | A61B-0006/032 | A61B-2090/365 | A61B-2576/00 | G06T-2207/30004 | G06T-2207/30204 | G06T-2210/41","G06T-007/00","G06T-007/00 | A61B-090/30 | A61B-005/00 | G06F-003/01 | G06F-003/03 | G06F-003/04815 | G06F-003/0346 | G06T-007/50 | G06T-007/70 | A61B-005/055 | G06T-019/00 | A61B-090/00 | A61B-006/03","","","","","","4923027004835"
"US","US","P","B2","Artificial intelligence systems and methods for generating land responses from biological extractions","An artificial intelligence system for generating land responses from biological extractions. The artificial intelligence system includes a computing device, configured to retrieve, a biological extraction pertaining to a user, wherein the biological extraction contains at least an element of physiological data. The computing device is configured to locate, a land descriptor wherein the land descriptor identifies a property. The computing device is configured to generate, a land machine-learning model, wherein the land machine-learning model utilizes the biological extraction as an input and outputs property elements. The computing device is configured to determine the suitability of the property utilizing the output property elements.","1. An artificial intelligence system for generating land responses from biological extractions, the system comprising a computing device, the computing device designed and configured to: retrieve a biological extraction pertaining to a user, wherein the biological extraction contains at least an element of physiological data;locate a land descriptor wherein the land descriptor identifies a property;generate a land machine-learning model, wherein generating the land machine-learning model comprises training the land machine-learning model with training data correlating biological extractions to property elements of a property, wherein the land machine-learning model receives the biological extraction and the land descriptor as an input and outputs property elements;determine suitability of the property utilizing the output property elements;determine that the output property elements indicate that the property is unsuitable for the user;recommend a neutralizer element to make the property suitable for the user based on the user'ss biological extraction;indicate a period when the neutralizer element will make the property suitable for the user; andupdate the land machine-learning model by training the land machine-learning model with the biological extraction as an input and the suitable property as an output.","18","16/825200","2020-03-20","2021-0290174","2021-09-23","11684316","2023-06-27","KPN INNOVATIONS, LLC","Kenneth Neumann","","","","A61B-0005/7264","A61B-0005/7264 | G06N-0005/04 | G06N-0020/00 | G06Q-0030/0631 | G06Q-0050/16 | G06Q-0050/165 | H04W-0004/021 | H04W-0004/029","A61B-005/00","A61B-005/00 | G06N-020/00 | G06Q-050/16 | H04W-004/029 | H04W-004/021 | G06N-005/04 | G06Q-030/0601","","","","","","4923026001051"
"US","US","P","B2","Method for controlling moving body based on collaboration between the moving body and human, and apparatus for controlling the moving body thereof","The present disclosure relates to technology that controls a remote moving body based on collaboration between the moving body and human, and a method for controlling a moving body includes acquiring a first biosignal indicating an intention to start operation of the moving body from a user, operating the moving body, determining a surrounding situation of the moving body that autonomously controls the driving, providing the user with surrounding information of the moving body for inducing path setting, acquiring a second biosignal evoked by recognition of the surrounding information from the user, setting a driving direction of the moving body, commanding the moving body to automatically perform a driving operation to be carried out in the set driving direction, and acquiring a third biosignal responsive to recognition of a driving error from the user and correcting the driving direction of the moving body to induce driving path resetting.","1. A method for controlling a moving body, comprising: acquiring, by an apparatus for controlling a moving body, a first biosignal indicating an intention to start operation of the moving body from a user, and operating the moving body;determining, by the apparatus for controlling a moving body, a surrounding situation of the moving body that autonomously controls driving of the moving body, providing the user with surrounding information about the surrounding situation of the moving body for inducing path setting, acquiring a second biosignal evoked by recognition of the surrounding information from the user, and setting a driving direction of the moving body;commanding, by the apparatus for controlling a moving body, the moving body to automatically perform a driving operation to be carried out in the set driving direction; andacquiring, by the apparatus for controlling a moving body, a third biosignal responsive to recognition of a driving error from the user and correcting the set driving direction of the moving body to induce driving path resetting.","20","17/025187","2020-09-18","2022-0004184","2022-01-06","11687074","2023-06-27","KOREA INSTITUTE OF SCIENCE AND TECHNOLOGY","Laehyun Kim | Da-Hye Kim | Seung-jun Oh | Eon Jo Hong","10-2020-0082945","KR","2020-07-06","G05D-0001/0016","G05D-0001/0016 | A61B-0005/378 | A61B-0005/7282 | B60W-0040/02 | B60W-0040/08 | B60W-0050/08 | B60W-0060/001 | G05D-0001/0088 | G05D-0001/0246 | B60W-2040/089 | B60W-2040/0872 | B60W-2050/0064 | B60W-2420/42 | G06F-0003/015 | G06F-0003/017 | G06V-0040/20 | G10L-0015/22 | G10L-2015/223","G05D-001/00","G05D-001/00 | G05D-003/00 | G06F-007/00 | G06F-017/00 | A61B-005/00 | A61B-005/378 | G05D-001/02 | B60W-050/08 | B60W-040/08 | B60W-060/00 | B60W-040/02 | G06F-003/01 | G10L-015/22 | G06V-040/20 | B60W-050/00","","","","","","4923026003797"
"US","US","P","B2","Method and apparatus for predicting a race time","A method for providing at least one of the following information to an athlete during a race such as a running or a cycling race: a race time prediction; a probability to achieve a target time at the end of the race; and/or an indication whether the pace followed by the athlete is too fast, adequate or too slow in order to achieve a target time. The method includes measuring during the race a plurality of intermediate times with an inertial sensor and/or a positional sensor in a wearable device; causing a processing unit in the wearable device to retrieve, based on the intermediate time and on previous races of other athletes, a race profile as non-linear function of time over distance (t=f(d)); and using the retrieved race profile for determining the information.","1. A method for providing to an athlete during a race such as a running or a cycling race, at least one of: a race time prediction;a probability to achieve a target time at the end of the race;and/or an indication whether the pace followed by the athlete is too fast, adequate or too slow in order to achieve the target time,said method comprisingmeasuring during said race a plurality of intermediate times or speeds with an inertial sensor and/or a positional sensor in a wearable device;causing a processing unit in said wearable device to retrieve a race profile as a non-linear function of time over distance (t=f(d));using the retrieved race profile for determining said information;wherein said step of retrieving the race profile comprises selecting the race profile from among a plurality of standardized race profiles, said plurality of standardized race profiles comprising: a first starter race profile wherein said non-linear function represents a more rapid pace during an initial section than during a last section of the race; and/ora second starter race profile wherein said non-linear function represents a more rapid pace during a last section than during an initial section of the race.","23","16/339256","2017-10-05","2019-0266505","2019-08-29","11687809","2023-06-27","SLYDE ANALYTICS LLC","Cyrille Gindre | Frederic Lamon | Christophe Ramstein | Patrick Flaction","CH01331/16","CH","2016-10-06","G06N-0005/048","G06N-0005/048 | A61B-0005/112 | A61B-0005/1112 | A61B-0005/1118 | A61B-0005/1122 | A61B-0005/681 | A61B-0005/7264 | A61B-0005/7275 | A63K-0003/00 | G06Q-0010/04 | G07C-0001/24 | G09B-0019/0038 | G16H-0020/30 | G16H-0050/30 | A61B-0005/6803 | A61B-0005/6807 | A61B-2503/10 | A61B-2562/0219","G06N-005/00","G06N-005/00 | G06N-005/048 | G16H-050/30 | A61B-005/11 | A61B-005/00 | G06Q-010/04 | G09B-019/00 | G16H-020/30 | A63K-003/00 | G07C-001/24","","","","","","4923026004528"
"US","US","P","B2","Avatar generator","Systems and methods are disclosed for recommending products or services by receiving a three-dimensional (3D) model of one or more products; performing motion tracking and understanding an environment with points or planes and estimating light or color in the environment; and projecting the product in the environment.","1. A method, comprising: performing motion tracking and camera scanning of an environment with points or planes and estimating light or color in the environment;optimizing features extracted from an image;creating correspondences between a subject avatar from the image and from among different avatars or products;recommending a body design or avatar by looking up the correspondences among different avatars or products; andprojecting the recommended body design or avatar in the environment.","20","17/546258","2021-12-09","2022-0101599","2022-03-31","11688129","2023-06-27","Bao Tran","Bao Tran","","","","G06T-0017/00","G06T-0017/00 | A61B-0005/0205 | A61B-0005/1036 | A61B-0005/1079 | A61B-0005/1112 | A61B-0005/1118 | A61B-0005/1128 | A61B-0005/165 | A61B-0005/6829 | A61B-0005/6898 | A61B-0005/742 | G01G-0019/44 | G06Q-0030/0643 | G06T-0003/0093 | G06T-0007/20 | G06T-0011/001 | G06T-0019/006 | G06T-0019/20 | A61B-2576/02 | A63B-2220/00 | G06T-2215/16 | G06T-2219/2012","G06T-017/00","G06T-017/00 | A61B-005/0205 | A61B-005/103 | A61B-005/107 | A61B-005/11 | A61B-005/16 | A61B-005/00 | G01G-019/44 | G06Q-030/0601 | G06T-003/00 | G06T-007/20 | G06T-011/00 | G06T-019/00 | G06T-019/20","","","","","","4923026004847"
"US","US","P","B2","Orthesis or prosthesis system and method for open-loop or closed-loop orthesis or prosthesis control","An orthosis or prosthesis system comprising at least one orthosis or prosthesis, at least one pair of electrodes for contacting the body of the user of the orthosis or prosthesis in order to capture muscle-related signals, at least one evaluation unit for muscle-related signals captured by the at least one electrode pair, at least one actuator for moving the at least one orthosis or prosthesis, and at least one control unit for controlling the at least one actuator. The at least one electrode pair is designed to capture at least a first muscle-related signal using a first measurement frequency and a second muscle-related signal using a second measurement frequency. The at least one evaluation unit evaluates a phase of the first signal and a phase of the second signal. The muscle-related signals can be bioimpedance signals. The system makes it possible to distinguish between muscle contractions and interfering signals.","1. An orthosis or prosthesis system, comprising: at least one orthosis or prosthesis;at least one pair of electrodes as an electrode pair configured for contacting a body of a user of the respective orthosis or prosthesis for detecting muscle-related signals;at least one evaluation unit for muscle-related signals detected by the at least one electrode pair;at least one actuator for moving the respective at least one orthosis or prosthesis;at least one control unit for controlling the at least one actuator;wherein the at least one electrode pair is configured to detect at least a first muscle-related signal using a first measurement frequency and a second muscle-related signal using a second measurement frequency;wherein the at least one evaluation unit is configured to:evaluate a phase of the first signal and a phase of the second signal;compare a phase of the first signal to a reference phase in order to ascertain a first phase change;compare a phase of the second signal to the reference phase in order to ascertain a second phase change;evaluate the first phase change and the second phase change.","20","17/045613","2019-04-01","2021-0161685","2021-06-03","11679009","2023-06-20","OTTOBOCK SE & CO. KGAA","Martin Ryschka | Roman Kusche","10-2018-205306","DE","2018-04-09","A61F-0002/72","A61F-0002/72 | A61B-0005/053 | A61B-0005/397 | A61B-0005/4851 | G06F-0003/015 | A61F-2002/5059 | A61F-2002/6827 | A61F-2002/701 | A61F-2002/704 | A61F-2002/765 | A61F-2005/0155","A61F-002/72","A61F-002/72 | A61B-005/053 | A61B-005/397 | G06F-003/01 | A61B-005/00 | A61F-002/70 | A61F-002/50 | A61F-002/76 | A61F-002/68 | A61F-005/01","","","","","","4923025001074"
"US","US","P","B2","Systems, devices, and methods for performing augmented reality responsive to monitoring user behavior","Systems, devices, and methods are described for performing augmented reality (AR) to assist user performing a task in an environment. An AR device may be configured to capture real-time data. An AR engine may be configured to monitor user behavior from the real-time data responsive to feature extraction from the real-time data, compare the user behavior to pre-defined work procedures, and generate augmented reality objects to be output by the AR device.","1. An augmented reality (AR) system, comprising: an AR device configured to capture real-time data; andan AR engine including a processor configured to execute computer-readable instructions configured to instruct the processor to: monitor user behavior from the real-time data responsive to feature extraction from the real-time data;compare the user behavior and another user behavior corresponding to another AR system to pre-defined work procedures in a coordinated manner to coordinate efforts of the user with the other user; andgenerate augmented reality objects to be output by the AR device responsive to the comparison of the user behavior, the other user behavior, and the pre-defined work procedures;wherein the generated augmented reality objects are at least partially related to the pre-defined work procedures.","22","17/250895","2019-10-02","2021-0341991","2021-11-04","11681357","2023-06-20","Battelle Energy Alliance, LLC","SuJong Yoon | Jeffery A. Aguiar | Johanna H. Oxstrand | Katya L. Le Blanc","","","","G06F-0003/011","G06F-0003/011 | A61B-0005/02438 | G06F-0003/017 | G06Q-0010/063114 | G06V-0020/20 | G06V-0040/20 | H04W-0004/80","G06F-003/01","G06F-003/01 | H04W-004/80 | A61B-005/024 | G06Q-010/0631 | G06V-020/20 | G06V-040/20","","","","","","4923025003405"
"US","US","P","B2","Data center selection for communication with an industrial testing device","The invention relates to an industrial testing device communicating with a data center located in a remote computer network, such as the cloud. Disclosed is a method of registering the device to the cloud and specifying the geographical location of the data center. The method includes selecting a data center from a list of available data centers based on regulations specific to a device type of the industrial testing device. Features are configured for communication between the device and the selected data center.","1. A method comprising: creating a user account in a computer network;registering a specific industrial testing device to the user account;based on registering the specific industrial testing device to the user account, causing a user interface to display a plurality of industrial testing devices that includes the specific industrial testing device;receiving input selecting the specific industrial testing device from the displayed plurality of industrial testing devices;configuring the specific industrial testing device based on receiving the input selecting the specific industrial testing device; andbased on input comprising a request to modify one or more features of the specific industrial testing device, enabling receipt of one or more updates for the specific industrial testing device at a specified location.","20","17/545227","2021-12-08","2022-0103637","2022-03-31","11683376","2023-06-20","OLYMPUS AMERICA INC.","Ehab Ghabour","","","","H04L-0067/12","H04L-0067/12 | G06F-0003/0482 | G06F-0003/1454 | G06F-0009/452 | G06F-0016/9535 | H04L-0045/126 | H04L-0067/10 | H04L-0067/52 | H04N-0001/00437 | H04N-0001/00509 | H04N-0001/00517 | H04W-0004/70 | A61B-0001/00016 | A61B-0008/00 | A61B-0008/42 | A61B-0008/4472 | A61B-2017/00221 | H04L-0041/22 | H04L-0067/1021 | H04L-0067/1097 | H04W-0004/02 | H04W-0004/024 | H04W-0004/029","H04L-067/12","H04L-067/12 | H04W-004/70 | H04L-067/10 | H04L-045/12 | H04N-001/00 | G06F-016/9535 | G06F-003/0482 | H04L-067/52 | G06F-009/451 | G06F-003/14 | H04W-004/024 | H04W-004/029 | H04L-067/1021 | A61B-001/00 | A61B-008/00 | A61B-017/00 | H04L-041/22 | H04L-067/1097 | H04W-004/02","","","","","","4923025005410"
"US","US","P","B2","Methods and systems for secure operation of implantable devices","Implantable devices, such as artificial organs, increasingly incorporate hardware, software, firmware, and/or wireless communication capabilities. For example, such implantable devices can utilize wireless technology to allow for efficient configuration, maintenance, and operational analysis. As these implantable devices become more connected, electronic security will become more important. This disclosure relates to implantable devices that may utilize a secure boot process and secure communication, both between artificial devices in the human body and between these devices and the external world. This disclosure provides secure communication approaches for maintaining the digital privacy and integrity of artificial devices, for protecting the individual from malicious hacking of data, and for controlling of such implantable devices.","1. A system comprising: a first implantable device configured to be implanted subcutaneously in a body;a second implantable device configured to be implanted subcutaneously in the body; anda control component coupled to the first implantable device, the control component including a processor having computer instructions that when executed cause the processor to perform operations comprising: establishing a secure boot process of the first implantable device implanted subcutaneously in the body;wherein a key is compared to authorized keys as a gating function for completion of the secure boot process, wherein confirmation of the key unique to the first implantable device facilitates the security of the secure boot process;identifying an authorized device;establishing a first secure communication pathway between the first implantable device and the authorized device;identifying an unauthorized device;blocking communications between the first implantable device and the unauthorized device;establishing a direct peer-to-peer secure communication pathway between the first implantable device and the second implantable device; andexchanging data between the first implantable device and the second implantable device via the direct peer-to-peer secure communication pathway.","19","16/358540","2019-03-19","2020-0305000","2020-09-24","11683690","2023-06-20","T-MOBILE USA, INC.","Ahmad Arash Obaidi","","","","H04W-0012/088","H04W-0012/088 | A61F-0002/022 | A61F-0002/14 | A61F-0002/38 | A61M-0005/14276 | A61N-0001/37254 | G06F-0016/22 | G06F-0021/575 | G16H-0040/67 | H04W-0012/50 | A61B-0005/14532 | A61F-2002/183 | A61F-2002/3067 | A61F-2250/0002 | A61M-2205/3523 | A61N-0001/36046 | A61N-0001/3956 | G06F-2221/034","H04W-012/088","H04W-012/088 | G06F-016/22 | G16H-040/67 | A61N-001/372 | A61F-002/02 | A61F-002/14 | A61F-002/38 | A61M-005/142 | G06F-021/57 | H04W-012/50 | A61B-005/145 | A61N-001/36 | A61F-002/18 | A61F-002/30 | A61N-001/39","","","","","","4923025005716"
"US","US","P","B2","System and method for noninvasive identification of cognitive and behavioral goals","A brain machine interface system for use with an electroencephalogram to identify a behavioral intent of a person is disclosed. The system includes an electroencephalogram configured to sense electromagnetic signals generated by a brain of a person. The electromagnetic signals include a time component and a frequency component. A monitor monitors a response of the person to a stimulus and a characteristic of the stimulus. A synchronization module synchronizes the sensed electromagnetic signals with the response and the characteristic to determine a set of electromagnetic signals corresponding to the monitored response and the characteristic. A processor processes the set of electromagnetic signals and extracts feature vectors. The feature vectors define a class of behavioral intent. The processor determines the behavioral intent of the person based on the feature vectors. A brain machine interface and a method for identifying a behavioral intent of a person is also disclosed.","1. A brain machine interface system for use with an electroencephalogram to identify a behavioral intent of a person, the system comprising: an electroencephalogram configured to sense electromagnetic signals generated by a brain of a person, wherein the electromagnetic signals comprise a time component and a frequency component;a physiological sensor configured to monitor a response of the person to a stimulus and a characteristic of the stimulus;a synchronization module configured to synchronize the sensed electromagnetic signals with the response and the characteristic to determine a set of electromagnetic signals corresponding to the monitored response of the person and the characteristic; anda processor configured to process the set of electromagnetic signals and to extract feature vectors, wherein the processor is configured to: extract spectral features from the electromagnetic signals;determine a first frequency band and a second frequency band of the spectral features; anddetermine a bicoherence element to define a measure of phase coupling based on the first frequency band and the second frequency band, wherein the measure of phase coupling corresponds to a first set of feature vectors and a second set of feature vectors derived from the at least one stimulus,wherein each of the feature vectors define a class of behavioral intent; anda prosthetic limb configured to perform an action based on machine executable instructions executed by the processor,wherein the processor is further configured to determine the behavioral intent of the person based on the feature vectors,wherein the machine executable instructions, executed by the processor, correspond to the determined behavioral intent, andwherein the prosthetic limb is configured to receive motion instructions from the processor and to move according to the behavioral intent of the person.","15","17/141920","2021-01-05","2021-0205104","2021-07-08","11672676","2023-06-13","TELEDYNE SCIENTIFIC & IMAGING, LLC","Patrick M. Connolly | Stephen Simons | Karen Zachery | Barry Ahrens | Mario Aguilar-Simon | William D. Reynolds, Jr. | David Krnavek","","","","A61F-0002/72","A61F-0002/72 | A61B-0003/113 | A61B-0005/11 | A61B-0005/16 | A61B-0005/246 | A61B-0005/316 | A61B-0005/374 | A61B-0005/378 | A61B-0005/38 | A61B-0005/7264 | G06F-0003/013 | G06F-0003/015 | G06F-0003/04842","A61F-002/72","A61F-002/72 | G06F-003/01 | A61B-005/16 | A61B-005/38 | A61B-005/246 | A61B-005/316 | A61B-005/374 | A61B-005/378 | A61B-003/113 | A61B-005/11 | A61B-005/00 | G06F-003/04842","","","","","","4923024001297"
"US","US","P","B2","Athletic performance sensing and/or tracking systems and methods","Athletic performance sensing and/or tracking systems include components for measuring or sensing athletic performance data and/or for storing and/or displaying desired information associated with the athletic performance to the user (or others). Such systems can allow users a wide variety of options in creating workouts, selecting and presenting media content during the athletic performance, etc., e.g., to help keep users entertained and motivated. In some instances, user feedback may be used, optionally in combination with objective data relating to a workout, to control features of the workout routine, to control the music or other media content selected and/or presented, and/or to control features of future workout routines and/or the presented media content.","1. An athletic performance tracking system, comprising: a processing system programmed and adapted to receive input indicating at least a first parameter associated with a user'ss performance during a workout routine, wherein the first parameter includes physical or physiological data associated with the workout routine; andan input system configured to receive subjective user input including a ranking of the workout routine,wherein the processing system is further adapted to: determine, during the user'ss performance of the workout routine, whether a predefined condition associated with the workout routine has been met; andin response to determining that the predefined condition has been met, prompt the user for the subjective ranking of the workout routine; andgenerate a future workout routine based on the subjective ranking of the workout routine.","20","17/746164","2022-05-17","2022-0277825","2022-09-01","11676699","2023-06-13","NIKE, INC.","Raymond W. Riley | Kevin W. Hoffer | William E. Berner | Allan M. Schrock | James A. Niegowski | William F. Rauchholz","","","","G16H-0020/30","G16H-0020/30 | A61B-0005/1118 | A61B-0005/74 | A61B-0005/7405 | A63B-0024/0006 | A63B-0024/0062 | A63B-0024/0075 | A63B-0024/0087 | A63B-0071/06 | A63B-0071/0616 | A63B-0071/0622 | A63B-0071/0686 | G05B-0015/02 | G06F-0003/165 | G06F-0016/4387 | G06Q-0010/0639 | G09B-0005/02 | G09B-0005/04 | G16Z-0099/00 | A61B-2503/10 | A63B-0024/0059 | A63B-0069/0028 | A63B-0069/16 | A63B-2024/0065 | A63B-2024/0068 | A63B-2024/0071 | A63B-2071/0625 | A63B-2071/0694 | A63B-2220/00 | A63B-2220/12 | A63B-2220/17 | A63B-2220/20 | A63B-2220/30 | A63B-2220/836 | A63B-2225/20 | A63B-2230/06 | G06F-0016/68","G16H-020/30","G16H-020/30 | A63B-024/00 | G06F-016/438 | G06Q-010/0639 | G16Z-099/00 | A63B-071/06 | G05B-015/02 | G06F-003/16 | G09B-005/02 | G09B-005/04 | A61B-005/11 | A61B-005/00 | G06F-016/68 | A63B-069/00 | A63B-069/16","","","","","","4923024005292"
"US","US","P","B2","Tracking wound healing progress using remote image analysis","Systems and methods for tracking healing progress of multiple adjacent wounds are provided. In one embodiment, a system may include a processor configured to receive a first image of a plurality of adjacent wounds near a form of colorized surface having colored reference elements, determine colors of the plurality of wounds, correct for local illumination conditions, receive a second image of the plurality of wounds near the form of colorized surface, to determine second colors of the plurality of wounds in the second image, match each of the plurality of wounds in the second image to a wound of the plurality of wounds in the first image, and determine an indicator of the healing progress for each of the plurality of wounds based on changes between the first image and the second image.","1. A non-transitory computer readable medium for tracking healing progress of multiple adjacent wounds, the computer readable medium containing instructions that when executed by a processor cause the processor to perform a method, the method comprising: receiving a first image of a plurality of adjacent wounds, wherein each wound has multiple segments of differing colors in the first image;determining first colors of the plurality of wounds;receiving a second image of the plurality of wounds, wherein capture of the second image occurs at least one day after capture of the first image;determining second colors of the plurality of wounds in the second image;matching each of the plurality of wounds in the second image to a corresponding wound of the plurality of wounds in the first image; anddetermining an indicator of the healing progress for each of the plurality of wounds based on changes between the first image and the second image.","22","17/482856","2021-09-23","2022-0013216","2022-01-13","11676705","2023-06-13","HEALTHY.IO LTD.","Yonatan Adiri | Ido Omer | Ron Zohar","","","","G16H-0030/40","G16H-0030/40 | A61B-0005/0013 | A61B-0005/1034 | A61B-0005/445 | G01N-0001/12 | G01N-0021/78 | G01N-0033/4833 | G01N-0033/48778 | G01N-0033/493 | G01N-0033/5094 | G01N-0033/52 | G01N-0033/6803 | G01N-0033/6827 | G01N-0033/70 | G06F-0016/5838 | G06Q-0020/385 | G06Q-0040/08 | G06T-0007/0012 | G06T-0007/0014 | G06T-0007/0016 | G06T-0007/11 | G06T-0007/74 | G06T-0007/90 | G16H-0010/40 | G16H-0010/60 | G16H-0030/20 | G16H-0040/20 | G16H-0070/00 | H04L-0063/126 | H04W-0004/12 | H04W-0012/06 | G01N-2001/2826 | G06T-2207/10024 | G06T-2207/20112 | G06T-2207/30004 | G16H-0015/00","G06T-007/00","G06T-007/00 | G16H-030/40 | G16H-040/20 | G16H-010/60 | G01N-033/70 | G01N-033/483 | G06Q-040/08 | G01N-001/12 | G01N-021/78 | G01N-033/487 | G01N-033/493 | G01N-033/52 | G01N-033/68 | G16H-010/40 | G16H-030/20 | G01N-033/50 | H04L-009/40 | H04W-004/12 | H04W-012/06 | G06T-007/90 | G16H-070/00 | G06F-016/583 | A61B-005/103 | A61B-005/00 | G06Q-020/38 | G06T-007/73 | G06T-007/11 | G16H-015/00 | G01N-001/28","","","","","","4923024005298"
"US","US","P","B2","System and method for controlling and selecting sources in a room on a network","A system and method for controlling and selecting sources in a room on a network. The system allows a remote viewer to create a virtual presence within the room by providing the available displays, corresponding to the sources, to the remote viewer. The system includes a standardizing technique for improving the communication and overall switching of data for streaming on a network. The system can include a recording server for performing dual recording of the video files in each of a local database and a remote database. A graphical user interface (GUI) display is provided to guide a local user through a medical procedure in the standardized system.","1. A system for medical collaboration with a remote viewer, the system comprising: a medical videoscope configured to provide an internal video stream of an internal view from within a human or animal subject during a medical procedure;an external camera configured to provide an external video stream of an external view including an outside of the human or animal subject during the medical procedure; anda monitor controller, coupled to the medical videoscope and the external camera, the monitor controller configured to: select and locally display a video stream, including at least one of the internal or external video streams, on a local monitor, wherein the local monitor is local to the human or animal subject, in response to a received first selection request, wherein the video stream is selected based on one or more predefined settings corresponding to at least one of a physician associated with the medical procedure, the human or animal subject undergoing the medical procedure, or a type of the medical procedure;transmit at least one of: the locally displayed video stream, the internal video stream, or the external video stream, to a remote device for remote display on the remote device, in response to a second selection request;change at least one of the locally displayed video stream, the internal video stream or the external video stream based on a status;receive, from the remote device, a third selection request selecting at least one of the internal video stream or the external video stream not currently locally displayed on the local monitor; andin response to the third selection request, update the display on the remote device in accordance with the third selection request.","20","17/188651","2021-03-01","2021-0225506","2021-07-22","11676716","2023-06-13","GYRUS ACMI, INC. D/B/A OLYMPUS SURGICAL TECHNOLOGIES AMERICA","Eddie E. Mitchell | Peter Renzi","","","","G16H-0040/67","G16H-0040/67 | A61B-0001/00011 | A61B-0001/3132 | A61B-0005/0002 | A61B-0006/56 | A61B-0008/56 | A61B-0034/20 | A61B-0034/25 | A61B-0090/20 | A61B-0090/361 | A61B-0090/37 | G06F-0003/0482 | G06Q-0010/109 | G09B-0023/285 | G16H-0040/63 | G16H-0080/00 | H04L-0065/60 | H04L-0067/12 | H04N-0005/2228 | H04N-0007/0125 | H04N-0007/147 | H04N-0007/155 | H04N-0007/181 | H04N-0009/79 | H04N-0023/62 | G16H-0030/20 | H04N-0023/555","G16H-040/67","G16H-040/67 | A61B-001/00 | A61B-001/313 | A61B-005/00 | A61B-034/20 | A61B-090/20 | A61B-090/00 | H04N-007/18 | G16H-080/00 | H04N-007/14 | H04N-007/15 | H04N-023/62 | A61B-034/00 | A61B-006/00 | A61B-008/00 | H04L-065/60 | H04N-005/222 | H04N-007/01 | H04N-009/79 | G16H-040/63 | G06F-003/0482 | G06Q-010/109 | G09B-023/28 | H04L-067/12 | G16H-030/20 | H04N-023/50","","","","","","4923024005308"
"US","US","P","B2","Information processing apparatus, program, cosmetic dispenser","Customized cosmetics corresponding to user-unique factors are provided. An information processing apparatus can communicate with a cosmetic dispenser configured to dispense at least one of a plurality of cosmetics based on recipe information indicating a usage amount of each of the plurality of cosmetics. The apparatus includes a retrieve module configured to retrieve user-unique information unique to the user, the user-unique information including at least one of user attribute information related to the user's attributes, environmental information related to the user's environment, action information related to the user's action, and psychosomatic information related to the user's psychosomatic, skin information related to the user's skin, and information related to cosmetics which the user has used, a selection module configured to select the recipe information based on the user unique information among a plurality of recipe information, and a transmission module configured to transmit the selected recipe information to the cosmetic dispenser.","1. An information processing apparatus capable of communicating with a cosmetic dispenser configured to dispense at least one of a plurality of cosmetics based on recipe information indicating a usage amount of each of the plurality of cosmetics, the apparatus comprising: a retrieve module configured to retrieve user-unique information unique to the user and prediction information, the user-unique information including at least one of user attribute information related to the user'ss attributes, environmental information related to the user'ss environment, action information related to the user'ss action, psychosomatic information related to the user'ss psychosomatic, skin information related to the user'ss skin, and information related to cosmetics which the user has used;a selection module configured to select the recipe information based on the user-unique information and the prediction information among a plurality of recipe information, the user-unique information including user log information indicating a history of the user-unique information;a calculator configured to calculate a first skin score based on the user log information and a second skin score based on the prediction information; anda transmission module configured to transmit the selected recipe information to the cosmetic dispenser,wherein the selection module selects the recipe information based on the first skin score and the second skin score.","18","16/635255","2018-08-01","2021-0085060","2021-03-25","11666133","2023-06-06","SHISEIDO COMPANY, LTD.","Yuko Matsui | Hideo Hata | Mayuri Tashiro | Mieko Nasu | Motoki Takata | Yohei Kobayashi | Miho Yajima | Youko Hayashi | Ayaka Nagai | Takanari Tsuda | Yuichiro Mori | Naoki Saito","2017-151692","JP","2017-08-04","A45D-0044/005","A45D-0044/005 | A45D-0034/00 | A61B-0005/4839 | B01F-0033/846 | B01F-0033/8442 | B01F-0035/2115 | B01F-0035/2135 | G06N-0005/04 | G06Q-0030/0201 | G16H-0010/60 | G16H-0020/13 | G16H-0040/67 | G16H-0050/30 | A45D-2034/005 | A45D-2044/007 | A61B-0005/024 | A61B-0005/1072 | A61B-0005/165 | A61B-0005/442 | A61B-0005/443 | A61B-0005/444 | A61B-0005/4806 | A61B-0005/4866 | A61B-0005/4872 | A61B-0010/0012 | B01F-2101/21 | G06Q-0030/0202 | G06Q-0030/0631 | H04L-0067/12","A45D-044/00","A45D-044/00 | G16H-040/67 | G16H-010/60 | G16H-050/30 | A45D-034/00 | A61B-005/00 | G06N-005/04 | G06Q-030/0201 | G16H-020/13 | B01F-033/84 | B01F-035/21 | A61B-005/024 | A61B-005/107 | A61B-005/16 | A61B-010/00 | G06Q-030/0202 | G06Q-030/0601 | H04L-067/12 | B01F-101/21","","","","","","4923023000872"
"US","US","P","B2","Eye-tracking system for detection of cognitive load","A visual tracking system, comprises an eye-tracking device and a cognitive load detection device disposed in electrical communication with the eye-tracking device, the cognitive load detection device comprising a controller having a memory and a processor. The controller is configured to receive eye-movement data from the eye-tracking device, the eye-movement data comprising pupil dilation event data and at least one of saccade event data and fixation event data, apply a classification function to the eye-movement data to detect a cognitive load associated with the eye-movement data and corresponding to a visual location of a field of view of the user, and output a notification regarding the cognitive load associated with the eye-movement data.","1. A visual tracking system, comprising: an eye-tracking device; anda cognitive load detection device disposed in electrical communication with the eye-tracking device, the cognitive load detection device comprising a controller having a memory and a processor, the controller configured to: receive eye-movement data from the eye-tracking device, the eye-movement data comprising pupil dilation event data taken during a saccade event and pupil dilation event data taken during a fixation event,apply a processing function to the eye-movement data to generate processed eye-movement data as a ratio of the pupil dilation event data taken during the saccade event relative to the pupil dilation event data taken during the fixation event,apply a classification function to the processed eye-movement data to detect a cognitive load associated with the processed eye-movement data and corresponding to a visual location of a field of view of the user, the cognitive load indicating an amount of cognitive resources used by a user when processing information, andoutput a notification regarding the cognitive load associated with the processed eye-movement data.","16","16/523147","2019-07-26","2020-0029806","2020-01-30","11666258","2023-06-06","WORCESTER POLYTECHNIC INSTITUTE","Mina Shojaeizadeh | Soussan Djamasbi | Randy C. Paffenroth | Andrew C. Trapp","","","","A61B-0005/163","A61B-0005/163 | A61B-0003/113 | G06F-0003/013 | G06F-0003/017 | G06F-0009/542 | G06F-0018/214 | G06N-0020/00 | G06T-0007/20 | G06V-0040/19","A61B-005/16","A61B-005/16 | A61B-003/113 | G06F-003/01 | G06F-009/54 | G06T-007/20 | G06N-020/00 | G06F-018/214 | G06V-040/19","","","","","","4923023000997"
"US","US","P","B2","Removable smartphone case for radio wave based health monitoring that includes an alignment feature","A removable smartphone case is disclosed. The removable smartphone case includes a case body configured to receive a smartphone, a radio frequency (RF) front-end connected to the case body and including a semiconductor substrate and an antenna array including at least one transmit antenna configured to transmit radio waves below the skin surface of a person and a two-dimensional array of receive antennas configured to receive radio waves, the received radio waves including a reflected portion of the transmitted radio waves, wherein the semiconductor substrate includes circuits configured to generate signals in response to the received radio waves, a communications interface connected to the case body and configured to transmit digital data that corresponds to the signals generated by the semiconductor substrate from the removable smartphone case, and an alignment feature integrated into the case body and configured to align the antenna array with an object.","1. A removable smartphone case comprising: a case body configured to receive a smartphone;a radio frequency (RF) front-end connected to the case body and including a semiconductor substrate and an antenna array including at least one transmit antenna configured to transmit radio waves below the skin surface of a person and a two-dimensional array of receive antennas configured to receive radio waves, the received radio waves including a reflected portion of the transmitted radio waves, wherein the semiconductor substrate includes circuits configured to generate signals in response to the received radio waves;a communications interface connected to the case body and configured to transmit digital data that corresponds to the signals generated by the semiconductor substrate from the removable smartphone case; andan alignment feature integrated into the case body and configured to align the antenna array with a blood vessel of the person, wherein the alignment feature is a visual marking on the case body that is collocated with the antenna array such that aligning the visible marking with the blood vessel of the person also aligns the antenna array directly over the blood vessel of the person, wherein the visual marking on the case body is collocated with the antenna array in that the visible marking on the body case and the antenna array overlap with each other when viewed from a plan view.","8","16/683026","2019-11-13","2020-0187817","2020-06-18","11666279","2023-06-06","MOVANO INC.","Michael A. Leabman","","","","A61B-0005/6824","A61B-0005/6824 | A45C-0011/00 | A61B-0005/0004 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/021 | A61B-0005/022 | A61B-0005/02116 | A61B-0005/02444 | A61B-0005/14532 | A61B-0005/489 | A61B-0005/681 | A61B-0005/683 | A61B-0005/684 | A61B-0005/6815 | A61B-0005/6898 | A61B-0005/7257 | A61B-0005/7264 | A61B-0005/742 | A61B-0005/7405 | A61B-0005/7455 | A61B-0008/488 | G01S-0007/4004 | G01S-0007/4026 | G01S-0013/87 | G01S-0013/88 | G06F-0001/163 | G06F-0003/016 | G06F-0003/04812 | G06F-0003/167 | G06F-0017/142 | H01Q-0001/273 | H01Q-0001/38 | H01Q-0021/061 | H04B-0001/3888 | H04B-0007/0617 | A45C-2011/002 | A61B-0005/0265 | A61B-0005/05 | A61B-0005/6833 | G01S-0007/028 | G06N-0020/00","A61B-005/00","A61B-005/00 | A61B-005/021 | A61B-005/024 | A61B-005/145 | G01S-013/88 | G01S-007/40 | G06F-001/16 | G06F-003/01 | G06F-003/04812 | G06F-003/16 | H01Q-001/38 | H01Q-021/06 | A61B-005/022 | G06F-017/14 | H04B-007/06 | H01Q-001/27 | A45C-011/00 | G01S-013/87 | H04B-001/3888 | A61B-008/08 | A61B-005/05 | G06N-020/00 | A61B-005/0265 | G01S-007/02","","","","","","4923023001018"
"US","US","P","B2","Remote interface for digital configuration and security of safety equipment","In some examples, a system includes: a plurality of different articles of personal protection equipment (PPE) that are all controlled by a particular user, and a computing device. The computing device may include one or more computer processors configured to receive sets of data from each of the different articles of PPE, wherein each set of data is based at least in part on a type of each of the different articles of PPE; generate for display a user interface that contemporaneously includes a plurality of graphical elements that are based at least in part on at least two sets of data that correspond to at least two different articles of PPE of the plurality of different articles of PPE; and in response to receiving an indication of user input that corresponds to at least one of the plurality of graphical elements, perform at least one operation.","1. A system comprising: a plurality of different articles of personal protection equipment (PPE) (13, 326, 328) that are all controlled by a particular user (10), wherein each respective article of PPE comprises a respective communication device (14);a computing device (16, 302, 302) comprising: a second communication device (306);one or more computer processors (304); anda memory (324, 318, 322) comprising instructions that when executed by the one or more computer processors cause the one or more computer processors to:receive sets of data (314) from each of the different articles of PPE, wherein each set of data is based at least in part on a type of each of the different articles of PPE;generate for display a user interface (800) that contemporaneously includes a plurality of graphical elements (814, 802) that are based at least in part on at least two sets of data that correspond to at least two different articles of PPE of the plurality of different articles of PPE;in response to receiving an indication of user input that corresponds to at least one of the plurality of graphical elements, perform at least one operation;receive an audio signal in response to sound generated by the particular user;determine that the audio signal comprises a voice command to change a configuration associated with a particular article of PPE of the at least two different articles of PPE based at least in part on the information in the audio signal corresponds to a particular type of the particular article of PPE;in response to the determination that the audio signal comprises the voice command to change the configuration associated with the particular article of PPE, generate a message comprising data to change the configuration associated with the particular article of PPE; and send the message to the particular article of PPE.","21","16/645108","2018-09-10","2021-0121330","2021-04-29","11666486","2023-06-06","3M INNOVATIVE PROPERTIES COMPANY","Eric C. Lobner | Kiran S. Kanukurthy | Micayla A. Johnson | Susan R. Johnson | Patric Anveg?rd | Emil R. Eriksson | Daniel E. G. Gullberg | Anton P. Backlund | Derek S. Baker | John M. Kruse","","","","A61F-0009/067","A61F-0009/067 | A42B-0003/225 | A61F-0009/061 | G05B-0019/048 | G06F-0001/163 | G06Q-0010/063114 | G06Q-0050/00 | G06V-0040/20 | G08B-0013/22 | G08B-0021/02 | G08B-0031/00 | G16Y-0010/00 | G16Y-0020/00 | H04L-0067/12 | H04W-0012/009 | H04W-0012/033 | H04W-0012/0433","G08B-013/22","G08B-013/22 | G08B-021/02 | G05B-019/048 | G06Q-010/0631 | G06F-001/16 | A61F-009/06 | H04W-012/00 | H04W-012/0433 | H04W-012/033 | A42B-003/22 | H04L-067/12 | G06V-040/20 | G08B-031/00 | G06Q-050/00 | G16Y-020/00 | G16Y-010/00","","","","","","4923023001224"
"US","US","P","B2","Method for estimating a quantity of a blood component in a fluid receiver and corresponding error","A method and system for communicating estimated blood loss parameters of a patient to a user, the method comprising: receiving data representative of an image, of a fluid receiver; automatically detecting a region within the image associated with a volume of fluid received at the fluid receiver, the volume of fluid including a blood component; calculating an estimated amount of the blood component present in the volume of fluid based upon a color parameter represented in the region, and determining a bias error associated with the estimated amount of the blood component; updating an analysis of an aggregate amount of the blood component and an aggregate bias error associated with blood loss of the patient, based upon the estimated amount of the blood component and the bias error; and providing information from the analysis of the aggregate amount of the blood component and the aggregate bias error, to the user.","1. A method comprising: accessing, by one or more processors, an image including a representation of a fluid receiver having fluid from a patient, the fluid including a blood component;identifying, by the one or more processors, that the fluid receiver in the image is of a specific type;detecting, by the one or more processors, a region in the image that is within the fluid receiver and includes the fluid;determining, by the one or more processors, an amount of the blood component in the fluid receiver and a corresponding error, based on the region and the specific type of fluid receiver; andproviding, by the one or more processors, information based on the determined amount of the blood component and the corresponding error, to a user.","20","17/175897","2021-02-15","2021-0192917","2021-06-24","11670143","2023-06-06","GAUSS SURGICAL, INC.","Siddarth Satish | Andrew T. Hosford | Kevin J. Miller | Milton McColl | Juan Carlos Aragon","","","","G08B-0021/0272","G08B-0021/0272 | A61B-0005/02 | A61B-0005/02042 | G06F-0003/0488 | G06K-0007/0004 | G06K-0007/084 | G06K-0007/087 | G06K-0007/10297 | G06K-0019/06187 | G06K-0019/06206 | G06K-0019/07 | G06K-0019/0702 | G06K-0019/0704 | G06K-0019/0723 | G06K-0019/0725 | G06K-0019/0775 | G06K-0019/07345 | G06K-0019/07703 | G06K-0019/07705 | G06K-0019/07707 | G06K-0019/07709 | G06K-0019/07749 | G06K-0019/07766 | G06K-0019/07769 | G06K-0019/07773 | G06K-0019/083 | G06Q-0020/18 | G06Q-0020/20 | G06Q-0020/34 | G06Q-0020/341 | G06Q-0020/3415 | G06Q-0020/352 | G06Q-0020/385 | G06Q-0020/401 | G06Q-0030/0222 | G06Q-0030/0241 | G06Q-0030/0277 | G06Q-0030/0641 | G06T-0007/62 | G06V-0010/24 | G06V-0010/25 | G06V-0040/172 | G07F-0007/0806 | G07F-0007/1008 | G08B-0013/1427 | G08B-0021/0244 | G08B-0021/0247 | G08B-0021/0269 | G08B-0021/0277 | H04W-0004/02 | H04W-0004/021 | H04W-0004/029 | H04W-0004/20 | H04W-0004/80 | H04W-0008/005 | G06T-2207/10024 | G06T-2207/30004 | G06V-2201/03 | G08B-0005/22 | G08B-0025/016 | H04W-0084/18","G06T-007/62","G06T-007/62 | G08B-021/02 | H04W-004/021 | H04W-004/80 | H04W-004/029 | G08B-013/14 | H04W-004/02 | H04W-004/20 | A61B-005/02 | H04W-008/00 | G06V-010/25 | G06V-040/16 | G06V-010/24 | G06F-003/0488 | G06K-007/00 | G06K-007/08 | G06K-007/10 | G06K-019/06 | G06K-019/07 | G06K-019/073 | G06K-019/077 | G06K-019/08 | G06Q-020/18 | G06Q-020/20 | G06Q-020/34 | G06Q-020/38 | G06Q-020/40 | G06Q-030/0207 | G06Q-030/0241 | G06Q-030/0601 | G07F-007/08 | G07F-007/10 | G08B-025/01 | G08B-005/22 | H04W-084/18","","","","","","4923023004863"
"US","US","P","B2","Method and system for analyzing human gait","The present invention relates to methods for analyzing gait of a subject. In particular, the present invention relates to a method for analyzing gait of a subject, said method comprising: providing data representing the 3D-movement of a foot of said subject over time; identifying within said data first data segments that each represent of at least one stride; determining one or more stride features for each of said first data segments; and defining one or more clusters on the basis of at least one stride feature of said one or more stride features. Each of the defined clusters represents a class of strides, e.g. a class may represent the typical stride of a subject. The present invention also provides for corresponding systems that are configured to perform the methods of the present invention and the use of these systems for analyzing in assessing gait of a subject, preferably a subject suffering from a movement-impairment.","1. A method for analyzing gait of a subject, the method comprising: (a) generating, from 3D-accelerometers and/or 3D-gyroscopes mounted on a left foot and a right foot, or a left shoe and a right shoe of the subject, data representing 3D-movement of the left foot and the right foot of the subject over time, wherein the 3D-movement comprises a plurality of strides of the left foot and the right foot;(b) identifying the plurality of strides in the data and defining first data segments for the left foot and the right foot, wherein each of the first data segments comprises one identified stride or a sequence of consecutive identified strides;(c) determining one or more stride features for each of the first data segments; and(d) defining one or more clusters on the basis of at least one stride feature of the one or more stride features, wherein each cluster represents a class of strides,wherein the one or more clusters are used to determine a gait impairment of the subject.","23","16/308764","2017-06-12","2019-0150793","2019-05-23","11660024","2023-05-30","PORTABILES HEALTHCARE TECHNOLOGIES GMBH","Jens Barth | Bjoern Eskofier | Julius Hannink | Jochen Klucken | Ralph Steidl | J?rgen Winkler","2016-174268","EP","2016-06-13","A61B-0005/112","A61B-0005/112 | A61B-0005/6807 | A61B-0005/7267 | G06F-0003/011 | G06F-0003/017 | G06N-0003/08 | G06N-0020/10 | G06V-0040/25 | A61B-0005/7246 | A61B-2562/0219","A61B-005/11","A61B-005/11 | A61B-005/00 | G06V-040/20 | G06N-020/10 | G06F-003/01 | G06N-003/08","","","","","","4923022000956"
"US","US","P","B2","Biomechatronic data communication systems","A data transmission system for transmitting an electrical data to a nerve cell. A data receiving system for receiving an electrical data from a nerve cell has at least two phototransistor crystals that is stimulated by light to form an electrical signal; an image source that allows the light to be sent to the phototransistor crystals and allows controlling the amount of light transmitted to each phototransistor crystal independently of each other, and at least one control unit that is connected to the image source that controls the amount of light transmitted from the image source to each of the phototransistor crystals.","1. A data transmission system for transmitting an electrical data to a nerve cell (N), characterized by comprising: at least two phototransistor crystals (T), each of which is stimulated by light (l) to form an electrical signal, and comprising:at least one signal output (1), at which an electrical signal is generated and which is configured to be coupled to the nerve cell (N) to which the electrical signal is to be transmitted,at least one voltage input common terminal (3), andat least one threshold light intensity control input (2) configured for controlling a level of the electrical signal generated at the at least one signal output (1) according to an amount of light (l) received, wherein the electrical signal is obtained at the at least one signal output (1) only when a level of the light (l) rises above a threshold value,at least one image source (4) configured for sending the light (l) to the at least two phototransistor crystals (T) and configured for controlling the amount of light (l) transmitted to the at least two phototransistor crystals (T) independently of each other; andat least one control unit which is connected to said image source (4) and is configured to control the amount of light (l) transmitted from said at least one image source (4) to the at least two phototransistor crystals (T).","28","16/331037","2017-04-17","2019-0216328","2019-07-18","11660033","2023-05-30","Utku Buyuksahin","Utku Buyuksahin","2016/12947","TR","2016-09-09","A61B-0005/24","A61B-0005/24 | A61B-0005/307 | A61B-0005/4029 | A61B-0005/6877 | A61B-0005/4851 | A61B-0005/6811 | A61B-2562/0209 | A61B-2562/0233 | A61B-2562/04 | A61B-2562/223 | A61F-2002/6827 | G06F-0003/015","A61B-005/24","A61B-005/24 | A61B-005/00 | A61B-005/307 | G06F-003/01 | A61F-002/68","","","","","","4923022000965"
"US","US","P","B2","Dynamic triggering of augmented reality assistance mode functionalities","Various embodiments of the present invention provide methods, apparatus, systems, computing devices, computing entities, and/or the like for performing augmented reality assistance mode functionalities. Certain embodiments utilize systems, methods, and computer program products that perform augmented reality assistance mode functionalities by using at least one of environment familiarity predictions, assistance mode triggering need determinations, and threat detection machine learning models.","1. A computer-implemented method for dynamically providing augmented reality assistance mode functionalities, the computer-implemented method comprising: generating, by one or more processors and based at least in part on (a) current location data associated with a current end-user physical environment of an augmented reality device and (b) a real-time video stream of the current end-user physical environment, an environment familiarity prediction for the current end-user physical environment, wherein (a) the augmented reality device is associated with an end user profile, (b) the end-user profile comprises a conditional severity score associated with an end user, and (c) the conditional severity score is determined by: (i) identifying a plurality of emotional detection vectors for a plurality of challenge responses, wherein the plurality of emotional detection vectors comprises one or more affirmative-labeled emotional detection vectors and one or more negative-labeled emotional detection vectors,(ii) for each of the one or more affirmative-labeled emotional detection vector, determining a distance measure between the affirmative-labeled emotional detection vector and a negative-labeled centroid of the one or more negative-labeled emotional detection vectors, and(iii) determining the conditional severity score based at least in part on each determined distance measure;determining, by the one or more processors and based at least in part on the environment familiarity prediction, an assistance mode triggering need for augmented reality assistance mode functionalities; andin response to determining that the assistance mode triggering need is an affirmative assistance mode triggering need: (a) generating, by the one or more processors and by using a threat detection machine learning model, a threat indication prediction for the current end-user physical environment, wherein the threat indication prediction is based at least in part on the real-time video stream, and(b) providing, by the one or more processors, the threat indication prediction to the augmented reality device to present using one or more augmented reality visualizations.","18","17/405583","2021-08-18","2023-0059399","2023-02-23","11663790","2023-05-30","OPTUM, INC.","Kartik Chaudhary | Sudeep Choudhary | Raghav Bali | Anurag Das | Subhadip Maji","","","","G06T-0019/006","G06T-0019/006 | G06F-0003/011 | G06F-0003/017 | G06F-0018/22 | G06F-0018/24137 | G06V-0040/28 | A61B-0005/165 | G06N-0020/00","G06T-019/00","G06T-019/00 | G06F-003/01 | G06V-040/20 | G06F-018/22 | G06F-018/2413 | A61B-005/16 | G06N-020/00","","","","","","4923022004693"
"US","US","P","B2","Skin detection device and product information determination method, device and system","A skin detection device, including a processor and a texture recognition sensor, is disclosed. The texture recognition sensor is configured to detect surface texture of skin to be detected; and the processor is configured to determine surface smoothness of the skin to be detected according to the surface texture of the skin to be detected. The skin detection device can independently perform the health detection of the skin to be detected. A product information determination method, a product information determination device and a product information determination system are further disclosed.","1. A skin detection device, comprising: a processor and a texture recognition sensor, wherein the processor is electrically connected with the texture recognition sensor;the texture recognition sensor is configured to detect surface texture of skin to be detected; andthe processor is configured to determine surface smoothness of the skin to be detected according to the surface texture of the skin to be detected,the skin detection device further comprises a first pressure sensor, whereinthe first pressure sensor is electrically connected with the processor and is configured to acquire pressure applied by the skin to be detected to the first pressure sensor when the skin detection device slides on the skin to be detected; andthe processor is further configured to determine a value of elasticity of the skin to be detected according to the pressure acquired by the first pressure sensor,the skin detection device further comprises: a detection electrode electrically connected with the processor, whereinthe detection electrode is configured to acquire conductivity of the skin to be detected by contacting the skin to be detected; andthe processor is further configured to determine moisture content and oil content of the skin to be detected according to the conductivity,the skin detection device further comprises: a base, whereinthe base comprises a contact surface and a side surface, and the contact surface and the side surface are connected;the contact surface is configured to contact the skin to be detected;the texture recognition sensor and the detection electrode are disposed on the contact surface; andthe first pressure sensor is disposed on the side surface,the skin detection device further comprises: a second pressure sensor, whereinthe second pressure sensor is disposed on the contact surface and configured to detect pressure applied to the second pressure sensor by a portion of the skin to be detected in contact with the contact surface;the processor is further configured to determine a target detection value according to the pressure detected by the first pressure sensor and the pressure detected by the second pressure sensor; andthe processor is further configured to determine the value of elasticity of the skin to be detected according to the target detection value.","16","16/309269","2018-01-11","2019-0374156","2019-12-12","11653873","2023-05-23","BOE TECHNOLOGY GROUP CO., LTD.","Wenchu Dong","2017-10517873","CN","2017-06-29","A61B-0005/442","A61B-0005/442 | A61B-0005/0053 | A61B-0005/0077 | A61B-0005/053 | A61B-0005/165 | A61B-0005/443 | A61B-0005/444 | A61B-0005/7275 | G06Q-0030/0623 | G06T-0007/90 | G06V-0040/174 | G09B-0019/00 | G06T-2207/10024 | G06T-2207/30088 | G06V-0040/178","A61B-005/00","A61B-005/00 | G06T-007/90 | A61B-005/053 | A61B-005/16 | G06Q-030/0601 | G09B-019/00 | G06V-040/16","","","","","","4923021001008"
"US","US","P","B2","System and method for improving the emotional mindset of the user","A system for improving an emotional mindset of a person, the system includes a server to receive and process data; a computing device in data communication with the server; one or more software rules implemented through the server and computing device; a platform implemented through the computing device, the platform to perform the steps of: receive a user identified need; a meditation packet created by the computing device, server, and one or more software rules, the meditation packet having resources complied based on the user identified need, the meditation packet to aid the user with the user identified need; the meditation packet is provided to the user through the platform.","1. A system for improving an emotional mindset of a person, the system comprising: a server to receive and process data;a computing device in data communication with the server;one or more software rules implemented through the server and computing device;a crowdsourced connection to provide communication between the server and a plurality of secondary users, wherein the plurality of secondary users input user needs and user solutions, which create crowdsourced data to be processed and used by the one or more software rules, the crowdsourced data includes data regarding tested solutions and effectiveness of the tested solutions for one or more mindset needs, based on information collected from the plurality of secondary users;a platform implemented through the computing device, the platform performs the steps of: receive a user identified need, the user identified need being a mental or performance need that the user desires;determine two or more contents applicable to the user identified need; andoutput a meditation packet with the two or more contents to the user, wherein the user interacting with the meditation packet aids in alleviating the user identified need;the meditation packet created by the computing device, server, and one or more software rules, the meditation packet having the two or more contents, the two or more contents compiled based on correlating the user identified need to at least two separate sources of data, the at least two separate sources of data including: a first source being the crowdsourced connection; anda second source being obtained from a professional resource;wherein the first source is obtained through the crowdsourced connection and provides for data related to content that has been tested by at least one of the secondary users;wherein the platform analyzes the at least two separate sources of data to provide content that takes into account both secondary user solutions and feedback along with meditation and health content from professional sources;wherein the meditation packet is provided to the user through the platform; andwherein the one or more software rules utilize the data obtained by the platform to objectively assess effectiveness of the meditation packet and update subsequent meditation packets over time, when the data obtained by the platform indicates said update would benefit the subsequent meditation packets.","16","16/562196","2019-09-05","2020-0303056","2020-09-24","11646110","2023-05-09","Sean Sullivan","Sean Sullivan","","","","G16H-0020/70","G16H-0020/70 | A61B-0005/74 | A61N-0001/36092 | G06F-0003/011 | G06T-0019/006 | G06V-0020/20","G16H-020/70","G16H-020/70 | G06F-003/01 | G06T-019/00 | A61B-005/00 | A61N-001/36 | G06V-020/20","","","","","","4923019004960"
"US","US","P","B1","Skin condition analyzing and skin disease diagnosis device","The present invention relates to a device for skin condition analysis and skin disease diagnosis. The skin condition analysis and skin disease diagnosis device according to the present invention is an all-in-one type device that can perform not only cosmetic skin condition analysis but also diagnosis of medical skin disease items.","1. A device for skin condition analysis and skin disease diagnosis, comprising: a face interpolation region having a space in which the face of a user is inserted and secured;a photographing module that is designed to be placed in the face interpolation region and to acquire first user face image and second user face image for analyzing skin condition analysis items and skin disease diagnosis items;a processor that is designed to execute analysis of the skin condition analysis items including wrinkles, pigmentation, redness, sebum and pores using the first user face image, and also execute diagnosis of the skin disease diagnosis items including acne, atopy, psoriasis, rash and vitiligo using the first user face image and the second user face image; anda rotational touch display that provides the user with the analysis results of the skin condition analysis items and the diagnosis results of the skin disease diagnosis items executed in the above processor,wherein the photographing module includes:a module front region in which: a photographing unit including a first camera designed to acquire the first user face image while having a first polar film disposed on front of the first camera, and a second camera designed to acquire the second user face image; a first white light emitting light source unit including a first white light source that has a second polar film disposed on front of the first white light source with the same polar axis as that of the first polar film, as well as a second white light source that has a third polar film disposed on front of the second white light source with a polar axis different from that of the first polar film; and an ultraviolet light emitting light source unit including an ultraviolet light source that has a visible light shielding filter disposed on front of the ultraviolet light source, are provided; anda module rear region as a rear area of the module front region, in which a second white light emitting light source unit including a third white light source disposed on a site corresponding to the first white light source unit and the ultraviolet light source unit is provided,wherein the processor includes:an auto-focusing unit that controls the first camera to form auto-focusing on the face of the user and photograph the same;a blur sensor unit to determine whether the first user face image exceeds a preset blur value;a first deep learning unit that executes encoding to extract features of a face area after extracting the face area from the first user face image acquired by the first camera and having passed through the blur sensor unit, and then, executes decoding to acquire a first face feature map provided with a class value set for each pixel;a second deep learning unit that forms a second face feature map through feature combination of an encoding result for extracting features in regard to an extracted face area after extracting the face area from the first user face image acquired by the first camera and another encoding result in regard to an extracted face area after extracting the face area from the second user face image acquired by the second camera;a skin condition analysis unit that executes analysis of skin condition analysis items including wrinkles, pigmentation, redness, sebum and pores using the first face feature map;a skin disease diagnosis unit that executes diagnosis of skin disease diagnosis items including acne, atopy, psoriasis, rash and vitiligo using the second face feature map; anda simulation unit that creates simulation images for improvement and worsening of skin conditions along with solutions for improvement and prescription of the skin conditions based on the results of the skin condition analysis and the skin disease diagnosis in the skin condition analysis unit and the skin disease diagnosis unit, respectively.","9","17/732615","2022-04-29","","","11638553","2023-05-02","LULULAB INC.","Sangwook Yoo | Yongjoon Choe | Seong Taek Kim | Sijun Roh | Pil Soo Kim","","","","A61B-0005/441","A61B-0005/441 | A61B-0005/0075 | A61B-0005/0079 | A61B-0005/1079 | A61B-0005/7275 | A61B-0005/743 | A61B-0005/7435 | G06F-0003/0488 | G06T-0007/0012 | G06V-0010/446 | G16H-0050/20 | G16H-0050/50 | A61B-2090/309 | G06T-2207/30088","G06K-009/00","G06K-009/00 | A61K-035/12 | A61B-005/00 | A61B-005/107 | G06V-010/44 | G16H-050/20 | G06F-003/0488 | G06T-007/00 | G16H-050/50 | A61B-090/30","","","","","","4923018000628"
"US","US","P","B2","Systems and methods for hair analysis","Disclosed are hair analysis systems and methods comprising: (a) capturing an image of a user at as captured by a camera and sending the image to a hair analysis processor; (b) analyzing the user's hair condition at the hair analysis processor, based on the image from the camera by using a model that is trained using a plurality of images of users that each feature one or more micro features of a respective user's hair, and providing an analysis result to a display; and (c) displaying at the display the analysis result to the user. The present invention provides the system and the method with an improved sensitivity.","1. A hair analysis system comprising: (a) a camera to capture an image of a user and to send the image to a hair analysis processor;(b) the hair analysis processor: to analyze the user'ss hair condition based on the image by using a model that is trained using a plurality of images of users that each feature one or more of the following features of a respective user'ss hair: frizz, cleanliness, moisture, curliness, length, damage, or shine; and to provide an analysis result to a display wherein the analysis result is at least one of the following: the analyzed hair condition;a hair prediction based on the analyzed hair condition;a hair product recommendation based on the analyzed hair condition;a hair product usage recommendation based on the analyzed hair condition; ora hair style recommendation based on the analyzed hair condition; and(c) the display to display the analysis result to the user.","14","17/386580","2021-07-28","2021-0353215","2021-11-18","11633148","2023-04-25","The Procter & Gamble Company","Ankur Purwar | Faiz Feisal Sherman | Raghunandan Melkote Kainkaryam","","","","A61B-0005/448","A61B-0005/448 | A45D-0044/005 | A61B-0005/0077 | G06N-0003/02 | G06Q-0030/0631 | A45D-2044/007 | G06T-0007/0012 | G06T-2207/20081 | G06T-2207/20084","A61B-005/00","A61B-005/00 | G06Q-030/0601 | G06N-003/02 | A45D-044/00 | G06T-007/00","","","","","","4923017000994"
"US","US","P","B2","3D electrical activity representation","In one embodiment, a medical system includes a catheter including electrodes, and configured to be inserted into a chamber of a heart and maneuvered among sampling sites to sample electrical activity, a display, and processing circuitry to receive signals provided by the catheter, and compute, for each sampling site, a sampling position of the catheter and respective electrode positions of the catheter electrodes, render to the display a 3D representation of the chamber including respective sampling-site markers indicating the computed sampling position of the catheter at respective ones of the sampling sites, receive a user input selecting one sampling-site marker, and update the 3D representation to include electrode markers indicating the respective electrode positions of the respective catheter electrodes while the catheter was sampling the electrical activity of the tissue at the sampling site corresponding to the selected sampling-site marker.","1. A medical system, comprising: a catheter including a distal portion and catheter electrodes disposed at respective locations on the distal portion, and configured to be inserted into a chamber of a heart of a living subject and maneuvered among multiple sampling sites to sample electrical activity of tissue of the chamber with the catheter electrodes;a display; andprocessing circuitry configured to: receive signals provided by the catheter, and in response to the signals, compute, for each of the sampling sites, a sampling position of the catheter and respective electrode positions of each of the catheter electrodes at that sampling position of the catheter;render to the display a three-dimensional (3D) representation of the chamber including sampling-site markers indicating the computed sampling position of the catheter at each of the sampling sites without including electrode markers indicating the computed electrode positions, wherein each of the sampling site markers and each of the electrode markers are annotations included on the rendered 3D representation of the chamber;receive a user input selecting one of the sampling-site markers; andupdate the rendered 3D representation, responsively to the received user input, to selectively display all the electrode markers associated with the selected sampling-site marker, wherein the electrode markers associated with the selected sampling-site marker indicate positions of each the catheter electrodes while the catheter was at a sampling site associated with the user selected sampling-site marker.","21","16/594668","2019-10-07","2021-0100612","2021-04-08","11633229","2023-04-25","BIOSENSE WEBSTER (ISRAEL) LTD.","Oded Baron | Stanislav Goldberg | Shmuel Auerbach","","","","A61B-0018/1492","A61B-0018/1492 | A61B-0005/061 | A61B-0006/466 | G06F-0003/04815 | G06T-0007/0012","A61B-018/14","A61B-018/14 | A61B-005/06 | A61B-006/00 | G06F-003/04815 | G06T-007/00","","","","","","4923017001075"
"US","US","P","B2","Providing users with access to routes for traveling","Among other things, one or more techniques and/or systems are provided for providing users with access to a route for travelling. A user, of a client device, may send a request for access to the route to a route planning service. The route may correspond to a starting location and an ending location. The route planning service may query a route database to identify an entry indicating that a restricted access road segment (e.g., a high occupancy vehicle lane, a shoulder lane, a bus lane, etc.) and/or a road segment (e.g., comprising a traffic light alteration capability) exists between the starting location and the ending location. Responsive to successfully authorizing the user for travelling the restricted access road segment and/or the road segment, the route, comprising the restricted access road segment and/or the road segment, may be provided to the client device.","1. A non-transitory computer-readable medium comprising instructions that when executed by a processor perform operations, the operations comprising: receiving, from a first client device of a first user, a request for a route corresponding to a starting location and an ending location;querying a route database to identify an entry indicating that a restricted access road segment with a traffic light alteration capability exists between the starting location and the ending location;receiving, from a second client device of a second user, a second request for a second route corresponding to the restricted access road segment;based upon (i) a determination that allowance of one of the first user or the second user to use the restricted access road segment would not exceed an allowed allocation threshold, and (ii) a determination that allowance of both the first user and the second user to use the restricted access road segment would exceed the allowed allocation threshold: assigning a first score to the first user; andassigning a second score to the second user;authorizing the first user to use the restricted access road segment with the traffic light alteration capability based upon the first user submitting a method of payment for access to the traffic light alteration capability; andresponsive to successfully authorizing the first user to use the restricted access road segment with the traffic light alteration capability and determining that the first score of the first user exceeds the second score of the second user: providing the route, comprising the restricted access road segment, to the first client device but not the second client device;maintaining a current location of the first client device; andaltering operation of a traffic light along the restricted access road segment based upon the current location of the first client device being within a threshold distance of the traffic light.","20","16/853215","2020-04-20","2020-0250976","2020-08-06","11634143","2023-04-25","INRIX, INC.","Christopher Scofield | Dominic Jordan | Uri Lavee | Kevin James Foreman | William Schwebel","","","","B60W-0040/04","B60W-0040/04 | A61B-0005/02055 | A61B-0005/369 | A61B-0005/4845 | B60R-0016/0236 | B60W-0030/143 | B60W-0040/08 | B60W-0040/09 | B64C-0039/024 | G01C-0021/3415 | G01C-0021/3469 | G01C-0021/3617 | G01C-0021/3655 | G01C-0021/3667 | G01C-0021/3682 | G05D-0001/0011 | G05D-0001/0088 | G05D-0001/021 | G06F-0016/29 | G06N-0020/00 | G06Q-0020/102 | G06Q-0030/0283 | G06Q-0040/08 | G07B-0015/00 | G07B-0015/063 | G07C-0005/008 | G08G-0001/012 | G08G-0001/0112 | G08G-0001/0129 | G08G-0001/0141 | G08G-0001/0145 | G08G-0001/065 | G08G-0001/07 | G08G-0001/093 | G08G-0001/097 | G08G-0001/0962 | G08G-0001/0965 | G08G-0001/0967 | G08G-0001/096725 | G08G-0001/096741 | G08G-0001/096775 | G08G-0001/096791 | G08G-0001/096811 | G08G-0001/096822 | G08G-0001/096838 | H04B-0001/3822 | H04B-0007/18504 | H04L-0009/3247 | H04L-0067/02 | H04L-0067/306 | H04M-0015/60 | H04W-0004/024 | H04W-0004/029 | H04W-0004/40 | H04W-0004/42 | H04W-0004/50 | H04W-0012/08 | A61B-0005/024 | A61B-0005/0531 | B60W-2040/0809 | B60W-2040/0872 | B60W-2540/22 | B60W-2552/00 | B60W-2555/20 | B60W-2710/1044 | B60W-2710/18 | B60W-2720/10 | B64C-2201/123 | G01C-0021/3608 | G06Q-0050/30 | G06Q-2240/00 | H04W-0004/48","B60W-040/04","B60W-040/04 | H04W-004/40 | G06N-020/00 | G06F-016/29 | H04W-004/024 | H04W-004/029 | G08G-001/01 | B60W-040/08 | B60W-040/09 | G08G-001/09 | G08G-001/0967 | G07B-015/06 | G08G-001/0968 | G08G-001/097 | H04W-012/08 | A61B-005/369 | B60W-030/14 | G05D-001/00 | G07C-005/00 | A61B-005/0205 | A61B-005/00 | G05D-001/02 | H04B-001/3822 | H04L-067/02 | H04L-067/306 | B60R-016/023 | B64C-039/02 | H04B-007/185 | G06Q-020/10 | G06Q-030/02 | H04M-015/00 | G06Q-040/08 | H04L-009/32 | G08G-001/065 | G01C-021/34 | G01C-021/36 | G08G-001/0962 | H04W-004/48 | G06Q-050/30 | H04W-004/50 | G06Q-030/0283 | G07B-015/00 | H04W-004/42 | G08G-001/07 | G08G-001/0965 | A61B-005/024 | A61B-005/0531","","","","","","4923017001976"
"US","US","P","B2","OnScene command vision","Embodiments use holographic projection in augmented reality to visualize a building in 3D and show, as a holographic figure, the position of personnel in the building. In some embodiments, a user can ""tap"" on a holographic figure to view data on that person, such as skin temperature, room temperature, heart rate, etc.","1. A system for identifying the location of an agent within an interior of a building, the system comprising: a set of triangulation reference receivers disposed outside of the building, the set of triangulation reference receivers disposed to receive locator information from a transmitter with the agent;a model receiver configured to procure a 3D model of the interior of the building;a correlator configured to correlate the 3D model to a reference frame, and to correlate the location of the agent to the reference frame, to produce a correlated location representing the location of the agent within the building;a rendering module configured to render a 3D image from the 3D model and correlated location, the 3D image including an avatar representing the agent at the correlated location within the 3D image; anda 3D display device in communication with the rendering module, the 3D display device configured to receive and display, to a user, the 3D image.","31","16/846093","2020-04-10","2020-0241142","2020-07-30","11635519","2023-04-25","INTERGRAPH CORPORATION","Andrew James England | Laura Beth Ezzell | Thomas Overfield | Renz Angelo Santos | Edward Michael Sieja | Charles Carlton Barnes","","","","G01S-0017/48","G01S-0017/48 | A61B-0005/002 | A61B-0005/0008 | A61B-0005/08 | G06Q-0010/06 | G06Q-0010/08 | G06Q-0050/28 | G06T-0017/20","G01S-017/48","G01S-017/48 | G01S-017/89 | A61B-005/00 | A61B-005/08 | G06T-017/20 | G06Q-010/06 | G06Q-010/08 | G06Q-050/28","","","","","","4923017003343"
"US","US","P","B2","Systems and methods for collecting, analyzing, and sharing bio-signal and non-bio-signal data","A computer network implemented system for improving the operation of one or more biofeedback computer systems is provided. The system includes an intelligent bio-signal processing system that is operable to: capture bio-signal data and in addition optionally non-bio-signal data; and analyze the bio-signal data and non-bio-signal data, if any, so as to: extract one or more features related to at least one individual interacting with the biofeedback computer system; classify the individual based on the features by establishing one or more brain wave interaction profiles for the individual for improving the interaction of the individual with the one or more biofeedback computer systems, and initiate the storage of the brain waive interaction profiles to a database; and access one or more machine learning components or processes for further improving the interaction of the individual with the one or more biofeedback computer systems by updating automatically the brain wave interaction profiles based on detecting one or more defined interactions between the individual and the one or more of the biofeedback computer systems. A number of additional system and computer implemented method features are also provided.","1. A brainwave monitoring system comprising: a plurality of client computing devices capturing bio-signal data fora plurality of users, each of the plurality of client computing devices in communication with at least one bio-signal sensor, wherein the plurality of client computing devices comprises a first client computing device capturing bio-signal data from a first user and a second client computing device capturing bio-signal data from a second user, wherein each of the plurality of client computing devices is executing a respective application of a plurality of applications, wherein the plurality of applications comprises a first application and a second application, wherein the first client computing device is executing the first application, and wherein the second client computing device is executing the second application;at least one user effector to provide a real-time biofeedback output; andat least one computer server in communication with the plurality of computing devices over a communications network, the at least one computer server configured to: receive time-coded bio-signal data from each of the plurality of client computing devices capturing the bio-signal data for the plurality of users, the time-coded bio-signal data associated with a plurality of user identifiers for the plurality of users;acquire time-coded feature event data;extract feature events from the time coded feature event data at feature event time codes, each feature event being a set of variables and corresponding values at one or more feature event time codes;automatically search the feature events to identify patterns that are statistically significant, each pattern linked to a feature event time code of the one or more feature event time codes, the pattern representing user response associated with the feature event data at the feature event time code;using the feature event time codes linked to the patterns identified in the feature-events, label segments in the time-coded bio-signal data having bio-signal time-codes during a same time segment as the feature event time codes linked to the patterns;update or create pipelines associated with the plurality of applications using bio-signal features extracted from labelled segments of the time-coded bio-signal data from each of the plurality of client computing devices, the pipelines defined by pipeline parameters for predicting brain states, wherein the update or create pipelines comprises updating or creating the pipeline associated with the second application and the second user using bio-signal features extracted from labelled segments of the time-coded bio-signal data from the first user;determine and output a response classification of segments of time-coded bio-signal data from at least one user of the plurality of users using a respective pipeline defined by the pipeline parameters, the response classification being an automatic prediction of a brain state at the bio-signal time codes, wherein the respective pipeline is associated with the at least one user and a respective application of the at least one user, wherein the determine and output the response classification of segments of time-coded bio-signal data from at least one user comprises determining and outputting a response classification of segments of time-coded bio-signal data from the second user using a respective pipeline defined by the pipeline parameters, wherein the respective pipeline is associated with the second application and the second user;the at least one user effector configured to provide the real-time biofeedback output as an indication of the response classification.","29","15/966930","2018-04-30","2018-0246570","2018-08-30","11635813","2023-04-25","INTERAXON INC.","Trevor Ce Coleman | Christopher Allen Aimone | Ariel Stephanie Garten | Locillo (Lou) Giuseppe Pino | Paul Harrison Baranowski | Raul Rajiv Rupsingh | Kapil Jay Mishra Vidyarthi","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/0006 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/369 | A61B-0005/486 | G06F-0016/00 | G16H-0040/60 | G16H-0040/67 | H04L-0012/16 | H04L-0067/01","G06F-016/00","G06F-016/00 | G06F-003/01 | H04L-012/16 | A61B-005/00 | G16H-040/67 | A61B-005/369 | H04L-067/01 | G16H-040/60","","","","","","4923017003632"
"US","US","P","B2","Contactless cough detection and attribution","Methods, devices, and systems for contactless cough detection and attribution are presented herein. Audio data may be received using a microphone. A cough may be identified as having occurred based on the received audio data. Radar data may be received indicative of reflected radio waves from a radar sensor. A state analysis process may be performed using the received radar data. The detected cough may be attributed to a particular user based at least in part on the state analysis process performed using the radar data.","1. A contactless cough detection device, comprising: a housing;a microphone housed by the housing;a wireless network interface housed by the housing;an electronic display housed by the housing, wherein the electronic display is visible through a screen;a radar sensor housed by the housing, wherein the radar sensor emits radio waves through the screen; anda processing system housed by the housing, comprising one or more processors, that receives data from the microphone and the radar sensor and is in communication with the wireless network interface and electronic display, wherein the processing system is configured to: receive audio data from the microphone;detect that a cough has occurred based on the received audio data;receive radar data indicative of reflected radio waves from the radar sensor;perform a state analysis process based on the received radar data for a user within a bed to select a state from a plurality of states;attribute the detected cough to the user within the bed based at least in part on the state selected for the user based on the received radar data;cause cough data for the user corresponding to the attributed detected cough to be stored; andoutput the stored cough data via the electronic display.","20","17/846438","2022-06-22","2022-0322966","2022-10-13","11627890","2023-04-18","Google LLC","Dongeek Shin | Michael Dixon | Jake Garrison | Andrew William Goldenson","","","","A61B-0005/0823","A61B-0005/0823 | A61B-0005/0022 | A61B-0005/05 | A61B-0005/0803 | A61B-0005/4806 | A61B-0005/7267 | A61B-0005/7282 | A61B-0005/742 | A61B-0005/749 | A61B-0007/003 | G01S-0007/415 | G06F-0003/167 | G06N-0005/04 | G06N-0020/00 | G10L-0015/22 | G10L-0015/30 | G10L-0025/51 | H04R-0001/025 | H04R-0001/04 | A61B-2505/07 | A61B-2562/06 | G06F-0003/14 | G10L-2015/223 | H04R-2420/07","A61B-005/08","A61B-005/08 | G10L-025/51 | H04R-001/04 | G10L-015/22 | G10L-015/30 | H04R-001/02 | G01S-007/41 | G06N-020/00 | G06N-005/04 | A61B-007/00 | A61B-005/05 | A61B-005/00 | G06F-003/16 | G06F-003/14","","","","","","4923016000812"
"US","US","P","B2","Blockchain","An Internet of Thing (IoT) device includes a camera coupled to a processor; and a wireless transceiver coupled to the processor. Blockchain smart contracts can be used with the device to facilitate secure operation.","1. A device, comprising: a device body;an accelerometer coupled to the body; a camera to capture an image;a wireless transceiver; anda processor coupled to the body and associated with a blockchain with a blockchain address for a secured transaction by accessing data, content, or application stored in a cloud storage; authorizing a first client device; receiving an authorization request from the first client device; generating an authorization key for accessing a cloud server and storing the key in a blockchain; providing the authorization key to the first client device; receiving the authorization key from a remote device as a second client device working as an agent of the first client device; granting access to the second client device based on the authorization key; receiving code and data associated with an application or content identified in a blockchain, and running code with the data.","20","17/497475","2021-10-08","2022-0023742","2022-01-27","11628351","2023-04-18","Bao Tran | Ha Tran","Bao Tran | Ha Tran","","","","A63B-0071/145","A63B-0071/145 | A42B-0003/0433 | A61B-0005/11 | A61B-0005/6804 | A63B-0024/0006 | A63B-0024/0021 | A63B-0024/0062 | A63B-0024/0075 | A63B-0043/004 | A63B-0060/46 | A63B-0069/36 | A63B-0069/38 | A63B-0071/06 | A63F-0011/00 | B33Y-0010/00 | G01L-0005/0052 | G06F-0001/163 | G06F-0001/1694 | G06F-0003/00 | G06F-0003/011 | G06F-0003/012 | G06F-0003/013 | G06F-0003/014 | G06F-0003/017 | G06V-0020/20 | G06V-0040/23 | G06V-0040/28 | G09B-0019/003 | G16H-0020/30 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/30 | H04L-0067/12 | H04N-0005/2253 | H04N-0005/232 | H04Q-0009/00 | H04W-0084/18 | A61B-0005/0022 | A61B-0005/01 | A61B-0005/024 | A61B-0005/053 | A61B-0005/055 | A61B-0005/0533 | A61B-0005/318 | A61B-0005/4872 | A61B-0005/6806 | A61B-0005/6895 | A61B-2503/10 | A61B-2562/0219 | A63B-0021/072 | A63B-0021/0724 | A63B-0021/0726 | A63B-0069/0002 | A63B-0069/0026 | A63B-0069/0028 | A63B-0069/0048 | A63B-0069/0071 | A63B-0069/02 | A63B-0069/06 | A63B-0069/16 | A63B-0069/3632 | A63B-0071/085 | A63B-0071/10 | A63B-0071/1216 | A63B-0071/1291 | A63B-0071/141 | A63B-2071/125 | A63B-2071/1233 | A63B-2071/1283 | A63B-2208/0204 | A63B-2220/12 | A63B-2220/13 | A63B-2220/16 | A63B-2220/20 | A63B-2220/24 | A63B-2220/30 | A63B-2220/40 | A63B-2220/51 | A63B-2220/53 | A63B-2220/56 | A63B-2220/72 | A63B-2220/74 | A63B-2220/75 | A63B-2220/76 | A63B-2220/803 | A63B-2220/806 | A63B-2220/807 | A63B-2220/833 | A63B-2220/836 | A63B-2225/30 | A63B-2225/50 | A63B-2225/54 | A63B-2225/74 | A63B-2230/04 | A63B-2230/06 | A63B-2230/50 | A63B-2230/60 | A63B-2230/70 | A63B-2243/007 | A63B-2243/0025 | A63B-2243/0037 | A63B-2243/0054 | A63B-2243/0066 | A63B-2243/0095 | A63B-2244/102 | A63B-2244/18 | A63B-2244/19 | A63B-2244/20 | A63B-2244/203 | A63F-0013/211 | G06F-2203/011 | G06F-2203/0384 | G06Q-0010/08 | G06Q-0030/0241 | G06Q-0030/06 | G09B-0019/0038 | H04B-0001/04 | H04L-0067/10 | H04N-0005/247 | H04N-0007/18 | H04Q-2209/40 | H04W-0088/02","A63B-071/14","A63B-071/14 | G16H-050/20 | G01L-005/00 | G06F-003/01 | A63F-011/00 | G16H-030/20 | B33Y-010/00 | A63B-069/36 | A63B-024/00 | H04Q-009/00 | H04N-005/225 | A63B-069/38 | G06F-001/16 | H04W-084/18 | A42B-003/04 | A63B-071/06 | G06F-003/00 | A63B-060/46 | A63B-043/00 | G09B-019/00 | A61B-005/11 | G16H-020/30 | A61B-005/00 | H04L-067/12 | H04N-005/232 | G16H-030/40 | G16H-050/30 | G06V-020/20 | G06V-040/20 | A63B-069/02 | A61B-005/0533 | A61B-005/024 | A63B-069/16 | H04W-088/02 | A63B-071/12 | A63B-069/00 | A63B-071/08 | A61B-005/053 | A63B-069/06 | A63B-071/10 | A63B-021/072 | H04B-001/04 | A61B-005/01 | H04N-007/18 | H04L-067/10 | A61B-005/055 | A63F-013/211 | G06Q-030/06 | G06Q-030/0241 | G06Q-010/08 | H04N-005/247 | A61B-005/318","","","","","","4923016001271"
"US","US","P","B2","Determination device, determination method, program, and information storage medium","In each trial, brain electrical activity at multiple points of a target person is measured. An acquirer of a determination device acquires response matrices for n trials under a first condition and response matrices form trials under a second condition. An analyzer performs canonical correlation analysis on the acquired response matrices to obtain first canonical variable time series. A distance calculator calculates a distance between the trials from the obtained first canonical variable time series to obtain a distance matrix. A determiner obtains a possibility that the n trials and the m trials are classified into two different clusters from the distance matrix and determines whether the first condition and the second condition are substantially different. It is possible to provide to a single target person a first content in n trials and a second content in m trials so as to determine a difference in interest of the single target person. It is possible to provide the same content to a first subject who is the target person in n trials and to a second subject who is the target person in m trials so as to determine whether the two are different or the same.","1. A determination device, comprising: an electroencephalograph that measures brain electrical activity of a target person with D electrodes in n trials under a first condition and m trials under a second condition;an acquirer that acquires (n+m) response matrices from the measured brain electrical activity of the n and m trials;an analyzer that performs canonical correlation analysis on response matrices included in the acquired (n+m) response matrices to obtain first canonical variable time series for the response matrices;a distance calculator that calculates a distance between the n and m trials for the response matrices from the obtained first canonical variable time series; anda determiner that obtains a possibility that the n trials and the m trials are classified into two different clusters from the calculated distance anddetermines whether the first condition and the second condition are different or not by comparing the possibility with a specific value,and whereinbrain electrical activity at D points of a target person is measured, with the D electrodes respectively, T times in the n trials 1, 2, . . . , n and in the m trials n+1, n+2, n+m,for each of an integer i (i=1, 2, . . . , n, n+1, n+2, . . . , n+m), an integer p (p=1, 2, . . . , D), and an integer t (t=1, 2, . . . , T), an element X(i)p(t) in a row p and a column t of a response matrix X(i) obtained in the trial i is a value measured at a p-th point among the D points at a t-th sampling time since the trial i starts,for each of the integer i (i=1, 2, . . . , n, n+1, n+2, . . . , n+m) and an integer j (j=1, 2, . . . , n, n+1, n+2, . . . , n+m), canonical correlation analysis is performed on the response matrix X(i) for the trial i and the response matrix X(j) for the trial j to obtain a first canonical variable time series ui,j for the response matrix X(i), and a first canonical variable time series vi,j for the response matrix X(j), andfor each of the integer i (i=1, 2, . . . , n, n+1, n+2, . . . , n+m) and the integer j (j=1, 2, . . . , n, n+1, n+2, . . . , n+m), an element in a row i and a column j of a distance matrix S is assumed to be a distance between the trial i and the trial j that is calculated from the first canonical variable time series ui,j and the first canonical variable time series vi,j,and wherein the determiner obtains the possibility from the distance matrix S by:placing the n and m trials 1, 2, . . . , n, n+1, n+2, n+m in a low-dimensional space by multidimensional scaling and applying a support vector machine and leave-one-out cross-validation to the n and m trials 1, 2, . . . , n, n+1, n+2, n+m placed in the low-dimensional space;placing the n and m trials 1, 2, . . . , n, n+1, n+2, . . . , n+m in a low-dimensional space from the distance matrix S by multidimensional scaling and measuring overlap between a distribution of the n trials and a distribution of the m trials in the low-dimensional space; orcreating from the distance matrix S a dendrogram in which the n and m trials 1, 2, . . . , n, n+1, n+2, . . . , n+m are clustered by hierarchical clustering measuring placement of the n trials and placement of the m trials in the dendrogram.","12","15/576632","2016-05-27","2019-0025918","2019-01-24","11630512","2023-04-18","RIKEN","Keiichi Kitajo | Hiromichi Suetani","2015-108664","JP","2015-05-28","G06F-0003/015","G06F-0003/015 | A61B-0005/117 | A61B-0005/369 | A61B-0005/377 | A61B-0005/40 | A61B-0005/4064 | G06F-0017/15 | G06F-0018/231 | A61M-2021/005 | A61M-2021/0027 | G06F-2203/011","A61B-005/377","A61B-005/377 | A61B-005/117 | A61B-005/369 | G06F-018/231 | G06F-017/15 | G06F-003/01 | A61B-005/00 | A61M-021/00","","","","","","4923016003412"
"US","US","P","B2","End-to-end deep neural network for auditory attention decoding","In one aspect of the present disclosure, method includes: receiving neural data responsive to a listener's auditory attention; receiving an acoustic signal responsive to a plurality of acoustic sources; for each of the plurality of acoustic sources: generating, from the received acoustic signal, audio data comprising one or more features of the acoustic source, forming combined data representative of the neural data and the audio data, and providing the combined data to a classification network configured to calculate a similarity score between the neural data and the acoustic source using one or more similarity metrics; and using the similarity scores calculated for each of the acoustic sources to identify, from the plurality of acoustic sources, an acoustic source associated with the listener's auditory attention.","1. A method comprising: receiving neural data responsive to a listener'ss auditory attention;receiving an acoustic signal responsive to a plurality of acoustic sources;for each of the plurality of acoustic sources: generating, from the received acoustic signal, audio data comprising one or more features of the acoustic source,forming combined data representative of the neural data and the audio data, andproviding the combined data to a convolutional deep neural network (DNN) configured to calculate a similarity score between the neural data and the acoustic source using one or more similarity metrics; andusing the similarity scores calculated for each of the acoustic sources to identify, from the plurality of acoustic sources, an acoustic source associated with the listener'ss auditory attention.","17","16/720810","2019-12-19","2020-0201435","2020-06-25","11630513","2023-04-18","MASSACHUSETTS INSTITUTE OF TECHNOLOGY | THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK","Gregory Ciccarelli | Christopher Smalt | Thomas Quatieri | Michael Brandstein | Paul Calamia | Stephanie Haro | Michael Nolan | Joseph Perricone | Nima Mesgarani | James O'Sullivan","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/121 | A61B-0005/31 | A61B-0005/369 | G06F-0003/017 | G06F-0018/22 | G06F-0018/251 | G06N-0003/02","G06F-003/01","G06F-003/01 | G06K-009/62 | G06K-009/00 | G06N-003/04 | G06N-003/08 | G10L-021/0208 | G10L-021/0272 | A61B-005/12 | A61B-005/31 | A61B-005/369 | H04R-001/10 | H04R-025/00 | G06N-003/02 | G06F-018/22 | G06F-018/25","","","","","","4923016003413"
"US","US","P","B2","Hospital support system, hospital support method, hospital support program, and control device","A hospital support system includes a management monitor and an operation terminal functioning as a first controller that performs control for displaying a status management table in a state in which items corresponding to a series of predetermined multiple stages performed so as to correspond to a diseased animal at least while the diseased animal stays in a hospital from when the diseased animal arrives at the hospital to when payment is finished are arranged in a sequence of time on the management monitor and a second controller that performs control for displaying an icon indicating which stage of the multiple stages in the status management table is a stage being performed at a current point of time on the management monitor.","1. A hospital support system comprising: a display device;a tag that store identification information for identifying each subject;a reader that is provided at a predetermined position for each of a plurality of stages scheduled in a series, that reads the identification information from the tag, and that transmits the identification information to an operation terminal;the operation terminal comprising a processor, configured to: obtain appointment information of each of a plurality of appointed subjects of a current day;generate and display a table according to the appointment information of the appointed subjects in a sequence of appointed time on the display device;in response to a first tag of a first subject among the appointed subjects being read by a first reader, provided at a first predetermined position for a first stage, among the readers and first identification information of the first tag being received from the first reader, update and display the table in a state in which items corresponding to the series of scheduled stages performed based on the first identification information of the first subject read by the first reader at the first predetermined position for the first stage at least while the first subject stays in a hospital from when the first subject arrives at the hospital to when payment is finished are arranged in a sequence of time on the display device;display stage information indicating which stage of the scheduled stages in the table is a stage being performed at a current point of time on the display device for each of the appointed subjects, wherein the stage information comprises the first subject being at the first predetermined position for the first stage and a second subject among the appointed subjects being at the first predetermined position for the first stage; anddisplay information indicating a running status of a first examination device being used by the first subject at the first predetermined position and the number of waiting subjects among the appointed subjects waiting for the first examination device at the current point of time on the display, wherein the second subject is one of the waiting subjects for the first examination device.","18","16/398288","2019-04-30","2019-0348172","2019-11-14","11631492","2023-04-18","FUJIFILM CORPORATION","Shigetoshi Ishikawa | Hirona Yumbe | Akemi Oda | Yasuhisa Kaneko | Haruyasu Nakatsugawa | Keiji Tsubota","2018-092393","JP","2018-05-11","G16H-0040/20","G16H-0040/20","A61B-005/00","A61B-005/00 | G06Q-010/00 | G06Q-050/00 | G16H-040/20","","","","","","4923016004389"
"US","US","P","B2","Coordinate input processing apparatus, emotion estimation apparatus, emotion estimation system, and building apparatus for building emotion estimation-oriented database","A coordinate input processing apparatus includes a position detection apparatus and a communication circuit. The position detection apparatus includes a sensor which detects a position pointed to by an electronic pen, and circuitry which acquires pen state information regarding a state of the electronic pen held by a person. The communication circuit transmits to an emotion estimation apparatus coordinates corresponding to the position pointed to by the electronic pen and the pen state information in an emotional state estimation request, and receives from the emotion estimation apparatus the coordinates corresponding to the position pointed to by the electronic pen, the pen state information included in the emotional state estimation request, and the information regarding the distracted state of the person holding the electronic pen in an emotional state estimation response having the same format as the emotional state estimation request.","1. A coordinate input processing apparatus comprising: a position detection apparatus that includes a sensor which, in operation, detects a position pointed to by an electronic pen, and circuitry which, in operation, acquires pen state information regarding a state of the electronic pen held by a person; anda communication circuit which, in operation, transmits to an emotion estimation apparatus coordinates corresponding to the position pointed to by the electronic pen and the pen state information in an emotional state estimation request having a format that includes a plurality of fields configured to respectively store the coordinates corresponding to the position pointed to by the electronic pen, the pen state information, and information regarding a distracted state of the person holding the electronic pen, and receives from the emotion estimation apparatus the coordinates corresponding to the position pointed to by the electronic pen, the pen state information included in the emotional state estimation request, and the information regarding the distracted state of the person holding the electronic pen in an emotional state estimation response having the format that includes the plurality of fields configured to respectively store the coordinates corresponding to the position pointed to by the electronic pen, the pen state information, and the information regarding the distracted state of the person holding the electronic pen.","26","17/556898","2021-12-20","2022-0113816","2022-04-14","11625110","2023-04-11","Wacom Co., Ltd.","Akiyuki Kake | Heidi Wang","2016-170574 | 2016-249336","JP | JP","2016-09-01 | 2016-12-22","G06F-0003/03545","G06F-0003/03545 | A61B-0005/0022 | A61B-0005/16 | A61B-0005/165 | A61B-0005/6814 | A61B-0005/6898 | A61B-0005/7267 | G06F-0003/01 | G06F-0003/011 | G06F-0003/015 | G06F-0003/03 | G06F-0003/038 | G06F-0003/041 | G06F-0003/046 | G06F-0003/0488 | G06K-0009/00536 | G06K-0009/6217 | G06V-0030/1423 | G06V-0030/36 | G06V-0040/37 | A61B-0005/1114 | A61B-0005/7246 | G06F-2203/011","G06F-003/0354","G06F-003/0354 | G06F-003/01 | G06F-003/03 | A61B-005/16 | G06F-003/041 | G06F-003/0488 | G06K-009/62 | G06F-003/046 | A61B-005/00 | G06K-009/00 | G06V-030/32 | G06V-030/142 | G06V-040/30 | G06F-003/038 | A61B-005/11","","","","","","4923015003224"
"US","US","P","B2","System and method for organic cognitive response (OCR) feedback for adaptive work instructions","A method includes obtaining multiple inputs associated with a worker in a manufacturing environment. The method also includes performing a fuzzy logic process on the multiple inputs to generate multiple outputs, where the multiple outputs are associated with performance by the worker of a task in the manufacturing environment. The method further includes providing instructions to an electronic device to display a specified output among the multiple outputs while the worker performs the task in the manufacturing environment, where the specified output includes instruction information for performing the task.","1. A method comprising: obtaining multiple inputs associated with a worker in a manufacturing environment;performing a fuzzy logic process on the multiple inputs to generate multiple outputs, the multiple outputs associated with performance by the worker of a task in the manufacturing environment, wherein a specified output of the multiple outputs generated by the fuzzy logic process comprises (i) a first default setting indicating a detail level of instruction information for performing the task and (ii) a second default setting indicating a position of a user control of a user interface, wherein performing the fuzzy logic process comprises implementing a Mamdani fuzzy inference system on the multiple inputs to generate the first default setting and the second default setting; andproviding instructions to an electronic device to display the specified output while the worker performs the task in the manufacturing environment, the specified output also comprising the instruction information for performing the task, the instruction information initially presented according to the first default setting, the specified output further comprising the user control, the user control configured to adjustably control the detail level of the instruction information based on the position of the user control selected by the worker, wherein an initial setting of the user control is set according to the second default setting.","20","16/836529","2020-03-31","2021-0304048","2021-09-30","11625636","2023-04-11","RAYTHEON COMPANY","Kristen M. Stone | James S. Neil","","","","G06N-0007/02","G06N-0007/02 | A61B-0005/18 | A61B-0005/7264 | G06F-0009/451 | G06N-0020/00 | G06Q-0010/04 | G06Q-0010/06316 | G06Q-0010/06398 | G06Q-0050/04 | G16H-0040/67 | G16H-0050/20","G06N-007/02","G06N-007/02 | G16H-040/67 | G16H-050/20 | G06F-009/451 | G06N-020/00 | A61B-005/18 | A61B-005/00 | G06Q-010/04 | G06Q-010/06 | G06Q-050/04 | G06Q-010/0631 | G06Q-010/0639","","","","","","4923015003744"
"US","US","P","B2","Method and apparatus for treating a joint, including the treatment of cam-type femoroacetabular impingement in a hip joint and pincer-type femoroacetabular impingement in a hip joint","A computer visual guidance system for guiding a surgeon through an arthroscopic debridement of a bony pathology, wherein the computer visual guidance system is configured to: (i) receive a 2D image of the bony pathology from a source; (ii) automatically analyze the 2D image so as to determine at least one measurement with respect to the bony pathology; (iii) automatically annotate the 2D image with at least one annotation relating to the at least one measurement determined with respect to the bony pathology so as to create an annotated 2D image; and (iv) display the annotated 2D image to the surgeon so as to guide the surgeon through the arthroscopic debridement of the bony pathology.","1. An arthroscopic visual guidance system for guiding a surgeon through an arthroscopic removal of bone, wherein the arthroscopic visual guidance system comprises an intra-operative imaging device, an electronic communication interface, a display, and at least one processor that is configured to: (i) image the bone during a bone removal procedure using the intra-operative imaging device to produce a first intraoperative image;(ii) automatically annotate, by the at least one processor, the intraoperative image with at least one anatomic measurement relating to the bone so as to create an annotated image;(iii) display, by the display, the annotated image to the surgeon so as to guide the surgeon through the arthroscopic removal of the bone;(iv) image the bone during a bone removal procedure using the intra-operative imaging device to produce a second intraoperative image showing partial removal of the bone;(v) automatically annotate, by the processor, the second intra-operative image with at least anatomic measurement determined with respect to the bone so as to create a second annotated image; and(vi) display, by the display, in real-time the second annotated image to the surgeon so as to guide the surgeon through the arthroscopic removal of the bone.","29","17/143091","2021-01-06","2021-0169503","2021-06-10","11612402","2023-03-28","Stryker Corporation","Brian Fouts | Brady Woolford | Christopher Zeh","","","","A61B-0017/1703","A61B-0017/1703 | A61B-0017/175 | A61B-0034/10 | A61B-0034/20 | A61B-0034/25 | G06T-0007/0012 | G06T-0011/00 | G06T-0011/60 | A61B-0017/1659 | A61B-2017/1602 | A61B-2034/107 | A61B-2034/2051 | A61B-2034/2065 | A61B-2090/067 | A61B-2090/365 | A61B-2090/3966 | G06F-0003/0488 | G06F-0003/04845 | G06T-2207/10116 | G06T-2207/30008","A61B-017/17","A61B-017/17 | A61B-034/10 | G06T-011/00 | A61B-034/20 | A61B-034/00 | G06T-007/00 | G06T-011/60 | A61B-090/00 | A61B-017/16 | G06F-003/04845 | G06F-003/0488","","","","","","4923013000925"
"US","US","P","B2","User-guidance system based on augmented-reality and/or posture-detection techniques","A user-guidance system that utilizes augmented-reality (AR) components and human-posture-detection techniques is presented. The user-guidance system can help users to use smart devices to conduct 3D body scans more efficiently and accurately. AR components are computer generated for the on-screen guidance to guide a camera operator to position the camera in a particular location in relation to a target object with a particular tilt orientation in relation to the target object to capture an image that includes a region of the target object for 3D reconstruction of the target object. Human-posture-detection techniques are used to detect a human user's real-time posture and provide real-time on-screen guidance feedback and instructions to the human user to adopt an intended best posture for 3D reconstruction of the human user.","1. A method for use in generating a three-dimensional model of a target object using a camera and a display screen with augmented-reality on-screen guidance, comprising: using augmented reality, computer generating and displaying augmented-reality components, including a virtual carton at a position on the display screen to guide a camera operator to align the camera with the position of the virtual carton on the display screen, wherein the position of the virtual carton corresponds to a particular location in relation to the target object at which the camera can capture an image that includes a target region of the target object;adjusting at least one of the augmented-reality components to provide an indication on the display screen of a particular tilt orientation of the camera at which the camera can capture the image that includes the target region of the target object; andreceiving the image from the camera after the camera operator uses the camera to capture the image with the camera aligned with the position of the virtual carton and having the tilt orientation indicated by the at least one augmented-reality component on the display screen.","20","16/837485","2020-04-01","2020-0311429","2020-10-01","11615616","2023-03-28","Jeff Jian Chen","Jeff Jian Chen","","","","G06V-0020/20","G06V-0020/20 | A61B-0005/1116 | G06F-0003/011 | G06F-0003/167 | G06T-0007/74 | G06V-0040/67 | G06T-2207/30196","G06V-020/20","G06V-020/20 | A61B-005/11 | G06F-003/16 | G06T-007/73 | G06F-003/01 | G06V-040/60","","","","","","4923013004114"
"US","US","P","B2","User interface for custom patterned electrical stimulation","A neurostimulation system includes a programming control circuit and a user interface. The programming control circuit may be configured to generate a plurality of stimulation parameters controlling delivery of neurostimulation pulses according to one or more neurostimulation programs each specifying a pattern of the neurostimulation pulses. The user interface includes a display screen, a user input device, and a neurostimulation program circuit. The neurostimulation program circuit may be configured to allow for construction of one or more pulse trains (PTs) and one or more train groupings (TGs) of the one or more neurostimulation programs, and to allow for scheduling of delivery of the one or more neurostimulation programs, using the display screen and the user input device. Each PT includes one or more pulse blocks each including a plurality of pulses of the neurostimulation pulses. Each TG includes one or more PTs.","1. A method for neurostimulation, comprising: delivering neurostimulation pulses from a stimulation device;storing one or more pulse blocks (PBs) each including a plurality of pulses of the neurostimulation pulses, one or more pulse trains (PTs) each including one or more PBs each selected from the stored one or more PBs, and one or more train groupings (TGs) each including a sequence of PTs each selected from the stored one or more PTs;constructing a TG selected from the stored one or more TGs using a user interface, including: receiving a selection of each PT of the PTs of the selected TG from the stored one or more PTs;receiving a selection of an order of the one or more PBs in the each PT, the selection including a randomized order; andreceiving a selection of a number of repetitions of each PB of the one or more PBs in the each PT; andcontrolling the delivery of the neurostimulation pulses according to at least the constructed selected TG.","20","17/355755","2021-06-23","2021-0316150","2021-10-14","11607551","2023-03-21","Boston Scientific Neuromodulation Corporation","David Ernest Wechter","","","","A61N-0001/37247","A61N-0001/37247 | A61B-0005/24 | A61N-0001/0556 | A61N-0001/36053 | A61N-0001/36114 | A61N-0001/37264 | G06F-0003/0483 | G06F-0003/0488 | G06F-0003/04842 | G16H-0040/63 | A61B-0005/4035 | A61B-0005/7475","A61N-001/36","A61N-001/36 | A61N-001/372 | A61N-001/05 | G16H-040/63 | A61B-005/24 | G06F-003/0483 | G06F-003/04842 | G06F-003/0488 | A61B-005/00","","","","","","4923012001286"
"US","US","P","B2","Monitoring of biometric data to determine mental states and input commands","Various embodiments of an apparatus, methods, systems and computer program products described herein are directed to an Analytics Engine that receives one more signal files that include neural signal data of a user based on voltages detected by one or more electrodes on a set of headphones worn by a user. The Analytics Engine preprocesses the data, extracts features from the received data, and feeds the extracted features into one or more machine learning models to generate determined output that corresponds to at least one of a current mental state of the user and a type of facial gesture performed by the user. The Analytics Engine sends the determined output to a computing device to perform an action based on the determined output.","1. A computer-implemented method, comprising: receiving one or more signal files of data based on voltages of one or more human body electrical processes related to from one or more facial gestures physically performed by a user, the voltages detected by one or more electrodes on a set of headphones worn by the user, wherein the one or more received signal files of data comprise a channel of electrode data for each of the one or more electrodes on the set of headphones, wherein a channel of electrode data for a respective electrode comprises: an average amplitude of voltage during a window of time detected by the respective electrode, a high powerband frequency of the respective electrode and a low frequency powerband of the respective frequency;extracting features from the received data and feeding the extracted features into one or more machine learning models, wherein the extracted features comprise at least one feature set based on channel data from a plurality of electrodes, wherein feeding the extracted features comprises: feeding one or more of the extracted EMG features into a facial gesture machine learning model for determining a type of facial gesture from a plurality of types of facial gestures, wherein a respective type of facial gesture includes a sequence of facial gestures including at least one jaw clench user fingerprint and representing an attempt to match a passcode;generating a determined output, via feeding the extracted features, comprising one of:(i) identifying a respective sequence of facial gestures represented by the extracted EMG features represents an invalid passcode instance due at least to a detected jaw clench facial gesture in the respective sequence of facial gestures being different than a valid respective jaw clench user fingerprint of the passcode;(ii) identifying the respective sequence of facial gestures represented by the extracted EMG features represents a valid passcode instance due at least to the detected jaw clench facial gesture in the respective sequence of facial gestures matching the respective jaw clench user fingerprint of the passcode; andbased on identifying the valid passcode instance, sending output to a computing device indicating an instance of verification of an identity associated with the passcode.","29","17/122085","2020-12-15","2022-0187912","2022-06-16","11609633","2023-03-21","NEURABLE, INC.","Ramses Eduardo Alcaide | David Stanley | Dereck Padden | James Hamet | Adam Molnar | Jamie Alders | Jegan Candassamy | Arjun Daniel Srinivas","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/165 | A61B-0005/291 | A61B-0005/296 | A61B-0005/369 | A61B-0005/375 | A61B-0005/389 | A61B-0005/486 | A61B-0005/6803 | A61B-0005/742 | G01R-0019/0084 | G05B-0013/0265 | G06F-0009/542 | G06F-0021/32 | G10L-0025/63 | G16H-0050/20 | H04R-0001/08 | H04R-0001/1008 | H04R-0001/1041 | G06F-2203/011 | G16H-0020/30 | G16H-0020/70","G06F-003/01","G06F-003/01 | G06F-009/54 | G06F-021/32 | G05B-013/02 | G01R-019/00 | A61B-005/291 | A61B-005/296 | A61B-005/369 | A61B-005/389 | A61B-005/16 | A61B-005/00 | H04R-001/10 | H04R-001/08 | G10L-025/63 | G16H-050/20 | A61B-005/375 | G16H-020/70 | G16H-020/30","","","","","","4923012003354"
"US","US","P","B2","Device and method for analyzing the state of a system in a noisy context","A computer-implemented method for determining the state of a system, which includes steps of: collecting data relating to a system, the data being noisy data comprising data of interest and noise; generating a signal to be analyzed from the collected data, the signal being a noisy signal comprising a signal of interest and noise; analyzing the regularity of the signal of interest by compensating the influence of the noise in the computation of the power of the difference between the integrated noisy signal and its trend; and determining the state of the system depending on the result of the analysis of the regularity of the signal of interest.","1. A computer-implemented method for determining a state of a system, the method comprising steps of: collecting data relating to a system, said data being noisy data comprising data of interest and noise;generating a signal to be analyzed from the collected data, said signal being a noisy signal comprising a signal of interest and noise;analyzing a regularity of the signal of interest based on the noisy signal by compensating an influence of the noise in the computation of a power of a difference between an integrated noisy signal and its trend; anddetermining the state of said system depending on a result of the analysis of the regularity of the signal of interest estimated from the noisy signal.","15","16/858556","2020-04-24","2020-0342199","2020-10-29","11610073","2023-03-21","THALES | UNIVERSITE DE BORDEAUX | INSTITUT POLYTECHNIQUE DE BORDEAUX | CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE","Pierrick Legrand | Eric Grivel | Jean-Marc Andre | Bastien Berthelot","2019-004422","FR","2019-04-26","G06K-0009/0051","G06K-0009/0051 | A61B-0005/7203 | G06F-0017/15 | G06F-0017/18","G06F-017/15","G06F-017/15 | G06K-009/00 | A61B-005/00 | G06F-017/18","","","","","","4923012003792"
"US","US","P","B2","Brain-computer interface based robotic arm self-assisting system and method","Disclosed are a brain-computer interface based robotic arm self-assisting system and method. The system comprises a sensing layer, a decision-making layer and an execution layer. The sensing layer comprises an electroencephalogram acquisition and detection module and a visual identification and positioning module and is used for analyzing and identifying the intent of a user and identifying and locating positions of a corresponding cup and the user's mouth based on the user intent. The execution layer comprises a robotic arm control module that performs trajectory planning and control for a robotic arm based on an execution instruction received from a decision-making module. The decision-making layer comprises the decision-making module that is connected to the electroencephalogram acquisition and detection module, the visual identification and positioning module and the robotic arm control module to implement the acquisition and transmission of data of an electroencephalogram signal, a located position and a robotic arm status and the sending of the execution instruction for the robotic arm. The system combines the visual identification and positioning technology, a brain-computer interface and a robotic arm to facilitate paralyzed patients to drink water by themselves, improving the quality of life of the paralyzed patients.","1. A robotic arm self-assisting system based on brain-computer interface, characterized in that the system is set up based on a three-layer structure including a sensing layer, a decision-making layer and an execution layer, wherein the sensing layer comprises an electroencephalogram acquisition and detection module and a visual identification and positioning module, the electroencephalogram acquisition and detection module being used for acquiring an electroencephalogram signal and analyzing and identifying an intent of a user, and the visual identification and positioning module being used for identifying and locating positions of a cup and the user'ss mouth based on the intent; the execution layer comprises a robotic arm control module, the robotic arm control module performs trajectory planning and control for a robotic arm based on an execution instruction received from a decision-making module; and a decision-making layer comprises the decision-making module, the decision-making module is connected to the electroencephalogram acquisition and detection module, the visual identification and positioning module and the robotic arm control module to implement an acquisition and a transmission of data of the electroencephalogram signal, a located position and a robotic arm status, and a sending of the execution instruction for the robotic arm.","10","16/471505","2017-10-11","2019-0387995","2019-12-26","11602300","2023-03-14","SOUTH CHINA UNIVERSITY OF TECHNOLOGY","Zhijun Zhang | Yongqian Huang | Yuanqing Li","2016-11180477","CN","2016-12-20","A61B-0005/375","A61B-0005/375 | B25J-0003/04 | B25J-0013/087 | G05B-0019/4097 | G06F-0003/015 | H04W-0080/06 | G05B-2219/35444 | G05B-2219/39451","G06F-017/00","G06F-017/00 | A61B-005/375 | B25J-003/04 | B25J-013/08 | G05B-019/4097 | G06F-003/01 | H04W-080/06","","","","","","4923011000732"
"US","US","P","B2","Methods and systems for individualized content media delivery","Aspects relate to systems and methods for individualized content media delivery. An exemplary system includes a sensor configured to detect a biofeedback signal as a function of a biofeedback of a user, a display configured to present content to the user, and a computing device configured to control an environmental parameter for an environment surrounding the user as a function of the biofeedback signal, wherein controlling the environmental parameter additionally includes generating an environmental machine-learning model as a function of an environmental machine-learning algorithm, training the environmental machine-learning model as a function of an environmental training set, wherein the environmental training set comprises biofeedback inputs correlated to environmental parameter outputs and generating the environmental parameter as a function of the biofeedback signal and the environmental machine-learning model.","1. A method of individualized content media delivery, the method comprising: detecting, using at least a sensor, at least a biofeedback signal as a function of a biofeedback of a user;presenting, using at least a display, content to the user; andcontrolling, using at least a computing device, at least an environmental parameter for an environment of the user as a function of the at least a biofeedback signal, wherein the at least an environmental parameter comprises a thermal parameter and controlling the at least an environmental parameter further comprises: generating an environmental machine-learning model as a function of an environmental machine-learning algorithm;training the environmental machine-learning model as a function of an environmental training set, wherein the environmental training set comprises biofeedback inputs correlated to environmental parameter outputs; andgenerating the at least an environmental parameter as a function of the at least a biofeedback signal and the environmental machine-learning model.","18","17/387491","2021-07-28","2023-0035334","2023-02-02","11604513","2023-03-14","GMECI, LLC","Bradford R. Everman | Brian Scott Bradke","","","","G06F-0003/015","G06F-0003/015 | G06K-0009/6292 | G09B-0005/06 | A61B-0005/486","G06F-003/01","G06F-003/01 | G09B-005/06 | G06K-009/62 | A61B-005/00","","","","","","4923011002920"
"US","US","P","B2","System for determining anatomical feature orientation","The systems and methods disclosed herein provide determination of an orientation of a feature towards a reference target. As a non-limiting example, a system consistent with the present disclosure may include a processor, a memory, and a single camera affixed to the ceiling of a room occupied by a person. The system may analyze images from the camera to identify any objects in the room and their locations. Once the system has identified an object and its location, the system may prompt the person to look directly at the object. The camera may then record an image of the user looking at the object. The processor may analyze the image to determine the location of the user's head and, combined with the known location of the object and the known location of the camera, determine the direction that the user is facing. This direction may be treated as a reference value, or ""ground truth."" The captured image may be associated with the direction, and the combination may be used as training input into an application.","1. An apparatus for determining an orientation of an anatomical feature of a subject, the apparatus comprising: memory;instructions;processor circuitry to execute the instructions to: identify an anatomical feature of the subject;determine a first location of the anatomical feature at a first time;determine a second location of the anatomical feature at a second time;determine a motion of the anatomical feature based on the first location and the second location; anddetermine the orientation of the anatomical feature of the subject based on a location of a target and the motion.","20","17/840077","2022-06-14","2022-0392100","2022-12-08","11605179","2023-03-14","Intel Corporation","Glen J. Anderson | Giuseppe Raffa | Carl S. Marshall | Meng Shi","","","","G06T-0007/70","G06T-0007/70 | A61B-0090/361 | G06F-0003/012 | G06K-0009/6271 | G06V-0010/454 | G06V-0030/194 | G06V-0040/168 | A61B-0005/70 | G06T-2207/30196","G06K-009/00","G06K-009/00 | G06T-007/70 | A61B-090/00 | G06F-003/01 | G06K-009/62 | G06V-010/44 | G06V-030/194 | G06V-040/16 | A61B-005/00","","","","","","4923011003578"
"US","US","P","B2","Systems and methods for machine learning-based state prediction and visualization","Determining input data for at least one machine learning model based on electronic data of a user. Predicting, based on the input data and the at least one machine learning model, a mental state of the user, the mental state comprising mood values, uncertainty values, and magnitude values, each mood value being associated with a corresponding uncertainty value of the uncertainty values and a corresponding magnitude value of the magnitude values, the magnitude value indicating a relative strength or weakness of the associated mood value. Selecting and arranging, based on the predicted mental state, a subset of graphical elements, each graphical element being associated with a corresponding mood value of the set of mood values, and each graphical element of the subset of graphical elements being associated with the predicted mental state of the user.","1. A computing system comprising: one or more processors; andmemory storing instructions that, when executed by the one or more processors, cause the computing system to perform: obtaining electronic data of a user;determining input data for at least one machine learning model based on the electronic data of the user;predicting, based on the input data and the at least one machine learning model, a mental state of the user, the mental state comprising a set of mood values, a set of uncertainty values, and a set of a magnitude values, each mood value of the set of mood values being associated with a corresponding uncertainty value of the set of uncertainty values and a corresponding magnitude value of the set of magnitude values, the corresponding magnitude value indicating a relative strength or weakness of the mood value associated with the corresponding uncertainty value and the corresponding magnitude value, wherein the predicting, based on the input data and the at least one machine learning model, the mental state of the user further causes the computing system to perform: mapping the set of mood values, the set of uncertainty values, and the set of magnitude values to a coordinate system, the coordinate system comprising a plurality of different mood regions, wherein each of the set of mood values is mapped to the coordinate system as a corresponding user point in the coordinate system, and wherein each of the corresponding uncertainty values is mapped as a corresponding radius originating at the corresponding point in the coordinate system;identifying at least a first mood region of the plurality of different mood regions that includes at least one corresponding user mapped therein;identifying at least a second mood region of the plurality of different mood regions that does not include any corresponding user points mapped therein, and includes at least a portion of a first radius of the corresponding radii mapped in the coordinate system; andwherein the mental state of the user is predicted based on the identified at least a first mood region of the plurality of different moods regions, the identified at least a second mood region of the plurality of different mood regions, and the magnitude values associated with the at least one corresponding user point mapped in the at least a first mood region of the plurality of different moods regions and the first radius of the corresponding radii mapped in the coordinate system;selecting and arranging, by the computing system based on the predicted mental state of the user, a subset of graphical elements from a set of graphical elements, each graphical element of the set of graphical elements being associated with a corresponding mood value of the set of mood values, and each graphical element of the subset of graphical elements being associated with the predicted mental state of the user;facilitating presentation, via a graphical user interface (GUI), of the subset of graphical elements according to the selection and arrangement of the subset of graphical elements;receiving, in response to the user interacting with the GUI presenting the subset of graphical elements according to the selection and arrangement of the subset of graphical elements, a user selection of a particular graphical element of the subset of graphical elements; andfacilitating presentation, via the GUI in response to the user selection, of the user selected graphical element of the subset of graphical elements.","17","17/661530","2022-04-29","2022-0350468","2022-11-03","11605464","2023-03-14","MARVIN BEHAVIORAL HEALTH CA, P.C. | MARVIN BEHAVIORAL HEALTH INC.","Patrick Ohiomoba","","","","G16H-0050/20","G16H-0050/20 | A61B-0005/165 | G06F-0003/04842 | G06N-0020/00 | G16H-0050/30 | G16H-0020/70","G16H-050/20","G16H-050/20 | G16H-050/30 | G06N-020/00 | A61B-005/16 | G06F-003/04842 | G16H-020/70","","","","","","4923011003859"
"US","US","P","B2","Determining intended user movement to control an external device","A user-specific model of muscular activity can be used to control an external device based on muscular activity within a limb of a user. The user-specific model of muscular activity can include single movements and corresponding one or more primary muscle patterns. New single movements can be added to the user-specific model of muscular activity can be by a system that includes a processor by receiving user-specific EMG signals (including one or more EMG patterns that indicate a single movement); decomposing the user-specific EMG signals into the one or more EMG patterns in EMG feature space that indicate the single movement; and updating the user-specific model of muscular activity to include the single movement and corresponding one or more primary muscle patterns based on the one or more EMG patterns in EMG feature space.","1. A method, comprising: receiving, by a system comprising a processor, user-specific EMG signals recorded by electrodes located on a limb of a user, wherein the user-specific EMG signals comprise one or more EMG patterns that indicate a single movement;decomposing, by the system, the user-specific EMG signals into the one or more EMG patterns in EMG feature space that indicate the single movement;updating, by the system, a user-specific model of muscular activity to include the single movement and corresponding one or more primary muscle patterns based on the one or more EMG patterns in EMG feature space; andcontrolling, by the system, the external device based on a predicted intent of the user to move the external device using the user-specific model of muscular activity by: receiving an unknown EMG signal from the user,identifying a partition of the EMG feature space where the unknown EMG signal is located,bounding the partition of the EMG feature space where the unknown EMG signal is located by known single movements with a corresponding one or more primary muscle patterns, anddetermining a ratio of movement vectors that should be combined to determine the direction and the magnitude of the intent of the user to move the external device, wherein the ratio of movement vectors corresponds to the unknown EMG signal.","21","16/818185","2020-03-13","2020-0289016","2020-09-17","11596339","2023-03-07","CASE WESTERN RESERVE UNIVERSITY","Platon Lukyanenko | Matthew R. Williams | Dustin J. Tyler","","","","A61B-0005/296","A61B-0005/296 | A61B-0005/4851 | A61F-0002/72 | G06F-0003/015","A61B-005/296","A61B-005/296 | G06F-003/01 | A61F-002/72 | A61B-005/00","","","","","","4923010000969"
"US","US","P","B2","System and method of manufacturing a medical implant","A system and method for forming a medical implant using a printing device. The printing device includes a print head having a heated nozzle, a heated build plate for receiving the printed material thereon, and a reflective plate having an active heater. A method for forming a medical device includes extruding a printing material by contiguous deposition to form a porous object having a lattice-like structure. The medical device, such as a spinal implant, may have interconnected pores and different regions, each having a different porosity for encouraging bone growth therein. The printed medical implant may be designed to be patient-specific, customized, and printed on-demand.","1. A selectively porous customizable medical implant made by a process of fused filament fabrication using a 3-D printer, the implant comprising: a continuous strand comprising: at least a first region having a lattice structure comprising rows deposited by a continuous flow defining interconnected asymmetrical openings of varying shape and size creating a first porosity; andat least a second region having a lattice structure comprising rows deposited by a continuous flow defining interconnected asymmetrical openings of varying shape and size creating a second porosity, wherein the first porosity has openings of a different size than the second porosity; anda plurality of layers, wherein each layer of the plurality of layers includes at least one of the first region or the second region, such that the continuous strand progresses between each layer, and a successive layer connects to a prior layer between planes in a Z-axis,wherein every layer is deposited through a contiguous deposition of a print material, andwherein each layer is deposited completely prior to a next layer being deposited.","20","17/370740","2021-07-08","2022-0324170","2022-10-13","11597148","2023-03-07","CURITEVA, INC.","Todd Reith | Eric Linder | Ryan Heskett","","","","B29C-0064/245","B29C-0064/245 | A61F-0002/3094 | B29C-0064/118 | B29C-0064/295 | B33Y-0010/00 | B33Y-0030/00 | G06Q-0040/08 | G06T-0007/0002 | G06T-0007/90 | G06V-0010/751 | H04L-0051/02 | H04L-0067/306 | H04L-0067/52 | A61F-0002/4455 | A61F-2002/30985 | B29C-0064/393 | B29K-2101/12 | B29L-2031/753 | B33Y-0050/02 | B33Y-0080/00 | G06T-2207/30168 | G06T-2207/30252 | G06V-0030/10 | G06V-2201/08 | G06V-2201/09 | G06V-2201/10 | H04L-0067/12","B29C-064/245","B29C-064/245 | A61F-002/30 | B33Y-010/00 | B33Y-030/00 | B29C-064/118 | B29C-064/295 | G06T-007/90 | G06V-010/75 | H04L-067/52 | G06Q-040/08 | G06T-007/00 | H04L-051/02 | H04L-067/306 | B33Y-080/00 | B33Y-050/02 | B29C-064/393 | B29K-101/12 | B29L-031/00 | A61F-002/44 | G06V-030/10 | H04L-067/12","","","","","","4923010001773"
"US","US","P","B2","Authentication method and system","A method for authenticating an object, comprising determining a physical dispersion pattern of a set of elements, determining a physical characteristic of the set of elements which is distinct from a physical characteristic producible by a transfer printing technology, determining a digital code associated with the object defining the physical dispersion pattern, and authenticating the object by verifying a correspondence of the digital code with the physical dispersion pattern, and verifying the physical characteristic.","1. A handheld optical code reader, comprising: a wireless digital data communication network interface configured to communicate over a digital data communication network;a camera configured to acquire a set of pixels representing optical characteristics of at least one feature of a physical object;at least one automated digital processor, configured to: recognize symbols in the set of pixels;convert the set of pixels from the camera to a surface projection of an image;determine characteristics of an expected image of a portion of the physical object distinct from the symbols, dependent on the recognized symbols;perform a stochastic analysis of at least a portion of the image with respect to deviations of the image from the characteristics of the expected image; andauthenticate the physical object based on at least the stochastic analysis and an acceptable error metric; andan output display configured to present an authentication status.","20","17/549524","2021-12-13","2022-0101029","2022-03-31","11600056","2023-03-07","CoPilot Ventures III LLC","Jay Fraser","","","","G06V-0010/147","G06V-0010/147 | G06K-0007/10722 | G06K-0007/1413 | G06K-0009/6201 | G06V-0010/88 | G06V-0020/80 | G06V-0030/224 | G06V-0030/40 | G06V-0030/418 | G06V-0040/1324 | G06V-0040/172 | G07D-0007/0043 | G07D-0007/2008 | G07D-0007/2033 | H04L-0009/3236 | H04L-0009/3247 | A61B-0005/1172 | G07D-0007/12 | G07D-0007/1205 | H04L-2209/72","G06V-010/147","G06V-010/147 | G07D-007/2033 | G07D-007/0043 | G06V-010/88 | G06V-020/80 | G06V-030/40 | G06V-030/224 | G06V-030/418 | G06V-040/16 | G06V-040/13 | G07D-007/20 | H04L-009/32 | G06K-007/10 | G06K-007/14 | G06K-009/62 | G07D-007/1205 | A61B-005/1172 | G07D-007/12","","","","","","4923010004665"
"US","US","P","B1","Increased dynamic range sensor with fast readout","Embodiments relate to a sensor system for a brain computer interface (BCI) that enable detection and decoding of brain activity by optical tomography. The sensor system includes an array of pixels arranged as grouped pixel units to provide increased dynamic range. One or more of the grouped pixel units can operate in a saturated mode while providing information useful for decoding brain activity. Furthermore, the grouped pixel units are arranged to enable fast readout by a pixel scanner, thereby increasing detection and decoding ability by systems implementing the sensor design. The grouped pixel units of the sensor system are aligned with optical fibers of an interface to a body region of a user, where the optical fibers can be retained in position relative to the grouped pixel units by an optically transparent substrate that provides mechanical support while minimizing factors associated with divergence of light transmitted through optical fibers.","1. A sensor system comprising: an array of pixels arranged linearly along an axis as a set of grouped pixel units;a set of optical fibers comprising first end regions mapped to a multidimensional region of interest and second end regions aligned linearly with the set of grouped pixel units along the axis; anda pixel scanner;wherein the sensor system is operable in a line scanning mode in which, during characterization of the region of interest, a central region of a first grouped pixel unit of the set of grouped pixel units is saturated by light from one of the set of optical fibers, and the pixel scanner transmits light-derived signals from the central region and an unsaturated region of the first grouped pixel unit for characterization of the region of interest; andwherein the sensor system is operable in a long exposure mode and short exposure mode, wherein, in each of the long exposure mode and the short exposure mode, the pixel scanner generates full frame and reduced line scans of the set of grouped pixel units.","20","16/773045","2020-01-27","","","11600093","2023-03-07","META PLATFORMS, INC.","Tobias Gerard Tiecke","","","","G06V-0040/10","G06V-0040/10 | A61B-0005/0042 | A61B-0005/0062 | A61B-0005/165 | G06F-0003/015 | G06N-0005/04 | G06N-0020/00 | G06V-0040/15","G06F-003/01","G06F-003/01 | G06V-040/10 | G06N-005/04 | G06N-020/00 | A61B-005/00 | A61B-005/16","","","","","","4923010004701"
"US","US","P","B2","System and method for estimating the brain blood volume and/or brain blood flow and/or depth of anesthesia of a patient","A system (1) for estimating the brain blood volume and/or brain blood flow and/or depth of anesthesia of a patient, comprises at least one excitation electrode (110E) to be placed on the head (20) of a patient (2) for applying an excitation signal, at least one sensing electrode (110S) to be placed on the head (20) of the patient (2) for sensing a measurement signal caused by the excitation signal, and a processor device (12) for processing said measurement signal (VC) sensed by the at least one sensing electrode (110S) for determining an output indicative of the brain blood volume and/or the brain blood flow. Herein, the processor device (12) is constituted to reduce noise in the measurement signal (VC) by applying a non-linear noise-reduction algorithm. In this way a system for estimating the brain blood volume and/or the brain blood flow of a patient is provided which may lead to an increased accuracy and hence more exact estimates.","1. A system for estimating the brain blood volume and/or brain blood flow and/or depth of anesthesia of a patient, comprising: at least one excitation electrode to be placed on a temple of the head of a patient for applying an excitation signal,at least one sensing electrode to be placed on the other temple of the head of the patient for sensing a measurement signal (VC) caused by the excitation signal,at least one electrode to be placed on the scalp of the patient'ss head to receive an EEG signal of spontaneous electrical activity of the brain of the patient, anda processor device for processing said measurement signal (VC) sensed by the at least one sensing electrode for determining an output indicative of the brain blood volume and/or the brain blood flow, said measurement signal (VC) being processed in the processor device in a first processing path comprising an amplification device for amplifying the measurement signal (VC) and an analog-to-digital converter for digitizing the measurement signal (VC), and said EEG signal received by the at least one EEG electrode being processed in the processor device in a second processing path for receiving and processing the EEG signal, wherein the processor device is constituted to reduce noise in the measurement signal (VC) by applying a non-linear noise-reduction algorithm based on a Poincare map analysis, andwherein the processor device is constituted to determine, based on the noise-reduced version of the measurement signal (VC), a correlate of the brain blood volume according to an area (A) obtained from integration of the measurement signal (VC), and wherein the area (A) is obtained from integration of the measurement signal (VC) over the left ventricular ejection time (LVET), which is estimated as the period from a point (B), defined as the minimum of the derivative of the measurement signal (VC) prior to a maximum point (C), to a point (X), defined as the minimum of the derivative of the measurement signal (VC) immediately after said maximum point (C), andwherein the processor device is further constituted (i) to determine a correlate of the brain blood flow by multiplying said correlate of the brain blood volume with a value indicative of the heart rate of the patient, (ii) to feed the correlate of the brain blood volume and/or the correlate of the brain blood flow into a first non-linear model comprising a fuzzy logic model or a quadratic equation model, to obtain output values indicative of the brain blood volume and/or the brain blood flow, and (iii) to feed features derived from the EEG signal and said output values into a second non-linear model to obtain final output values for the brain blood volume and/or the brain blood flow, and/or an output value indicative of a depth of anaesthesia.","8","16/621411","2018-05-29","2020-0222008","2020-07-16","11589824","2023-02-28","QUANTIUM MEDICAL SL","Erik Weber Jensen | Carmen Gonzalez Pijuan","2017-382364","EP","2017-06-14","A61B-0005/7217","A61B-0005/7217 | A61B-0005/026 | A61B-0005/0245 | A61B-0005/316 | A61B-0005/369 | A61B-0005/4821 | A61B-0005/7225 | A61B-0005/7246 | A61B-0005/7264 | A61B-0005/7275 | G06F-0017/18 | G06N-0003/0436 | G06N-0003/08 | G06N-0007/02","A61B-005/026","A61B-005/026 | A61B-005/00 | A61B-005/0245 | G06F-017/18 | G06N-003/04 | G06N-003/08 | G06N-007/02 | A61B-005/316 | A61B-005/369","","","","","","4923009001054"
"US","US","P","B2","Automated physical training system","Systems, methods and computer readable media comprising a virtual exercise board, which is represented by images on the screen of a pad device; wearable devices configured to attach to each shoe of a user and to collect and transmit touch data to the pad device; cameras for tracking movement and calibrating with the data collected by the wearable devices; and computer programs for collecting user data, processing user data, and generating outputs. In embodiments, features include augmented reality; ratings of performance; automated workouts/protocols; real-time progress bar; multi-location database capabilities; and reports.","1. A system for providing physical training routines for a user via a virtual sensor board on a display and on a floor, the system comprising: a tablet device having a processor and a display, the processor having a computer readable medium programmed to administer a physical training routine program to the user via the display;the physical training routine program configured to display a virtual sensor board on the display, the virtual sensor board including a plurality of virtual sensor units for indicating locations for foot touches by said user on said floor, the plurality of sensor units including a center sensor unit and at least four peripheral sensor units spaced peripherally from the center sensor unit, the virtual sensor board replicating a floor virtual sensor board located on said floor;a pair of wearable devices for executing a physical training routine and collecting user physical data during the physical training routine and transmitting user physical data to the tablet device, each of the wearable devices attachable to a foot or shoe of said user, the user physical data comprising foot touches of the user on a floor surface relative to changes on the virtual sensor board on the display;a camera system for executing an augmented reality physical training routine and collecting visual user data during the routine and for transmitting the visual user data to the tablet device; andthe physical training routine program configured to process the user physical data and the user visual data to administer physical training functionalities, administering physical training functionalities including varying an appearance of the virtual sensor units on the virtual sensor board to correspond with target physical touch areas on the floor virtual sensor board and foot touches registered on the floor virtual sensor board.","12","16/428658","2019-05-31","2020-0054931","2020-02-20","11590402","2023-02-28","The Quick Board, LLC","Kevin L Martin | Jason Fisher | Daniel Ilinca","","","","A63B-0071/0622","A63B-0071/0622 | A63B-0024/0062 | A63B-0024/0075 | A63B-0071/0686 | G06F-0003/011 | G06F-0003/0334 | G06F-0016/95 | G06T-0007/20 | A61B-0005/1038 | A61B-0005/6807 | A63B-2024/0068 | A63B-2071/065 | A63B-2220/53 | A63B-2220/56 | A63B-2220/62 | A63B-2220/806 | A63B-2220/836 | A63B-2225/52 | G06T-2207/10016 | G06T-2207/30196","A63B-071/06","A63B-071/06 | G06F-016/95 | A63B-024/00 | G06F-003/01 | G06F-003/033 | G06T-007/20 | A61B-005/103 | A61B-005/00","","","","","","4923009001629"
"US","US","P","B2","System and method for identifying user","A method may include acquiring first physiological data relating to a first subject, extracting at least one first physiological feature from the first physiological data relating to the first subject, determining a first model relating to at least one first reference physiological feature, generating, based on the first model and the at least one first physiological feature, a second model, the second model relating to at least one second reference physiological feature corresponding to the second model, and determining, based on the second model and the at least one first physiological feature, at least one identification physiological feature relating to the first subject. In some embodiments, the at least one identification physiological feature may correspond to the at least one second reference physiological feature.","1. A method implemented on a computing device having at least one processor, at least one computer-readable storage medium, and a communication port connected to a measuring device, the method comprising: acquiring first physiological data relating to a first subject;extracting at least one first physiological feature from the first physiological data relating to the first subject;determining a first model relating to at least one first reference physiological feature;generating, based on the first model and the at least one first physiological feature, a second model, the second model relating to at least one second reference physiological feature corresponding to the second model;determining, based on the second model and the at least one first physiological feature, at least one identification physiological feature relating to the first subject, wherein the at least one identification physiological feature corresponds to the at least one second reference physiological feature;identifying a second subject based on the at least one identification physiological feature relating to the first subject;wherein the identifying a second subject based on the at least one first physiological feature relating to the first subject comprises: acquiring the at least one identification physiological feature relating to the first subject;determining at least one second physiological feature relating to the second subject;comparing the at least one second physiological feature relating to the second subject and the at least one identification physiological feature relating to the first subject; andidentifying the second subject based on the comparison; andwherein the comparing the at least one second physiological feature relating to the second subject and the at least one identification physiological feature relating to the first subject comprises: determining a first feature vector based on the at least one identification physiological feature relating to the first subject;determining a second feature vector based on the at least one second physiological feature relating to the second subject;determining a distance between the first feature vector relating to the first subject and the second feature vector relating to the second subject; andcomparing the distance between the first feature vector relating to the first subject and the second feature vector relating to the second subject with a threshold.","9","16/804640","2020-02-28","2020-0201971","2020-06-25","11593468","2023-02-28","VITA-COURSE TECHNOLOGIES CO., LTD. | VITA-COURSE TECHNOLOGIES (HAINAN) CO., LTD.","Chuanmin Wei | Heng Peng | Jiwei Zhao","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/0006 | G06N-0003/08 | G06N-0020/10 | G06V-0040/70 | G06V-0040/15","G06F-021/32","G06F-021/32 | G06N-020/10 | A61B-005/00 | G06N-003/08 | G06V-040/70 | G06V-040/10","","","","","","4923009004669"
"US","US","P","B2","Methods for training and using a neurome that emulates the brain of a user","A system for training a neurome that emulates a brain of a user comprises a non-invasive brain interface assembly configured for detecting neural activity of the user in response to analog instances of a plurality of stimuli peripherally input into the brain of the user from at least one source of content, memory configured for storing a neurome configured for outputting a plurality of determined brain states of an avatar in response to inputs of the digital instances of the plurality of stimuli, and a neurome training processor configured for determining a plurality of brain states of the user based on the detected neural activity of the user, and modifying the neurome based on the plurality of determined brain states of the user and the plurality of determined brain states of the avatar.","1. A method of training a neurome that emulates a brain of a user, comprising: storing a neurome comprising a brain state regression model;peripherally inputting analog instances of a plurality of stimuli into the brain of the user from at least one source of content;detecting neural activity of the user in response to the analog instances of the plurality of stimuli peripherally input into the brain of the user;determining a plurality of brain states of the user based on the detected neural activity of the user;inputting digital instances of the plurality of stimuli into the neurome, such that the neurome outputs a plurality of determined brain states of an avatar; andmodifying the neurome based on the plurality of determined brain states of the avatar and the plurality of determined brain states of the user by: extracting single-dimensional vectors of content features from the digital instances of the plurality of stimuli characterizing the at least one source of content;inputting the single-dimensional vectors of content features into a first input of the brain state regression model;inputting single-dimensional vectors of brain state features characterizing the determined brain states of the user into a second input of the brain state regression model, such that the brain state regression model outputs single-dimensional vectors of brain state features of the avatar that substantially match the single-dimensional vectors of brain state features of the user.","33","17/408121","2021-08-20","2021-0383277","2021-12-09","11593715","2023-02-28","HI LLC","Bryan Johnson | Ethan Pratt | Jamu Alford | Husam Katnani | Julian Kates-Harbeck | Ryan Field | Gabriel Lerner | Antonio H. Lara","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/7264 | G06F-0003/015 | G06K-0009/00496 | G06N-0003/004 | G06N-0003/006 | G06N-0003/02 | G06N-0003/08 | G06Q-0030/0201 | A61B-0005/165 | G06Q-0030/0242","G06F-003/01","G06F-003/01 | G06N-020/00 | A61B-005/00 | G06K-009/00 | G06N-003/004 | G06N-003/006 | G06N-003/02 | G06N-003/08 | G06Q-030/0201 | A61B-005/16 | G06Q-030/0242","","","","","","4923009004915"
"US","US","P","B2","Arrangement for determining body surface properties by means of Multiple Spatially Resolved Reflection Spectroscopy (MSRRS)","An arrangement and a computer program product are provided for determining body surface characteristics. An arrangement includes an acquisition unit configured to detect body surface features by Multiple Spatially Resolved Reflection Spectroscopy (MSRRS) in a wavelength range between about 300 nm and about 1500 nm; a data storage unit to interrogate data using the characteristics; and a user interface comprising an output unit, wherein the user interface is configured to interact with a user. Further, the arrangement includes a portable computing unit configured for: interacting with a user and for evaluating the features and for determining the characteristics based on the features; obtaining from the data storage unit features of treatment products and/or application instructions for non-therapeutic treatment of a body surface according to the characteristics; and instructing the output unit to output information on the treatment products and/or application instructions to a user.","1. An arrangement for determining body surface characteristics, the arrangement comprising: an acquisition unit configured to detect body surface features by Multiple Spatially Resolved Reflection Spectroscopy (MSRRS) in a wavelength range between about 300 nm and about 1500 nm;a data storage unit to interrogate data using the determined body surface characteristics;a user interface comprising an output unit, wherein the user interface is configured to interact with a user; anda portable computing unit configured for: interacting with a user and for evaluating the determined body surface features and for determining the body surface characteristics based on the determined body surface features;obtaining from the data storage unit features of treatment products and/or application instructions for non-therapeutic treatment of a body surface according to the determined body surface characteristics; andinstructing the output unit to output information on the treatment products and/or application instructions to a user,wherein the portable computing unit is further configured to compare the features of treatment products for non-therapeutic treatment of a body surface with the determined body surface characteristics and to determine an effect of the treatment products on the body surface, taking into account the determined body surface characteristics.","13","16/652360","2018-09-25","2020-0300754","2020-09-24","11585750","2023-02-21","HENKEL AG & CO. KGAA","Andreas Bock | Thomas Welss","10-2017-219625","DE","2017-11-06","G01N-0021/33","G01N-0021/33 | A61B-0005/0075 | A61B-0005/44 | A61B-0005/443 | A61B-0005/448 | G01N-0021/35 | G06Q-0010/06316 | G06Q-0030/0623 | G06Q-0030/0631 | G06Q-0030/0633 | G16H-0070/20 | A45D-2044/007 | G01N-2201/062 | G01N-2201/06113","G01N-021/33","G01N-021/33 | G16H-070/20 | A61B-005/00 | G01N-021/35 | G06Q-010/0631 | G06Q-030/0601 | A45D-044/00","","","","","","4923008003553"
"US","US","P","B2","Image processing methods and arrangements useful in automated store shelf inspections","Imagery captured by an autonomous robot is analyzed to discern digital watermark patterns. In some embodiments, identical but geometrically-inconsistent digital watermark patterns are discerned in an image frame, to aid in distinguishing multiple depicted instances of a particular item. In other embodiments, actions of the robot are controlled or altered in accordance with image processing performed by the robot on a digital watermark pattern. The technology is particularly described in the context of retail stores in which the watermark patterns are encoded, e.g., on product packaging, shelving, and shelf labels. A great variety of other features and arrangements are also detailed.","1. A method of checking inventory in a retail store, the store including products stocked on shelves positioned adjacent an aisle, the method including the acts: producing shelf imagery using a robot that navigates along said aisle of said retail store, the robot being equipped with an imaging system comprising one or more cameras and at least one illumination source;processing a low-resolution counterpart to said shelf imagery to sense presence of machine-readable information at a particular location within the shelf imagery;based on said particular location, identifying a region of interest containing the machine-readable information; andanalyzing a high-resolution counterpart to said region of interest, depicted in said shelf imagery, to decode an identifier from said machine-readable information;wherein processing of the shelf imagery is reduced due to analysis of said region of interest, rather than all said shelf imagery, to decode said identifier;the method further including detecting, from said low-resolution counterpart to said shelf imagery, spatial frequency impulses associated with an orientation signal of a digital watermark, said spatial frequency impulses indicating presence of machine-readable digital watermark information.","25","16/752337","2020-01-24","2020-0234394","2020-07-23","11587195","2023-02-21","Digimarc Corporation","Sean Calhoon | Tony F. Rodriguez | Joel R. Meyer | William Y. Conwell","","","","G06T-0001/0092","G06T-0001/0092 | G06Q-0010/087 | A61B-0017/00234 | G05D-0001/0088 | G06K-0007/10861 | G06K-0007/1417 | G06K-0007/1421 | G06N-0003/049 | G06Q-0020/208 | G06Q-0030/00 | G06T-0001/005 | G06T-0001/0007 | G06T-2201/0065 | G06V-0010/10","G06T-001/00","G06T-001/00 | G06Q-010/087 | G06K-007/14 | A61B-017/00 | G06N-003/049 | G06K-007/10 | G06Q-020/20 | G05D-001/00 | G06Q-030/00 | G06V-010/10","","","","","","4923008004988"
"US","US","P","B1","Systems and methods for preventing errors in medical imaging","A method for preventing wrong-patient errors includes receiving a selection of a current imaging subject. The current imaging subject is selected for a current image acquisition session comprising capturing one or more current images of the current imaging subject utilizing at least a first image sensor system of a first imaging modality. The method includes accessing one or more previous images of a previous imaging subject. The one or more previous images depict the previous imaging subject according to at least a second imaging modality that is different from the first imaging modality. The method includes presenting the one or more previous images on a display system and, in response to determining that the previous imaging subject matches the current imaging subject based upon the one or more previous images, performing the current image acquisition session.","1. A computer-implemented method for preventing wrong-patient errors, comprising: receiving a selection of a current imaging subject, the current imaging subject being selected for a current image acquisition session comprising ci) capturing one or more current images of the current imaging subject utilizing at least a first image sensor system of a first imaging modality and (ii) capturing one or more additional current images of the current imaging subject utilizing at least a second image sensor system of a second imaging modality that is different from the first imaging modality, the second image sensor system being mounted to the first image sensor system;accessing a set of one or more previous images of a previous imaging subject, the one or more previous images of the previous imaging subject being associated with one or more image capture timepoints that temporally precede the current image acquisition session, the one or more previous images depicting the previous imaging subject according to at least the second imaging modality that is different from the first imaging modality;presenting the one or more previous images on a display system, the display system comprising a user interface associated with control of the second image sensor; andin response to determining that the previous imaging subject matches the current imaging subject based upon the one or more previous images of the second imaging modality, performing the current image acquisition session by (i) capturing the one or more current images of the current imaging subject utilizing at least the first image sensor system of the first imaging modality and (ii) capturing one or more additional current images of the current imaging subject utilizing at least the second image sensor system mounted to the first image sensor system.","15","17/738932","2022-05-06","","","11587232","2023-02-21","CAMERAD TECHNOLOGIES | THE UAB RESEARCH FOUNDATION","Carson Arthur Wick | Srini Tridandapani","","","","G06T-0007/0016","G06T-0007/0016 | A61B-0005/0035 | G06F-0003/165 | G10L-0017/04 | G10L-0017/06 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/30004","G06T-007/00","G06T-007/00 | G06F-003/16 | G10L-017/06 | G10L-017/04 | A61B-005/00","","","","","","4923008005025"
"US","US","P","B2","Medical data processing apparatus and medical data processing method","In one embodiment, a medical data processing apparatus includes processing circuitry. The processing circuitry obtains medical data relating to a subject, and outputs medical diagnostic image data obtained by performing predetermined processing on the medical data, along with standardized medical image data based on the medical data, the standardized medical image data being standardized for machine learning without performing part or all of the predetermined processing.","1. A medical data processing apparatus comprising: processing circuitry configured to: obtain medical data relating to a subject; andoutput medical diagnostic image data that is obtained by performing predetermined processing on the medical data and is used for medical diagnosis, not for machine learning, along with standardized medical image data based on the medical data, the standardized medical image date being standardized for machine learning without performing part or all of the predetermined processing.","19","16/894208","2020-06-05","2020-0402661","2020-12-24","11587680","2023-02-21","CANON MEDICAL SYSTEMS CORPORATION","Hidenori Takeshima","2019-113447","JP","2019-06-19","G16H-0050/20","G16H-0050/20 | A61B-0005/7267 | G06F-0021/6245 | G06N-0020/00 | G16H-0010/60 | G16H-0015/00 | G16H-0030/20 | G16H-0030/40 | G16H-0050/70 | A61B-0001/00009 | A61B-0005/0066 | A61B-0005/0095 | A61B-0005/055 | A61B-0006/5217 | A61B-0008/5223 | G06T-0007/0012","G16H-050/20","G16H-050/20 | G16H-010/60 | G16H-030/20 | G16H-030/40 | G06N-020/00 | G16H-015/00 | G06F-021/62 | A61B-005/00 | G16H-050/70 | G06T-007/00 | A61B-005/055 | A61B-001/00 | A61B-006/00 | A61B-008/08","","","","","","4923008005470"
"US","US","P","B2","System monitor and method of system monitoring to predict a future state of a system","System monitors and methods of monitoring a system are disclosed. In one arrangement a system monitor predicts a future state of a system. A data receiving unit receives system data representing a set of one or more measurements performed on the system. A first statistical model is fitted to the system data. The first statistical model is compared to each of a plurality of dictionary entries in a database. Each dictionary entry comprises a second statistical model. The second statistical model is of the same general class as the first statistical model and obtained by fitting the second statistical model to data representing a set of one or more previous measurements performed on a system of the same type as the system being monitored and having a known subsequent state. A prediction of a future state of the system being monitored is output based on the comparison. The first statistical model and the second statistical model are each a stochastic process or approximation to a stochastic process.","1. A system monitor configured to predict a future state of a system being monitored, comprising: a data receiving unit configured to receive system data representing a set of one or more measurements performed on the system; anda processing unit configured for: obtaining a first statistical model of the system data by fitting the first statistical model to the system data;obtaining a comparison of the first statistical model to each of a plurality of dictionary entries in a database, each dictionary entry comprising a second statistical model, the second statistical model being of the same general class as the first statistical model and obtained by fitting of the second statistical model to data representing a set of one or more previous measurements performed on a system of a same type as the system being monitored and having a known subsequent state; andoutputting a prediction of a future state of the system being monitored based on the comparison, wherein:the first statistical model and the second statistical model are each a stochastic process or approximation to a stochastic process;the plurality of dictionary entries comprises a plurality of groups of dictionary entries, each group exclusively containing dictionary entries having a common known subsequent state that is different to the known subsequent state of each other group; andthe comparison comprises determining which of the plurality of groups the first statistical model is most similar to.","22","16/239144","2019-01-03","2019-0156233","2019-05-23","11580432","2023-02-14","OXFORD UNIVERSITY INNOVATION LIMITED","David Andrew Clifton | Glen Wright Colopy | Marco Andre Figueiredo Pimentel","2016013318","GB","2016-08-02","G06N-0007/005","G06N-0007/005 | A61B-0005/725 | A61B-0005/7275 | G06F-0017/18 | G06K-0009/00496 | G06K-0009/00563 | G06K-0009/6255 | G06K-0009/6284 | G06V-0030/242 | G16H-0050/70","G06N-007/00","G06N-007/00 | G06F-017/18 | G06K-009/00 | G06K-009/62 | G16H-050/70 | G06V-030/242 | A61B-005/00","","","","","","4923007004783"
"US","US","P","B1","Computer apparatus and methods for generating color composite images from multi-echo chemical shift-encoded MRI","A computer apparatus and methods generate multi-parametric color composite images from multi-echo chemical shift encoded (CSE) MRI. Some embodiments use inherently co-registered images (i.e., image maps) that are combined into a single intuitive composite color image. The color (e.g., brightness, hue, and/or saturation) reflects in part the water and fat content, and other properties, particularly T2* relaxation (related to magnetic susceptibility) of the tissue.","1. A computer apparatus and color encoding scheme to facilitate the interpretation and analysis of a multi-echo chemical shift encoded (CSE) MRI of a vertebrate organism, comprising: (a) a non-transitory memory storing images generated from a gradient echo (GRE containing) multi-echo CSE MRI sequence;(b) a program stored in the non-transitory memory and operatively configured to generate a three-dimensional color space composite output image utilizing at least three stored images as inputs, wherein: (i) the at least three input images are inherently co-registered and have at least three different image types selected from the group consisting of in-phase (IP), opposed-phase (OP), water (W), fat (F), R2* (1/T2*), and derivatives of these;(ii) output image voxels whose MRI signal derives at least in part from water protons approach grayscale as the contribution of lipid protons and T2* signal decay approach zero;(iii) output image voxels exhibit a color that deviates from gray based in part on lipid proton content and T2* signal decay; and(iv) at least a portion of the output image voxels exhibit a color that includes and provides information from corresponding voxels in each of the input images; and(c) a computer processor in communication with the non-transitory memory and configured to perform the program by executing computer executable instructions.","20","16/795557","2020-02-19","","","11580626","2023-02-14","Kenneth L. Weiss","Kenneth L. Weiss","","","","G06T-0005/50","G06T-0005/50 | A61B-0005/055 | A61B-0005/4872 | A61B-0005/7267 | G06T-0007/0012 | G06T-0007/90 | G06T-0009/00 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | G16H-0050/70 | A61B-2576/00 | G06F-0003/0482 | G06T-2207/10024 | G06T-2207/10088 | G06T-2207/20081 | G06T-2207/20084 | G06T-2210/41","G06K-009/00","G06K-009/00 | G06T-005/50 | G06T-009/00 | G06T-007/00 | G16H-030/40 | G16H-030/20 | G16H-050/70 | G16H-050/20 | A61B-005/00 | A61B-005/055 | G06T-007/90 | G06F-003/0482","","","","","","4923007004973"
"US","US","P","B2","Ambulatory medicament pump voice operation","Ambulatory medical devices that provide therapy to a subject, such as blood glucose control, are disclosed. Disclosed systems and devices can transmit a request to modify blood glucose control therapy delivered to a subject. The request can be transmitted via a voice-activated control system, the ambulatory medicament pump can include a medicament reservoir, a pump controller, a wireless data interface, and/or other elements. The device can receive an indication that the request to modify therapy is approved and, in response to the indication that the request to modify the blood glucose control therapy is approved, instruct the pump controller to modify the blood glucose control therapy delivered to the subject.","1. A method for executing a glucose level control system command via an ambulatory medicament pump, the glucose level control system command provided by a user via a voice-user interface, the method comprising: by an electronic processor of an ambulatory medicament pump executing specific computer-executable instructions stored in a non-transitory memory in communication with the electronic processor, receiving, via a network interface, an execution command from a pump management system, the execution command comprising computer-executable instructions to perform a verbal command;performing, via the ambulatory medicament pump, the execution command; andtransmitting, via the network interface, a verification signal to the pump management system, the verification signal indicating that the execution command was successfully performed.","30","17/657612","2022-03-31","2022-0226571","2022-07-21","11581080","2023-02-14","BETA BIONICS, INC.","Michael J. Rosinko | Firas H. El-Khatib | John R. Costik","","","","G16H-0020/17","G16H-0020/17 | A61B-0005/14532 | A61M-0005/14244 | A61M-0005/14248 | A61M-0005/172 | A61M-0005/1723 | G05B-0023/0283 | G06Q-0020/401 | G06Q-0030/0631 | G06Q-0030/0633 | G16H-0015/00 | G16H-0040/20 | G16H-0040/40 | G16H-0040/63 | G16H-0040/67 | G16H-0050/70 | A61M-2005/1402 | A61M-2005/14208 | A61M-2205/18 | A61M-2205/33 | A61M-2205/3553 | A61M-2205/3569 | A61M-2205/3576 | A61M-2205/3584 | A61M-2205/502 | A61M-2205/505 | A61M-2205/52 | A61M-2205/609 | A61M-2205/80 | A61M-2209/088 | A61M-2230/201","A61M-005/172","A61M-005/172 | G16H-020/17 | A61B-005/145 | G16H-040/67 | G16H-040/63 | A61M-005/142 | G16H-040/40 | G06Q-030/0601 | G16H-040/20 | G05B-023/02 | G06Q-020/40 | G16H-015/00 | G16H-050/70 | A61M-005/14","","","","","","4923007005422"
"US","US","P","B2","Detecting and displaying stent expansion","A method for processing an intravascular image including a plurality of image frames acquired during a pullback of an imaging catheter inserted into a vessel, the method including displaying on a graphical user interface (GUI) an image including detected results of lumen borders and at least one stent, the image including an evaluated stent expansion and an evaluated stent apposition determined from the intravascular image. The method also includes determining whether a modification to the detected results of the stent has been received by the GUI. Then, re-evaluating stent length, stent expansion and stent apposition when it is determined that the detected results of the stent has been modified via the GUI and displaying the re-evaluated stent expansion and the re-evaluated stent apposition on the GUI.","1. A method for processing an intravascular image including a plurality of image frames acquired during a pullback of an imaging catheter inserted into a vessel, the method comprising: displaying on a graphical user interface (GUI) an image including detected results of lumen borders and at least one stent, the image including an evaluated stent expansion and/or an evaluated stent apposition determined from the intravascular image including the plurality of image frames acquired during the pullback of the imaging catheter inserted into the vessel, and the at least one stent having an evaluated stent length;determining whether a modification to the displayed detected results of the at least one stent has been received by the GUI;re-evaluating stent length, stent expansion, and stent apposition in a case where it is determined that the displayed detected results of the at least one stent have been modified via, and using, the GUI; anddisplaying the re-evaluated stent expansion and the re-evaluated stent apposition on the GUI after the modification has been received via, and using, the GUI.","20","16/148421","2018-10-01","2019-0099080","2019-04-04","11571129","2023-02-07","CANON U.S.A., INC.","Mie Kunio | Daisuke Yamada | Yohei Minatoya","","","","A61B-0005/0066","A61B-0005/0066 | A61B-0005/0084 | A61B-0005/6852 | A61B-0005/7435 | A61B-0008/12 | A61F-0002/92 | G06F-0003/048 | G06F-0003/0488 | G16H-0030/40 | A61B-0005/0035 | A61B-0005/06 | A61B-0005/1076 | G06T-0007/0012 | G06T-2207/10101 | G16H-0010/60 | G16H-0030/20","A61B-005/00","A61B-005/00 | A61F-002/92 | A61B-008/12 | G06F-003/048 | G06F-003/0488 | G16H-030/40 | G06T-007/00 | G16H-010/60 | G16H-030/20 | A61B-005/06 | A61B-005/107","","","","","","4923006000762"
"US","US","P","B2","System and method for making a recommendation for a user of a life management system","A life management system receives data from a client device worn by a user, the data comprising biotelemetry data and activity data collected about a user wearing the client device. The life management system generates snapshot information using information from a group consisting of: the biotelemetry data, activity data, social data associated with the user, and user profile information associated with the user. The life management system generates a recommendation using portions of the snapshot information, and updates the snapshot information with the recommendation. The life management system executes a recommendation associated with the snapshot information in accordance with the user controls associated with the user.","1. A computer implemented method for generating a recommendation on a device of a user of a life management system, the life management system comprising a processor configured to execute non-transitory machine readable instructions, wherein execution of the non-transitory machine readable instructions by the processor causes the life management system to: receive data relating to the user, the user data comprising biotelemetry data received from a biotelemetry device configured to be worn by the user and collect one or more of the biotelemetry data and activity data via a wireless connection between the biotelemetry device and the life management system, the activity data collected about the user and social data of the user, the social data comprising calendar entries on a calendar for the user;automatically change a graphical user interface of a display of the user device to represent a detected mood of the user based on the collected biotelemtry data and the activity data by changing at least one of a color scheme of the display or change the graphical user interface to include different types of information associated with the detected mood;access the calendar on a client device of the user and a calendar on a client device of at least one other user by remotely connecting to the client device of the user and the client device of the at least one other user, wherein the at least one other user is identified as a contact by the life management system in a corresponding contact application of the client device of the user;analyze the calendar on the client device of the user and the calendar on the client device of the at least one other user;identify an open time slot in the calendar of the user that is common to an open time slot identified by the life management system in the calendar of the at least one other user;generate an entry in the open time slot in the calendar of the user reserving the open time slot for a new activity and generate an entry in the open time slot in the calendar of the at least one other user reserving the open time slot for the new activity;generate snapshot information using the biotelemetry data, the activity data and user profile information associated with the user;determine a momentum score for the user using portions of the snapshot information;compare the momentum score to a threshold value, and determine a requirement for the new activity based on the comparison;develop a travel-related recommendation to both the user and the at least one other user based on the requirement, wherein the new activity is based on the travel-related recommendation;engage with a third party service that is configured to carry-out the new activity associated with the travel-related recommendation;automatically update an entry in the calender of the user with the new activity in the open time slot in the calendar of the user and an entry in the open time slot in the calendar of the at least one other user with the travel-related recommendation and information related to the third-party service.","20","15/059573","2016-03-03","2017-0000348","2017-01-05","11564571","2023-01-31","ZERO360, INC.","Peter Karsten | George Arriola | Kouji Kodera","2013015764 | 2013015765 | 2014000225","GB | GB | GB","2013-09-04 | 2013-09-04 | 2014-01-07","A61B-0005/0022","A61B-0005/0022 | A61B-0005/0048 | A61B-0005/0205 | A61B-0005/02055 | A61B-0005/369 | A61B-0005/389 | A61B-0005/681 | A61B-0005/7445 | A61B-0005/7475 | G06F-0001/163 | G06F-0003/015 | G06F-0003/017 | G06F-0003/0346 | G06F-0003/0484 | G06F-0003/04883 | G06Q-0010/109 | G06Q-0010/1093 | G06Q-0010/1095 | G08B-0001/08 | G08B-0006/00 | G08B-0007/06 | G08B-0025/016 | G08B-0025/10 | G16H-0010/60 | G16H-0015/00 | G16H-0020/30 | G16H-0040/20 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | H04Q-0009/00 | A61B-0005/021 | A61B-0005/02438 | A61B-0005/0533 | A61B-0005/486 | A61B-0005/4866 | A61B-0005/4875 | G06F-2203/011 | G06F-2203/04808 | H04L-0067/306 | H04L-0067/535 | H04Q-2209/40","A61B-005/00","A61B-005/00 | G08B-006/00 | G08B-007/00 | G08B-001/00 | G06Q-010/00 | G16H-040/00 | G16H-050/00 | G16H-015/00 | G16H-020/00 | G06F-003/00 | G08B-025/00 | G06F-001/00 | H04Q-009/00 | H04L-067/00 | A61B-005/0205 | G08B-007/06 | G08B-001/08 | G06Q-010/10 | G16H-040/63 | G16H-050/20 | G16H-040/20 | G16H-010/60 | G16H-020/30 | G16H-040/67 | A61B-005/369 | A61B-005/389 | G06F-003/01 | G06F-003/0484 | G08B-025/01 | G08B-025/10 | G06F-001/16 | G06F-003/0346 | G06F-003/04883 | A61B-005/024 | H04L-067/50 | H04L-067/306 | A61B-005/021 | A61B-005/0533","","","","","","4923005000956"
"US","US","P","B2","Generating simulated anatomies of an electromagnetic source","Systems are provided for generating data representing electromagnetic states of a heart for medical, scientific, research, and/or engineering purposes. The systems generate the data based on source configurations such as dimensions of, and scar or fibrosis or pro-arrhythmic substrate location within, a heart and a computational model of the electromagnetic output of the heart. The systems may dynamically generate the source configurations to provide representative source configurations that may be found in a population. For each source configuration of the electromagnetic source, the systems run a simulation of the functioning of the heart to generate modeled electromagnetic output (e.g., an electromagnetic mesh for each simulation step with a voltage at each point of the electromagnetic mesh) for that source configuration. The systems may generate a cardiogram for each source configuration from the modeled electromagnetic output of that source configuration for use in predicting the source location of an arrhythmia.","1. A method performed by one or more computing systems for generating a simulated anatomy of an electromagnetic source within a body, the method comprising: accessing seed anatomies of the electromagnetic source, each seed anatomy having a seed value for each of a plurality of anatomical parameters of the electromagnetic source;accessing a plurality of sets of weights, each set of weights including weight for each seed anatomy;for each of the sets of weights, for each of the anatomical parameters, generating a simulated value for that anatomical parameter by combining the seed values for that anatomical parameter, factoring in the weight for each seed anatomy;generating a three-dimensional mesh representing the electromagnetic source based on the simulated values for the anatomical parameters; andsimulating electrical activity of the electromagnetic source based on the generated three-dimensional mesh.","30","16/042984","2018-07-23","2019-0333641","2019-10-31","11564641","2023-01-31","VEKTOR MEDICAL, INC.","Christopher Villongco","","","","A61B-0005/7435","A61B-0005/7435 | A61B-0005/02028 | A61B-0005/318 | A61B-0005/319 | A61B-0005/339 | A61B-0005/341 | A61B-0005/7275 | A61B-0005/743 | A61B-0034/10 | G06V-0010/00 | G09B-0023/30 | A61B-0005/1075 | A61B-0005/25 | A61B-0005/316 | A61B-0005/349 | A61B-0005/361 | A61B-0005/363 | A61B-0005/364 | A61B-0005/6805 | A61B-0005/725 | A61B-0005/7235 | A61B-0005/7267 | A61B-0005/7425 | A61B-0005/7445 | A61B-0006/032 | A61B-0006/503 | A61B-2034/105 | G06F-0030/20 | G06K-0009/6215 | G06N-0003/0454 | G06N-0003/08 | G06N-0005/04 | G06N-0005/046 | G06N-0020/00 | G06T-0003/4007 | G06T-0017/205 | G06T-0019/20 | G06T-2210/41 | G09B-0023/285 | G16H-0050/20 | G16H-0050/50","G06F-003/048","G06F-003/048 | A61B-005/00 | G09B-023/30 | A61B-034/10 | A61B-005/02 | A61B-005/318 | A61B-005/319 | A61B-005/339 | A61B-005/341 | G06V-010/00 | G16H-050/50 | G16H-050/20 | G06N-020/00 | A61B-006/03 | G06N-005/04 | G09B-023/28 | G06K-009/62 | G06N-003/08 | G06T-019/20 | A61B-005/107 | G06T-003/40 | G06T-017/20 | G06F-030/20 | A61B-005/316 | A61B-005/349 | A61B-005/364 | A61B-005/25 | A61B-005/361 | A61B-005/363 | A61B-006/00 | G06N-003/04","","","","","","4923005001026"
"US","US","P","B2","Wearable device for communication with an ophthalmic device","A system can include an aural computing system in communication with the ophthalmic device. In some embodiments, the aural computing system can include a wireless communication device in communication with the ophthalmic device. In some embodiments, the ophthalmic device comprises a contact lens, which can inserted into the user's eye. The wireless communication device can comprise wearable technology.","1. A system for communicating with an ophthalmic device, comprising: the ophthalmic device including a contact lens body,a transmitter coupled with the contact lens body, andan output device coupled with the contact lens body;a sensor coupled to the contact lens body and in communication with the transmitter, wherein the sensor detects at least one of a health condition, an ambient condition, a lens fit, or a biometric data related to the eye; andan aural computing system in wireless communication with the ophthalmic device.","25","16/353173","2019-03-14","2019-0285911","2019-09-19","11567345","2023-01-31","MENICON SINGAPORE PTE LTD.","Stephen D. Newman","","","","G02C-0007/04","G02C-0007/04 | A61B-0003/16 | A61B-0005/6821 | A61F-0002/16 | A61F-0002/1613 | A61F-0009/0008 | G02C-0011/10 | G08B-0005/22 | G08B-0021/182 | H04R-0001/10 | A61B-0003/101 | A61B-0005/02 | A61B-0005/14532 | A61F-2250/0002 | A61F-2250/008 | G02C-0007/083 | G06F-0003/013","G02C-007/04","G02C-007/04 | A61B-005/00 | A61F-002/16 | A61B-003/16 | H04R-001/10 | A61F-009/00 | G02C-011/00 | G08B-005/22 | G08B-021/18 | G02C-007/08 | A61B-005/02 | G06F-003/01 | A61B-003/10 | A61B-005/145","","","","","","4923005003703"
"US","US","P","B2","Terminal","A terminal is disclosed. The terminal according to an embodiment of the present invention comprises: an output unit for outputting a notification; a storage unit for storing a database; a control unit for controlling the outputting of the notification; and an artificial intelligence unit for acquiring information regarding a user's context, and outputting a notification when the user's context corresponds to information included in the database, wherein the database includes at least one of a user's personal database, a standard activity database, and an accident type database.","1. A terminal comprising: an output interface configured to output a notification;a memory configured to store a database;a controller configured to control output of the notification; andan artificial intelligence interface configured to acquire information on a situation of a user and output a notification when the situation of the user corresponds to information included in the database,wherein the database includes at least one of a personal database of the user, a standard behavior database or an accident type database,wherein the standard behavior database includes information on a behavior of a driver while driving, andwherein the personal database of the user includes information on a behavior pattern acquired from a past behavior of the user,wherein the artificial intelligence interface is configured to acquire information on a behavior of the user,wherein, when the behavior of the user corresponds to the behavior of the driver included in the standard behavior database, the artificial intelligence interface is configured to determine whether the behavior of the user corresponds to the behavior pattern included in the personal database,wherein, based on the determination, the artificial intelligence interface is further configured to adjust an output intensity of the notification to be output.","13","16/634561","2018-03-23","2020-0265741","2020-08-20","11568758","2023-01-31","LG ELECTRONICS INC.","Eunjoo Rhee | Choil Lee","10-2017-0095570","KR","2017-07-27","G09B-0019/00","G09B-0019/00 | A61B-0005/18 | A61B-0005/7264 | G01C-0021/3484 | G01C-0021/3492 | G06F-0016/22 | G06N-0020/00 | G06Q-0030/0631 | H04M-0001/72454 | G09B-0019/14","G09B-019/00","G09B-019/00 | G06F-016/22 | G06N-020/00 | A61B-005/18 | A61B-005/00 | G01C-021/34 | G06Q-030/06 | H04M-001/72454 | G09B-019/14","","","","","","4923005005103"
"US","US","P","B2","Optimization tool for auditory devices","An optimization system for testing a patient's hearing comprises a controller, an ear piece, and a memory. The controller: provides a series of tones to the ear piece; receives feedback from the patient between each tone; generates a data point to be used in an audiogram after receiving each feedback; after each data point is generated, computes a statistical distribution based on the generated data points; identifies an area of the statistical distribution most in need of additional data; and selects a subsequent tone to provide in the series of tones. Each feedback indicates whether the respective tone was detected or not detected, and each data point is based on the respective feedback. Each subsequent tone provided in the series of tones is a tone represented in the area of the statistical distribution most in need of additional data at the time of selection.","1. An optimization system for testing a patient'ss hearing comprising: a controller;an ear piece in audible communication with the controller;a memory in communication with the controller and including instructions that, when executed by the controller, cause the controller to: provide a series of tones to the ear piece;receive feedback from the patient between each tone provided, wherein each feedback indicates that the respective tone was detected in the ear piece or that the respective tone was not detected;generate a data point to be used in an audiogram after receiving each feedback, wherein each data point is based on the respective feedback;after each data point is generated, compute a statistical distribution based on the generated data points;identify an area of the statistical distribution most in need of additional data; andselect a subsequent tone to provide in the series of tones, wherein each subsequent tone provided in the series of tones is a tone represented in the area of the statistical distribution most in need of additional data at the time of selection.","18","17/391560","2021-08-02","2021-0361193","2021-11-25","11559224","2023-01-24","SOUNDWAVE HEARING, LLC","Christopher Boven | Reagan John Roberts","","","","A61B-0005/123","A61B-0005/123 | A61B-0005/6803 | A61B-0005/743 | A61N-0001/36038 | H04R-0025/30 | H04R-0025/305 | H04R-0025/50 | H04R-0025/70 | G06F-0003/165 | H04R-2225/41 | H04R-2225/67","H04R-025/00","H04R-025/00 | A61B-005/12 | A61B-005/00 | A61N-001/36 | G06F-003/16","","","","","","4923004000820"
"US","US","P","B2","Method for measuring hair movement characteristics","A method of measuring changes in hair movement characteristics, predictive of consumer response includes: i) providing an apparatus for measuring hair movement characteristics of hair; ii) measuring the hair movement characteristics using the apparatus to obtain a first hair movement characteristic; iii) applying a treatment to the hair or an assault to the hair; iv) measuring the hair movement characteristics using the apparatus after step iii) to obtain a second hair movement characteristic; v) comparing the first hair movement characteristic and the second hair movement characteristic; and vi) assessing a change in movement occurring as a result of the application of the treatment or the assault.","1. A method of measuring changes in hair movement characteristics, predictive of consumer response, the method comprising the steps of: i) providing an apparatus, including a camera for capturing images of hair, for measuring hair movement characteristics of hair;ii) measuring the hair movement characteristics using the apparatus to obtain a first hair movement characteristic;iii) applying a treatment to the hair or an assault to the hair;iv) measuring the hair movement characteristics using the apparatus after step iii) to obtain a second hair movement characteristic;v) comparing the first hair movement characteristic and the second hair movement characteristic; andvi) assessing a change in movement occurring as a result of the application of the treatment or the assault.","17","16/623166","2018-06-20","2021-0137448","2021-05-13","11559249","2023-01-24","CONOPCO, INC., D/B/A UNILEVER","Fraser Ian Bell | LIyr Glyndwr Griffiths | Eric Gordon Mahers | Julie Marie Roberts | Graham John Cleaver | Aneta Magdalena Stasik","2017-177722","EP","2017-06-23","A61B-0005/448","A61B-0005/448 | G06Q-0030/0201 | G06T-0007/20 | A61B-0005/44","A61B-005/00","A61B-005/00 | G06Q-030/02 | G06T-007/20","","","","","","4923004000844"
"US","US","P","B2","Operation and control of magnetic resonance imaging apparatus","A combined MRI and radiotherapy apparatus comprises a radiotherapeutic source, an MRI system, a patient support, drive motors for the patient support arranged to adjust the position of the patient support while a patient is on the support, a control panel having a user-operable input interface for controlling the drive motors, and a display unit. A mounting arrangement for a display device comprises a transparent cover, a display panel held against a rear face of the cover so as to be visible through a front face of the cover, and a retaining structure for holding the display panel in place. The retaining structure comprises a chassis fixable in position relative to the cover, the chassis having at least one resilient finger extending therefrom alongside a rear face of the display panel, a part of which bears against the rear face of the display panel to resiliently urge the display panel against the rear face of the cover. The radiotherapeutic source, MRI system, patient support and the control panel will usually be located within an enclosed space, to confine the therapeutic radiation; a second control panel is provided outside the enclosed space, able to control at least the radiotherapy source.","1. A mounting arrangement for a display device comprising a display panel, the mounting arrangement comprising: a cover having a transparent section arranged for receiving the display panel against a rear face of the transparent section so as to be visible through a front face of the transparent section, anda retaining structure for holding the display panel in place, the retaining structure comprising a chassis fixable in position relative to the cover, the chassis having at least one resilient finger arranged to extend therefrom alongside a rear face of the display panel, wherein:a part of the at least one resilient finger is arranged to bear against the rear face of the display panel to resiliently urge the display panel against the rear face of the transparent section of the cover when the display panel is received by the cover; andthe retaining structure is removably mounted to an external component independently of both the cover and the display panel.","20","16/179175","2018-11-02","2019-0133538","2019-05-09","11559262","2023-01-24","ELEKTA LIMITED","Duncan Bourne | Anthony Williams","2017018288","GB","2017-11-03","A61B-0005/7445","A61B-0005/7445 | A61B-0005/0036 | A61B-0005/055 | A61B-0005/7475 | A61N-0005/10 | A61N-0005/1048 | G01R-0033/4808 | G01R-0033/546 | A61N-2005/1074 | A61N-2005/1092 | G06F-0003/0488","A61B-005/055","A61B-005/055 | G01R-033/54 | G01R-033/48 | A61N-005/10 | A61B-005/00 | G06F-003/0488","","","","","","4923004000857"
"US","US","P","B2","Generating attribute preference models based on disparate attribute spectrums","An attribute correlation system reduces network traffic and processing cycles associated with impromptu item selections by generating attribute preference models based on disparate attribute spectrums. The attribute correlation system deploys the attribute preference models to select individual items from various disparate ""candidate item categories."" Generally described, the attribute preference models facilitate analyzing item sets across a wide variety of disparate ""candidate"" item categories to preemptively identify individual items for a user. In this way, the individual items may be identified and, ultimately, selected for the user even absent any indication that the user has searched for otherwise identified these items or even other items from within the disparate ""candidate"" item categories. The ""candidate"" item categories may be determined to be disparate from one another based on a relationship void existing such that predefined relationships are missing between these ""candidate"" item categories.","1. A computer-implemented method, comprising: identifying preferred attribute value ranges for a user based on at least one first attribute spectrum that defines first values for one or more attributes of at least one tangible item of interest to the user;generating, based on the preferred attribute value ranges for the user, an attribute preference model in association with the user;selecting a candidate item category from a plurality of item categories each defined with item category data, wherein the candidate item category includes a plurality of candidate tangible items;determining at least one second attribute spectrum that defines second values for the one or more attributes of each of the plurality of candidate tangible items in the candidate item category;analyzing the at least one second attribute spectrum based on the attribute preference model to designate a selected tangible item of the plurality of candidate tangible items; andcommunicating the selected tangible item to the user.","20","17/008196","2020-08-31","2020-0404072","2020-12-24","11563818","2023-01-24","EBAY INC.","Steve Yankovich | Sergio Pinzon Gonzales, Jr.","","","","H04L-0067/306","H04L-0067/306 | G06F-0016/9535 | G06N-0020/00 | G06Q-0010/083 | G06Q-0030/0202 | G06Q-0030/0241 | G06Q-0030/0256 | G06Q-0030/0603 | G06Q-0030/0621 | G06Q-0030/0627 | G06Q-0030/0631 | G06Q-0030/0641 | G06Q-0030/0643 | G06Q-0050/04 | G06V-0020/20 | G06V-0020/80 | G06V-0040/20 | H04L-0067/535 | G06Q-0050/01","A61N-001/00","A61N-001/00 | H04L-067/306 | G06Q-030/06 | G06N-020/00 | G06Q-030/02 | G06Q-050/04 | G06V-020/20 | G06V-020/80 | G06V-040/20 | H04L-067/50 | G06F-016/9535 | G06Q-010/08 | G06Q-050/00","","","","","","4923004005359"
"US","US","P","B2","Performance monitoring systems and methods","Systems and methods for electronically creating and modifying a fitness plan are disclosed. The method may include receiving electronic user data, collecting electronic fitness data, and displaying a suggestion for a fitness activity based on the electronic user data and the electronic fitness data.","1. A method of creating and modifying a fitness plan, the method comprising: receiving electronic user data related to a fitness activity on a portable electronic device, the portable electronic device including a microprocessor, a display screen, a user input, a satellite positioning system receiver, and a wireless communication transceiver, wherein the portable electronic device is configured to be carried by a user during the fitness activity;after receiving the electronic user data related to the fitness activity on the portable electronic device, collecting electronic fitness data during a second fitness activity on the portable electronic device, wherein the electronic fitness data comprises location information based on position data as determined by the satellite positioning system receiver, and wherein the portable electronic device is configured to be carried by the user during the second fitness activity;displaying automatically a recommendation on the display screen for a route for a third fitness activity based on the electronic user data and the electronic fitness data.","20","16/190510","2018-11-14","2019-0078891","2019-03-14","11557388","2023-01-17","ADIDAS AG","Michael Ellis","","","","G16H-0020/30","G16H-0020/30 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/0205 | A61B-0005/0833 | A61B-0005/1038 | A61B-0005/1118 | A61B-0005/411 | A61B-0005/4836 | A63B-0024/0021 | A63B-0024/0062 | A63B-0024/0075 | A63B-0024/0084 | A63B-0024/0087 | A63B-0069/16 | A63B-0071/06 | A63B-0071/0686 | G01C-0021/20 | G01S-0019/19 | G06F-0003/16 | G06F-0011/328 | G06F-0017/40 | G09B-0005/02 | G09B-0005/06 | G09B-0019/0038 | G09B-0019/0092 | G16H-0010/20 | G16H-0020/10 | G16H-0020/40 | G16Z-0099/00 | H04L-0063/0236 | H04L-0063/08 | H04L-0063/1408 | H04W-0004/024 | H04W-0004/029 | H04W-0012/02 | H04W-0012/06 | A45F-2005/008 | A61B-0005/7475 | A63B-0022/0076 | A63B-0022/02 | A63B-0069/0028 | A63B-2024/0012 | A63B-2024/0025 | A63B-2024/0065 | A63B-2024/0093 | A63B-2071/063 | A63B-2071/0625 | A63B-2071/0627 | A63B-2071/0658 | A63B-2071/0661 | A63B-2071/0663 | A63B-2220/12 | A63B-2220/14 | A63B-2220/17 | A63B-2220/20 | A63B-2220/22 | A63B-2220/30 | A63B-2220/836 | A63B-2225/15 | A63B-2225/20 | A63B-2225/50 | A63B-2225/60 | A63B-2225/685 | A63B-2230/04 | A63B-2230/202 | A63B-2230/207 | A63B-2230/30 | A63B-2230/42 | A63B-2230/50 | A63B-2230/65 | A63B-2230/70 | A63B-2244/20 | G16H-0020/00 | G16H-0020/60 | H04B-0001/385 | H04L-0063/0428 | H04M-0001/6041 | H04M-2250/02 | H04W-0084/18 | Y10S-0482/901","G16H-020/30","G16H-020/30 | H04W-004/029 | H04W-004/024 | A63B-024/00 | G01C-021/20 | G01S-019/19 | A61B-005/0205 | A61B-005/11 | G09B-019/00 | A61B-005/103 | A61B-005/00 | A63B-069/16 | H04L-009/40 | H04W-012/02 | H04W-012/06 | G16Z-099/00 | A63B-071/06 | G06F-003/16 | G06F-017/40 | G06F-011/32 | G09B-005/06 | G09B-005/02 | A61B-005/083 | G16H-020/40 | G16H-010/20 | G16H-020/10 | A45F-005/00 | A63B-022/00 | A63B-022/02 | A63B-069/00 | H04B-001/3827 | H04M-001/60 | H04W-084/18 | G16H-020/00 | G16H-020/60","","","","","","4923003004323"
"US","US","P","B2","Personalized network searching","Personalized network searching, in which a search query is received from a user, and a request is received to personalize a search result. Responsive to the search query and the request to personalize the search result, a personalized search result is generated by searching a personalized search object. Responsive to the search query, a general search result is generated by searching the general search object. The personalized search result and the general search result are provided to a client device, an advertisement is selected based at least in part upon the personalized search object, and the advertisement, the personalized search result, and the general search result are displayed.","1. A computer-implemented method performed by at least one processor, the computer implemented method comprising: identifying a user;receiving user input from the user through an interface of a client device, the user input indicating a modification to a set of favorite items for the user;in response to receiving the user input: modifying the set of favorite items at the client device, the modification to the set of favorite items at the client device initiating a synchronization process to synchronize the set of favorite items modified responsive to the user input with a server-side storage system configured to synchronize favorite items for the user with one or more other client devices, the server-side storage system remote from the client-side storage;presenting through a single interface of the client device, in response to a query, a combined search results set generated via one or more search sub-processes, the combined search results set comprising one or more favorite items from the set of favorite items synchronized for the user and comprising at least one of: one or more search results from a first global index, orone or more search results from a second global index.","24","17/740034","2022-05-09","2022-0266020","2022-08-25","11547853","2023-01-10","GOOGLE LLC","Gregory Joseph Badros | Stephen Lawrence","","","","A61N-0001/36071","A61N-0001/36071 | A61N-0001/36003 | A61N-0001/36017 | A61N-0001/36021 | A61N-0001/36057 | G06F-0016/9535 | G06F-0016/972 | G06Q-0030/0201 | G06Q-0030/0241 | Y10S-0707/99933 | Y10S-0707/99943 | Y10S-0707/99945","G06F-016/00","G06F-016/00 | A61N-001/36 | G06F-016/9535 | G06F-016/958 | G06Q-030/02","","","","","","4923002001538"
"US","US","P","B2","System and method for automated detection of neurological deficits","The disclosed embodiments provide systems and methods for predicting presence of one or more neurological deficits. The system may include a microphone, a camera, one or more memory devices storing instructions, and one or more processors configured to execute the instructions to extract audio information including a period density entropy coefficient and a mel frequency cepstral coefficient from an audio feed received from the microphone. Additionally, the instructions may cause the processor to determine position and depth information of eye movement from a video feed received from the camera and detect features of interest including facial landmarks, spatial orientation of limbs, and positional information of limb movements from the video feed. The one or more processors may further extract the features of interest from the video feed and process the extracted features of interest by aligning the extracted features of interest to a common reference.","1. A system for predicting presence of one or more neurological deficits, comprising: a microphone;a camera;one or more memory devices storing instructions; andone or more processors configured to execute the instructions to: extract audio information including a period density entropy coefficient and a mel frequency cepstral coefficient from an audio feed received from the microphone;determine position and depth information of eye movement from a video feed received from the camera;detect features of interest including facial landmarks, spatial orientation of limbs, and positional information of limb movements from the video feed;extract the features of interest from the video feed;process the extracted features of interest by aligning the extracted features of interest to a common reference;generate a set of coordinates representing the features of interest;generate intensity-based features including a histogram of oriented gradients representing the extracted features of interest;provide the audio information, the position and depth information of eye movement, the set of coordinates representing the features of interest, and the intensity-based features to a predictive model to classify the audio information, the position and depth information of eye movement, the set of coordinates representing the features of interest, and the intensity-based features; andin response to the classification, predict a presence of one or more neurological deficits.","20","15/733244","2019-01-22","2021-0093231","2021-04-01","11540749","2023-01-03","UNIVERSITY OF VIRGINIA","Omar Uribe | Mark McDonald | Andrew M. Southerland | Gustavo Rohde | Yan Zhuang","","","","A61B-0005/1128","A61B-0005/1128 | A61B-0005/4076 | A61B-0005/4803 | G06F-0003/013 | G06N-0020/00 | G06V-0040/171 | G06V-0040/176","A61B-005/11","A61B-005/11 | G06N-020/00 | A61B-005/00 | G06F-003/01 | G06V-040/16","","","","","","4923001001047"
"US","US","P","B1","Decision support tool for mitigating emergency department (ED) congestion","A technology is provided for predicting congestion or crowding of services over a future time interval, and may be utilized for forecasting congestion in a hospital emergency department. One embodiment of this technology comprises a decision support tool for resources management to prevent overcrowding and long waiting times, or for mitigating ED congestion by, for example, warning hospital managers that a significant likelihood exists of ED congestion over a future time frame, or automatically initiating mitigative actions. A time series of consecutive ED arrivals timestamps is processed to determine a presence (or absence) of positive autocorrelation or self-similarity and estimate Hurst exponent values to generate a forecast model. The forecast model is utilized to determine future ED demand.","1. A computer-implemented method for predicting congestion or crowding of services over a future time interval, comprising utilizing a processor for executing computer-readable media having instructions stored thereon that cause the processor to: receive a time series having a plurality of historical patient arrival events, each historical patient arrival event of the plurality of historical patient arrival events having a date-time stamp;from the time series, determine a plurality of epochs of a fixed length of time;determine a number of historical patient arrival events for each epoch of the plurality of epochs;determine an ensemble model of Hurst exponent values based on at least a portion of the number of historical patient arrival events for each epoch of the plurality of epochs;determine a forecast model from at least a portion of the ensemble model of the Hurst exponent values;utilize the forecast model, predicting future values of Hurst exponents to forecast future patient arrival events;based on the future values of Hurst exponents, generate a notification indicating risk of congestion or crowding over the future time interval, wherein the future time interval comprises one or more future epochs; andautomatically perform, by a computerized decision- support tool, a mitigative action based on the future values of Hurst exponents, wherein the mitigative action comprises causing a diversion of new patient arrival events, notifying individuals awaiting services of anticipated delays, or initiating triage services.","18","15/858443","2017-12-29","","","11544603","2023-01-03","CERNER INNOVATION, INC.","Douglas S. McNair","","","","G06N-0005/045","G06N-0005/045 | G06Q-0010/04 | G16H-0010/60","G16H-050/20","G16H-050/20 | G16H-050/50 | A61B-005/00 | G06N-005/04 | G06Q-010/04 | G16H-010/60","","","","","","4923001004874"
"US","US","P","B2","Web service method","Disclosed herein is a web service system and method comprising a determine best result engine useful for presenting intelligent objective decisions for at least one scenario for at least one collection of criteria across many different industries and market segments including but not limited to healthcare, manufacturing, and financial services. In some forms, the web service system also or alternatively provides automatic configuration of at least one of an array of items and services and results that a customer/user requires in preparation of completing a task. The web service system can utilize a plurality of knowledge data engines to capture and analyze information of a predetermined type to produce knowledge data for consideration by the determine best result engine.","1. A method performed by one or more computing devices of a web service system facilitating users to select a configured package of results comprising the steps of: saving, on one or more storage portions of said one or more computing devices of said web service system, a product-element table referencing one or more of products, services, and results;saving, on one or more storage portions of said one or more computing devices of said web service system, one or more configured packages of products, services, and results available from said product-element table;saving, on one or more storage portions of said one or more computing devices of said web service system, a plurality of selections representing various types of scenarios from which one or more users may choose;saving, on one or more storage portions of said one or more computing devices of said web service system, one or more configured packages matched to the plurality of selections;displaying consumer facing options on a client system of a first user;in response to displaying consumer facing options, receiving, from the first user through the client system of the first user, selections of the consumer facing options;collecting knowledge data from a plurality of contributing knowledge engines into a determine best results engine whereas the determine best result engine filters out from output of the determine best results engine at least one of: a truth statement and a truth table;utilizing the knowledge data from the plurality of contributing knowledge data engines to generate one or more intelligent configured packages of results corresponding to consumer facing option selections received by said first user;displaying on the client system of the first user, said one or more intelligent configured packages of results;receiving from the first user instructions for one or more of: processing a sale and saving information input by the first user; andactivating a computer processor of said web service system to execute said instructions from the first user.","22","16/910075","2020-06-24","2020-0320602","2020-10-08","11544756","2023-01-03","Bruce Zak","Bruce Zak","","","","G06Q-0030/0621","G06Q-0030/0621 | G06Q-0030/0623 | G06Q-0030/0641 | A61B-0017/20 | G06Q-0010/06395 | G06Q-0010/087 | G06Q-0040/06 | G06Q-0050/01","G06Q-030/06","G06Q-030/06 | A61B-017/20 | G06Q-050/00 | G06Q-040/06 | G06Q-010/06 | G06Q-010/08","","","","","","4923001005026"
"US","US","P","B1","Posture detection and correction","Various implementations disclosed herein include devices, systems, and methods for detecting and correcting the posture of users of electronic devices. In some implementations, an image capture device or other sensor is used to estimate or otherwise determine a posture of a user. As a specific example, a head mounted device (HMD) may include a camera that captures an image of the user wearing the device and the image may be analyzed to identify 3D joint locations representing the current posture of the user relative to the HMD. The user's posture is analyzed to assess whether a posture correction or change is desirable, for example, by classifying the posture as good or bad or by scoring the posture on a numerical scale. If a posture correction or change is desirable, appropriate feedback to encourage the user to adopt the posture correction or otherwise change his or her posture is identified and provided.","1. A method, comprising: at an electronic device having a processor: determining a posture of a user using an electronic device based on an image of the user captured by an image capture device on the electronic device;determining a posture condition based on the posture;providing a view of three dimensional (3D) environment comprising a barrier and visual content on the electronic device, wherein the barrier is positioned to block at least a portion of the visual content in the view based on a viewpoint of the user in the 3D environment, wherein positioning of the barrier encourages the user to correct the posture condition by changing the posture to change the viewpoint in order to continue viewing the visual content;changing the viewpoint based on the user changing the posture; andproviding an updated view of the 3D environment, wherein the at least a portion of the visual content that was blocked by the barrier is visible in the updated view based on the changed viewpoint.","21","16/732790","2020-01-02","","","11544865","2023-01-03","APPLE INC.","Daniel Kurz","","","","G06T-0007/70","G06T-0007/70 | G06F-0003/14 | G06T-2207/20084 | G06T-2207/30196","G06T-007/246","G06T-007/246 | G06T-007/70 | A61B-005/11 | G06F-003/14","","","","","","4923001005135"
"US","US","P","B2","Camera calibration method using human joint points","A novel multiple camera calibration algorithm uses human joint points for matched key points. A recent machine-learning based human joint detector provides joint positions with labels (e.g. left wrist, right knee, and others). In single person situation, it directly provides matched key points between multiple cameras. Thus, the algorithm does not suffer a key-point matching problem, even in a very sparse camera configuration, which is challenging in the traditional image feature-based method. This algorithm provides easy setup for a multiple camera configuration for marker-less pose estimation.","1. A method comprising: setting a plurality of cameras;moving a target around; andperforming camera calibration of the plurality of cameras by: collecting human joint positions in 2D images of each of the cameras by using a joint detector, wherein collecting the human joint positions includes image processing to determine approximately adjoining, moving segments and an approximately, relatively stationary joint point, wherein determining the approximately adjoining, moving segments and the approximately, relatively stationary joint point includes analyzing movement of segments to determine if an amount of movement of a segment is above a threshold, and when the amount of movement of the segment is above the threshold, then the segment is a moving part, and if the amount of movement of a point is below the threshold, then the point is a joint;fixing a gauge;estimating camera positions and orientations by minimizing a summation of triangulation errors; andfitting a floor plane.","21","16/594385","2019-10-07","2021-0104069","2021-04-08","11544872","2023-01-03","SONY GROUP CORPORATION","Daisuke Tahara | Nobuhiro Tsunashima","","","","G06T-0007/80","G06T-0007/80 | A61B-0005/107 | G01B-0021/042 | G06F-0003/017 | G06F-0003/0484 | G06T-0007/0012 | G06T-0007/251 | G06T-0007/73 | G06T-2207/10028","G06T-007/80","G06T-007/80 | G06F-003/01 | G01B-021/04 | G06F-003/0484 | G06T-007/246 | G06T-007/73 | A61B-005/107 | G06T-007/00","","","","","","4923001005142"
"US","US","P","B2","Case display apparatus, case displaying method, and storage medium background to seamlessly present diagnostic images captured at different times for comparative reading","A user input obtainer receives an image movement instruction including identification information specifying a position shift or an image capture time shift to be performed and also including a displacement amount. When the identification information specifies the position shift, a slice position selector determines a tomographic image at a destination of the position shift based on the displacement amount from a set of tomographic images captured at the same time. On the other hand, when the identification information specifies the image capture time shift, the image capture time selector determines a tomographic image at a destination of the shift based on the displacement amount from sets of tomographic images that are identical to each other in terms of a patient, an examination portion, and a modality. A displaying image obtainer reads out the determined tomographic image from an image storage device and gives it to a display information generator.","1. A case display apparatus, comprising: at least one memory configured to store a program; andat least one processor configured to execute the program and control the case display apparatus to:generate display information displayed on a display device, including display information associated with a first tomographic image set and a second tomographic image set;perform position registration by determining images in the second tomographic image set having a highest correlation to images in the first tomographic image set;when the display information includes a first tomographic image, receive an image movement instruction including identification information and a displacement amount, the identification information indicating a slice position shift or an image capture time shift, wherein there is no medical image except the first tomographic image being displayed on the display when a user gives the image movement instruction;when the identification information specifies that the slice position shift is to be performed, determine a second tomographic image at a destination of the slice position shift from the first tomographic image set including the first tomographic image, based on a position movement amount corresponding to the displacement amount, the first tomographic image set being a first plurality of tomographic images;when the identification information specifies that the image capture time shift is to be performed, select, by an image capture time selector, a third tomographic image from the second tomographic image set, the second tomographic image set being a second plurality of tomographic images, a target person of the first tomographic image and target persons of the second tomographic image set being identical, examination portions captured in the first tomographic image set and examination portions captured in the second tomographic image set being identical, a modality for the first tomographic image and modalities for the second tomographic image set being identical, and image capture times of the first tomographic image set and image capture times of the second tomographic image set being different, the third tomographic image selected from the second tomographic image set having a highest correlation with the first tomographic image from the first tomographic image set, an image capture time of the third tomographic image being shifted from an image capture time of the first tomographic image based on a time movement amount corresponding to the displacement amount, wherein the difference between the image capture time of the third tomographic image and the image capture time of the first tomographic image is proportional to the displacement amount; andread out the second tomographic image or the third tomographic image from an image storage device, and that gives a read out tomographic image to the display information generator;add importance information to a part of tomographic images stored in the image storage device,and when the identification information specifies that the image capture time shift is to be performed and there is a fourth tomographic image added with the importance information indicating that the fourth tomographic image is an important image between the first tomographic image and the third tomographic image, read out the fourth tomographic image with the importance information from the image storage device for display instead of the third tomographic image selected by the image capture time selector.","6","16/654066","2019-10-16","2020-0046451","2020-02-13","11538572","2022-12-27","PANASONIC INTELLECTUAL PROPERTY MANAGEMENT CO., LTD.","Kazuki Kozuka | Kenji Kondo | Kazutoyo Takata","2013-164154 | 2014-127893","JP | JP","2013-08-07 | 2014-06-23","G16H-0030/20","G16H-0030/20 | A61B-0006/03 | A61B-0006/032 | A61B-0006/463 | A61B-0006/469 | A61B-0090/37 | G06F-0003/04842 | G06F-0003/04845 | G06F-0003/04847 | G06T-0007/0016 | A61B-0005/055 | A61B-2090/374 | A61B-2090/378 | A61B-2090/3762 | A61B-2560/0487 | G06T-2200/24 | G06T-2207/10081 | G06T-2207/20108","A61B-006/03","A61B-006/03 | A61B-006/00 | G06F-003/04842 | G06F-003/04845 | G06T-007/00 | A61B-005/055 | A61B-090/00 | G06F-003/04847 | G16H-030/20","","","","","","4922052005421"
"US","US","P","B2","Device for controlling an operating state of at least one medical device in a medical data network as well as medical device for a medical data network","A device, such as a data network device for a medical data network, controls an operating state of a second medical device in such a way that by sending a request message, the second medical device is prompted to change over into the operating state of the combined therapy and hence into an operating state in which its actuator is controlled as a function of an information signal of the first medical device. The information signal is based on physiological measured values. As a result, a clinician does not have to configure the second medical device himself/herself directly on site at the medical device by inputting an input signal, but this can be carried out by the device.","1. A state controlling device for controlling an operating state of at least one medical device for a combined therapy of a patient by the interaction of at least two medical devices in a medical data network, wherein a first medical device is configured to detect physiological measured values of a patient in an operating state of a combined therapy, further to generate an information signal on the basis of the physiological measured values as well as to provide the information signal via the data network, and wherein a second medical device has an actuator for physiologically influencing the patient and is further configured to select at least one operating parameter of the actuator as a function of the information signal of the first medical device, which is received via the data network, in the operating state of the combined therapy, wherein the state controlling device comprises: a state controlling device structure for controlling the operating state of at least one medical device, the state controlling device structure comprising: a data network interface;an input unit for inputs of a user;an optical display unit for outputting information to the user; anda control unit configured:to determine a current joint assignment of the first medical device and of the second medical device to a common group in the data network on the basis of data messages received via the data network interface;to display to the user the current joint assignment of the first and second medical devices to the group via the display unit in case of a positive result of the determination; andupon receipt of a confirmation signal from the input unit, to send a request message to the second medical device, which indicates a request to activate the operating state of the combined therapy in the second medical device, wherein the combined therapy is configured to be carried out via at least the first medical device and the second medical device.","20","15/850559","2017-12-21","2018-0177398","2018-06-28","11529052","2022-12-20","DR?GERWERK AG & CO. KGAA","G?tz Kullik | Stefan Schlichting | Volker Mildner | Joshua Abell","","","","A61B-0005/0022","A61B-0005/0022 | G16H-0040/60 | G16H-0040/67 | A61B-0005/0004 | A61M-0005/1723 | G16H-0020/10","G06Q-040/08","G06Q-040/08 | A61B-005/00 | G16H-040/60 | G16H-040/67 | A61M-005/172 | G16H-020/10","","","","","","4922051000733"
"US","US","P","B2","Systems and methods for emotional-imaging composer","Systems and methods for Emotional-Imaging Composer are disclosed. The method may include recording a real-time biosignal from a plurality of biosignal sensors. The method may further include determining an emotion that is associated with the real-time biosignal. The method may further include outputting a display feature corresponding to the emotion, wherein the display feature is a lighting effect on a graphical user interface.","1. A method for an Emotional-Imaging Composer, the method comprising: recording a real-time biosignal from a plurality of biosignal sensors;determining an emotion that is associated with the real-time biosignal;outputting a display feature corresponding to the emotion, wherein the display feature is a lighting effect on a graphical user interface; anddetermining a second emotion that is associated with a second real-time biosignal at a second time interval, wherein determining the second emotion comprises determining, on a design quadrant, a location of the second emotion.","17","17/408209","2021-08-20","2022-0075450","2022-03-10","11531394","2022-12-20","EMOTIONAL IMAGING, INC.","Jordan Deitcher | Mitchel Benovoy | Johnty Wang","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/165 | G06N-0020/00 | G06T-0011/001 | B25J-0011/001 | G05B-2219/36488 | G06F-2203/011 | G06T-2200/24 | G06T-2210/56","G06F-003/01","G06F-003/01 | G06T-011/00 | G06N-020/00 | A61B-005/16 | B25J-011/00","","","","","","4922051003057"
"US","US","P","B2","Puncture planning apparatus and puncture system","A puncture planning apparatus has: a simulation unit that simulates movement of an organ and a puncture needle by simulation using an organ model; and a planning unit that plans, based on the simulation result, how to move the puncture needle when an actual organ is punctured. The simulation unit executes a plurality of times of the simulation of an operation to advance the puncture needle while correcting an angle of the puncture needle so as to follow the movement of the target segment due to deformation of the organ, conditions of an advancement speed of the puncture needle are changed for each of the plurality times of the simulation, and the planning unit performs planning using the best simulation result out of the plurality of simulation results acquired under different conditions of the advancement speed.","1. A puncture planning apparatus comprising: a processor configured to execute a program to cause the puncture planning apparatus to function as:a simulation unit that simulates movement of an organ and a puncture needle when the puncture needle is inserted toward a target segment inside the organ, by simulation using an organ model; anda planning unit that plans, based on a result of the simulation, how to move a puncture needle when an actual organ is punctured, and outputs a planning result,wherein the simulation unit executes a plurality of times of the simulation of an operation to advance the puncture needle while correcting a direction of the puncture needle so as to follow a movement of the target segment due to deformation of the organ, wherein conditions of an advancement speed of the puncture needle are changed for each of the plurality times of the simulation,wherein the planning unit performs planning using a simulation result that meets predetermined criteria out of the plurality of simulation results acquired under different conditions of the advancement speed of the puncture needle,wherein the simulation unit performs speed adjustment in the simulation, so as to reduce the advancement speed of the puncture needle in accordance with a puncture reaction force, which is a force that the puncture needle receives from the organ, and executes the plurality of times of simulation while changing, as the condition, a parameter which determines a reduction rate of the advancement speed in the speed adjustment, andwherein the advancement speed of the puncture needle is determined using a speed gain Kf, a value of Kf having the following correspondence with the puncture reaction force Fn:A) Kf=1, when Fn is equal to or less than Fnmin;B) Kf decreases from 1 to Kfmin as Fn increases, when Fn is greater than Fnmin; andC) Kf=Kfmin, when Fn is greater than a value in which Kf reaches Kfmin,where Fnmin is a predetermined threshold and Kfmin is a predetermined minimum value of the speed gain.","15","16/281171","2019-02-21","2019-0188234","2019-06-20","11531726","2022-12-20","CANON KABUSHIKI KAISHA","Kiyoshi Takagi","2014-142425 | 2015-083663","JP | JP","2014-07-10 | 2015-04-15","G06F-0017/10","G06F-0017/10 | A61B-0017/3403 | A61B-0034/10 | A61B-0034/30 | G06F-0030/00 | G16H-0050/50 | G16Z-0099/00 | A61B-0005/065 | A61B-0008/0841 | A61B-0017/3478 | A61B-0034/20 | A61B-0090/11 | A61B-2017/3413 | A61B-2034/104 | A61B-2034/107 | G01R-0033/287","G06F-030/00","G06F-030/00 | G06F-017/10 | A61B-017/34 | A61B-034/30 | A61B-034/10 | G16H-050/50 | G16Z-099/00 | A61B-008/08 | A61B-090/11 | A61B-005/06 | A61B-034/20 | G01R-033/28","","","","","","4922051003388"
"US","US","P","B2","Marker element and application method with ECG","The present invention relates to a method to be performed by a computing device part of or coupled to an ECG device for applying a marker element in a process of determining positions of a set of ECG electrodes as placed on a human torso relative to a 3D model of a body, preferably a torso model of the torso of the human body. The method includes receiving an imaging information recording relating to the human body from an optical imaging device, preferably a 3D imaging device. The optical imaging information includes imaging information of the exterior of the body, such as imaging information of the exterior of the torso, and imaging information of the marker element. The method also includes performing an image recognition on the imaging information for obtaining a presence determination, preferably a positive or negative determination, of the marker element in the imaging information.","1. A method performed by a computing device part of or coupled to an ECG device for applying a marker element in a process of determining positions of a set of ECG electrodes as placed on a human torso relative to a 3D model of a human body, the method comprising: receiving an imaging information recording relating to the human body from an optical 3D imaging device, the optical imaging information comprising:imaging information of the exterior of the human torso,imaging information of the marker element, the marker element being arranged in an area comprising an actual position of placement thereof on the human body, andimaging information of the ECG electrodes as placed on the human torso, andperforming an image recognition on the imaging information for obtaining a positive or negative presence determination of the marker element in the imaging information.","26","16/651804","2018-11-27","2020-0242802","2020-07-30","11532101","2022-12-20","PEACS INVESTMENTS B.V.","Peter Michael van Dam | Eelco Matthias van Dam | Samir Alioui","2019635","NL","2017-09-27","G06T-0007/75","G06T-0007/75 | A61B-0005/0062 | A61B-0005/0064 | A61B-0005/0077 | A61B-0005/25 | A61B-0005/318 | A61B-0005/684 | A61B-0005/7405 | A61B-0090/98 | G06K-0007/10722 | G06K-0007/1413 | G06K-0007/1417 | G06T-0007/0012 | G06T-0007/73 | G06T-2207/10024 | G06T-2207/10028 | G06T-2207/20021 | G06T-2207/30048 | G06T-2207/30168 | G06T-2207/30204","G06K-007/14","G06K-007/14 | G06T-007/73 | A61B-090/98 | A61B-005/00 | G06K-007/10 | G06T-007/00 | A61B-005/25 | A61B-005/318","","","","","","4922051003762"
"US","US","P","B2","Hyperspectral scanning to determine skin health","A system, method, and computer readable media are provided for obtaining a first set of skin data from an image capture system including at least one ultraviolet (UV) image of a user's skin. Performing a correction on the skin data using a second set of skin data associated with the user. Quantifying a plurality of skin parameters of the user's skin based on the first skin data, including quantifying a bacterial load. Quantifying the bacterial load by applying a brightness filter to isolate portions of the at least one UV image containing fluorescence, applying a dust filter, identifying portions of the at least one UV image that contain fluorescence due to bacteria, and determining a quantity of bacterial load in the users skin. Determining, using a machine learning model, an output associated with a normal skin state of the user and a current skin state of the user.","1. A system for assessing skin health, comprising: one or more processors;one or more tangible, non-transitory media operably connectable to the one or more processors and storing instructions that, when executed, cause the one or more processors to perform operations comprising: obtaining, from an imaging system, first skin data associated with a user, wherein the first skin data comprises an ultraviolet (UV) image of the user'ss skin;performing a correction on the first skin data, based on second skin data associated with the user, the second skin data having been collected at a time prior to obtaining the first skin data, wherein the second skin data is stored in a repository;determining features associated with anatomy present in the first skin data;segmenting the first skin data into a plurality of portions based on the features associated with the anatomy;quantifying, separately for each portion of the user'ss skin, a plurality of skin parameters of the user'ss skin, wherein the at least one of the plurality of skin parameters is a bacterial load, the quantifying comprising: applying a brightness filter that isolates portions of at least one UV image that contain fluorescence,applying a dust filter that removes portions of the at least one UV image that contain fluorescence caused by dust particles, wherein the dust filter comprises an expectation maximization algorithm and a two-component Gaussian mixture model,applying a loopy belief propagation algorithm that isolates portions of the at least one UV image that contain fluorescence due to bacteria, anddetermining a bacterial load quantity associated with the user'ss skin based on an output of the loopy belief propagation algorithm;determining, based on providing the plurality of quantified skin parameters as input features to a machine learning model, an output associated with a normal skin state of the user and a current skin state of the user; andproviding, for display on a user computing device, data indicating the output.","18","16/705676","2019-12-06","2021-0174965","2021-06-10","11532400","2022-12-20","X DEVELOPMENT LLC","Anupama Thubagere Jagadeesh | Brian Lance Hie","","","","G16H-0050/30","G16H-0050/30 | A61B-0005/0071 | A61B-0005/442 | A61B-0005/443 | A61B-0005/445 | A61B-0005/486 | A61B-0005/7267 | G06N-0003/04 | G06N-0003/08 | G06Q-0030/0631 | G06T-0005/40 | G06T-0005/50 | G06T-0007/0016 | G06T-0007/11 | G06T-0007/44 | G06T-0007/90 | G16H-0010/60 | G16H-0020/00 | A61B-2576/02 | G06N-0003/0445 | G06T-2207/10064 | G06T-2207/20084 | G06T-2207/30088","G16H-050/00","G16H-050/00 | G16H-050/30 | G06Q-030/06 | G16H-010/60 | G06N-003/08 | G06N-003/04 | G06T-007/44 | G06T-005/40 | G06T-007/00 | G06T-005/50 | G06T-007/11 | G06T-007/90 | A61B-005/00 | G16H-020/00","","","","","","4922051004059"
"US","US","P","B2","Methods and systems for providing an episode of care","Systems and methods for determining a care plan for a patient are disclosed. Data from a one or more databases are received or retrieved and used to determine a preferred care plan used to perform a surgical procedure or otherwise treat a patient. The data may include data pertaining to a patient, a healthcare professional, a healthcare facility, an implant, economic data, simulation data, imaging data, and/or the like. The data may be used to determine a plan that provides a positive outcome and patient satisfaction. The data may be updated over time or in real time to improve or refine the determination of care plans for the current patient or other patients.","1. A system for optimizing outcomes and patient satisfaction during an episode of care comprising: one or more processors; anda non-transitory processor-readable storage medium in operable communication with the one or more processors, comprising one or more instructions that, when executed, cause the one or more processors to: obtain, from one or more sources, case plan data comprising at least one of: patient data, healthcare professional data, facility data, or healthcare economy data,perform, using a neural network, a simulation for a new episode of care based on the case plan data, wherein the neural network is trained using at least a first training set comprising historical case data from a database,generate, based on the simulation, a predictor equation, wherein the predictor equation comprises one or more weighting values associated with at least one implant component and an associated patient anatomy,receive, from a user input device, at least one user input associated with the new episode of care,modify, based on the at least one user input, the predictor equation,determine, based on the modified predictor equation, an optimized case plan comprising a volumetric representation of bone to be removed from the associated patient anatomy, andtrain the neural network using a second training set comprising the historical case data and one or more of the case plan data, the optimized case plan, and outcome data associated with the new episode of care.","18","16/847183","2020-04-13","2020-0243199","2020-07-30","11532402","2022-12-20","SMITH & NEPHEW, INC. | SMITH & NEPHEW ORTHOPAEDICS AG | SMITH & NEPHEW ASIA PACIFIC PTE. LIMITED","Daniel Farley | Sied W. Janna | Scott K. Laster | Zachary C. Wilkinson","","","","G16H-0050/50","G16H-0050/50 | A61B-0005/4528 | A61B-0034/10 | A61F-0002/46 | G06N-0003/0427 | G06N-0003/08 | G06Q-0030/0201 | G16H-0010/20 | G16H-0010/60 | G16H-0020/00 | G16H-0020/40 | G16H-0040/20 | G16H-0050/20 | G16H-0050/70 | G16H-0070/20 | A61B-2034/108 | A61F-2002/4633","G16H-050/50","G16H-050/50 | G16H-040/20 | G16H-010/60 | G16H-050/70 | G16H-050/20 | G16H-020/00 | G16H-010/20 | G16H-070/20 | A61F-002/46 | G06N-003/04 | G06N-003/08 | G06Q-030/02 | A61B-034/10 | G16H-020/40 | A61B-005/00","","","","","","4922051004061"
"US","US","P","B2","Arousal state estimation apparatus and arousal state estimation method","An arousal state estimation apparatus includes: a feature value acquisition unit acquiring a plurality of types of feature values regarding an arousal state of a human body from physiological data obtained by measuring the human body; and an estimation unit estimating the arousal state of the human body by using a principal feature value that is some type among the plurality of types of feature values. In a case where the principal feature value is unacquirable due to a defect of the physiological data, the estimation unit estimates the arousal state of the human body by using a different type of feature value than the principal feature value among the plurality of types of feature values acquired by the feature value acquisition unit instead of the unacquirable principal feature value.","1. An arousal state estimation apparatus comprising: a feature value acquisition unit acquiring a plurality of types of feature values regarding an arousal state of a human body from physiological data obtained by measuring the human body; andan estimation unit estimating the arousal state of the human body by using principal feature values, wherein the principal feature values are some types among the plurality of types of feature values,wherein, in a case where one of the principal feature values is unacquirable due to a defect of the physiological data, the estimation unit estimates the arousal state of the human body by using a substitute feature value instead of the unacquirable principal feature value, wherein the substitute feature value is a different type of feature value than the principal feature values among the plurality of types of feature values acquired by the feature value acquisition unit.","8","15/931735","2020-05-14","2020-0367797","2020-11-26","11523760","2022-12-13","HONDA MOTOR CO., LTD. | KYUSHU UNIVERSITY, NATIONAL UNIVERSITY CORPORATION","Tadahiro Kubota | Tomohiro Imai | Shigekazu Higuchi | Kosuke Okusa | Hisao Yoshida | Yuka Egashira | Yuki Nishimura","2019-095167","JP","2019-05-21","A61B-0005/165","A61B-0005/165 | G06F-0003/015 | G06F-0003/017 | G06K-0009/6232 | G06F-2203/011","A61B-005/16","A61B-005/16 | G06F-003/01 | G06K-009/62","","","","","","4922050000813"
"US","US","P","B2","Energy transfer system for spinal implants","An energy transfer system includes a spinal implant having an antenna, an antenna extender attached to a portion of the spinal implant in proximity to the antenna, and a reader device configured to send energy to the spinal implant via the antenna extender. The antenna extender extends away from the spinal implant. The spinal implant is configured to be positioned within a spinal area of a patient.","1. An energy transfer system comprising: a spinal implant having: an antenna,an antenna extender attached to a portion of the spinal implant in proximity to the antenna, wherein the antenna extender extends away from the spinal implant, wherein the antenna extender comprises a flexible, dielectric material;a pedicle screw having a set screw and an anchoring member;a second set screw and a second anchoring member, the second set screw having a second antenna extender attached to it; anda reader device configured to send energy to the spinal implant via the antenna extender,wherein the spinal implant is configured to be positioned within a spinal area of a patient.","18","16/733738","2020-01-03","2021-0205046","2021-07-08","11517398","2022-12-06","WARSAW ORTHOPEDIC, INC.","Newton Metcalf | Arjun S. Kurian | Kevin T. Foley","","","","A61B-0090/98","A61B-0090/98 | A61B-0017/7032 | A61B-0017/7082 | H02J-0050/10 | H04B-0005/0037 | H04B-0005/0056 | H04B-0005/0081 | G06K-0007/10297 | H05K-0001/0277","H04B-005/00","H04B-005/00 | A61B-005/00 | A61B-090/98 | A61B-017/70 | H02J-050/10 | G06K-007/10 | H05K-001/02","","","","","","4922049001165"
"US","US","P","B2","Inventory system and methods of using the same","The present disclosure is also directed to a system for managing inventory within a dispenser that is configured to maintain a known inventory and is configured to transmit inventory values from the dispenser so that other users can review the inventory values. Also, the dispenser is configured to be accessed by users locally and through an internet connection so that the inventory can be reviewed.","1. A dispenser, the dispenser comprising: a movable door, movable between an open position, and a closed position, when the movable door is in the open position, the dispenser is configured to receive a medical product into an interior space of the dispenser, wherein the medical product comprises an identifier;a holder in the interior space, the holder configured to store the medical product in a fixed location;a dispensing area configured to hold a medical product, wherein the dispenser further comprises an openable door extending over the dispensing area;a transport mechanism configured to move the medical product from the holder to the dispensing area, wherein the transport mechanism comprises a first reader configured to scan the identifier;a processor;an error area, wherein the error area is configured to hold at least one medical product;an error door extending over the error area; andan electronic storage device, wherein the electronic storage device is configured to store a location of the holder within the dispenser, at least one of the first reader and a second reader, the second reader configured to read the identifier, being configured to read the identifier of the medical product upon insertion of the medical product into the dispenser, or configured to read the identifier of a container storing one or more medical products, and store a quantity of the medical product in the electronic storage device as an available inventory of medical product, and whereinthe first reader is configured to read the identifier of the medical product upon removal of the medical product from the holder and transmit to the electronic storage device to subtract the removed medical product from the available inventory of the medical product.","26","16/887908","2020-05-29","2020-0377300","2020-12-03","11518615","2022-12-06","ETHICON, INC.","Matthew Chila | Jonathan Addeo Syby | Allen Keith On | David Mickle Wade","","","","B65G-0001/1371","B65G-0001/1371 | A61B-0090/90 | B65G-0001/0485 | G06K-0007/10366 | G06K-0007/1413 | G06K-0007/1417 | G06Q-0010/0875 | G06Q-0030/014 | G16H-0040/20 | G16H-0040/67 | H04N-0005/76 | A61B-0017/06114","B65G-001/137","B65G-001/137 | A61B-090/90 | G16H-040/67 | G16H-040/20 | B65G-001/04 | G06K-007/10 | G06K-007/14 | G06Q-010/08 | G06Q-030/00 | H04N-005/76 | A61B-017/06","","","","","","4922049002373"
"US","US","P","B2","Interactive 3D cursor for use in medical imaging","An interactive 3D cursor facilitates selection and manipulation of a three-dimensional volume from a three-dimensional image. The selected volume image may be transparency-adjusted and filtered to remove selected tissues from view. Qualitative and quantitative analysis of tissues in a selected volume may be performed. Location indicators, annotations, and registration markers may be overlaid on selected volume images.","1. A method for displaying a structure in a head display unit, the method comprising: obtaining image data representing the structure in a three-dimensional (3D) image space;obtaining an initial representation of a 3D cursor in the 3D image space, the 3D cursor having a 3D shape with an initial position in the 3D image space, and the 3D cursor containing the structure;obtaining an initial viewing angle for orienting the 3D cursor and the structure in the 3D image space;obtaining an initial left eye viewpoint for a left eye and an initial right eye viewpoint for a right eye for viewing the 3D cursor and the structure, wherein the initial right eye viewpoint is offset from the initial left eye viewpoint;displaying, by the head display unit, a left eye image for the left eye based on the initial left eye viewpoint, the initial viewing angle, the 3D cursor, and the structure, and a right eye image for the right eye based on the initial right eye viewpoint, the initial viewing angle, the 3D cursor, and the structure;obtaining an input to apply a rotation of the 3D cursor containing the structure about the 3D cursor'ss center;responsive to the input, generating an updated viewing angle of the 3D cursor and the structure contained within the 3D cursor to reorient the 3D cursor and the structure in the 3D image space based on the rotation; anddisplaying, by the head display unit, an updated left eye image for the left eye based on the initial left eye viewpoint, the updated viewing angle, the 3D cursor, and the structure, and an updated right eye image for the right eye based on the initial right eye viewpoint, the updated viewing angle, the 3D cursor and the structure.","23","17/339341","2021-06-04","2021-0294435","2021-09-23","11520415","2022-12-06","D3D TECHNOLOGIES, INC.","Kathleen M. Douglas | Robert E. Douglas | David Byron Douglas","","","","G06F-0003/0346","G06F-0003/0346 | A61B-0005/489 | A61B-0006/466 | G06F-0003/03543 | G06F-0003/04812 | G06F-0003/04842 | G06T-0007/0012 | G06T-0007/0014 | G06T-0007/62 | G06T-0019/00 | G06T-0019/006 | H04N-0013/183 | H04N-0013/344 | G06T-2207/10072 | G06T-2207/10081 | G06T-2207/10104 | G06T-2207/20104 | G06T-2207/30096 | G06T-2207/30204 | G06T-2210/41","G06F-003/0346","G06F-003/0346 | G06F-003/04812 | G06F-003/04842 | G06F-003/0354 | G06T-019/00 | A61B-006/00 | G06T-007/62 | G06T-007/00 | H04N-013/344 | H04N-013/183 | A61B-005/00","","","","","","4922049004160"
"US","US","P","B2","Communication control for a surgeon controlled secondary display and primary display","A tiered multi-display control scheme may provide various communication control options for a surgeon-controlled secondary display and primary operating room display. A powered surgical tool may be in operative communication with a local display and at least one main monitor within the operating room outside the sterile field for displaying multiple data and/or imaging sources. The local display may be interactable by the surgeon within the sterile field. The display outside the sterile field may show an image of an aspect of the laparoscopic scope and may contain superimposed other data streams from other devices besides the scope. The secondary display could be used to direct from its displayed content up onto the primary display or remove it from the display. The added or removed data streams may be originated from the secondary display, passed through the secondary display, or be networked with the secondary display.","1. A powered surgical instrument comprising: a communication module operably connected to a first display inside a surgical sterile field and a second display outside of the surgical sterile field; anda processor configured to: obtain a multi-display control parameter;identify a current multi-display control mode based on the multi-display control parameter;determine, based on the current multi-display control mode, whether to generate visualization control data configured to control display content on the second display outside of the surgical sterile field in response to a user indication at the first display inside the surgical sterile field; andinteract with the first display inside the surgical sterile field and the second display outside of the surgical sterile field based on the determination.","20","17/062507","2020-10-02","2022-0104889","2022-04-07","11510743","2022-11-29","CILAG GMBH INTERNATIONAL","Frederick E. Shelton, IV | Jason L. Harris","","","","A61B-0034/25","A61B-0034/25 | A61B-0017/072 | A61B-0017/320092 | A61B-0018/14 | G06F-0003/1423 | G16H-0040/63 | A61B-0005/021 | A61B-0034/37 | A61B-2017/00039 | A61B-2017/00199 | A61B-2017/00221 | A61B-2017/00398 | A61B-2017/07235 | A61B-2017/07285 | A61B-2017/32007 | A61B-2017/320084 | A61B-2017/320095 | A61B-2017/320097 | A61B-2018/0063 | A61B-2018/00541 | A61B-2018/00565 | A61B-2018/00601 | A61B-2018/00994 | A61B-2034/252 | A61B-2034/256 | A61B-2034/258 | A61B-2218/002 | A61B-2218/007 | A61B-2218/008 | G06F-0003/14","G06F-003/147","G06F-003/147 | A61B-034/00 | G16H-040/63 | A61B-017/072 | A61B-017/32 | A61B-018/14 | G06F-003/14 | A61B-034/37 | A61B-005/021 | A61B-017/00 | A61B-018/00","","","","","","4922048001127"
"US","US","P","B2","Modeling method for screening surgical patients","A modeling method for screening surgical patients, used in analysis modeling for heart rate variability (HRV). Low-cost, portable and wearable signal acquisition equipment is utilized to acquire an electrocardiography (ECG) signal of an epileptic 24 hours before surgery; a multiscale entropy (MSE) of the ECG is calculated by means of a programmed HRV analysis method, wherein characteristic parameters representing heart rate complexity are extracted on the basis of an MSE curve, and a medical refractory epileptic suitable for vagus nerve stimulation (VNS) surgery is accurately and efficiently screened, thus avoiding unnecessary expenditures and avoiding delaying an optimal opportunity for treatment. Meanwhile, the curative effects of the VNS treatment may be wholly improved by means of clearly selecting VNS surgical indication patients according to the characteristic parameters of the MSE complexity of the ECG.","1. A method for screening surgical patients with Vagus Nerve Stimulation (VNS) indications, wherein the method comprises: collecting electrocardiography (ECG) data in vitro from patients with vagus nerve-related diseases;selecting sinus normal-to-normal (NN) interval data using the ECG data;performing a multiscale entropy (MSE) calculation on the selected sinus normal-to-normal (NN) interval data, wherein performing the MSE calculation comprises: performing coarse grained processing on the normal-to-normal (NN) interval data {x1, . . . , xi, . . . , xN}, to obtain reconstructed sequences using an equation 1≤j≤N/τ, where τ is a given scale factor, τ=1, 2, 3, . . . q; calculating a sample entropy for each reconstructed sequence with a different scale factor τ; anddrawing a MSE curve of the sample entropy with respect to the different scale factors with the scale factor as a horizontal axis and the sample entropy as a vertical axis;extracting parameters representing heart rate complexity using the MSE curve, wherein extracting the parameters representing heart rate complexity further includes: for the MSE curve and the different scale factors including scale n1, scale n2, and scale n3, wherein scale n1<scale n2<scale n3, obtaining at least slope n1 by linearly fitting points of the curve that corresponds with the scale 1 to scale n1, wherein scale n3<40;dividing the curve from the scale n2 to the scale n3 into a plurality of segments; andcalculating area parameters for each area encompassed by each segment of the plurality of segments of the curve to obtain the parameters representing heart rate complexity;setting thresholds for the parameters representing heart rate complexity, the thresholds indicating patients who are suitable for VNS implantation surgery and those who are not suitable for VNS implantation surgery;constructing a model for patients with vagus nerve-related diseases based on heart rate variability as represented by the extracted parameters and the thresholds; andidentifying patients who are suitable for VNS implantation surgery using the model;providing the VNS implantation surgery to the identified patients.","5","16/086428","2017-08-04","2019-0090803","2019-03-28","11504052","2022-11-22","BEIJING PINS MEDICAL CO., LTD | TSINGHUA UNIVERSITY","Luming Li | Hongyun Liu | Zhao Yang","2016-10708479","CN","2016-08-23","A61B-0005/4094","A61B-0005/4094 | A61B-0005/0245 | A61B-0005/02405 | A61B-0005/349 | A61B-0005/7278 | G06Q-0010/04 | G06Q-0050/22 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | G16H-0050/50 | A61B-0005/316 | A61B-0005/339 | A61B-0005/341 | A61B-0005/369 | A61B-0005/4035 | A61B-2505/05","A61B-005/00","A61B-005/00 | A61B-005/0245 | G16H-050/50 | G16H-040/63 | G06Q-050/22 | G16H-050/30 | G06Q-010/04 | G16H-050/20 | A61B-005/349 | A61B-005/024 | A61B-005/316 | A61B-005/339 | A61B-005/341 | A61B-005/369","","","","","","4922047001021"
"US","US","P","B1","Brainwave compatibility","A system and method for identifying individuals that compatibly contribute to high-performing teams is disclosed. Teams may range in size and complexity from a two-person team of roommates to many hundreds in a commercial product development team. Candidates for new or existing teams are identified by matching brainwave response of candidates to the brainwave signature of high-performing, compatible teams. The stimuli of stimulus datasets are rapidly presented to candidates and sensed by any of the five human senses. The signature of a team type is a set of brainwave response characteristics extracted from one or more high-performing, compatible exemplar teams presented with the stimulus dataset and is also distinctly different from other team types or the general population. Closeness of fit between a candidate's brainwave response and the signature of the exemplar team provides an indication of the likely compatible fit and contribution of a candidate to a high-performing team.","1. A method for determining the compatibility of one or more candidates for potential association with a team-of-interest comprised of two or more persons that share in at least one of an interests, objective and task, the method comprising: a. developing a stimulus dataset composed of one or more stimuli to elicit one or more of a psychophysiologic response from members of said team-of-interest and one or more of said candidate for association with said team-of-interest to corroborate one or more psychological characteristics of said team-of-interest;b. presenting stimuli of said stimulus dataset to each of one or more selected high-performing persons of said team-of-interest by one or more of a sensory presentation device to produce a psychophysiological response for each of said high-performing members of said team-of-interest;c. presenting stimuli of said stimulus dataset to each of one or more selected low-performing persons of said team-of-interest by one or more of said sensory presentation devices to produce a psychophysiological response of said low-performing members of said team-of-interest;d. training a machine learning algorithm with said psychophysiological response for each of said high-performing members of said team-of-interest and said psychophysiological response for each of said low-performing members of said team-of-interest;e. extracting a psychophysiological response signature common to high-performing members of said team-of-interest using said machine learning algorithm;f. extracting a psychophysiological response signature common to low-performing persons of said team-of-interest using said machine learning algorithm;g. developing a matching algorithm using machine learning techniques to assess the degree of similarity of psychophysiological responses;h. presenting stimuli of said stimulus dataset to each of said one or more candidates for potential association with said team-of-interest by one or more of said sensory presentation devices to produce a psychophysiological response for each of said one or more candidates;i. computing a compatibility score using said matching algorithm for each of said one or more candidates by comparing said psychophysiological response of said one or more candidates to said psychophysiological response signature common to high-performing persons of said team-of-interest and with said psychophysiological response signature common to low-performing persons of said team-of-interest; andj. determining if said compatibility score of said one or more candidates exceeds a selection threshold score correlated with high-performing persons in said team-of-interest.","9","17/004900","2020-08-27","","","11507924","2022-11-22","Robert William Kocher | Loran Dean Ambs","Robert William Kocher | Loran Dean Ambs","","","","G06Q-0010/1053","G06Q-0010/1053 | A61B-0005/0057 | A61B-0005/0816 | A61B-0005/163 | A61B-0005/167 | A61B-0005/369 | A61B-0005/742 | A61B-0005/7405 | G06Q-0010/0631 | G16H-0040/63 | G16H-0050/70","G06Q-010/10","G06Q-010/10 | G06Q-010/06 | G06Q-030/08 | G16H-040/63 | G16H-050/70 | A61B-005/16 | A61B-005/08 | A61B-005/00 | A61B-005/369","","","","","","4922047004865"
"US","US","P","B2","Methods for signal-embedded signatures","In accordance with one embodiment, a method for a health scanning system is disclosed. The method includes receiving at least one electrical physiological data signal (PDS); suppressing a Direct Current (DC) signal component of the PDS to emphasize the Alternating Current (AC) signal component of the PDS; isolating the signal noise in the AC signal component of the PDS; and extracting a noise signature from the signal noise in the PDS. The noise signature, after calibration, can be used to uniquely identify a known user from other users.","1. A method for a health scanning system, the method comprising: receiving at least one electrical physiological data signal (PDS);suppressing a direct current (DC) signal component of the PDS to emphasize an alternating current (AC) signal component of the PDS;isolating a signal noise in the AC signal component of the PDS;extracting a noise signature from the signal noise in the PDS;comparing the noise signature to a predetermined signature; andin response to determining a match between the noise signature and the predetermined signature, allowing access to at least one of a secured system, a device, and a database.","6","16/740439","2020-01-11","2021-0000387","2021-01-07","11497419","2022-11-15","HEALTHY.IO LTD.","Martin Zizi","","","","A61B-0005/117","A61B-0005/117 | A61B-0005/318 | A61B-0005/6898 | G06F-0021/32 | G06F-0021/44 | G06K-0009/00 | G16H-0010/60 | G16H-0040/63 | H04B-0015/005","G06F-021/44","G06F-021/44 | G06K-009/00 | G06F-021/32 | A61B-005/117 | H04B-015/00 | A61B-005/00 | G16H-040/63 | G16H-010/60 | A61B-005/318","","","","","","4922046000985"
"US","US","P","B2","Cards and devices with magnetic emulators with zoning control and advanced interiors","A payment card (e.g., credit and/or debit card) is provided with a magnetic emulator operable to act as a magnetic stripe read-head detector and a data transmitter. A multiple layer flexible PCB may be fabricated to include multiple magnetic emulators. An emulator may include a coil that includes magnetic, ferromagnetic, or ferromagnetic, material in the coil's interior. Coils may be associated with zones. As a read-head is detected to move from zone-to-zone, coils may be activated to transmit information in those zones.","1. A device comprising: a first and a second zone;a first and a second read-head detector each operable to detect a read-head of a magnetic stripe reader; anda magnetic emulator operable to couple with and wirelessly communicate with the read-head of the magnetic stripe reader comprising a first and a second portion;wherein the first portion and the first read-head detector are located in the first zone and the second portion and the second read-head detector are located in the second zone.","4","15/583356","2017-05-01","2017-0286817","2017-10-05","11494606","2022-11-08","DYNAMICS INC.","Jeffrey D. Mullen | David Lambeth | Bruce Cloutier","","","","G06K-0019/083","G06K-0019/083 | A61B-0005/02 | A61B-0005/02042 | G06F-0003/0488 | G06K-0007/0004 | G06K-0007/084 | G06K-0007/087 | G06K-0007/10297 | G06K-0019/06187 | G06K-0019/06206 | G06K-0019/07 | G06K-0019/0702 | G06K-0019/0704 | G06K-0019/0723 | G06K-0019/0725 | G06K-0019/0775 | G06K-0019/07345 | G06K-0019/07703 | G06K-0019/07705 | G06K-0019/07707 | G06K-0019/07709 | G06K-0019/07749 | G06K-0019/07766 | G06K-0019/07769 | G06K-0019/07773 | G06Q-0020/18 | G06Q-0020/20 | G06Q-0020/34 | G06Q-0020/3415 | G06Q-0020/352 | G06Q-0020/385 | G06Q-0020/401 | G06Q-0030/0222 | G06Q-0030/0241 | G06Q-0030/0277 | G06Q-0030/0641 | G06T-0007/62 | G06V-0010/24 | G06V-0010/25 | G07F-0007/0806 | G07F-0007/1008 | G06T-2207/10024 | G06T-2207/30004 | G06V-2201/03","G06K-019/06","G06K-019/06 | G06K-019/08 | G06K-019/07 | G06K-019/077 | G06Q-020/18 | G06Q-020/20 | G06Q-020/34 | G06Q-020/38 | G06Q-030/02 | G06Q-030/06 | G07F-007/08 | G07F-007/10 | G06T-007/62 | A61B-005/02 | G06V-010/24 | G06V-010/25 | G06Q-020/40 | G06K-007/08 | G06K-019/073 | G06K-007/00 | G06F-003/0488 | G06K-007/10","","","","","","4922045004752"
"US","US","P","B2","Device and system for real-time gait modulation and methods of operation thereof","Apparatus, systems, and methods for real-time gait modulation are disclosed. In one embodiment, a functional electrical stimulation (FES) device is disclosed comprising one or more elastic wearable articles, a control unit comprising a wireless communication module, one or more processors, one or more memory units, a portable power supply, an electrical muscle stimulation (EMS) generator, and an inertial measurement unit (IMU) comprising at least a gyroscope and an accelerometer. The FES device can also comprise one or more electrode arrays configured to be in physical contact with the limb of the user. The processors can be programmed to execute instructions to retrieve readings from the IMU, calculate a gait cycle percentage by inputting at least the IMU readings into a machine learning algorithm, and instruct the EMS generator to provide electrical stimulation via the one or more electrode arrays based in part on the gait cycle percentage calculated.","1. A functional electrical stimulation (FES) device, comprising: one or more elastic wearable articles configured to be worn on a limb of a user;a control unit comprising a wireless communication module, one or more processors, one or more memory units, a portable power supply, an electrical muscle stimulation (EMS) generator, and an inertial measurement unit (IMU), wherein a housing of the control unit is coupled to at least one of the one or more elastic wearable articles, wherein the IMU comprises at least a gyroscope and an accelerometer;one or more electrode arrays configured to be in electrical communication with the EMS generator, wherein at least part of each of the one or more electrode arrays is configured to be in physical contact with the limb of the user, wherein the one or more processors are programmed to execute instructions stored in the one or more memory units to: retrieve gyroscope readings from the IMU,retrieve accelerometer readings from the IMU,calculate a gait cycle percentage by inputting at least the gyroscope readings into a machine learning algorithm,map the gyroscope readings and the accelerometer readings to three-dimensional angles of at least one of a hip, a knee, and a foot of the user throughout a gait cycle,determine at least one of a foot strike pattern, a foot inclination angle at initial contact, a tibia angle at loading response, a hip extension during late stance, a trunk lean, a heel eversion, a foot progression angle, a pelvic drop, a knee flexion during stance, a stride length, a knee window, a vertical displacement of the center mass, and a heel whip of the user based in part on the gait cycle percentage calculated, the gyroscope readings, the accelerometer readings, and the mapped three-dimensional angles, andinstruct the EMS generator to provide electrical stimulation to nerves and muscles of the limb via the one or more electrode arrays based in part on the gait cycle percentage calculated.","19","16/730336","2019-12-30","2020-0215324","2020-07-09","11484710","2022-11-01","EVOLUTION DEVICES, INC.","Pierluigi Alfredo Mantovani | Juan Manuel Rodriguez | Petr Karashchuk | Andrew Ekelem | Mohammed Aashyk Mohaiteen Hebsur Rahman","","","","A61N-0001/36003","A61N-0001/36003 | A61B-0005/0022 | A61B-0005/112 | A61B-0005/6828 | A61N-0001/0452 | A61N-0001/0456 | A61N-0001/0476 | A61N-0001/3603 | G06F-0003/011 | G06F-0003/017 | G06N-0003/08 | G06N-0020/00 | A61B-2562/0219","A61N-001/36","A61N-001/36 | A61N-001/04 | A61B-005/11 | A61B-005/00 | G06F-003/01 | G06N-003/08 | G06N-020/00","","","","","","4922044001488"
"US","US","P","B2","Authentication device, authentication system, authentication method, and program","An authentication device includes: a wearing position determination unit that determines a wearing position, the wearing position being a position at which a wearable article comprising a sensor is being worn on a body; and an authentication unit that performs authentication by using biometric information of the body, the biometric information being detected by the sensor at the wearing position.","1. An authentication system comprising: a finger ring; andan information processing device,the finger ring comprising: a first sensor configured to detect a fingerprint;a second sensor configured to detect acceleration of the finger ring;a lamp; anda first processor configured to: perform authentication based on stored fingerprint information and the detected fingerprint;enable use of a function of the finger ring based on a result of the authentication and the detected acceleration; andcontrol the lamp based on information received from the information processing device; andthe information processing device comprising: a display configured to: display a screen for accepting instructions to store the fingerprint information; andreceive instruction to store the fingerprint information based on a user input to the display; anda second processor configured to: send instruction information to store fingerprint information to the finger ring based on the user input to the display,wherein the finger ring receives the instruction information, and stores the fingerprint information detected by the first sensor.","15","15/744471","2016-06-30","2018-0211020","2018-07-26","11487855","2022-11-01","NEC CORPORATION","Hiroshi Fukuda","2015-141177","JP","2015-07-15","G06F-0021/32","G06F-0021/32 | A61B-0005/117 | G06T-0001/00 | G06V-0010/147 | G06V-0040/13 | G06V-0040/18 | G06V-0040/67 | G06V-0040/70 | G06F-2221/2111 | G06V-0040/14","G06K-009/00","G06K-009/00 | G06F-021/32 | A61B-005/117 | G06V-010/147 | G06V-040/13 | G06V-040/18 | G06V-040/60 | G06V-040/70 | G06T-001/00 | G06V-040/14","","","","","","4922044004604"
"US","US","P","B2","Method and device for monitoring","A method for monitoring a primary variable is carried out in a device having access to a set of sensors. The method includes the steps of receiving, from a network service, a series of forecasted values for the primary variable, each forecasted value being associated with one of a series of future time points; for at least one of the future time points, predicting a value for the primary variable using data of at least one secondary variable captured by a subset of the set of sensors, comparing the predicted value to the forecasted value associated with the future time point, and switching to a different subset of the set of sensors, if the predicted value deviates from the forecasted value with more than a specified threshold value.","1. A method for monitoring a primary variable, the method being carried out in a device having access to a set of sensors, comprising receiving, from a network service, a series of forecasted values for the primary variable, each forecasted value being associated with one of a series of future time points; and;for at least one of the future time points, predicting a value for the primary variable using data of at least one secondary variable captured by a subset of the set of sensors, comparing the predicted value to the forecasted value associated with the future time point, and switching to a different subset of the set of sensors, if the predicted value deviates from the forecasted value with more than a specified threshold value.","20","16/825828","2020-03-20","2020-0311577","2020-10-01","11488038","2022-11-01","SONY NETWORK COMMUNICATIONS EUROPE B.V.","Peter Exner | Anders Isberg","2019-50398","SE","2019-03-29","G06N-0005/04","G06N-0005/04 | A61B-0005/14532 | A61B-0005/7264 | A61B-0005/7275 | G01R-0021/133 | G06N-0020/00 | G06Q-0010/04 | G06Q-0050/06 | G16H-0040/67 | H04Q-0001/03","G06Q-010/04","G06Q-010/04 | A61B-005/00 | G05B-013/04 | G06N-005/04 | G16H-040/67 | G06N-020/00 | A61B-005/145 | G01R-021/133 | G06Q-050/06 | H04Q-001/02","","","","","","4922044004785"
"US","US","P","B2","Method and apparatus for neuroenhancement to enhance emotional response","A method of transplanting a desired emotional state from a donor to a recipient, comprising determining an emotional state of the donor; recording neural correlates of the emotional state of the donor who is in the desired emotional state; analyzing neural correlates of the emotional state of the donor to decode at least one of a temporal and a spatial pattern corresponding to the desirable emotional state; converting said at least one of a temporal and a spatial pattern corresponding to the desirable emotional state into a neurostimulation pattern; storing the neurostimulation pattern in the nonvolatile memory; retrieving the neurostimulation pattern from the nonvolatile memory; stimulating the recipients brain with at least one stimulus modulated with the neurostimulation pattern to induce the desired emotional state in the recipient.","1. A system for increasing emotional response to an audiovisual media presentation comprising consciously perceptible content, by neurostimulation of a subject concurrent with experiencing of the audiovisual media presentation by the subject, comprising: a first memory configured to store a set of information associated with a target emotional state, derived from recorded brainwaves of at least one donor while engaged in the respective target emotional state;a second memory configured to store an identified target emotional state associated with each of a sequence of portions of the audiovisual media presentation, the audiovisual media presentation having a plurality of portions, wherein at least two portions of the audiovisual media presentation are associated with different target emotional states;at least one automated processor configured to: determine a characteristic sequence of brainwave patterns associated with the identified target emotional state, based on at least the set of information corresponding to a respective portion of the audiovisual media presentation for the sequence of portions; andembed a neurostimulation pattern in each respective portion of the sequence of portions, the embedded neurostimulation pattern for each portion initially corresponding to an optimal transition sequence of brainwave patterns from a respective brainwave pattern associated with the current emotional state of the subject to the identified target emotional state of the subject, and subsequently corresponding to the determined sequence of characteristic brainwave patterns, having transitions between respective brainwave patterns synchronized with changes of the consciously perceptible content of the audiovisual media presentation,the embedded neurostimulation patterns being adapted to entrain a brainwave pattern of the subject with the determined characteristic sequence of brainwave patterns for each portion synchronized with the audiovisual media presentation, to achieve the identified target emotional state with each of the sequence of portions of the audiovisual media presentation, wherein the defined characteristic neurostimulation pattern is subliminally encoded as at least one of an audio and a visual stimulus within the audiovisual media presentation; andan output port configured to output the sequence of portions having the embedded neurostimulation pattern for each respective portion for the sequence of portions to the subject subliminally encoded within an audiovisual content of the audiovisual media presentation.","41","16/237471","2018-12-31","2019-0201691","2019-07-04","11478603","2022-10-25","NEUROENHANCEMENT LAB, LLC | NEUROLIGHT INC","Alexander Poltorak","","","","A61M-0021/00","A61M-0021/00 | A61B-0005/0006 | A61B-0005/165 | A61B-0005/316 | A61B-0005/4836 | A61M-0021/02 | A61N-0001/36082 | G06F-0003/015 | G16H-0020/70 | A61M-2021/005 | A61M-2021/0016 | A61M-2021/0022 | A61M-2021/0027 | A61M-2021/0044 | A61M-2021/0055 | A61M-2021/0066 | A61M-2021/0072 | A61M-2021/0077 | A61M-2205/50 | A61M-2205/52 | A61M-2230/10 | A61N-0001/0456 | A61N-0001/0529 | A61N-0001/3603 | A61N-0001/36025 | A61N-0002/004 | G06F-2203/011","A61N-001/36","A61N-001/36 | A61M-021/00 | A61B-005/00 | A61B-005/16 | A61M-021/02 | A61B-005/316 | G06F-003/01 | G16H-020/70 | A61N-002/00 | A61N-001/04 | A61N-001/05","","","","","","4922043001144"
"US","US","P","B2","Predicting clinical parameters from fluid volumes determined from OCT imaging","Systems and methods are provided for evaluating an eye using retinal fluid volumes to provide a clinical parameter. An optical coherence tomography (OCT) image of an eye of a patient is obtained. The OCT image is segmented to produce a total retinal volume and one or both of a subretinal fluid volume and an intraretinal fluid volume for a region of interest within the eye. A metric is generated as a function of the total retinal volume and one or both of the subretinal fluid volume and the intraretinal fluid volume. A clinical parameter for the patient is determined from the metric. The determined clinical parameter is provided to a user at a display.","1. A method comprising: obtaining an optical coherence tomography (OCT) image of an eye of a patient;segmenting the OCT image to produce a total retinal volume and one of a subretinal fluid volume and an intraretinal fluid volume for a region of interest within the eye;generating a metric as a function of the total retinal volume and the one of the subretinal fluid volume and the intraretinal fluid volume;determining at least one clinical parameter for the patient from the metric; andproviding the determined at least one clinical parameter to a user at a display.","20","16/569434","2019-09-12","2020-0077883","2020-03-12","11471037","2022-10-18","THE CLEVELAND CLINIC FOUNDATION","Justis P. Ehlers | Atsuro Uchida | Sunil Srivastava","","","","A61B-0003/0025","A61B-0003/0025 | A61B-0003/102 | A61B-0005/004 | A61B-0005/0066 | G06F-0017/18 | G06N-0020/00 | G06T-0007/0012 | G06T-0007/11 | G06T-2207/30041","G06T-007/00","G06T-007/00 | A61B-003/00 | A61B-003/10 | A61B-005/00 | G06T-007/11 | G06F-017/18 | G06N-020/00","","","","","","4922042001267"
"US","US","P","B2","Method for expert system to dynamically adapt fitness training plans","A method for an expert system to develop fitness training plans includes operating a dynamic exertion system to receive a rate of perceived exertion (RPE) through a user interface of a display device, combines the RPE with a movement, a movement load, and movement repetitions into movement set data, and operates a dynamic exertion algorithm. The method then displays an adjusted movement information display including the prescribed load and the prescribed movement repetitions through the user interface. The dynamic exertion algorithm generates a prescribed load and prescribed movement repetitions, determines a difference in RPE from the expected RPE through operation of a comparator, recalculates the one repetition maximum load value using the calibration and adjustment model when the difference in RPE is greater than an RPE threshold value, and generates a display control comprising the prescribed load and the prescribed movement repetitions.","1. A method comprising: receiving a rate of perceived exertion (RPE) through a user interface of a display device;combining the RPE with a movement, a movement load, and movement repetitions into movement set data;operating a dynamic exertion algorithm to: generate a prescribed load and prescribed movement repetitions from a one repetition maximum load value, historical movement set data, a relative exertion model and a calibration and adjustment model, wherein the relative exertion model defines a relationship between the movement load, the movement repetitions, and the RPE, and the relative exertion model determines an expected RPE;determine a difference in RPE from the expected RPE through operation of a comparator;recalculate the one repetition maximum load value using the calibration and adjustment model when the difference in RPE is greater than an RPE threshold value; andgenerate a display control comprising the prescribed load and the prescribed movement repetitions; anddisplaying an adjusted movement information display comprising the prescribed load and the prescribed movement repetitions through the user interface, in response to configuration of a user interface controller with the display control.","18","16/813386","2020-03-09","2020-0281482","2020-09-10","11471059","2022-10-18","VOLT ATHLETICS","Trevor William Watkins | Daniel Roven Giuliani | Brian James McNaboe | Jace Atom Derwin","","","","A61B-0005/02028","A61B-0005/02028 | G06N-0020/00 | G16H-0020/30","G06F-003/048","G06F-003/048 | A61B-005/02 | G06N-020/00 | G16H-020/30","","","","","","4922042001288"
"US","US","P","B2","Mobile application for wearable device","Featured are devices (e.g., peripheral devices) and systems that interact with a device having sensors (e.g., a wearable device or device configured for use with a piece of equipment, such as a vehicle) to receive and process data (e.g., physiological data) from the sensors. Also featured are computer implemented methods including software (e.g., an application) for receiving and processing data by the peripheral device. An application that communicates with the device (e.g., wearable device or equipment device) and sensor information processed by the application or a device running or accessing the application provides situational awareness for users in adverse conditions, such as during combat or wartime. The wearable device may include one or more inflatable bladders configured to apply pressure to a wound site for treatment.","1. A computer implemented method for presenting physiological data regarding a health state of a subject, wherein the method is performed using an application operating on a peripheral device comprising a graphical user interface, the method comprising: receiving the physiological data by the peripheral device, wherein the physiological data comprise information generated upon activation of at least one impact detection sensor located within or on a wearable device that is adorned by the subject, wherein the wearable device comprises a plurality of non-overlapping zones, each of which comprises at least one said impact detection sensor, wherein said at least one impact detection sensor of each of the plurality of zones is configured to independently activate upon an impact thereto, and each said impact detection sensor is independently connected to an information processing unit (IPU) that produces the information;displaying on the graphical user interface a visual representation of the plurality of zones of the wearable device; anddisplaying a signal in at least one of the plurality of zones of the visual representation corresponding to activation of at least one said impact detection sensor of the wearable device, wherein the signal identifies the occurrence of a physical impact to the wearable device within at least one of the plurality of zones, thereby indicating the health state of the subject; andwherein the physiological data displayed on the graphical user interface further comprise one or more of ballistic impact site, impact force, and source or direction of impact.","25","16/682044","2019-11-13","2020-0237318","2020-07-30","11471112","2022-10-18","LEGIONARIUS, LLC","Alexander Gruentzig","","","","A61B-0005/7282","A61B-0005/7282 | A61B-0005/0022 | A61B-0005/02055 | A61B-0005/4836 | A61B-0005/7425 | A61B-0017/12 | G06F-0003/048 | G16H-0040/67 | H04L-0063/083 | H04W-0012/33 | A61B-0005/021 | A61B-0005/026 | A61B-0005/02438 | A61B-0005/0816 | A61B-0005/14542 | A61B-0005/6804 | A61B-2017/00557 | A61B-2017/12004 | H04W-0012/63","A61B-005/00","A61B-005/00 | G16H-040/67 | A61B-005/0205 | A61B-017/12 | G06F-003/048 | H04L-009/40 | H04W-012/33 | A61B-005/021 | A61B-005/024 | A61B-005/08 | A61B-005/026 | A61B-005/145 | A61B-017/00 | H04W-012/63","","","","","","4922042001341"
"US","US","P","B2","Extended reality grasp controller","In example implementations, an apparatus is provided. The apparatus includes a body portion and a plurality of legs movably coupled to the body portion. The body portion is to rest on a backside of a hand of a user. Each one of the plurality of legs include a curved portion to fit between fingers of a user. Respective ends of the plurality of legs are to contact a palm of the user.","1. An apparatus, comprising: a body portion to rest on a backside of a hand of a user; anda plurality of legs movably coupled to the body portion, wherein each one of the plurality of legs comprise a curved portion to fit between fingers of a user and respective ends of the plurality of legs are to contact a palm of the user, wherein the plurality of legs comprises at least one of: a shape morphing material that contracts when in contact with heat from the palm of the user to close, electro-mechanical joints coupled to a switch to open and close the plurality of legs, or spring loaded joints.","11","17/414346","2019-06-12","2022-0100273","2022-03-31","11474603","2022-10-18","HEWLETT-PACKARD DEVELOPMENT COMPANY, L.P.","Mithra Vankipuram | Hiroshi Horii | Rafael Ballagas","","","","G06F-0003/014","G06F-0003/014 | A61B-0005/313 | A61B-0005/6825 | A61N-0001/0456 | A61N-0001/0472 | G06F-0003/015 | G06F-0003/016 | G06F-2203/011","G06F-003/01","G06F-003/01 | A61B-005/313 | A61B-005/00 | A61N-001/04","","","","","","4922042004810"
"US","US","P","B2","Methods based on an analysis of drawing behavior changes for cognitive dysfunction screening","Methods for screening, diagnosing, or predicting presence, progression, or treatment effects of a cognitive dysfunction such as dementia based on an analysis of drawing behavior changes by a pre-trained Na?ve Bayes method are provided. The methods include steps of obtaining drawing data of at least one image created by a test subject on a digital device and obtaining personal data of the test subject; reconstructing the at least one image based on the drawing data obtained; converting the drawing data to drawing features comprising a plurality of motion features and a plurality of geometric features; and determining probability that the test subject has a cognitive dysfunction based on the drawing features and the personal data by a pre-trained Na?ve Bayes method with a greedy variable selection.","1. A method for screening, diagnosing, or predicting presence, progression, or treatment effects of cognitive dysfunction based on analysis of drawing behavior changes, comprising: a) obtaining drawing data of at least one image created by a test subject on a digital device and obtaining personal data of the test subject;b) reconstructing the at least one image based on the drawing data obtained;c) converting the drawing data to drawing features comprising a plurality of motion features and a plurality of geometric features; andd) determining a probability that the test subject has a cognitive dysfunction based on the drawing features and the personal data by a pre-trained Na?ve Bayes methodwherein the reconstructing the image created comprises:a) filtering an entire drawing trajectory of the at least one image to reassemble separated drawing segments;b) re-drawing the figure pixel by pixel based on drawing segments obtained in the filtering and stored for processing; andc) resizing the image to a predetermined size and converting the image into greyscale.","20","16/696270","2019-11-26","2021-0153801","2021-05-27","11464443","2022-10-11","THE CHINESE UNIVERSITY OF HONG KONG","Kam Fai Tsoi | Wing Yip Lam | Tsz Kan Christopher Chu | Ka Ho Tsang","","","","A61B-0005/4088","A61B-0005/4088 | G06F-0017/16 | G06K-0009/6218 | G06K-0009/6259 | G06K-0009/6278 | G06K-0009/6296 | G06V-0010/751 | G16H-0010/20","G06K-009/00","G06K-009/00 | A61B-005/00 | G16H-010/20 | G06K-009/62 | G06F-017/16 | G06V-010/75","","","","","","4922041001022"
"US","US","P","B2","Method and apparatus of context-based patient similarity","A computer apparatus to assist diagnosis of a patient including: a memory storing instructions for execution and an output for results of a processor that provides a patient builder and a vertex filter. Where the builder inputs patient data including historical clinical; and open data, and to create a patient clinical object, PCO, graph; and a full patient graph PCOs for each patient. The filter includes: a context builder to build a domain based on a specification and open data; a context-based vertex filter ranking vertices in the full patient graph based on domain affiliation, retaining any vertices with high domain affiliation; and computing a similarity between the patient PCO and other PCOs in the full patient graph using the retained vertices; and a patient ranker ranks the PCOs according to t similarity where the output lists patients similar to the patient to suggest patient diagnoses.","1. A computer, comprising: a processor to couple to a memory, the memory storing instructions for execution by the processor, the processor configured by the instructions to provide an automated patient builder and an automated patient vertex filter;wherein: the automated patient builder is to, input, for a population of patients, electronic patient data of a patient, among the patients, the electronic patient data including historical clinical data and open data, the historical data including raw non-textual data,create patient clinical objects (PCOs) representing the population of patients, respectively, based on the electronic patient data, andenrich the electronic patient data of each patient of the patients, respectively, based on the open data,the PCOs being created by pre-processing the electronic patient data for automated filtering, the pre-processing including extracting, using an automated extraction technology, textual information from the raw non-textual data of the electronic patient data to store each PCO of the PCOs in the memory in form of pieces of at least the extracted textual information modeled as vertices with edges among the vertices forming a graph, resulting in the extracted textual information being a labeled vertex of a corresponding PCO among the PCOs, each vertex of the vertices electronically encapsulating knowledge about the patient in form of the textual data of the electronic patient data, and including a pointer leading to an address in the memory to link each vertex to where the raw non-textual data represented by the vertex is retrievable from the memory,the vertices to form the graph centered on a patient ID vertex, among the vertices, indicating a patient ID, with the edges representing relationships among the vertices by linking the patient ID vertex to the vertices labeled to represent the historical clinical data of the electronic patient data belonging to a category according to the enriched electronic patient data, respectively, andeach PCO of the PCOs representing the patient as a subgraph, wherein vertices corresponding to a plurality of subgraphs respectively corresponding to each PCO of the PCOs together compose vertices corresponding to a full patient graph of the PCOs representing the population of patients;the patient vertex filter is to perform the automated filtering on the vertices corresponding to the full patient graph to indicate at least one diagnosis corresponding to a target PCO of a target patient, from among the full patient graph, by implementing: a patient context builder to input, for the target PCO of the target patient, a specification of a medical domain according to a context and the open data used to enrich the patient data of the target patient, to a singular value decomposition (SVD) technology and/or artificial neural network (ANN) to extract feature vectors to obtain a context-based domain corpus based on the context and the open data,a context-based vertex filter to, filter based on ranking the vertices in the full patient graph based on vertex domain affiliation with respect to the context-based domain corpus for the target PCO, to retain vertices, in each PCO of the other PCOs in the full patient graph, with high context-based domain affiliation to the target PCO, andcompute similarity values between the target PCO and the other PCOs on basis of the retained vertices, anda patient ranker to rank the PCOs in the full patient graph according to the computed similarity values between the target PCO and the other PCOs, resulting in a contextualized list of similar PCOs; andthe processor is further configured by the instructions to indicate the at least one diagnosis corresponding to the target PCO, based on the ranked computed similarity values between the target PCO and the other PCOs within the full patient graph in the contextualized list of similar PCOs.","13","15/718759","2017-09-28","2018-0098737","2018-04-12","11464455","2022-10-11","FUJITSU LIMITED","Boris Villaz?n-Terrazas | Bo Hu | Victor De La Torre","10-2016-219432","DE","2016-10-06","A61B-0005/7275","A61B-0005/7275 | A61B-0005/0022 | G06N-0005/022 | G06N-0005/04 | G06N-0020/00 | G06Q-0010/10 | G16H-0010/60 | G16H-0020/10 | G16H-0020/70 | G16H-0020/90 | G16H-0050/20 | G16H-0050/50 | G16H-0050/70","G16H-050/70","G16H-050/70 | G16H-050/20 | A61B-005/00 | G06N-020/00 | G16H-010/60 | G16H-020/10 | G16H-020/90 | G16H-050/50 | G16H-020/70 | G06N-005/02 | G06N-005/04 | G06Q-010/10","","","","","","4922041001034"
"US","US","P","B1","Identifying object of user focus with eye tracking and visually evoked potentials","A brain computer interface system includes a wearable interface, an eye tracking device, and a client device for determining what object a user is looking at on an electronic display. The client device determines a region on the electronic display based on an estimated user gaze direction received from the eye tracking device. For each virtual object in the gaze region, the client device displays a visual stimulus with a unique frequency. The client device receives from the wearable interface an electrical potential signal measured at the user's brain and evoked by a visual stimulus on the electronic display. The client device identifies the object in the gaze region with a stimulus frequency matching a frequency derived from the potential signal, and executes instructions relating to the object.","1. A system comprising: an interface for a head region of a user, the interface including a plurality of electrodes operable to measure signals evoked by visual stimuli that the user is looking at, wherein the wearable interface is operable to transmit data derived from the signals;an electronic display including a plurality of pixels operable to display a plurality of objects using the plurality of pixels; anda client device operable to receive data from the interface, the client device storing instructions that when executed by the client device cause the client device to perform steps including: determine a region on the electronic display approximating where a user is looking, the electronic display including a plurality of pixels representing a plurality of objects, wherein the region includes a portion of the plurality of pixels;identify a subset of objects of the plurality of objects represented by pixels of the portion of the plurality of pixels within the region on the electronic display;responsive to identifying the subset of objects, display a visual stimulus for each object of the subset of objects on the electronic display by altering the pixels representing each object, wherein each visual stimulus has a unique frequency relative to the visual stimuli for other objects of the subset of objects;receive a visually evoked signal from the wearable interface;compare a frequency derived from the visually evoked signal to the frequency of each visual stimulus of the subset of objects;identify, based on the comparison, an object of the subset of objects corresponding to a visual stimulus having a frequency that matches the frequency derived from the visually evoked signal; and perform an action associated with the identified object.","20","17/370745","2021-07-08","","","11467662","2022-10-11","META PLATFORMS, INC.","Yu-Te Wang | Mark Allan Chevillet","","","","G06F-0003/013","G06F-0003/013 | A61B-0005/291 | A61B-0005/378 | A61B-0005/6803 | G06T-0003/60 | G06T-0007/70 | G06T-0011/001 | G06F-0003/015 | G06T-2207/30201","G09G-005/00","G09G-005/00 | G06F-003/01 | A61B-005/00 | G06T-007/70 | A61B-005/291 | G06T-003/60 | A61B-005/378 | G06T-011/00","","","","","","4922041004215"
"US","US","P","B2","Digital signal processing using sliding windowed infinite fourier transform","Systems and methods for digital signal processing using a sliding windowed infinite Fourier transform (""SWIFT"") algorithm are described. A discrete-time Fourier transform (""DTFT"") of an input signal is computed over an infinite-length temporal window that is slid from one sample in the input signal to the next. The DTFT with the temporal window at a given sample point is effectively calculated by phase shifting and decaying the DTFT calculated when the temporal window was positioned at the previous sample point and adding the current sample to the result.","1. A method for controlling a neural stimulation device, the steps of the method comprising: (a) providing to a computer system, input signal data comprising a plurality of samples acquired with a measurement device, wherein the input signal data comprise physiological signal data acquired with a physiological measurement device, wherein the physiological measurement device is an EEG device and the physiological signal data indicates neural signals in a subject;(b) generating signal feature data with the computer system by applying a sliding windowed infinite Fourier transform to the input signal data by: (i) selecting with the computer system, a window function comprising an infinite-length function having a time constant with units of number of samples;(ii) initializing a discrete-time Fourier transform (DTFT) matrix with the computer system;(iii) updating the DTFT matrix by: phase shifting the DTFT matrix by an angular frequency;decaying an amplitude of the DTFT matrix using the time constant;selecting a next sample in the input signal data and adding the next sample to the DTFT matrix;(iv) repeating step (iii) for a selected number of samples in the input signal data in order to generate the signal feature data;(c) storing the signal feature data generated by the computer system, wherein the signal feature data comprises at least one of amplitude information of the input signal data, phase information of the input signal data, or frequency information of the input signal data;(d) transforming the signal feature data into a set of control signals for controlling the neural stimulation device, wherein the set of control signals are indicative of one or more operating parameters for delivering an electrical stimulation by the neural stimulation device; and(e) controlling the neural stimulation device using the set of control signals.","13","16/009829","2018-06-15","2018-0365194","2018-12-20","11468144","2022-10-11","REGENTS OF THE UNIVERSITY OF MINNESOTA","Logan L. Grado | Matthew D. Johnson | Theoden I. Netoff","","","","G06F-0017/147","G06F-0017/147 | A61B-0005/316 | A61B-0005/7257 | A61N-0001/36196 | G06F-0017/141 | A61B-0005/318 | A61B-0005/374 | A61B-0005/389 | A61B-0005/4836 | A61B-0005/4851 | A61B-0005/686 | A61B-2562/0219 | A61N-0001/36038 | A61N-0001/36171 | H03H-2017/009","G06F-017/14","G06F-017/14 | A61B-005/00 | A61N-001/36 | A61B-005/316 | H03H-017/00 | A61B-005/318 | A61B-005/374 | A61B-005/389","","","","","","4922041004692"
"US","US","P","B2","Medical scan co-registration and methods for use therewith","A medical scan viewing system is conFIG.d to: receive a first medical scan and a second medical scan from a medical picture archive system, the first medical scan associated with a unique patient ID and a first scan date and the second medical scan associated with the unique patient ID and a second scan date; identify locations of anatomical landmarks in the first medical scan; identifying corresponding locations of the anatomical landmarks in the second medical scan; co-register the first medical scan with the second medical scan based on the locations of the anatomical landmarks in the first medical scan with the corresponding locations of the anatomical landmarks in the second medical scan; and present for display, via an interactive user interface, the first medical scan with the second medical scan, wherein the first medical scan and the second medical scan are synchronously presented, based on the co-registering.","1. A medical scan viewing system, comprising: a network interface; a processing system that includes a processor; and a memory device that stores executable instructions that, when executed by the processing system, configure the processor to perform operations comprising: receiving, via the network interface, a first medical scan and a second medical scan from a medical picture archive system, the first medical scan associated with a first unique patient ID and a first scan date and the second medical scan associated with the first unique patient ID and a second scan date that is more recent than the first scan date, wherein the first medical scan includes a first plurality of image slices, and wherein the second medical scan includes a second plurality of image slices; identifying, via first input received via an interactive user interface, locations of a plurality of anatomical landmarks in the first medical scan; identifying, via second input received via the interactive user interface, corresponding locations of the plurality of anatomical landmarks in the second medical scan; co-registering the first medical scan with the second medical scan based on the locations of the plurality of anatomical landmarks in the first medical scan with the corresponding locations of the plurality of anatomical landmarks in the second medical scan; presenting for display, via the interactive user interface, abnormality change measurement data of the first medical scan with the second medical scan, wherein the first medical scan and the second medical scan are synchronously presented based on the co-registering, wherein the identifying the locations of the plurality of anatomical landmarks in the first medical scan includes: presenting the first medical scan for display via a first portion of the interactive user interface, the first portion of the interactive user interface facilitating selective display of the first plurality of image slices; prompting a user, via the interactive user interface, to identify the location of a first anatomical landmark of the plurality of anatomical landmarks in the first medical scan; receiving, via the interactive user interface, the first input indicating the location of the first anatomical landmark in the first medical scan; prompting the user, via the interactive user interface, to identify a location of a second anatomical landmark of the plurality of anatomical landmarks in the first medical scan; and receiving, via the interactive user interface, the second input indicating the location of the second anatomical landmark in the first medical scan, wherein the location of the first anatomical landmark is in a fixed position in an X-Y plane of the first medical scan.","16","16/695657","2019-11-26","2021-0158936","2021-05-27","11462315","2022-10-04","ENLITIC, INC.","Shankar Rao | Jordan Francis | Kevin Lyman","","","","G16H-0030/40","G16H-0030/40 | A61B-0005/0022 | A61B-0005/055 | A61B-0005/7264 | A61B-0006/032 | A61B-0006/5217 | A61B-0006/563 | G06T-0007/0016 | G06T-0007/337 | G06F-0003/0485 | G06T-2200/24 | G06T-2207/20101","G16H-030/40","G16H-030/40 | A61B-006/03 | A61B-006/00 | A61B-005/055 | A61B-005/00 | G06T-007/00 | G06T-007/33 | G06F-003/0485","","","","","","4922040005458"
"US","US","P","B2","OCT signal processing device and recording medium","An OCT signal processing device has an acquisition unit for acquiring three or more OCT signals being temporally different from each other with respect to the same position on a subject, from an OCT device that detects an OCT signal based on reflection light of measurement light applied to a subject and reference light corresponding to the measurement light, and a calculation unit for calculating motion contrast based on a plurality of OCT signals. The calculation unit selects two or more sets of OCT signals having different time interval between the OCT signals among sets obtained by extracting a plurality of OCT signals out of the three or more OCT signals, and calculates motion contrast for each of the selected two or more sets.","1. An optical coherence tomography (OCT) signal processing device configured to process an OCT signal, comprising: a central processing unit (CPU) configured to: acquire three or more OCT signals, each of the three or more OCT signals being temporally different from each other with respect to a same position on a subject, from an OCT device that detects OCT signals based on reflection light of measurement light applied to the subject and reference light corresponding to the measurement light;extract a plurality of OCT signals from the three or more OCT signals;select two or more sets of OCT signals from the plurality of OCT signals, the two or more sets of OCT signals having different time intervals, and calculate motion contrast for each of the selected two or more sets, wherein the two or more sets of OCT signals having different time intervals include a temporally separated set of extracted OCT signals which are not temporally adjacent, andwherein the CPU is further configured to synthesize motion contrasts calculated for each of the two or more sets.","10","16/068452","2017-01-05","2019-0380588","2019-12-19","11452452","2022-09-27","NIDEK CO., LTD.","Naoki Takeno | Masaaki Hanebuchi | Yasuhiro Furuuchi | Hajime Namiki","2016-002074","JP","2016-01-07","A61B-0005/0066","A61B-0005/0066 | A61B-0003/102 | A61B-0003/12 | A61B-0005/004 | G06F-0017/18 | G06T-0007/20 | G06T-2207/10101","A61B-005/00","A61B-005/00 | A61B-003/10 | A61B-003/12 | G06T-007/20 | G06F-017/18","","","","","","4922039000775"
"US","US","P","B1","Predicting and preventing caregiver musculoskeletal injuries","Systems, methods and computer-readable media are provided for determining the modality for transferring, lifting, or repositioning (TLR) a human patient in a health care setting contexts. In some cases, a model-based recursive partitioning and Bradley-Terry regression is applied, which may be optionally parallelized so as to determine statistical associations with various factors, such as caregiver attributes, care venue, and patient attributes. One embodiment determines a Bradley-Terry regression model from the recursive partitioning which may be incorporated into a TLR selection decision-support tool or otherwise utilized to identify the optimal modality.","1. A system for providing an optimal transferring, lifting, and/or repositioning (TLR) modality option for a particular context to a computing device with a user interface, the system comprising: one or more processors; andcomputer-readable media having computer-usable instructions embodied thereon that, when executed by the one or more processor, cause the one or more processor to:receive raw count TLR usage information and context attributes associated with the TLR usage information, at least a portion of the context attributes including an explanatory analysis of context associated with the variable, wherein the explanatory analysis of context associated with the variable includes a history of previous injury; determine one or more statistical distributions that fit the TLR usage information;estimate a set of parameters for the one or more statistical distributions;determine a set of distribution quantiles for the set of parameters;based on the quantiles, transform the TLR usage information into a paired-comparison of TLR modalities;determine one or more regression models by performing model-based recursive partitioning on the context attributes, each model having a set of model coefficients;determine model convergence and model stability;for each model of the one or more regression models, determine a statistical significance for the set of model coefficients;based at least on the statistical significance for each of the one or more regression models, determine a model from the one or more regression models to be utilized in a decision-support application for determining an optimal TLR modality option for the particular context;utilize the decision-support application for automatically determining the optimal TLR modality option for the particular context; andprovide to the computing device having the user interface the optimal TLR modality option, wherein the computing device is associated with the decision-support application.","19","15/824765","2017-11-28","","","11452652","2022-09-27","CERNER INNOVATION, INC.","Douglas S. McNair","","","","A61G-0007/10","A61G-0007/10 | G06Q-0050/22 | A61B-0005/7275 | A61B-0005/746 | A61B-0005/7465 | A61G-0007/001 | G16H-0050/70","G06F-017/10","G06F-017/10 | A61G-007/10 | G06Q-050/22 | A61B-005/00 | A61G-007/00 | G16H-050/70","","","","","","4922039000974"
"US","US","P","B2","Apparatus, systems, and methods for gathering and processing biometric and biomechanical data","Apparatus, systems, and methods are provided for measuring and analyzing movements of a body and for communicating information related to such body movements over a network. In certain embodiments, a system gathers biometric and biomechanical data relating to positions, orientations, and movements of various body parts of a user performed during sports activities, physical rehabilitation, or military or law enforcement activities. The biometric and biomechanical data can be communicated to a local and/or remote interface, which uses digital performance assessment tools to provide a performance evaluation to the user. The performance evaluation may include a graphical representation (e.g., a video), statistical information, and/or a comparison to another user and/or instructor. In some embodiments, the biometric and biomechanical data is communicated wirelessly to one or more devices including a processor, display, and/or data storage medium for further analysis, archiving, and data mining. In some embodiments, the device includes a cellular telephone.","1. An article of apparel, comprising: a fabric configured to conform to a body of a wearer;a plurality of ultrasonic positioning sensors secured with respect to the fabric at a first set of predetermined locations, each of the ultrasonic positioning sensors configured to emit a sound wave configured to be detected by other ones of the plurality of ultrasonic positioning sensors and output an electronic signal indicative of having emitted or detected a sound wave; anda plurality of feedback devices secured with respect to the fabric, each of the feedback devices configured to output a feedback signal configured to be detectable by the wearer of the article of apparel;wherein a processor is configured to: determine positional values of the plurality of ultrasonic positioning sensors based, at least in part, on electronic signals output by the plurality of ultrasonic positioning sensors; andcause at least one of the plurality of feedback devices to output the feedback signal based, at least in part, on the positional values as determined and a performance metric for a physical activity performed by the wearer.","19","17/003249","2020-08-26","2020-0391078","2020-12-17","11452914","2022-09-27","NIKE, INC.","Lee Norman Cusey | Jay Allen Shears | Harold Dan Stirling","","","","A63B-0024/0062","A63B-0024/0062 | A61B-0005/1124 | A61B-0005/6804 | A63B-0024/0006 | A63B-0069/3608 | A61B-0005/1114 | A61B-0005/1127 | A61B-0005/4528 | A61B-0005/6824 | A61B-0005/7405 | A61B-2562/0219 | A63B-0005/11 | A63B-0069/3667 | A63B-2024/0012 | A63B-2209/08 | A63B-2209/10 | A63B-2220/10 | A63B-2220/13 | A63B-2220/24 | A63B-2220/30 | A63B-2220/40 | A63B-2220/803 | A63B-2220/836 | A63B-2225/20 | A63B-2225/50 | G06F-0003/011 | G06K-0009/00 | G06V-0040/23 | G16H-0020/30 | G16H-0040/67 | G16H-0080/00","A63B-024/00","A63B-024/00 | A61B-005/11 | A61B-005/00 | A63B-069/36 | A63B-005/11 | G06F-003/01 | G16H-040/67 | G06V-040/20 | G06K-009/00 | G16H-020/30 | G16H-080/00","","","","","","4922039001236"
"US","US","P","B2","Method and system for correlating an image capturing device to a human user for analysis of cognitive performance","A method capturing eye movement data for detection of cognitive anomalies, includes a computer application displaying a frame on a display, capturing and displaying a video image of a user's face and eyes, while the user aligns the face to the frame, capturing and processing the face image to initiate an image eye movement capture process, outputting an indication on a display and moving the indication spatially to one of a plurality of images, capturing a video of each user eye, to track the position of the indication of the display, the image of each eye comprising a sclera portion, an iris portion, and a pupil portion, parsing the video to determine a reference images corresponding to eye positions, capturing the user's eyes while the user views familiar and novel images, and correlating the images of the user's eyes to the familiar or novel images using the reference images.","1. A method for playing a matching game on a host computer comprising: uploading from the host computer to a remote computer system, a computer network address for a plurality of static images, wherein the plurality of static images comprises a first plurality of static images and a second plurality of static images;uploading from the host computer to the remote computer system, remote computer system executable software code comprising: first remote computer system executable software code that directs the remote computer system to display on a display of the remote computer system to a player, only static images from the first plurality of static images but not static images from the second plurality of static images, wherein each of the static images from the first plurality of static images is displayed upon at most half of the display for a first predetermined amount of time;second remote computer system executable software code that directs the remote computer system to inhibit displaying on the display of the remote computer system to the player, at least one static image from the first plurality of static images to the player, for a second predetermined amount of time;third remote computer system executable software code that directs the remote computer system to simultaneously display on the display of the remote computer system to the player, a first static image from the first plurality of static images and a second static image from the second plurality of static images, wherein the first static image and the second static image are displayed upon at most half of the display for a third predetermined amount of time;fourth remote computer system executable software code that directs the remote computer system to capture using a web camera of the remote computer system video data of the player, wherein the video data captures eye movements of the player while the display of the remote computer system is displaying to the player the first static image and the second static image;fifth remote computer system executable software code that directs the remote computer system to create edited video data from a subset of the video data in response to a pre-defined two dimensional area of interest from the video data, wherein the edited video data has a lower resolution than the video data; andsixth remote computer system executable software code that directs the remote computer to provide to the host computer, the edited video data;determining with the host computer a first amount of time representing an amount of time the player views the first static image and a second amount of time representing an amount of time the player views the second static image, in response to the edited video data;determining with the host computer a viewing relationship for the player between the second amount of time and the first amount of time, in response to the first amount of time and the second amount of time;determining with the host computer whether the viewing relationship for the player between the second amount of time and the first amount of time exceeds a first threshold and generating a success flag in response thereto; andproviding from the host computer to the remote computer system, an indication that the player is successful, in response to the success flag.","9","17/136950","2020-12-29","2021-0161449","2021-06-03","11445955","2022-09-20","NEUROTRACK TECHNOLOGIES, INC.","Nick Bott | Alexander R. Lange | Rob Cosgriff | Roger Hsiao | Brad Dolin","","","","A61B-0005/163","A61B-0005/163 | A61B-0005/165 | A61B-0005/4088 | A63F-0013/822 | A63F-0013/85 | G06F-0003/013 | G06K-0009/00 | G06T-0007/11 | G06T-0007/248 | G06V-0020/59 | G06V-0040/166 | G06V-0040/193 | G09B-0019/00 | G09B-0019/18 | H04N-0005/23206 | H04N-0005/23222 | H04N-0005/23293 | H04N-0007/183 | H04N-0007/185 | A63F-0013/213 | G06T-2207/30041","A61B-005/16","A61B-005/16 | H04N-007/18 | G06K-009/00 | G06T-007/246 | H04N-005/232 | A63F-013/822 | A63F-013/85 | G06V-020/59 | G06V-040/16 | G06V-040/18 | G06T-007/11 | A61B-005/00 | G06F-003/01 | G09B-019/00 | G09B-019/18 | A63F-013/213","","","","","","4922038001020"
"US","US","P","B2","System for management of insurance risk and insurance events","This system is directed to the management of insurance information, risks and coverage, wherein a set of non-transitory computer readable instructions included in a kiosk disposed at the construction site can include instructions for creating a certificate of insurance according to a determination that the insurance requirements have been met and storing the certificate of insurance in the distributed ledger.","1. A computerized system for management of insurance risk and insurance events comprising: a kiosk having a kiosk computer readable medium uniquely associated with a construction site and in communication with a distributed ledger;a sensor in communications with the kiosk for detecting a presence of workers at a construction site, materials at the construction site, events and conditions occurring at the construction site;a set of non-transitory computer readable instructions included in the kiosk computer readable medium for: for each worker present at the construction site, determining an arrival time, a departure time, an amount of time worked and worker class,identifying a set of construction materials delivered to the construction site,detecting an installation action performed by a worker when installing the set of construction materials,detecting an insurance event,identifying a set of environmental conditions associated with the insurance event,associating the insurance event with the set of environmental conditions,creating an insurance record according to the insurance event, and transmitting the insurance record to a third party computer device.","31","16/997840","2020-08-19","2021-0035231","2021-02-04","11449949","2022-09-20","SCIENTIA POTENTIA EST., LLC","Jeremy Blackburn | Tim McVicker | Justin Southward | W. Kurt Taylor | Karl David | Austi Critchfield | Michael Lu","","","","G06Q-0040/08","G06Q-0040/08 | A61B-0005/6887 | B64C-0039/024 | B64D-0047/08 | G06K-0019/0723 | G06Q-0010/0875 | G06Q-0010/103 | G06Q-0010/109 | G06Q-0010/1057 | G06Q-0040/125 | G06Q-0050/08 | G06Q-0050/265 | G06V-0020/41 | G16H-0040/67 | G16H-0050/30 | H04N-0005/76 | A61B-2503/20 | B64C-2201/127 | G06V-0020/44","G06Q-040/08","G06Q-040/08 | G06Q-050/08 | G06Q-010/10 | G06Q-050/26 | G16H-040/67 | G16H-050/30 | G06Q-040/00 | G06Q-010/08 | G06K-019/07 | B64C-039/02 | B64D-047/08 | H04N-005/76 | A61B-005/00 | G06V-020/40","","","","","","4922038004986"
"US","US","P","B2","Skin condition measuring module","A skin condition measuring module according to the present invention can be designed to effectively diagnose a user's skin condition while being installed within or connected with: a kiosk; a skin condition measuring mirror; a vending machine for performing skin condition measurement and providing cosmetic products; or a module-embedded skin condition measuring device.","1. A skin condition measuring module installed in a skin condition measuring device or configured to interwork with the skin condition measuring device to diagnose a skin condition of a user, and including a user face capturing camera control unit and a light source control unit, the skin condition measuring module comprising: a memory in which software for driving the module is stored;a processor configured to form a single user face image for measuring the skin condition, which is acquired by a user face capturing camera, or configured to form an entire user face image for measuring the skin condition by extracting specific areas of a user face from a plurality of user face images acquired based on a plurality of capturing parameters set to extract the specific areas of the user face and composing the extracted specific areas of the user face;a sensor unit having a distance sensor for measuring a distance between the user and the camera; andan output control unit for controlling an output device which is configured to suggest the user to move a face position when a partial area of the user face deviates from the user face image acquired by the user face capturing camera, suggest the user to move the face position when the distance between the user and the camera, which is measured by the distance sensor for measuring the distance between the user and the camera, deviates from a preset value, or suggest the user to move the face position when the partial area of the user face deviates from the user face image acquired by the user face capturing camera, and the distance between the user and the camera, which is measured by the distance sensor for measuring the distance between the user and the camera, deviates from the preset value.","10","16/960100","2018-12-28","2021-0065369","2021-03-04","11449997","2022-09-20","LULULAB INC.","Sang Wook Yoo | Yong Joon Choe","10-2018-0002898","KR","2018-01-09","G06T-0007/0014","G06T-0007/0014 | A61B-0005/0013 | A61B-0005/0079 | A61B-0005/441 | A61B-0005/486 | A61B-0005/7207 | A61B-0005/7282 | G06Q-0030/0631 | G06T-0007/80 | G16H-0030/40 | G16H-0040/67 | G16H-0050/20 | H04N-0005/2351 | H04N-0005/265 | A61B-2576/02 | G06T-2207/20212 | G06T-2207/30088 | G06T-2207/30201","G06T-007/00","G06T-007/00 | G06T-007/80 | G16H-040/67 | G16H-050/20 | G16H-030/40 | A61B-005/00 | G06Q-030/06 | H04N-005/235 | H04N-005/265","","","","","","4922038005034"
"US","US","P","B2","SpO2 miniprogram: AI SpO2 measurement app","There is included an apparatus and system including video capture code configured to control a camera of a mobile phone to capture a video of at least a portion of skin, extraction code configured to extract at least one photoplethysmogram (PPG) signal from the video, determination code configured to determine a peripheral oxygen saturation (SpO2) value based on the PPG signal with respect to a blood flow at the portion of skin, and display code configured to control a display of the mobile phone to display the SpO2 value.","1. An apparatus comprising: at least one memory configured to store computer program code; andat least one hardware processor configured to access said computer program code and operate as instructed by said computer program code, said computer program code including:video capture code configured to cause the at least one hardware processor to control a camera of a mobile phone to capture a video of at least a portion of skin;extraction code configured to cause the at least one hardware processor to extract at least one photoplethysmogram (PPG) signal from the video;analysis code configured to cause the at least one hardware processor to implement a statistical processing of the PPG signal and to remove a portion of the PPG signal based on determining the portion to be noisy data by comparing a result of the statistical processing to at least one predetermined threshold;determination code configured to cause the at least one hardware processor to determine a peripheral oxygen saturation (SpO2) value based on the PPG signal, after the portion is removed from the PPG signal, with respect to a blood flow at the portion of skin; anddisplay code configured to cause the at least one hardware processor to control a display of the mobile phone to display the SpO2 value.","20","16/869740","2020-05-08","2021-0345892","2021-11-11","11432727","2022-09-06","TENCENT AMERICA LLC","Lianyi Han | Chao Huang | Xiaozhong Chen | Zhimin Huo | Shih-Yao Lin | Hui Tang | Tao Yang | Kun Wang | Wei Fan","","","","A61B-0005/0205","A61B-0005/0205 | A61B-0005/0261 | A61B-0005/14552 | A61B-0005/6826 | A61B-0005/6898 | A61B-0005/742 | G06F-0003/14 | G06F-0017/18 | G06K-0007/10722 | G06K-0007/1413 | G06K-0007/1417 | G06Q-0050/01 | H04N-0005/23203 | A61B-2560/0431","A61B-005/1455","A61B-005/1455 | A61B-005/0205 | G06K-007/14 | G06K-007/10 | G06F-017/18 | G06Q-050/00 | G06F-003/14 | A61B-005/026 | A61B-005/00 | H04N-005/232","","","","","","4922036001002"
"US","US","P","B2","Mobile device avatar generation for biofeedback to customize movement control","A system has a sensor that senses baseline sensor data corresponding to one or more baseline movements of a user and senses dynamic sensor data corresponding to one or more dynamic movements of the user. The dynamic sensor data is captured after the baseline sensor data. Furthermore, the system has an image capture device integrated within a mobile computing device. The image capture device captures baseline image capture data corresponding to the one or more baseline movements of the user and captures dynamic image capture data corresponding to the one or more dynamic movements of the user. The dynamic image capture data is captured after the baseline image capture data. Additionally, the mobile computing device is in operable communication with the sensor.","1. A system comprising: a sensor configured to sense baseline sensor data corresponding to one or more baseline movements of a user and senses dynamic sensor data corresponding to one or more dynamic movements of the user, the dynamic sensor data being captured after the baseline sensor data;an image capture device integrated within a mobile computing device, the image capture device configured to capture one baseline image capture data corresponding to the one or more baseline movements of the user and configured to capture dynamic image capture data corresponding to the one or more dynamic movements of the user, the dynamic image capture data being captured after the baseline image capture data, the mobile computing device being in operable communication with the sensor;a processor that is programmed to generate user-specific baseline data based upon the baseline sensor data and the baseline image capture data, generate one or more predetermined biomechanical rules according to the user-specific baseline data, generate a virtual avatar based upon the one or more predetermined biomechanical rules, generate one or more cues corresponding to the virtual avatar based upon non-compliance of one or more real-world movements of the user with the one or more predetermined rules, filter the baseline sensor data according to an optimal sensor-specific plane that intersects the user during the one or more baseline movements, filter the baseline image capture data according to an optimal image capture device-specific plane that intersects the user during the one or more baseline movements, and composite the remaining data as the baseline data for generation of the virtual avatar, the one or more real-world movements being determined from the dynamic sensor data and the dynamic image capture data, the virtual avatar having virtual movements that are synchronized in real-time with the corresponding one or more real-world movements; anda memory device configured to store the user-specific baseline data in a baseline data structure for access by the processor.","18","16/837300","2020-04-01","2021-0307650","2021-10-07","11426099","2022-08-30","QUANTUM ATHLETE TECHNOLOGY LLC","Andrew Barr","","","","A61B-0005/1121","A61B-0005/1121 | A61B-0005/1116 | A61B-0005/1128 | G06F-0003/011 | G06T-0007/20 | G06V-0020/20 | G06V-0040/20 | G06V-0040/23 | G16H-0020/30 | G06T-2207/30196","A61B-005/11","A61B-005/11 | G06K-009/00 | G06V-040/20 | G06T-007/20 | G06V-020/20 | G16H-020/30 | G06F-003/01","","","","","","4922035000965"
"US","US","P","B2","System using eye tracking data for analysis and validation of data","The present invention generally relates to the field of automated and flexible information extraction for review and analysis of computer code. In particular, the novel present invention provides a unique platform for analyzing, classifying, extracting, and processing information using multichannel input from user devices and optical tracking sensors and employing the use of behavioral cloning network (BCN) technology. Embodiments of the inventions are configured to provide an end to end automated solution for extracting data from code review processes that can be used to automate and accelerate the code review and validation methods.","1. A system for analysis and validation of code data, the system comprising: at least one memory device with computer-readable program code stored thereon;at least one communication device;at least one electrooculography device;at least one processing device operatively coupled to the at least one memory device and the at least one communication device, wherein executing the computer-readable code is configured to cause the at least one processing device to: receive request data comprising code review data and code change data from a first data channel;generate eye movement data from a second data channel, wherein the eye movement data is generated using an electrooculography device;analyze the request data via a machine learning model to determine a relationship between code review data and code change data;determine contextual relationship between the eye movement data and analyzed request data;generate a modeling layer based on the analyzed request data and determined contextual relationship between the eye movement data and request data;apply the modeling layer to additional request data as a code review policy;back propagate the code review policy to compare results of the code review policy to labeled documentation generated by human review; andalter the modeling layer based on the results of the code review policy as compared to labeled documentation.","20","16/901402","2020-06-15","2021-0386313","2021-12-16","11426116","2022-08-30","BANK OF AMERICA CORPORATION","Madhusudhanan Krishnamoorthy","","","","A61B-0005/398","A61B-0005/398 | A61B-0003/113 | G06F-0003/013 | G06N-0003/084 | G06N-0020/00","A61B-005/398","A61B-005/398 | G06N-020/00 | A61B-003/113 | G06N-003/08 | G06F-003/01","","","","","","4922035000982"
"US","US","P","B2","Surgical guidance system and method with acoustic feedback","A surgical system includes a surgical tool, a tracking system configured to obtain tracking data indicative of positions of the surgical tool relative to an anatomical feature, an acoustic device, and a computer system programmed to control the acoustic device to provide acoustic feedback to a user based on the tracking data.","1. A surgical system, comprising: a cutting tool;a tracking system configured to obtain tracking data indicative of positions of the cutting tool relative to an anatomical feature as the cutting tool cuts into the anatomical feature;an acoustic device; anda computer system programmed to control the acoustic device to provide acoustic feedback to a user indicating a cutting depth of the cutting tool into the anatomical feature based on the tracking data by indicating when the cutting depth is too shallow, too deep, and correct relative to a planned implant depth.","10","17/093084","2020-11-09","2021-0068905","2021-03-11","11426245","2022-08-30","MAKO Surgical Corp.","Arthur Quaid | Hyosig Kang | Dennis Moses | Rony Abovitz | Scott Illsley","","","","A61B-0034/30","A61B-0034/30 | A61B-0017/1675 | A61B-0017/1695 | A61B-0017/1703 | A61B-0017/1764 | A61B-0034/10 | A61B-0034/20 | A61B-0034/35 | A61B-0034/37 | A61B-0034/70 | A61B-0034/71 | A61B-0034/74 | A61B-0034/76 | A61B-0090/36 | A61B-0090/361 | A61B-0090/37 | A61F-0002/30942 | A61N-0001/0534 | A61N-0001/3605 | A61N-0001/372 | G06F-0003/016 | G16Z-0099/00 | A61B-0005/1127 | A61B-0005/745 | A61B-0017/1677 | A61B-0017/17 | A61B-0017/1739 | A61B-0017/1767 | A61B-0034/25 | A61B-0090/14 | A61B-2017/00115 | A61B-2017/00119 | A61B-2017/00725 | A61B-2034/102 | A61B-2034/105 | A61B-2034/107 | A61B-2034/108 | A61B-2034/207 | A61B-2034/2048 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2059 | A61B-2034/2068 | A61B-2034/252 | A61B-2034/254 | A61B-2034/305 | A61B-2090/08021 | A61B-2090/365 | A61B-2090/3937 | A61B-2090/3983 | A61F-0002/38 | A61F-2002/4632 | A61F-2002/4633 | G05B-2219/36432 | G05B-2219/39196 | G05B-2219/40478 | G05B-2219/45117 | G05B-2219/45171","A61B-034/30","A61B-034/30 | A61F-002/30 | A61B-017/16 | A61B-090/00 | A61B-034/20 | A61B-034/00 | A61B-034/37 | A61B-034/10 | A61B-017/17 | G06F-003/01 | A61B-034/35 | A61N-001/05 | A61N-001/36 | A61N-001/372 | G16Z-099/00 | A61F-002/46 | A61B-005/00 | A61B-090/14 | A61B-005/11 | A61B-017/00 | A61F-002/38","","","","","","4922035001110"
"US","US","P","B2","Hand sanitizer station","A system and method for ultraviolet light-mediated hand sanitization and hands-free selection of content presented by a hand sanitizer station. The method includes detecting, using a sensor, an initiation event; responsive to the detection of the initiation event, activating an ultraviolet light source and sanitizing a surface of an object within a compartment adjacent to the ultraviolet light source; and visually presenting a graphic element associated with content on a display for presentation to a user, the display communicatively coupled to the ultraviolet light source.","1. A system comprising: an ultraviolet light source configured to be activated to sanitize a surface of an object within a compartment adjacent to the ultraviolet light source, the compartment receiving the object to be sanitized; anda display configured to visually present a graphic element associated with content for presentation to a user, the display communicatively coupled to the ultraviolet light source for presenting the content associated with the graphic element to the user when the ultraviolet light source is activated and the graphic element being touch-lessly selected from a plurality of graphic elements associated with the content based on a detection of the user'ss movement in proximity to the display.","16","16/890251","2020-06-02","2020-0298017","2020-09-24","11426601","2022-08-30","HYGENIA UVC LLC","George Scritchfield | Nigel Waites","","","","A61N-0005/0624","A61N-0005/0624 | G06F-0003/011 | G06V-0040/107 | G06V-0040/161 | A61N-2005/0627 | A61N-2005/0643 | A61N-2005/0661","A61B-005/06","A61B-005/06 | A61N-005/06 | G06F-003/01 | G06V-040/10 | G06V-040/16","","","","","","4922035001461"
"US","US","P","B1","Green Screen?health verification system (GS-HVS)","A system and method of testing, recording, tracking, and verifying the health of an individual to other persons while providing confidence that the individual user has taken and reported to at least one medical measurement, and this measurement has been identified as normal or not normal to the specific individual user, is provided. A medical instrument which is self-administered by the said individual user and a Green Screen Health Verification System (GS-HVS) server are provided. Software that contains prearranged decisions and protocols to provide guidance to said individual user and to an individual user's supervisor and records any abnormal reporting from said individual user and provides alerts based on said protocol. Software returns guidance from said GS-HVS server to said individual user, and upon said GS-HVS indicting that said individual user has good health, software displays a good health status or ""Green Screen"" on said individual user's said mobile device.","1. A Green Screen Health Verification System (GS-HVS) for testing, recording, tracking, and verifying the health of an individual user to other persons while providing confidence that the individual user has taken and reported at least one medical measurement, and this measurement has been determined to be normal or not normal to the specific individual user, the system comprising: information from a medical instrument designed to be self-administered by the said individual user or assisted by another person;a portable communication device comprising one of a cell phone, laptop or electronic tablet with a digital display and two-way communications with digital communication systems;a communications system that links said portable communications device with a GS-HVS server;said GS-HVS server includes software that communicates with said individual user'ss portable communication device;software that contains prearranged decisions and protocols to provide guidance to said individual user and to said individual user'ss supervisor or medical advisor as to what action said individual user should take;software that returns guidance from said GS-HVS server to said individual user stating when they are medically acceptable to go to work or should take other actions such as contacting said supervisor for additional direction;where said GS-HVS indicated that said individual user has good health, software that communicates with said individual user'ss said mobile device and displays a good health status or ""Green Screen"" on said individual user'ss said mobile device;software that allows said individual user to present said GS-HVS results as a screen view to a third party that said individual user desired to show the results to confirm health status;software that records all medical reports from said individual user; software that recognizes in accordance with computer protocol any abnormal reporting on a health question list from said individual user and provides any alerts based on said computer protocol; and,a supervisor or medical advisor that provides advice in accordance with an advice protocol to said individual user on any possible problems with said individual user based on said individual user'ss input to said GS-HVS.","20","17/128715","2020-12-21","","","11430553","2022-08-30","IDEAL INNOVATIONS INC.","Robert William Kocher | Douglas Earl Dyer | John Shelly Bowling, II","","","","G16H-0015/00","G16H-0015/00 | A61B-0005/7465 | G06K-0007/1095 | G06K-0007/1447 | G06K-0019/06112 | G06Q-0010/105 | G07C-0009/00 | G16H-0010/60 | G16H-0010/65 | G16H-0040/20 | G16H-0040/67 | G16H-0050/00 | G16H-0050/20 | G16H-0050/30 | G16H-0070/20 | G06K-0007/1417 | G06K-0019/06037 | G06Q-0050/265","G16H-015/00","G16H-015/00 | G06K-019/06 | G06K-007/10 | G06K-007/14 | G07C-009/00 | G16H-050/30 | G16H-050/00 | G16H-040/67 | G16H-050/20 | G16H-070/20 | G06Q-010/10 | G16H-040/20 | A61B-005/00 | G16H-010/65 | G16H-010/60 | G06Q-050/26","","","","","","4922035005386"
"US","US","P","B2","Controlled treatment of tissue and dynamic interaction with, and comparison of, tissue and/or treatment data","An interactive treatment mapping and planning system enables a user to more quickly, thoroughly, and efficiently aggregate fibroid and/or treatment information from a user and/or one or more sets of databases, construct a fibroid map providing a visual representation of the aggregated fibroid information, generate information from the aggregated information about the fibroid to be treated and/or treatment procedure, develop a treatment plan based on the fibroid and/or treatment procedure information, provide real-time information gathered from treatment devices during the treatment procedure, and allow the user to interact with the treatment data.","1. A system comprising: a non-transitory computer readable storage medium configured to store program instructions; anda processor programmed to execute the program instructions to cause the processor to: aggregate, from one or more data sources, a plurality of fibroid data records;extract, from the plurality of fibroid data records, fibroid data record items; andgenerate user interface data for rendering an interactive user interface on an electronic display, the interactive user interface including: a fibroid map displaying several views of a uterus including a first uterus view and a second uterus view, the first uterus view being different than the second uterus view, anda fibroid icon of a fibroid data record item, the fibroid icon comprising a first marker on the fibroid map at a location associated with the fibroid data record item, the fibroid icon configured to provide information relating to the fibroid data record item,wherein, when a user places a first marker in one of the first uterus view or the second uterus view, the system automatically populates another one of the first uterus view or the second uterus view with a second marker at the same location.","20","17/233838","2021-04-19","2021-0346095","2021-11-11","11419682","2022-08-23","GYNESONICS, INC.","Amer Hammudi | Jiayu Chen | Harry Kwan | David Blair Toub","","","","A61B-0034/10","A61B-0034/10 | A61B-0008/08 | A61B-0008/12 | A61B-0034/20 | A61B-0034/25 | A61B-0090/37 | G06F-0003/0484 | G16H-0040/63 | A61B-0005/4325 | A61B-2017/00053 | A61B-2017/4216 | A61B-2034/107 | A61B-2034/256 | A61B-2090/378 | A61B-2090/3975","A61B-034/10","A61B-034/10 | G16H-040/63 | G06F-003/0484 | A61B-034/20 | A61B-008/12 | A61B-008/08 | A61B-090/00 | A61B-034/00 | A61B-005/00 | A61B-017/00 | A61B-017/42","","","","","","4922034001153"
"US","US","P","B2","Method for determining a design of a customized footwear object","A method for determining a design of a customized footwear object for a wearer is provided. The method includes steps of: receiving a user input of a requirement data set regarding features that are related to the wearer; obtaining a cushion data set for producing a cushion of the footwear object based on the requirement data set, the cushion data set including data regarding a position of the cushion, data regarding a shape of the cushion, data regarding a material for the cushion and data regarding a hardness of the cushion; generating an order for production of the footwear object, the order including the cushion data set; and providing the order to a forming machine in order for the forming machine to produce the footwear object.","1. A method for determining a design of a customized footwear object to be implemented using an electronic device executing an application for ordering the footwear object, the footwear object being customized for a wearer, and having a substrate which is an elastic film and a cushion embedded in the substrate, the method comprising steps of: receiving a user input of a requirement data set regarding features that are related to the wearer;obtaining a cushion data set for producing the cushion of the footwear object based on the requirement data set, the cushion data set including data regarding at least one of a position of the cushion, data regarding a shape of the cushion, data regarding a material for the cushion, or data regarding a hardness of the cushion;generating an order for production of the footwear object, the order including the cushion data set; andproviding the order to a forming machine for the forming machine to produce the footwear object;wherein the step of obtaining a cushion data set includes obtaining the data regarding the hardness of the cushion by determining a value of density of the cushion based on the requirement data set, and implementing either a first calculation or a second calculation,wherein, in the first calculation, a value of volume of the cushion to be embedded in the substrate is calculated with a given value of weight of the cushion based on the value of density, and the data regarding the hardness of the cushion includes the value of volume of the cushion and the given value of weight of the cushion,wherein, in the second calculation, a value of weight of the cushion to be embedded in the substrate is calculated with a given value of volume of the cushion based on the value of density, and the data regarding the hardness of the cushion includes the value of weight of the cushion and the given value of volume of the cushion.","8","17/023213","2020-09-16","2021-0256585","2021-08-19","11423455","2022-08-23","CHAEI HSIN ENTERPRISE CO., LTD.","Shui-Mu Wang","2020105353","TW","2020-02-19","G06Q-0030/0621","G06Q-0030/0621 | A43D-0119/00 | G06F-0030/10 | G06K-0007/1413 | G06K-0019/06028 | G06Q-0010/0875 | G06Q-0030/0185 | G06Q-0030/0635 | G06Q-0050/04 | A61B-0005/112 | G06F-2119/18","G06Q-030/06","G06Q-030/06 | G06F-030/10 | A43D-119/00 | G06K-007/14 | G06K-019/06 | G06Q-010/08 | G06Q-030/00 | G06Q-050/04 | G06F-119/18 | A61B-005/11","","","","","","4922034004893"
"US","US","P","B2","System and method for waist circumference measurement and feedback for optimal placement of a smart belt","The present invention relates generally to the field of providing personalized health management to users, particularly for users suffering from obesity and obesity-related medical conditions, in order to enhance weight loss through a smart belt that is optimally positioned around the user's waist, and which facilitates weight loss and mindfulness through feedback delivered via the smart belt.","1. A system for providing feedback for properly positioning a smart belt around a waist of a user, comprising: a detection unit coupled to the smart belt, the detection unit configured to detect a height of the smart belt relative to a ground plane;a processor coupled to the detection unit, the processor configured to receive the detected height from the detection unit, the processor further configured to determine an optimal height for the smart belt based on the detected height and aggregate smart belt height data for other users with similar demographic or physiological characteristics of the user; anda feedback unit coupled to the smart belt, the feedback unit configured to deliver feedback to instruct the user to position the smart belt at the optimal height.","20","16/587895","2019-09-30","2021-0093030","2021-04-01","11412797","2022-08-16","Sunil Daniel","Sunil Daniel","","","","A41F-0009/025","A41F-0009/025 | A41D-0001/002 | A61B-0005/002 | A61B-0005/1072 | A61B-0005/684 | A61B-0005/6823 | A61B-0005/6831 | G06F-0001/163 | G06F-0003/011 | G06F-0003/016 | G06N-0020/00 | H04L-0067/306","G08B-021/00","G08B-021/00 | A41F-009/02 | G06N-020/00 | G06F-003/01 | G06F-001/16 | H04L-067/306 | A41D-001/00 | A61B-005/00 | A61B-005/107","","","","","","4922033000873"
"US","US","P","B2","Methods for computing a real-time step length and speed of a running or walking individual","A method for calculating an estimation of the speed {circumflex over (V)}[n′] of an individual walking along a path, over a time window including: computing a step length  [n′] of the individual, calculating the speed {circumflex over (V)}[n′] using the following formula: {circumflex over (V)}[n′]= [n′]× [n′].","1. A method for computing an estimation of a step length [n′] of an individual running along a path, over a time window n′, n′ being an integer higher than or equal to 1, said individual being equipped with a wrist-wearable device comprising: a calculation circuit,a GNSS transponder configured to deliver instant speeds of the wrist-wearable device,a barometer configured to deliver instant atmospheric pressure measures (P),an accelerometer configured to deliver instant acceleration measures along three perpendicular axes, namely: a first acceleration Sx along an X axis perpendicular to a wrist and parallel to a sagittal plane (P1) of the individual;a second acceleration Sy along a Y axis parallel to the wrist and parallel to the sagittal plane (P1) of the individual;a third acceleration Sz along a Z axis perpendicular to the wrist and parallel to a front plane (P2) of the individual;said method comprising the following steps, carried out by the calculation circuit: computing a vector of features X[n] over a time window n, n being an integer higher than or equal to 1, as follows: X[n]=[1F1[n]F2[n]F3[n]F4[n]F22[n] where: feature F1[n] is an instant cadence [n′] of the individual over time window n, computed from norms of the instant acceleration measures Sx[n],Sy[n],Sz[n] delivered by the accelerometer over time window n,feature F2[n] is an instant slope of the path computed from the instant atmospheric pressure measures (P[n]) delivered by the barometer over time window n,feature F3[n] is an instant energy of the second acceleration measures Sy[n] delivered by the accelerometer over time window n, andfeature F4[n] is an instant mean absolute jerk of the second acceleration measures Sy[n] delivered by the accelerometer over time window n, andcomputing a coefficient vector β[n] over time window n as follows: β[n]=β[n?1]+D[n]X[n](SL[n]?X[n]Tβ[n?1]) where: D[n] is a dispersion matrix of the instant vector of features X[n], V[n] is a speed of the wrist-wearable device, measured by the GNSS transponder over time window n,β[0] is an initial parameter,computing a vector of features X[n′] over time window n′, as follows: X[n′]=[1F1[n′]F2[n′]F3[n′]F4[n′]F22[n′]], and computing the step length [n′] as follows: [n′]=X[n′]β[n].","12","16/428001","2019-05-31","2020-0000374","2020-01-02","11412956","2022-08-16","THE SWATCH GROUP RESEARCH AND DEVELOPMENT LTD","Abolfazl Soltani | Hooman Dejnabadi | Kamiar Aminian","2018-180129","EP","2018-06-27","A61B-0005/112","A61B-0005/112 | A61B-0005/681 | G06F-0003/011 | G06F-0003/017 | G06F-0017/16 | A61B-2562/0219 | A61B-2562/0247","A61B-005/11","A61B-005/11 | A61B-005/00 | G06F-003/01 | G06F-017/16","","","","","","4922033001031"
"US","US","P","B2","Dynamically adjustable training simulation","Systems, methods and program products for administering and modifying a simulation that incorporates the use of biometric data collection and computer-based predictive modeling techniques to reliably identify changes in the stress level of simulation participants while predictively adapting the simulation environment to manage each participants' stress levels within a pre-set or desired range. Each participant may have their biometric data collected and continuously monitored by wearing a computing device comprising biometric sensors, camera systems and audio recording devices. As the participants biometric data changes, the system monitors the participant for changes in stress level and maintains a desired stress level in the participant by modifying the adjustable properties of one or more virtual objects or computer-accessible real physical objects of the simulation environment to increase or decrease the stress level of the simulation on participants.","1. A computer-implemented method comprising the steps of: selecting a simulation scenario comprising a simulation environment and an assigned role to a plurality of participants of the simulation scenario;assigning an objective for each of the plurality of participants, wherein said objective includes an acceptable stress level range for each of the plurality of participants;receiving biometric data collected by one or more biometric sensors associated with each of the plurality of participants;identifying a stress level as a function of the biometric data, associated with a participant selected from the plurality of participants, wherein said stress level is outside of the acceptable stress level range;predicting a modification to one or more elements of the simulation environment that would return the stress level of the participant back within the acceptable stress level range;applying a stochastic simulation or machine learning to predict an impact of the modification on the stress level of each of the plurality of participants;selecting the modification predicted to maintain stress levels of each of the plurality of participants within the acceptable stress level range; andapplying the modification to one or more elements of the simulation environment in real-time, during commencement of the simulation.","17","16/205260","2018-11-30","2020-0175123","2020-06-04","11416651","2022-08-16","INTERNATIONAL BUSINESS MACHINES CORPORATION","Melanie E. Roberts | Roslyn I. Hickson | Olivia J. Smith | Manoj Gambhir","","","","G06F-0030/20","G06F-0030/20 | A61B-0005/165 | G06F-0003/015 | G06N-0007/005 | G06N-0020/00 | G06F-2203/011","G06F-030/20","G06F-030/20 | G09G-005/08 | G06F-003/01 | G06N-007/00 | G06N-020/00 | A61B-005/16","","","","","","4922033004684"
"US","US","P","B2","Generating clinical forms","A machine learning system may be used to predict clinical questions to ask on a clinical form. A first encoder may encode first information and a second encoder may encoder second information from a medical record of a past appointment. The first and second encoded information and additional encoded information may be used to predict a clinical question to ask by using a reinforcement learning system. The reinforcement learning system may be trained by receiving ratings of questions from users.","1. A computer-implemented method for generating questions for a clinical form, the method comprising: training a first neural network autoencoder to encode information of a first type into a first vector representation, the first autoencoder comprising a first encoder that maps a first input vector to the first vector representation and a first decoder that maps the first vector representation to a first output vector, where the first decoder is trained to reduce the distance between the first output vector and the first input vector;training a second neural network autoencoder to encode information of a second type into a second vector representation, the second autoencoder comprising a second encoder that maps a second input vector to the second vector representation and a second decoder that maps the second vector representation to a second output vector, where the second decoder is trained to reduce the distance between the second output vector and the second input vector;providing a medical record of a past appointment of a patient;extracting from the medical record a first portion of information of the first type and a second portion of information of the second type;inputting the first portion of information to the first neural network autoencoder to output first encoded information;inputting the second portion of information to the second neural network autoencoder to output second encoded information;providing a database of clinical questions;inputting the first encoded information and second encoded information into a third neural network encoder to generate an aggregate encoding;predicting, by a question predictor, a clinical question to ask from the database of clinical questions based on the aggregate encoding.","28","16/457803","2019-06-28","2020-0035335","2020-01-30","11417417","2022-08-16","DRCHRONO INC.","Daniel Kivatinos | Michael Nusimow | Martin Borgt | Soham Waychal","","","","G16H-0010/20","G16H-0010/20 | G06F-0017/16 | G06K-0009/6215 | G06N-0003/0454 | G06N-0005/022 | G06N-0005/027","G16H-070/60","G16H-070/60 | G16H-050/20 | G16H-070/20 | A61B-005/11 | G16H-010/20 | G06N-003/04 | G06N-005/02 | G06K-009/62 | G06F-017/16","","","","","","4922033005442"
"US","US","P","B2","Medication adherence monitoring system and method","A medication management system is described that is operable to determine whether a user is actually following a protocol, provide additional assistance to a user, starting with instructions, video instructions, and the like, and moving up to contact from a medication administrator if it is determined that the user would need such assistance in any medical adherence situation, including clinical trial settings, home care settings, healthcare administration locations, such as nursing homes, clinics, hospitals and the like. Suspicious activity on the part of a patient or other user of the system is identified and can be noted to a healthcare provider or other service provider where appropriate.","1. A system for determining duplicate enrollment in a clinical trial, the system comprising: an image capture device configured to capture one or more video sequences associated with administration of medication by a first user;one or more processors configured to perform operations comprising receiving the captured one or more video sequences, anddetermining whether an image of the first user in the captured one or more video sequences is biometrically similar to an image of a second user included in another one or more captured video sequences associated with the second user; anda presentation device configured to present an alert to an operator to manually review images of the first user and the second user to confirm whether the first user and the second user are the same user.","20","17/166995","2021-02-03","2021-0158925","2021-05-27","11417422","2022-08-16","AIC INNOVATIONS GROUP, INC.","Adam Hanina | Gordon Kessler | Lei Guan","","","","G16H-0020/10","G16H-0020/10 | A61B-0005/0022 | A61B-0005/1128 | A61B-0005/4833 | G06K-0009/00 | G06Q-0050/22 | G06V-0020/52 | G16H-0020/13 | G16H-0040/67 | H04N-0007/183","G16H-020/10","G16H-020/10 | G06K-009/00 | G06Q-050/22 | A61B-005/11 | A61B-005/00 | G16H-020/13 | H04N-007/18 | G16H-040/67 | G06V-020/52","","","","","","4922033005447"
"US","US","P","B2","Orientation independent sensing, mapping, interface and analysis systems and methods","The disclosure relates generally to applications of Orientation Independent Sensing (OIS) and Omnipolar mapping Technology (OT) to various system, device and method embodiments as recited herein. Similarly, systems and methods suitable for supporting OIS and OT systems and methods are disclosed. Further, OIS and OT implementations that provide end user interfaces, diagnostic indicia and visual displays generated, in part, based on measured data or derived from measured data are also disclosed. Embodiments also describe applying optimization techniques to determine the greatest voltage difference of a local electric field associated with an electrode-based diagnostic procedure and a vector representation thereof. Various graphic user interface related features are also described to facilitate orientation and electrode clique signal display.","1. A system for mapping cardiac electrophysiology information from a subject using a plurality of electrodes, the system comprising: an electronic control unit configured to: receive cardiac electrogram signals from the plurality of electrodes;determine electric field data from the cardiac electrogram signals over a depolarization;define a vector {circumflex over (m)} based on the electric field data;determine a cardiac parameter by performing a vector operation upon (i) the vector {circumflex over (m)} or a vector perpendicular thereto {circumflex over (m)}⊥ and(ii) a diagnostic vector generated using the cardiac electrogram signals; andgenerate an output to a user or process based on the cardiac parameter, the output including a scalar output or a vector output; anda user interface configured to display the output or information correlated with the output.","20","16/904966","2020-06-18","2020-0315484","2020-10-08","11406312","2022-08-09","ST JUDE MEDICAL CARDIOLOGY DIVISION, INC","Don Curtis Deno | Dennis J. Morgan | Joshua C. Bush | Kumaraswamy Nanthakumar | Stephane Masse | Karl Magtibay","","","","A61B-0005/341","A61B-0005/341 | A61B-0005/287 | A61B-0005/316 | A61B-0005/333 | A61B-0005/339 | A61B-0005/361 | A61B-0005/7203 | A61B-0005/746 | G06F-0003/0482","A61B-034/20","A61B-034/20 | A61B-018/14 | A61B-005/341 | G06F-003/0482 | A61B-005/00 | A61B-005/287 | A61B-005/316 | A61B-005/333 | A61B-005/339 | A61B-005/361","","","","","","4922032000974"
"US","US","P","B2","Alert generation based on a cognitive state and a physical state","Provided are techniques for alert generation based on a cognitive state and a physical state. Wearables data, text communications, voice communications, and images of a caregiver providing care to a patient are obtained. The wearables data, the text communications, the voice communications, and the images are analyzed to identify a cognitive state and a physical state of the caregiver. The cognitive state and the physical state are compared to one or more care rules associated with tasks of the caregiver for the patient. In response to the comparison indicating any one of the cognitive state and the physical state prevent the caregiver from executing the tasks, an alert is generated. One or more recommendations for resolving the alert based on one or more recommendation rules are generated.","1. A computer-implemented method, comprising operations for: obtaining wearables data, text communications, voice communications, and images of a caregiver providing care to a patient;analyzing the wearables data, the text communications, the voice communications, and the images to identify a cognitive state and a physical state of the caregiver;comparing the cognitive state and the physical state to one or more care rules associated with tasks of the caregiver for the patient;training an alerts machine learning module by calibrating first weights;in response to the comparison indicating any one of the cognitive state and the physical state prevent the caregiver from executing the tasks, generating an alert using the trained alerts machine learning module that receives inputs of the cognitive state and the physical state and outputs the alert;training a recommendation machine learning module by calibrating second weights; andgenerating one or more recommendations for resolving the alert using the trained recommendation machine learning module that receives input of the cognitive state, the physical state, and the alert and outputs the one or more recommendations.","18","16/533550","2019-08-06","2021-0042680","2021-02-11","11410110","2022-08-09","INTERNATIONAL BUSINESS MACHINES CORPORATION","Fang Lu | Paul R. Bastide | Ishwarya Rajendrababu | SathyaNarayanan Srinivasan","","","","G06Q-0010/06398","G06Q-0010/06398 | A61B-0005/0024 | G06F-0001/163 | G06F-0040/20 | G06N-0005/02 | G06N-0020/00 | G10L-0015/183 | A61B-2505/01","G06Q-010/06","G06Q-010/06 | G06Q-010/10 | G06N-005/02 | G10L-015/183 | G06N-020/00 | A61B-005/00 | G06F-001/16 | G06F-040/20","","","","","","4922032004746"
"US","US","P","B1","Systems and methods for providing haptic feedback when interacting with virtual objects","Systems and methods for providing enhanced surface electrical neurostimulation and haptic feedback to a user within a simulation environment are provided. Enhanced surface electrical neurostimulation (eSENS) platforms are able to elicit distally referred tactile percepts while avoiding large charge densities as a method to deliver intuitive haptic feedback during functional tasks.","1. A system for providing enhanced surface electrical neurostimulation and haptic feedback to a user within a simulation environment, the system comprising: a neurostimulation subsystem,a contactless real time tracking subsystem,an object properties database,a processor; anda machine-readable medium in operable communication with the processor, the neurostimulation subsystem, the contactless real time tracking subsystem, and the object properties database, the machine-readable medium having instructions stored thereon that, when executed by the processor, perform the following steps: collecting, from at least one of the neurostimulation subsystem, the contactless real time tracking subsystem, and the object properties database, data comprising user input data, tracking data, and object properties data;processing the object properties data and the tracking data to create user object interaction information, a tactile contact model, and a neuromorphic mechanoreceptor activation model;processing the user input data and the neuromorphic mechanoreceptor activation model to create an enhanced neurostimulation response signal; anddelivering the enhanced neurostimulation response signal to the neurostimulation subsystem, thereby providing the enhanced surface electrical neurostimulation and haptic feedback,the neurostimulation response signal comprising one or more stimulation parameters derived from the user input data to fit the one or more stimulation parameters to an individual user based on user feedback, andthe one or more stimulation parameters comprising at least one of a pulse amplitude (PA) of less than 3000 microamps (μA), a pulse width (PW) of less than 800 microseconds (μs), and a pulse frequency (PF) of less than 300 Hz.","11","17/455817","2021-11-19","","","11402904","2022-08-02","THE FLORIDA INTERNATIONAL UNIVERSITY BOARD OF TRUSTEES","Ranu Jung | Andres E. Pena Serrada","","","","G06F-0003/014","G06F-0003/014 | A61B-0005/6824 | A61B-0005/6825 | A61N-0001/0456 | A61N-0001/0496 | A61N-0001/36034 | G06F-0003/015 | G06F-0003/016","G06F-003/01","G06F-003/01 | A61N-001/36 | A61B-005/00 | A61N-001/04","","","","","","4922031004155"
"US","US","P","B2","Brain computer interface for augmented reality","An apparatus, system, and method of a brain computer interface in a headset including an augmented reality display, one or more sensors, a processing module, at least one biofeedback device, and a battery. The interface may include a printed circuit board that has the sensors to read bio-signals, provides biofeedback, and performs the processing, analyzing, and mapping of bio-signals into output. The output provides feedback via stimulation of multiple sensory brain systems of a user, including audio and visual on the augmented reality display, or audio and haptic in terms of vibration patterns that a human user may feel. All together this forms a closed-loop system, by detecting the bio-signal, then providing sensory-feedback, which in turn enhances the bio-signal.","1. A system of a fully self-contained brain computer interface (BCI) in a wireless headset, the system comprising: an augmented reality display;at least two sensors for reading a bio-signal from a user, the bio-signal comprising at least one of EEG (Electroencephalography), ECG (Electrocardiography), functional near infrared spectroscopy (fNIRS), Magnetoencephalography (MEG), EMG (Electromyography), EOG (Electroocculography), and Time-Domain variants (TD-) of these bio-signal processing methods, a visually evoked potential, an audio evoked potential, a haptic evoked potential, and a motion evoked potential, and other bio-signals from multiple sources attached to other body parts other than a user'ss head;at least one processing module for the augmented reality display, including: a processor that renders a stimulation effect, wherein the stimulation effect is at least one of a timed visual stimulation on the augmented reality display, a timed audio stimulation, and a haptic stimulation on the fully self-contained BCI configured to evoke a measurable response in a user'ss brain; anda processor that analyzes and maps the bio-signal into a digital command, wherein the digital command includes at least one of instructions for a visual output configured for displaying on the augmented reality display and instructions for triggering a visual effect;at least one biofeedback device that produces at least one of a visual, audible, and tactile effect in communication with the processing module, wherein the at least one biofeedback device provides feedback to the user;a wireless network interface that transmits and receives data to and from other devices, wherein the data is at least one of stored, passed through, and processed on the fully self-contained BCI,a battery, wherein the battery provides power to one or more of the augmented reality display, the at least two sensors, the processing module, and the at least one biofeedback device;at least one of onboard storage or remote storage with enough memory to store, process and retrieve the data; anda printed circuit board, wherein the printed circuit board includes at least one of the at least two sensors, the processing module, the at least one biofeedback device, the battery, and combinations thereof,wherein the fully self-contained BCI in the wireless headset is an accessory apparatus that is configured to be temporarily or permanently mechanically integrated with another wearable device, the fully self-contained BCI configured to transfer data wirelessly or via a wired connection to the other wearable device.","7","17/222897","2021-04-05","2021-0223864","2021-07-22","11402909","2022-08-02","COGNIXION","Andreas Forsland | Leonard Zerman","","","","G06F-0003/015","G06F-0003/015 | G06F-0003/016 | G06F-0003/023 | G06F-0003/14 | G06F-0003/16 | G06N-0005/02 | G06N-0020/00 | A61B-0005/0075 | H04W-0084/18","G06F-003/01","G06F-003/01 | G06F-003/16 | G06F-003/023 | G06N-020/00 | G06N-005/02 | G06F-003/14 | A61B-005/00 | H04W-084/18","","","","","","4922031004160"
"US","US","P","B2","Fundus image capturing","An apparatus for producing a fundus image includes: a processor and a memory; an illumination component including a light source and operatively coupled to the processor; a camera including a lens and operatively coupled to the processor, wherein the memory stores instructions that, when executed by the processor, cause the apparatus to capture fundus images and provide controls for re-imaging the fundus.","1. A fundus imaging system, comprising: a handheld housing;a display supported by the handheld housing;a camera housed inside the handheld housing; anda computing device in communication with the display and the camera, the computing device including at least one processor and a memory storing instructions which, when executed by the at least one processor, cause the system to: capture a first image using the camera, the first image being captured at a first image point focus for a given eye;provide a control on the display;when the control is selected, capture a second image using the camera; anddisplay an exam summary interface on the display, the exam summary interface providing a summary of images captured by the camera including an image quality score for each captured image, wherein the image quality score includes a graphical representation of the image quality score, and wherein the image quality score for the second image is indented from the image quality score for the first image.","20","16/906057","2020-06-19","2020-0320703","2020-10-08","11403756","2022-08-02","WELCH ALLYN, INC.","Richard M. Farchione | Kristen L. Stebbins | Corrie A. Baum | Thomas A. Myers","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0003/0033 | A61B-0003/0041 | A61B-0003/1208 | G06F-0003/013 | G16H-0010/60 | G16H-0030/20 | G16H-0030/40 | G16H-0040/60 | A61B-0003/00 | A61B-0005/14555 | G06T-2207/30041","G06K-009/00","G06K-009/00 | G06T-007/00 | G06F-003/01 | G16H-010/60 | G16H-030/20 | G16H-040/60 | A61B-003/12 | A61B-003/00 | G16H-030/40 | A61B-005/1455","","","","","","4922031004998"
"US","US","P","B2","Strobing of active marker groups in performance capture","The present description relates to light patterns used in a live action scene of a visual production to encode information associated with objects in the scene, such as movement and position of the objects. A data capture system is enabled to differentiate between various groups of active markers attached to the objects in the scene. The groups of active markers emit particular wavelengths in strobing patterns predefined for the various groups. In some implementations, the groups are instructed to emit its assigned signature light pattern through a signal controller transmitting an initial key signature predefined for the group, followed by pattern signals to a control unit. The data representing the pattern is captured in illuminated and blank frames. Frames showing the light pattern are analyzed to extract information about the groups of active markers, such as distinguishing the groups and identifying the objects to which they are attached.","1. A method to distinguishing groups of active markers for performance capture associated with a visual production, the method comprising: specifying a first pattern for a first group of active markers to emit light of at least one wavelength and a second pattern different from the first pattern, for a second group of active markers to emit light of at least one wavelength;transmitting a first initial key sequence followed by first pattern signals, wherein the first initial key sequence is a trigger for receipt of the first pattern signals predefined for the first group of active markers, and further followed by a first terminal key sequence predefined for the first group of active markers to indicate an end of transmission of the first pattern signals;transmitting a second initial key sequence followed by second pattern signals, wherein the second initial key sequence is a trigger for receipt of the second pattern signals predefined for the second group of active markers, and further followed by a second terminal key sequence predefined for the second group of active markers to indicate an end of transmission of the second pattern signals;capturing, by a sensor device of a performance capture system, first data representing the light emitted by the first group of active markers, and second data representing the light emitted by the second group of active markers, in respective one or more illuminated frames and respective one or more blank frames of the performance capture system; anddistinguishing the first group of active markers and the second group of active markers by comparing the first data with the first pattern and the second data with the second pattern.","16","17/163276","2021-01-29","2021-0267493","2021-09-02","11403883","2022-08-02","UNITY SOFTWARE INC. | UNITY TECHNOLOGIES SF","Dejan Momcilovic | Jake Botting","","","","G06V-0040/23","G06V-0040/23 | A61B-0005/1113 | A61B-0005/1127 | A61B-0005/744 | A61B-0090/39 | G06F-0003/014 | G06F-0003/0308 | G06F-0003/0325 | G06T-0007/20 | G06T-0007/593 | G06T-0007/70 | G06V-0010/60 | H04N-0005/2353 | H04N-0005/23232 | H05B-0047/175 | A61B-2090/3945 | A61B-2090/3975 | G06T-2207/30196 | G06T-2207/30204 | G06T-2207/30208","G06K-009/00","G06K-009/00 | G06V-040/20 | G06T-007/593 | G06T-007/20 | G06T-007/70 | H04N-005/232 | H04N-005/235 | A61B-090/00 | A61B-005/11 | A61B-005/00 | G06V-010/60 | G06F-003/01 | G06F-003/03 | H05B-047/175","","","","","","4922031005123"
"US","US","P","B2","Display controller, display system including the display controller, and method of operating the display controller","A display chipset structure that is based on an integrated display controller is provided. The display controller includes including a display processor comprising a first digital circuit, and configured to receive image data from an application processor (AP) and output the image data to a first component driver chip configured to drive a gate line and a source line of a display panel; and a touch processor comprising a second digital circuit, and configured to receive touch data from a second component driver chip configured to drive sensing electrodes of a touch panel. The display controller is implemented as one semiconductor chip and separated from each of the first and second component driver chips, and the display processor and the touch processor communicate with each other through an internal interconnection of the one semiconductor chip.","1. A display controller comprising: a display processor comprising a first digital circuit, and configured to receive image data from an application processor (AP) and output the image data to a first component driver chip configured to drive a gate line and a source line of a display panel; anda touch processor comprising a second digital circuit, and configured to receive touch data from a second component driver chip configured to drive sensing electrodes of a touch panel,wherein the display controller is implemented as one semiconductor chip and separated from each of the first component driver chip and the second component driver chip,wherein the display processor and the touch processor communicate with each other through an internal interconnection of the one semiconductor chip,andwherein the display processor comprises:a frame memory configured to store the image data to be displayed on the display panel; andan image processing circuit configured to perform an image processing operation on the image data.","19","16/996391","2020-08-18","2021-0065602","2021-03-04","11403983","2022-08-02","SAMSUNG ELECTRONICS CO., LTD.","Changju Lee | Yoonkyung Choi | Jinbong Kim | Junho Huh","10-2019-0108470 | 10-2019-0164797","KR | KR","2019-09-02 | 2019-12-11","G09G-0003/20","G09G-0003/20 | A61B-0005/6898 | G06F-0003/0412 | G06F-0003/0416 | G06V-0040/1318 | G06F-0001/3265 | G09G-2310/0275 | G09G-2310/0278 | G09G-2310/08 | G09G-2330/023 | G09G-2370/08","G06F-003/041","G06F-003/041 | G06V-040/13 | G06K-009/00 | A61B-005/00 | G09G-003/20 | G06F-001/3234","","","","","","4922031005223"
"US","US","P","B2","Graphical user interface for a surgical navigation system and method for providing an augmented reality image during operation","A surgical navigation system includes: a 3D display system with a see-through visor; a tracking system comprising means for real-time tracking of: a surgeon's head, the see-through visor, a patient anatomy and a surgical instrument to provide current position and orientation data; a source of an operative plan, a patient anatomy data and a virtual surgical instrument model; a surgical navigation image generator configured to generate a surgical navigation image with a three-dimensional image representing simultaneously a virtual image of the surgical instrument corresponding to the current position and orientation of the surgical instrument and a virtual image of the surgical instrument indicating the suggested positions and orientation of the surgical instrument according to the operative plan data based on the current relative position and orientation of the surgeon's head, the see-through visor, the patient anatomy and the surgical instrument; wherein the 3D display system is configured to show the surgical navigation image at the see-through visor, such that an augmented reality image collocated with the patient anatomy in the surgical field underneath the see-through visor is visible to a viewer looking from above the see-through visor towards the surgical field.","1. An apparatus, comprising: a memory;a processor operatively coupled to the memory, the processor configured to: determine, based on data associated with an operative plan of a surgical procedure, a suggested position and orientation of a medical device in an anatomy of a patient, the medical device configured to be used in the surgical procedure;determine, based on tracking data associated with the medical device, an actual position and orientation of the medical device when the medical device is partially inserted into the patient and includes a hidden portion disposed below a skin of the patient; andgenerate a surgical navigation image including a three-dimensional (3D) image including (1) a virtual object indicative of the suggested position and orientation of the medical device, (2) a virtual representation of the medical device including a part corresponding to the hidden portion of the medical device, and (3) a virtual representation of a portion of the anatomy of the patient; anda display system configured to display the surgical navigation image on a partially transparent and partially reflective surface positionable between a head of an operator and a surgical field including the patient anatomy such that, when viewed by the operator, (1) the virtual representation of the portion of the anatomy is collocated with the portion of the anatomy and (2) the virtual representation of the medical device is overlaid on a visible portion of the medical device and has the part corresponding to the hidden portion of the medical device extending from the visible portion of the medical device.","20","17/145178","2021-01-08","2021-0267698","2021-09-02","11395705","2022-07-26","HOLO SURGICAL INC.","Krzysztof B. Siemionow | Cristian J. Luciano","2017-186307","EP","2017-08-15","A61B-0034/25","A61B-0034/25 | A61B-0005/7267 | A61B-0034/10 | A61B-0034/30 | A61B-0090/36 | A61B-0090/37 | G02B-0027/017 | G02B-0027/0172 | G06F-0003/012 | G06F-0003/013 | G06T-0005/002 | G06T-0007/11 | G06T-0019/006 | A61B-0034/20 | A61B-2017/00216 | A61B-2034/102 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2055 | A61B-2034/2063 | A61B-2034/2068 | A61B-2090/363 | A61B-2090/365 | A61B-2090/367 | A61B-2090/368 | A61B-2090/3618 | A61B-2090/372 | A61B-2090/3762 | A61B-2090/3983 | A61B-2090/502 | G02B-2027/0134 | G02B-2027/0136 | G02B-2027/0138 | G02B-2027/0178 | G02B-2027/0187 | G02B-2027/0196 | G06K-0009/6257 | G06T-0007/20 | G06T-2207/10081 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30012 | G06T-2207/30208 | G06T-2219/004 | G06V-2201/033","A61B-034/00","A61B-034/00 | G02B-027/01 | G06T-019/00 | G06F-003/01 | A61B-034/10 | A61B-090/00 | G06T-007/11 | A61B-005/00 | G06T-005/00 | A61B-034/30 | A61B-034/20 | A61B-090/50 | A61B-017/00 | G06K-009/62 | G06T-007/20","","","","","","4922030000884"
"US","US","P","B2","Motion tracking and strain determination","Described herein are systems, methods and instrumentalities associated with motion tracking and strain determination. A motion tracking apparatus as described herein may track the motion of an anatomical structure from a source image to a target image and determine corresponding points on one or more surfaces of the anatomical structure in both the source image and the target image. Using these surface points, the motion tracking apparatus may calculate one or more strain parameters associated with the anatomical structure and provide the strain parameters for medical diagnosis and/or treatment.","1. An apparatus, comprising one or more processors, wherein the one or more processors are configured to: obtain a source image of an anatomical structure and a target image of the anatomical structure;determine, in the source image, a first plurality of points on a first surface of the anatomical structure and a second plurality of points on a second surface of the anatomical structure, wherein the first plurality of points and the second plurality of points are determined based on a partial differential equation such that each point in the second plurality of points corresponds to a point in the first plurality of points;determine, based on the source image and the target image, a motion field that indicates a motion of the anatomical structure from the source image to the target image;estimate, based on the motion field and respective locations of the first plurality of points and the second plurality of points in the source image, respective locations of the first plurality of points and the second plurality of points in the target image;determine one or more strain parameters associated with the anatomical structure, wherein: the one or more strain parameters include at least one of a radial strain of the anatomical structure or a circumferential strain of the anatomical structure;if the one or more strain parameters include the radial strain, the radial strain is determined based on a change in a radial distance between one of the first plurality of points and one of the second plurality of points from the source image to the target image; andif the one or more strain parameters include the circumferential strain, the circumferential strain is determined based on a change in a circumferential distance from the source image to the target image, the circumferential distance being a projection of the radial distance in a circumferential direction of the anatomical structure.","20","17/070705","2020-10-14","2021-0158543","2021-05-27","11393092","2022-07-19","SHANGHAI UNITED IMAGING INTELLIGENCE CO., LTD.","Shanhui Sun | Hanchao Yu | Qiaoying Huang | Zhang Chen | Terrence Chen","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0005/0044 | A61B-0005/1128 | A61B-0005/7264 | G06F-0003/0485 | G06K-0009/6267 | G06N-0003/0454 | G06N-0003/08 | G06T-0003/0093 | G06T-0007/0014 | G06T-0007/11 | G06T-0007/248 | G06T-0007/55 | G06T-0007/73 | G06T-0011/206 | G06T-0013/80 | G06T-0019/00 | G16H-0030/40 | G16H-0050/30 | G16H-0050/50 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30048 | G06T-2210/41","G06T-007/00","G06T-007/00 | G06T-007/11 | G06K-009/62 | G06N-003/04 | G16H-050/50 | G16H-050/30 | G16H-030/40 | G06F-003/0485 | G06T-011/20 | G06T-013/80 | G06T-019/00 | G06T-007/55 | G06T-007/73 | G06T-007/246 | A61B-005/00 | A61B-005/11 | G06T-003/00 | G06N-003/08","","","","","","4922029004926"
"US","US","P","B2","Display control device, method for operating display control device, and program for operating display control device","A display control device includes: a first display control unit that display, on a display screen, a display region in which a plurality of display frames are arranged and a selection region in which a list of a plurality of thumbnail images is displayed; a first receiving unit that receives an operation of selecting the plurality of thumbnail images to select the plurality of examination images; a second receiving unit that receives an operation of designating an arrangement direction of the plurality of selected examination images in the display region in a state in which the plurality of thumbnail images are selected and before the plurality of examination images are laid out in the plurality of display frames; and a second display control unit that lays out the plurality of examination images in the plurality of display frames in the designated arrangement direction and displays the plurality of examination images on the display screen.","1. A display control device comprising: a processor, configured toperform first display control to display, on a display screen, a display region in which a plurality of examination images acquired for each examination are displayed and which has a plurality of display frames arranged in a grid shape, each of the plurality of display frames being capable of having one of the plurality of examination images laid out therein, and a selection region which is used to select examination images to be laid out in the plurality of display frames from the plurality of examination images and in which a list of a plurality of thumbnail images is displayed, each of the plurality of thumbnail images being obtained by respectively reducing one of the plurality of examination images;receive an operation of selecting thumbnail images from the plurality of thumbnail images displayed in the selection region to select the examination images corresponding to the selected thumbnail images;receive a designation operation of designating an arrangement direction in which the selected examination images are arranged in the plurality of display frames in the display region in a state in which the selected thumbnail images are displayed in the selection region and before the selected examination images are laid out in the plurality of display frames and of designating a first display frame of the plurality of display frames; andperform second display control to lay out the selected examination images from the first display frame in the plurality of display frames in the designated arrangement direction and to display the selected examination images laid out in the plurality of display frames on the display screen.","22","16/845027","2020-04-09","2020-0323504","2020-10-15","11382584","2022-07-12","FUJIFILM CORPORATION","Yuki Okabe | Eiichi Imamichi | Yuya Kudo | Masaki Miyamoto","2019-075795","JP","2019-04-11","A61B-0006/463","A61B-0006/463 | A61B-0005/743 | A61B-0005/7425 | A61B-0005/7435 | A61B-0005/7475 | A61B-0006/465 | G06F-0003/0482 | G06F-0003/1415 | G06T-0007/0014 | G06T-0007/38 | G06T-2207/10072","A61B-006/00","A61B-006/00 | G06F-003/0482 | G06F-003/14 | A61B-005/00 | G06T-007/00 | G06T-007/38","","","","","","4922028001053"
"US","US","P","B2","Web intelligent document was a data source","A report repository may store report results, and a web intelligence report server may include an SDK component to manage sessions, states, security, and resource access and to receive web intelligence data model authoring information, associated with a document, via an authoring API. The web intelligence report server may further include data sources associated with a plurality of data source types and data access associated with a plurality of data layers. A compound database platform of an in-memory database may create a report result via a data flow merge operation that combines multiple data sources into a single data source, based on the web intelligence data model authoring information, the data sources, and the data access. The report result may be stored in the report repository, and the web intelligence data model may be associated with a Web intelligence document as a data Source (""WaaS"") reusable in other documents.","1. A system associated with web intelligence reports, comprising: a report repository to store report results; anda web intelligence report server, coupled to the report repository, including: a Software Development Kit (""SDK"") component to manage sessions, states, security, and resource access and to receive web intelligence data model authoring information, associated with a document, via an authoring Application Programming Interface (""API""),data sources associated with a plurality of data source types,data access associated with a plurality of data layers, anda compound database platform of an in-memory database, coupled to the SDK component, data sources, and data access, including: a computer processor, anda memory storage device coupled to the computer processor and including instructions that, when executed by the computer processor, enable the compound database platform to: (i) create a report result via a data flow merge operation that combines multiple data sources into a single data source, based on the web intelligence data model authoring information, the data sources, and the data access,query facilities to receive web intelligence data model consumption information from a report engine and transmit a get metadata message to the compound database platform, anda calculation engine to execute calculation plans on top of data stored in compound database platform, wherein the calculation receives web intelligence data model consumption information from the query facilities and transmits a get data message to the compound database platform;wherein the report result is stored in the report repository and the web intelligence data model is associated with a Web intelligence document as a data Source (""WaaS"") reusable in other documents.","19","17/070114","2020-10-14","2022-0113949","2022-04-14","11386185","2022-07-12","BUSINESS OBJECTS SOFTWARE LTD.","Raphael Geoffroy | Sebastien Ducaule","","","","G06F-0016/958","G06F-0016/958 | G06F-0008/36 | G06F-0009/541 | G06F-0009/547 | G06Q-0030/0243 | A61B-0005/165 | G06F-0016/1734 | G06F-0016/24 | G06Q-0010/063","G06F-009/44","G06F-009/44 | G06F-016/958 | G06F-008/36 | G06F-009/54 | G06Q-030/02 | G06Q-010/06 | A61B-005/16 | G06F-016/24 | G06F-016/17","","","","","","4922028004626"
"US","US","P","B2","Biosignal acquisition method and algorithms for wearable devices","Apparatus, including a set of N electrodes (22), configured to be located in proximity to an epidermis (24) of a subject, and to acquire signals generated by electric sources within the subject. The apparatus also includes a set of M channels, configured to transfer the signals, where M is less than N, and a switch (40), configured to select, repetitively and randomly, M signals from the N electrodes and to direct the M signals to the M channels. The apparatus further includes a processor (28), configured to activate the switch, and to receive and analyze the M signals from the M channels so as to determine respective positions of the electric sources within the subject.","1. Apparatus, comprising: a set of N electrodes, configured to be located in proximity to an epidermis of a subject, and to acquire N respective input signals generated by electric sources within the subject;a set of M channels, where M is less than N;a switch, configured to multiply the N input signals by a quasirandom N X M matrix of ones and zeros to generate M output signals from the N electrodes, including short-circuiting selected ones of the N electrodes, and to direct the M output signals to the M channels; anda processor, configured to control the switch, and to receive and analyze the M output signals obtained from the M channels while modulating the quasirandom N×M matrix to compute, based on the output signals, respective timecourses of the electric sources within the subject.","23","16/316627","2017-07-12","2019-0290152","2019-09-26","11375939","2022-07-05","RAMOT AT TEL AVIV UNIVERSITY LTD.","Alex Bronstein | Evgeny Tsizin-Goldman","","","","A61B-0005/30","A61B-0005/30 | A61B-0005/0042 | A61B-0005/0531 | A61B-0005/1495 | A61B-0005/2415 | A61B-0005/291 | A61B-0005/296 | A61B-0005/316 | A61B-0005/369 | A61B-0005/389 | A61B-0005/681 | A61B-0005/6803 | A61B-0005/6822 | A61B-0005/6823 | A61B-0005/6824 | A61B-0005/6828 | A61B-0005/6829 | A61B-0005/6831 | A61B-0005/7207 | A61N-0001/0484 | A61N-0001/08 | A61N-0001/36 | A61N-0001/378 | G06F-0003/015 | A61B-2560/0209 | A61B-2562/0214 | G06F-0003/011 | G06F-0003/017","A61B-005/30","A61B-005/30 | A61B-005/00 | A61N-001/36 | A61B-005/0531 | A61N-001/04 | A61B-005/1495 | A61N-001/08 | G06F-003/01 | A61N-001/378 | A61B-005/291 | A61B-005/296 | A61B-005/316 | A61B-005/369 | A61B-005/389 | A61B-005/24","","","","","","4922027001013"
"US","US","P","B2","Medical device for sensing and or stimulating tissue","Devices, methods and systems for transmitting signals through a device located in a blood vessel of an animal for stimulating and/or sensing activity of media proximal to the devices. The media can include tissue and/or fluid. A method of controlling an apparatus in communication with a brain machine interface. The method can include measuring a first neural activity in a first neural area and measuring a second neural activity in a second neural area. The first neural activity can be associated with a first intent. The method can include creating and delivering, via the processor, one or more first control signals to the apparatus upon comparing the second neural activity with the first neural activity, and confirming, based on this comparison, that the second neural activity is associated with the first intent.","1. A method of controlling an apparatus in communication with a brain machine interface, the method comprising: measuring a first neural activity in a first neural area, where the first neural activity is associated with a first intent to control the apparatus, where measuring the first neural activity comprises using a first sensor of the brain machine interface;measuring a second neural activity in a second neural area using a second sensor of the brain machine interface;receiving, via a processor in wired or wireless communication with the brain machine interface, the first neural activity and the second neural activity;processing, via the processor, the received first neural activity and the second neural activity; andcreating and delivering, via the processor, one or more first control signals to the apparatus upon comparing the second neural activity with the first neural activity, and confirming, based on this comparison, that the second neural activity is associated with the first intent.","20","16/683077","2019-11-13","2020-0078195","2020-03-12","11376138","2022-07-05","THE UNIVERSITY OF MELBOURNE","Sam Emmanuel John | Nicholas Lachlan Opie | Thomas James Oxley","","","","A61F-0002/68","A61F-0002/68 | A61B-0005/24 | A61B-0005/4851 | A61B-0005/6862 | A61B-0005/6868 | A61B-0005/6876 | A61F-0002/72 | A61F-0002/86 | A61N-0001/36003 | B25J-0009/00 | G06F-0003/015 | A61B-0005/291 | A61B-2562/227 | A61F-2002/6827 | A61F-2002/705 | A61F-2250/0002 | A61N-0001/0531 | A61N-0001/0534 | A61N-0001/0536 | A61N-0001/36067 | A61N-0001/36078 | A61N-0001/36085 | A61N-0001/36089 | A61N-0001/36096 | A61N-0001/36178","A61F-002/68","A61F-002/68 | A61B-005/24 | A61N-001/36 | A61F-002/86 | A61B-005/00 | A61F-002/72 | B25J-009/00 | G06F-003/01 | A61F-002/70 | A61N-001/05 | A61B-005/291","","","","","","4922027001210"
"US","US","P","B2","Brain-computer interface method and system based on real-time closed loop vibration stimulation enhancement","Brain-computer interface method and system include displaying and providing a motor imagery task to a subject, and collecting a generated digital electroencephalogram signal; reading the digital electroencephalogram signal, performing interception if a preset time period is exceeded, and performing continuous reading if not; performing band-pass filtering, obtaining time-frequency characteristics of the digital electroencephalogram signal, and extracting a frequency value with highest frequency energy as a main frequency; obtaining an instantaneous phase of the digital electroencephalogram signal; generating predicted sine waves by respectively using the main frequency and the instantaneous phase as a frequency and an initial phase of sine waves, and predicting and obtaining real-time phase information; and judging whether the real-time phase is in a vibration stimulation application phase interval, generating and outputting a control instruction, and controlling a vibration motor to vibrate and to stimulate a sensory channel of the subject according to the control instruction.","1. A brain-computer interface method based on real-time closed loop vibration stimulation enhancement, comprising the following steps: displaying and providing a motor imagery task to a subject, and collecting a digital electroencephalogram signal of the subject generated during motor imagery;reading the collected digital electroencephalogram signal, judging whether a preset time period is exceeded or not, intercepting the digital electroencephalogram signal in the preset time period if YES, and continuously reading the collected digital electroencephalogram signal if NO;after band-pass filtering is performed on the intercepted digital electroencephalogram signal in the preset time period, obtaining time-frequency characteristics of the digital electroencephalogram signal of the period through calculation by fast Fourier transform, and extracting a frequency value with highest frequency energy as a main frequency; obtaining an instantaneous phase of the digital electroencephalogram signal of the period through calculation by Hilbert transform on the digital electroencephalogram signal subjected to band-pass filtering; generating predicted sine waves by respectively using the main frequency and the instantaneous phase of the digital electroencephalogram signal of the period as a frequency and an initial phase of sine waves, and predicting and obtaining real-time phase information at the current moment according to the predicted sine waves; andjudging whether the real-time phase is in a vibration stimulation application phase interval or not according to the predicted and obtained real-time phase information at the current moment, generating and outputting a control instruction according to the judging result, and controlling a vibration motor to vibrate and to stimulate a sensory channel of the subject according to the control instruction.","10","16/977751","2019-03-21","2020-0401226","2020-12-24","11379039","2022-07-05","SOUTHEAST UNIVERSITY","Aiguo Song | Wenbin Zhang | Hong Zeng | Baoguo Xu","2018-11553136","CN","2018-12-19","G06F-0003/015","G06F-0003/015 | A61B-0005/316 | A61B-0005/375 | A61B-0005/378 | A61B-0005/7455 | G06F-0003/016 | G06F-0017/142 | G06F-2203/011","G06F-003/01","G06F-003/01 | G06F-017/14 | A61B-005/375 | A61B-005/377 | A61B-005/24 | A61B-005/00 | A61B-005/378 | A61B-005/316","","","","","","4922027004097"
"US","US","P","B2","Nanowire characterization and identification","The techniques and systems described herein relate to manufacturing, characterizing, and/or identifying one or more types of magnetic nanowires (MNWs). One or more types of MNWs may be associated with different objects, and a system may identify the objects based on the magnetic nanowires associated with the objects. For example, such techniques may involve characterizing the types of MNWs based on magnetic field transmission characteristics and ferromagnetic resonance characteristics of each type of MNW. In some examples, the techniques described herein may enable the identification of each of a plurality of types of MNWs present in a sample or object based on a combined transmission value of the sample. Such techniques may enable the development and use of barcode-like systems of different types of MNWs for labeling and identifying objects of interest.","1. A method comprising: determining a magnetic field transmission characteristic corresponding to each type of magnetic nanowire (MNW) of a plurality of types of MNWs;determining a ferromagnetic resonance (FMR) characteristic of each type of MNW of the plurality of types of MNWs, wherein each type of MNW has an FMR characteristic that differs from an FMR characteristic of each other type of MNW of the plurality of types of MNWs;identifying each type of MNW of the plurality of types of MNWs based on the corresponding magnetic field transmission characteristic and the corresponding FMR characteristic; andassociating each type of MNW of the plurality of MNWs with a corresponding object of a plurality of objects, wherein each object differs from at least one other object of the plurality of objects.","20","16/151206","2018-10-03","2019-0102585","2019-04-04","11379677","2022-07-05","REGENTS OF THE UNIVERSITY OF MINNESOTA","Rhonda R. Franklin | Wen Zhou | Jaime F. Modiano | Bethanie J Stadler","","","","G06K-0007/10366","G06K-0007/10366 | A61B-0005/6852 | G01N-0033/587 | G01R-0033/0011 | G01R-0033/04 | G06K-0019/06187 | H01Q-0001/14 | H01Q-0001/38 | H01Q-0015/006 | H01Q-0019/005 | A61B-2562/0223","G06K-019/06","G06K-019/06 | G06F-017/00 | G06K-007/10 | G01N-033/58 | H01Q-001/14 | H01Q-001/38 | H01Q-015/00 | H01Q-019/00 | A61B-005/00 | G01R-033/04 | G01R-033/00","","","","","","4922027004733"
"US","US","P","B2","System and method for monitoring a work situation","The application relates to a system for monitoring a working situation, said system comprising at least one mobile sensor unit, at least one communication node, at least one sensor node which is formed by the communication node or is provided additionally to this, and a central evaluation unit. The at least one mobile sensor unit comprises at least one sensor for measuring at least one first physical variable which is suitable for describing an activity or a state of an individual carrying the mobile sensor unit, a transmitter for the wireless transmission of measurement values of the at least one first physical variable, which are detected by the at least one sensor of this mobile sensor unit, to the at least one sensor node. The at least one sensor node comprises at least one sensor for measuring at least one second physical variable which is suitable for describing a state of an environment of the sensor node. The at least one communication node comprises a receiver for receiving the measurement values of the at least one first physical variable which are transmitted by the transmitter of the mobile sensor unit, and a communication unit for transmitting the measurement values of the at least one first physical variable which are received by the receiver of this communication node and measurement values of the at least one second physical variable, which are detected by the at least one sensor of the sensor node, to the central evaluation unit. The application further relates to a method for use of the system.","1. A system for monitoring a working situation, said system comprising: at least one mobile sensor unit, configured to be carried by an individual;at least one communication node;at least one sensor node formed by the communication node or provided additionally; anda central evaluation unit;wherein the at least one mobile sensor unit comprises at least one sensor for measuring at least one first physical variable describing an activity or a state of the individual carrying the mobile sensor unit, wherein the system further comprises: a transmitter for wireless transmission of measurement values of the at least one first physical variable detected by the at least one sensor of the mobile sensor unit, to the at least one communication node; andan energy store to supply electricity to the transmitter, wherein the at least one sensor node comprises at least one sensor, different than the at least one sensor of the mobile sensor unit, for measuring at least one second physical variable describing a state of an environment of the sensor node;wherein the at least one communication node comprises: a receiver for receiving a measurement value of the at least one first physical variable transmitted by the transmitter of the mobile sensor unit; anda communication unit for transmitting the measurement value of the at least one first physical variable received by the receiver of the communication node, and a measurement value of the at least one second physical variable, detected by the at least one sensor of the sensor node, to the central evaluation unit;wherein the central evaluation unit comprises: a communication unit for receiving the measurement value of the at least one first physical variable and the measurement value of the at least one second physical variable, said measurement values being transmitted by the communication unit of the communication node;wherein the system is configured to assign the measurement value of the at least one second physical variable detected by the at least one sensor of the sensor node, to the mobile sensor unit located in the environment of the at least one sensor node, at least at one point in time and to transfer, store, and further process the assigned measurement value together with the measurement value of the at least one first physical variable detected by the at least one sensor of the mobile sensor unit located in the environment of the at least one sensor node, at least at the point in time, to the central evaluation unit as being assigned to one another, wherein the at least one second physical variable identifies a property of the environment, the property describing the state of the environment of the sensor node, and wherein a machine learning algorithm assigns a certain activity to the measurement value of the at least one first physical variable measured with the mobile sensor unit in the environment to identify a working process based on the measurement value of the at least one first physical variable measured with the mobile sensor unit in the environment, the assigned certain activity, and the property describing the state of the environment of the sensor node.","21","16/206329","2018-11-30","2019-0173953","2019-06-06","11381643","2022-07-05","FRAUNHOFER-GESELLSCHAFT ZUR F?RDERUNG DER ANGEWANDTEN FORSCHUNG E.V.","Sascha Feldhorst | Rene Grzeszick","10-2017-221852","DE","2017-12-04","H04L-0067/12","H04L-0067/12 | G01D-0021/00 | G01D-0021/02 | G06K-0009/6262 | G06Q-0010/0639 | H04J-0003/0638 | H04W-0004/80 | A61B-0005/0004","H04L-067/12","H04L-067/12 | G01D-021/02 | G01D-021/00 | G06Q-010/06 | H04W-004/80 | G06K-009/62 | H04J-003/06 | A61B-005/00","","","","","","4922027006685"
"US","US","P","B2","Systems and methods including a human-shaped graphical element","Graphical user interfaces for use with extracorporeal blood treatment systems may include a human-shaped graphical element and one or more process feature graphical elements. The human-shaped graphical element may be moved automatically or manually by users with respect to the process feature graphical elements to provide indications with respect to the process features corresponding to the human-shaped graphical element and the process feature graphical elements.","1. An extracorporeal blood treatment system comprising: extracorporeal blood treatment apparatus for use during an extracorporeal blood treatment comprising a blood pressure sensor to sense a patient'ss blood pressure;a display apparatus comprising a graphical user interface configured to depict a human-shaped graphical element and a heart-shaped graphical element at least partially within the human-shaped graphical element; anda computing apparatus comprising one or more processors, wherein the computing apparatus is operatively coupled to the extracorporeal blood treatment apparatus and the display apparatus, wherein the computing apparatus is configured to: display on the graphical user interface the human-shaped graphical element and the heart-shaped graphical element at least partially within the human-shaped graphical element using the one or more processors,allow a user to select the heart-shaped graphical element to initiate the blood pressure sensor to perform a blood pressure measurement on the patient,determine the patient'ss blood pressure using the blood pressure sensor in response to selection of the heart-shaped graphical element,display the measured blood pressure of the patient on the graphical user interface, anddisplay on the graphical user interface a blood process feature graphical element using the one or more processors, wherein the human-shaped graphical element and the blood process feature graphical element are separated by space, wherein the human-shaped graphical element is movable proximate the blood process feature graphical element to indicate that venous and arterial blood lines are connecting the patient to a blood circuit of the extracorporeal blood treatment apparatus,wherein the human-shaped graphical element further comprises a connection area corresponding to the blood process feature graphical element and the blood process feature graphical element comprises a connection area corresponding to the human-shaped graphical element, wherein the connection area of the human-shaped graphical element is at least proximate to the connection area of the blood process feature graphical element when the human-shaped graphical element is proximate the blood process feature graphical element to indicate that venous and arterial blood lines are connecting the patient to the blood circuit of the extracorporeal blood treatment apparatus.","12","15/740932","2016-06-30","2018-0184985","2018-07-05","11369320","2022-06-28","GAMBRO LUNDIA AB","Annmargret Hakansson | Par-Olof Hakansson | Maria Johnsson | Roger Nilsson | Bendik Torvin","2015-50939","SE","2015-07-02","A61B-0005/7435","A61B-0005/7435 | A61B-0005/0215 | A61B-0005/6866 | A61B-0005/743 | A61B-0005/744 | A61B-0005/7475 | A61M-0001/16 | A61M-0001/1607 | A61M-0001/3607 | A61M-0001/3609 | A61M-0001/3612 | A61M-0001/3639 | A61M-0001/3656 | G06F-0003/0481 | G06F-0003/0488 | G06F-0003/04842 | G06F-0003/04847 | G16H-0020/40 | G16H-0040/63 | A61B-0005/021 | A61M-2205/3331 | A61M-2205/3334 | A61M-2205/3344 | A61M-2205/50 | A61M-2205/502 | A61M-2205/505 | A61M-2230/06 | G06F-0003/04845 | G06F-0003/04886","A61B-005/00","A61B-005/00 | A61M-001/36 | A61B-005/0215 | G06F-003/04842 | G06F-003/0488 | G06F-003/04847 | G16H-040/63 | A61M-001/16 | G06F-003/0481 | G16H-020/40 | A61B-005/021 | G06F-003/04845 | G06F-003/04886","","","","","","4922026000999"
"US","US","P","B2","Managing audio for remote health services","A mobile health device includes a sensor that monitors values of a health metric of a person, a control component that controls the device to perform actions as part of providing a health service, and components of the device that receive the control signal and perform an action to provide the health service based on the control signal. The components include user output components for producing sound and/or displaying information and where providing the health service includes playing information as sound, displaying information as text, displaying information as motion video, and/or displaying information still images. The control component includes an input that receives audio signals corresponding to the monitored values of the health metric and produces a control signal based on information about the person, including the monitored values of the health metric of the person. The mobile health device may also include a network interface component.","1. A mobile health device, comprising: a sensor that monitors values of a health metric of a person;a control component that controls the mobile health device to perform actions as part of providing a health service, wherein the control component includes an input that receives audio signals corresponding to the monitored values of the health metric, and produces a control signal based on information about the person, including the monitored values of the health metric of the person, wherein the control component converts each received audio signal to an input value and determines the control signal based on the input value; andcomponents of the mobile health device that receive the control signal and perform an action to provide the health service based on the control signal, wherein the components include one or more user output components for producing sound and/or displaying information and wherein providing the health service includes one or more of: playing information as sound, displaying information as text, displaying information as motion video, and displaying information still images.","19","16/800007","2020-02-25","2020-0383571","2020-12-10","11363998","2022-06-21","TELADOC HEALTH, INC.","Tamer Fakhouri | Amar Kendale | Gene V. Kozin | Polina A. Segalova | Keval Mehta","","","","A61B-0005/7465","A61B-0005/7465 | A61B-0005/0004 | A61B-0005/0022 | A61B-0005/021 | A61B-0005/14532 | A61B-0005/7405 | G05B-0015/02 | G05D-0023/1917 | G06F-0001/26 | G06F-0001/3206 | G06F-0009/542 | G06F-0021/33 | G06N-0005/04 | G06N-0020/00 | G06T-0007/0002 | G16H-0010/60 | G16H-0015/00 | G16H-0020/00 | G16H-0030/20 | G16H-0030/40 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30 | G16H-0050/70 | G16H-0070/20 | G16H-0080/00 | G06F-0001/206 | G06T-2207/30168","G08B-001/08","G08B-001/08 | A61B-005/00 | A61B-005/145 | G16H-010/60 | G16H-040/67 | G16H-050/30 | A61B-005/021 | G16H-020/00 | G16H-070/20 | G16H-080/00 | G06N-020/00 | G06N-005/04 | G16H-040/63 | G05B-015/02 | G05D-023/19 | G16H-030/20 | G16H-050/20 | G16H-030/40 | G06T-007/00 | G16H-015/00 | G16H-050/70 | G06F-001/26 | G06F-009/54 | G06F-021/33 | G06F-001/3206 | G06F-001/20","","","","","","4922025000866"
"US","US","P","B2","System and method for detecting spikes in noisy signals","A method for determining a threshold for spike detection in a noisy signal in real-time is provided. The method includes estimating a current variability of noise in the noisy signal according to a measured instantaneous value and a window of previous instantaneous values using a sigma-delta control loop, determining the threshold based on the estimated variability of the noise; and repeating the steps to update the estimate of the variability of the noise and adjust the threshold in real-time as the noisy signal changes. A non-transitory computer-readable storage medium for executing the method on a processing unit, and a low-power digital system implementing the method are also provided.","1. A method for determining a threshold for spike detection in a noisy signal in real-time, the method comprising the steps of: a) receiving the noisy signal;b) measuring a current instantaneous value of the noisy signal;c) estimating a current variability of noise in the noisy signal according to the current instantaneous value and a window of previous instantaneous values using a sigma-delta control loop;d) determining the threshold based on the estimated variability of the noise; ande) repeating steps a) through d) to update the current estimate of the variability of the noise and adjust the threshold in real-time as the noisy signal changes.","31","15/753823","2016-08-23","2019-0012515","2019-01-10","11366977","2022-06-21","UNIVERSIT? LAVAL","Gabriel Gagnon-Turcotte | Benoit Gosselin","","","","G06K-0009/0053","G06K-0009/0053 | A61B-0005/24 | A61N-0001/362 | G06F-0003/01 | G06F-0017/10 | G06K-0009/00503","G06K-009/00","G06K-009/00 | G06F-017/10 | A61N-001/362 | A61B-005/24 | G06F-003/01","","","","","","4922025003818"
"US","US","P","B2","Patient management system","Methods and Systems implement patient management. In some cases, a patient management system 200 may include one or more respiratory pressure therapy devices to deliver respiratory pressure therapy to patients, and generate therapy data relating to a therapy session for a patient. The patient management system may include a data server communicating with the therapy device(s). The data server may compute, from therapy data, therapy summary data for the session, the summary data may include one or more statistics summarising therapy data. The patient management system may include a therapy management server communicating with the data server. The therapy management server may apply one or more rules to the summary data, update or generate one or more workflow groups of patients, each workflow group corresponding to a rule, depending on results of the respective rule applications; and/or serve a graphical layout representing one or more workflow groups.","1. A patient management system comprising: a data server, the data server being configured to: receive, from a respiratory pressure therapy device in communication with the data server, therapy data relating to a session of respiratory pressure therapy provided to a patient;compute, from the therapy data, therapy summary data for the session, the therapy summary data comprising a usage time and a leak flow rate; andapply a predictive model to the therapy summary data to compute a score indicating a predictive probability that the patient will meet a compliance standard, the compliance standard having a minimum amount of usage per session for a fraction of a number of consecutive sessions of respiratory pressure therapy; anda therapy management server in communication with the data server and a computing device, the therapy management server being configured to: obtain a plurality of patient workflow groups, wherein the patient workflow groups comprise a first patient workflow group of patients at risk of not being compliant with the compliance standard;update the first patient workflow group to include the patient based on a comparison between the score and a first threshold indicating that the patient should be added to the first patient workflow group;serve to the computing device a first graphical layout comprising an array of selectable headings, wherein each of the selectable headings represents one of the obtained patient workflow groups, and wherein the selectable headings comprise a first selectable heading for the first patient workflow group;receive a selection of the first selectable heading via an interaction with the first graphical layout;in response to receiving the selection of the first selectable heading, serve to the computing device a second graphical layout comprising details of one or more patients in the first patient workflow group, wherein the details of the one or more patients in the first patient workflow group comprise a selectable name for each of the one or more patients;receive a selection of any one of the selectable names via an interaction with the second graphical layout;in response to receiving the selection of the any one of the selectable names, serve to the computing device a corresponding patient menu comprising one or more selectable options relating to the corresponding patient, wherein one selectable option is a workflow history option;receive a selection of the workflow history option; andin response to receiving the selection of the workflow history option: obtain a list of patient workflow groups to which the corresponding patient has previously been added; andserve to the computing device the list of patient workflow groups to which the corresponding patient has previously been added.","30","15/329853","2015-07-31","2017-0186122","2017-06-29","11367023","2022-06-21","RESMED INC.","Robert Andrew Levings | Ryan Eric Belbin | Mark David Buckley | Michael Waclaw Colefax | Jason Connell | Cheryl Kazimer | Colin Bradley Kennedy | Susan Robyn Lynch | Rehana Nathwani | Timothy Semen | Rajwant Sodhi","","","","G06Q-0010/00","G06Q-0010/00 | A61M-0016/0633 | A61M-0016/0666 | A61M-0016/0683 | G16H-0020/40 | G16H-0040/67 | A61M-0005/00 | A61M-0016/06 | A61M-2016/003 | A61M-2016/0027 | A61M-2202/0208 | A61M-2205/3584 | A61M-2205/502 | G06Q-0050/20 | Y02A-0090/10","G06Q-010/00","G06Q-010/00 | G16H-020/40 | G16H-040/67 | A61M-016/06 | A61M-005/00 | A61M-016/00 | G06Q-050/20","","","","","","4922025003864"
"US","US","P","B2","Gesture-based detection of a physical behavior event based on gesture sensor data and supplemental information from at least one external source","An automated medication dosing and dispensing system includes: gesture sensors to detect physical movement of a user; computer-readable storage media with program code instructions; and at least one processor. The instructions are configurable to cause the at least one processor to perform a method that involves: detecting, based on analysis of gesture sensor readings obtained from the gesture sensors, an occurrence of a gesture-based physical behavior event; calculating an initial confidence level for the detected occurrence of the gesture-based physical behavior event; deriving a final confidence level, for the detected occurrence of the gesture-based physical behavior event, from the initial confidence level and from external information obtained from at least one source of data that is distinct from the gesture sensors; and adjusting medication dosage and/or medication dispensing parameters in response to the detected occurrence of the gesture-based physical behavior event and the final confidence level.","1. An automated medication dosing and dispensing system comprising: gesture sensors to detect physical movement of a user of the automated medication dosing and dispensing system;computer-readable storage media comprising program code instructions; andat least one processor, wherein the program code instructions are configurable to cause the at least one processor to perform a method comprising the steps of: detecting, based on analysis of gesture sensor readings obtained from the gesture sensors, an occurrence of a gesture-based physical behavior event;calculating an initial confidence level for the detected occurrence of the gesture-based physical behavior event, wherein the initial confidence level represents an initial probability that the occurrence of the gesture-based physical behavior event was correctly detected;deriving a final confidence level, for the detected occurrence of the gesture-based physical behavior event, from the initial confidence level and from external information obtained from at least one source of data that is distinct from the gesture sensors, wherein the final confidence level represents a final probability that the occurrence of the gesture-based physical behavior event was correctly detected; andadjusting medication dosage, medication dispensing parameters, or both medication dosage and medication dispensing parameters of the automated medication dosing and dispensing system in response to the detected occurrence of the gesture-based physical behavior event and the final confidence level.","20","16/886353","2020-05-28","2020-0294645","2020-09-17","11367517","2022-06-21","MEDTRONIC MINIMED, INC.","Katelijn Vleugels","","","","G16H-0020/13","G16H-0020/13 | A61B-0005/14532 | G06F-0003/017 | G06N-0007/005 | G06V-0040/20","G16H-020/13","G16H-020/13 | G06F-003/01 | G06N-007/00 | A61B-005/145 | G06V-040/20","","","","","","4922025004353"
"US","US","P","B2","Annotation histogram","Systems and methods for facilitating processing of cardiac information based on sensed electrical signals include a processing unit configured to receive a set of electrical signals; receive an indication of a measurement location corresponding to each electrical signal of the set of electrical signals; and generate, based on at least one of an annotation waveform corresponding to each electrical signal of the set of electrical signals and a set of annotation mapping values, an annotation histogram.","1. A system for facilitating processing of cardiac information based on sensed electrical signals, the system comprising: a processing unit configured to: receive a set of electrical signals;receive an indication of a measurement location corresponding to each electrical signal of the set of electrical signals;determine annotation waveforms for the set of electrical signals;generate, based on one or more values of the annotated waveforms, an annotation histogram;determine, based on number of values included in one or more bins of the annotation histogram, whether to present on a display device a representation of the annotation histogram as either a discrete histogram or a continuous histogram; andgenerate, on the display device, an annotated electroanatomical cardiac map representing the determined discrete or continuous histogram including the one or more values and corresponding locations of the one or more values.","16","16/950582","2020-11-17","2021-0059548","2021-03-04","11357438","2022-06-14","BOSTON SCIENTIFIC SCIMED, INC.","Brian Stewart | Vasiliy E. Buharin | Mordechai Perlman | Nathan H. Bennett","","","","A61B-0005/339","A61B-0005/339 | A61B-0005/0245 | A61B-0005/318 | A61B-0005/349 | A61B-0005/743 | A61B-0034/20 | A61B-0034/25 | G06F-0003/04842 | G06F-0040/169 | G06T-0011/206 | A61B-0005/0538 | A61B-0005/063 | A61B-0005/287 | A61B-0005/6852 | A61B-0005/7221 | A61B-0018/1492 | A61B-2018/00351 | A61B-2018/00577 | A61B-2018/00839 | A61B-2018/00875 | A61B-2034/2051 | A61B-2562/04 | A61B-2562/06 | G06T-2200/24","A61B-005/339","A61B-005/339 | G06T-011/20 | G06F-003/04842 | A61B-034/20 | A61B-005/00 | A61B-005/0245 | A61B-034/00 | G06F-040/169 | A61B-005/318 | A61B-005/349 | A61B-018/14 | A61B-018/00 | A61B-005/06 | A61B-005/0538 | A61B-005/287","","","","","","4922024001032"
"US","US","P","B2","Dialysis system with artificial intelligence","Constraining adaptive optimizations of a state of an operation module of a medical device includes determining if a new state has at least one operational parameter that is outside a constraint that has been provided to the medical device in a non-repudiable manner, accepting the new state if no operational parameters are outside any of the constraints, and reverting the medical device to a previous valid state if at least one operational parameter is outside at least one of the constraints. The adaptive optimizations may be provided using artificial intelligence along with relevant inputs thereto. The medical device may be a dialysis system. Constraint data may be provided to the medical device along with a one-way hash value of the constraint data using, for example, a SHA 256 hash. The one-way hash value may be digitally signed using a private key that is part of a public/private key pair.","1. A control unit for a medical device, operable for adaptive optimization of the medical device comprising: a processor;an input/output interface coupled to the processor that receives sensor data for the medical device and provides output to a display of the medical device; anda non-transitory computer readable medium coupled to the processor and containing software that, when executed by the processor, constrains adaptive optimizations of operational parameters of an operation module of the control unit for the medical device, wherein the software includes executable code that determines if a new state corresponding to an adjustment of values of at least one of the operational parameters results in at least one of the operational parameters being outside at least one of a plurality of constraints that have been provided to the medical device, the constraints being non-repudiable; executable code that accepts the new state if none of the operational parameters are outside any of the constraints; executable code that reverts the medical device to a previous valid state if at least one of the operational parameters is outside at least one of the constraints; and executable code that sets the constraints of the operational parameters of the medical device based on constraint data that is provided from a source that is external to the medical device and wherein the constraint data includes a one-way hash value of the constraints that is digitally signed.","14","16/939601","2020-07-27","2020-0353149","2020-11-12","11357896","2022-06-14","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Norbert Leinfellner | Joseph Manakkil","","","","A61M-0001/287","A61M-0001/287 | A61M-0001/3403 | B01D-0061/32 | A61M-0001/282 | A61M-2205/3584 | A61M-2205/502 | A61M-2205/52 | A61M-2205/6009 | G06F-0003/04847 | G06F-0009/451","A61M-001/28","A61M-001/28 | A61M-001/34 | B01D-061/32 | G06F-003/00 | G06F-009/00 | G06F-009/451 | G06F-003/04847","","","","","","4922024001486"
"US","US","P","B2","Method and system for automated medical records processing with telemedicine","A method and system for automated medical records processing with telemedicine is presented. The method and system includes plural electronic medical templates specifically designed such that they reduce the complexity and risk associated with collecting virtual patient encounter information, creating a medical diagnosis, tracking the patient through the medical processes during a telemedicine session and generate the appropriate number and type medical codes for a specific type of medical practice when processed. The medical codes and other types of processed virtual patient encounter information are displayed in real-time on electronic medical records and invoices immediately after a virtual patient encounter from a telemedicine visit.","1. A method for automated processing of medical information with telemedicine, comprising: creating on a plurality of cloud server network devices each with a telemedicine application and one or more processors on a cloud communications network, a plurality of pooled cloud hardware resources comprising: (1) automatic provisioning of a plurality of pooled cloud hardware resources usable in any amount at any time as needed and available via a plurality of cloud broadband network access components on the plurality of cloud server network devices; (2) automatic scaling of the plurality of pooled cloud hardware resources to obtain and release one or more of the plurality of pooled cloud hardware resources as required; (3) automatic controlling and optimizing the plurality of pooled cloud hardware resources with a metering method; (4) a plurality of cloud software services for automated processing of medical information including a cloud Platform as a Service (PaaS), a cloud computing Infrastructure as a Service (IaaS), and plurality of available cloud Software services as a Service (SaaS) including a plurality of different software services for automated risk reduction in processing of medical records information after virtual patient encounters via a telemedicine visit, the plurality of cloud software services for automated processing of medical information comprising: cloud networking services, storage services, virtualization services, operating system services, run-time services, data services and application services executed with the plurality of pooled cloud hardware resources;receiving on a first telemedicine application on a first cloud network device with one or more processor a request for a telemedicine visit and virtual patient encounter from a client telemedicine application on a client network device with one or more client network device processors;applying one or more balancing methods in real-time on the first telemedicine application on the first cloud server network device creating a balancing selection including: (1) balancing telemedicine visit workflow across the plurality of other telemedicine applications on the plurality of other cloud server network devices and across the plurality of pooled cloud hardware resources and the plurality of cloud software services, and (2) balancing telemedicine patient load across a plurality of medical facilities including an availability of plurality of medical personnel at the plurality of medical facilities and a current actual patient load and current telemedicine patient load at the plurality of medical facilities;selecting from the first telemedicine application on the first cloud server network device with the balancing selection from the one or more balancing methods a selected telemedicine application on a selected cloud server network device to accept the request for the telemedicine visit and the virtual patient encounter;sending a selection message from the first telemedicine application on the first cloud server network device via the cloud communications network to the selected telemedicine application on the selected cloud server network device indicating the selected telemedicine application on the selected cloud server network device has been selected to accept the telemedicine visit and the virtual patient encounter;selecting from the selected telemedicine application on the selected cloud server network device a PaaS and an IaaS provided by the plurality of pooled cloud hardware resources on the cloud communications network;selecting from the selected telemedicine application on the selected cloud server network device via the on the cloud communications network, a first set of SaaS cloud services from the plurality of available cloud SaaS services for automated electronic medical records processing including reducing risk and reducing a number of possible diagnostic decisions when processing medical records from the virtual patient encounter via the telemedicine visit;selecting from the selected telemedicine application on the selected cloud server network device, one or more SaaS services from the selected first set of SaaS cloud services a plurality of individual data structures comprising a plurality of medical information matrixes including a patient tracking (PT) matrix, historical information (HX) matrix, patient examination (PX) matrix, complexity risk (CX) information matrix and an Evaluation and Management (E/M) summary information matrix, each with a plurality of unique matrix information fields, the plurality of individual data structures stored in one or more cloud storage objects on the cloud communications network and available to the first set of SaaS cloud services selected by the selected telemedicine application on the selected cloud server network device;collecting data securely on the selected telemedicine application on the selected cloud server network device via the cloud communications network from the client telemedicine application on the client network device for virtual patient encounter during the telemedicine visit, into a plurality of data fields on one or more different electronic medical templates with the one or more SaaS services from the selected first set of SaaS cloud services;aggregating from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time only selected ones of data items from the plurality of data fields from the one or more different electronic medical templates storing the collected virtual patient encounter data from the telemedicine visit into the plurality of information matrixes stored in the one or more cloud storage objects, the plurality of information matrixes including: the patient tracking (PT) matrix, historical information (HX) matrix, patient examination (PX) matrix, complexity risk (CX) information matrix and the Evaluation and Management (E/M) summary information matrix, each with the plurality of unique matrix information data fields;eliminating from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services with the only selected ones of data items aggregated into the plurality of information matrixes an amount and complexity of the virtual patient encounter data from the telemedicine visit collected to be reviewed during the virtual patient encounter from the telemedicine visit;reducing from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services with the selected data items aggregated into the plurality of information matrixes a number of diagnostic options to be considered during the virtual patient encounter via the telemedicine visit, thereby reducing a medical risk associated with making a complex medical decision for the virtual patient encounter via the telemedicine visit and limiting an amount and complexity of patient data to be processed and reviewed after the virtual patient encounter via the telemedicine visit;calculating from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time a plurality of matrix summary values from the plurality of information matrixes;calculating from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time one or more medical and billing codes using the calculated plurality of matrix summary values;generating automatically from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time an electronic patient medical record with a plurality of data fields from the calculated plurality of matrix summary values and the one or more calculated medical and billing codes;generating automatically from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time a patient invoice for the virtual patient encounter via the telemedicine visit from the generated electronic medical record; anddisplaying securely from the selected telemedicine application on the selected cloud server network device via the cloud communications network with the one or more SaaS services from the selected first set of SaaS cloud services in real-time on the selected cloud server network device on a display component via a graphical user interface (GUI) selected ones of the plural data fields from the generated electronic patient medical record, including complexity risk information for the virtual patient encounter via the telemedicine visit and the generated patient invoice,wherein the generated electronic medical record provides a three hundred-sixty degree view of medical, billing, insurance and other information collected and generated from the virtual patient encounter via the telemedicine visit,wherein displaying the three hundred sixty degree view comprises displaying medical, billing, insurance and other information on the GUI surrounding a patient avatar for the virtual patient encounter via the telemedicine visit.","20","16/926898","2020-07-13","2020-0342966","2020-10-29","11361853","2022-06-14","PRACTICE VELOCITY, LLC","David E. Stern","","","","G16H-0010/60","G16H-0010/60 | G06F-0016/9538 | G06Q-0010/04 | G06Q-0010/0635 | G06Q-0010/06315 | G06Q-0010/10 | G06Q-0030/04 | G06Q-0040/08 | G16H-0015/00 | G16H-0040/20 | G16H-0040/67 | G16H-0050/20 | G16H-0050/70 | G16H-0070/20 | G16H-0070/60 | G16H-0080/00 | H04L-0067/12 | A61B-0005/002 | G06Q-0030/0185","G16H-010/60","G16H-010/60 | G16H-040/67 | G06Q-010/06 | G16H-040/20 | G16H-080/00 | G16H-050/20 | G16H-050/70 | G16H-070/20 | G06Q-030/04 | G06Q-040/08 | G16H-015/00 | G06Q-010/04 | G16H-070/60 | G06Q-010/10 | H04L-067/12 | G06F-016/9538 | A61B-005/00 | G06Q-030/00","","","","","","4922024005416"
"US","US","P","B2","Machine learning based depolarization identification and arrhythmia localization visualization","Techniques that include applying machine learning models to episode data, including a cardiac electrogram, stored by a medical device are disclosed. In some examples, based on the application of one or more machine learning models to the episode data, processing circuitry derives, for each of a plurality of arrhythmia type classifications, class activation data indicating varying likelihoods of the classification over a period of time associated with the episode. The processing circuitry may display a graph of the varying likelihoods of the arrhythmia type classifications over the period of time. In some examples, processing circuitry may use arrhythmia type likelihoods and depolarization likelihoods to identify depolarizations, e.g., QRS complexes, during the episode.","1. A device comprising processing circuitry and a storage medium, wherein the processing circuitry is configured to: apply one or more machine learning models to episode data for an episode associated with a period of time, wherein the episode data comprises an electrocardiogram (ECG) sensed during the period of time, and wherein the one or more machine learning models are configured to output a respective likelihood value for each of a plurality of arrhythmia type classifications, each of the respective likelihood values representing a likelihood that the respective arrhythmia type classification occurred at any point during the period of time; andderive, for each of the plurality of arrhythmia type classifications, class activation data indicating varying likelihoods of the respective arrhythmia type classification over the period of time based on the application of the one or more machine learning models to the episode data, wherein the class activation data for each of the arrhythmia type classifications comprises a respective set of arrhythmia type likelihood values for the period of time, each of the arrhythmia type likelihood values of the set representing a likelihood that the respective arrhythmia type classification occurred at a respective time during the period of time;generate data that indicates that at least one of the arrhythmia type classifications occurred at any point during the period of time based on the application of the one or more machine learning models to the episode data; andgenerate data that indicates at least one point during the period of time of higher likelihood for the at least one arrhythmia type classification relative to other points during the period of time based on the class activation data.","30","17/389831","2021-07-30","2021-0358631","2021-11-18","11355244","2022-06-07","MEDTRONIC, INC.","Tarek D. Haddad | Niranjan Chakravarthy | Donald R. Musgrove | Andrew Radtke | Eduardo N. Warman | Rodolphe Katra | Lindsay A. Pedalty","","","","G16H-0050/20","G16H-0050/20 | A61B-0005/076 | A61B-0005/339 | A61B-0005/349 | A61B-0005/686 | G06N-0005/04 | G06N-0020/00 | A61B-0005/7267","G06Q-050/00","G06Q-050/00 | G06T-007/00 | G16H-050/20 | G06N-020/00 | G06N-005/04 | A61B-005/07 | A61B-005/00 | A61B-005/339 | A61B-005/349","","","","","","4922023005393"
"US","US","P","B2","Accuracy of measuring nutritional responses in a non-clinical setting","Techniques are disclosed herein for improving the accuracy of nutritional responses measured in a non-clinical setting. Using the technologies described herein, different techniques can be utilized to improve the accuracy of test data associated with one or more ""at home"" tests. In some examples, more than one test is utilized to improve the accuracy of test data associated with a particular biomarker. In other examples, a data accuracy service can programmatically analyze data received from an individual and determine whether the data is accurate. In some examples, a computing device is utilized to assist in determining what food item(s) are consumed, as well as determine whether a test protocol was followed.","1. A method, comprising: generating, by one or more computing devices, a weighting of data to be used by a machine learning mechanism to be used to make predictions of an accuracy of a type of test performed in one or more non-clinical settings based, at least in part, on at least one of clinical measurements of nutritional responses or at home measurements of nutritional responses;receiving food data, wherein the food data indicates one or more foods consumed by an individual to evoke a nutritional response associated with a test performed in a non-clinical setting;receiving test data associated with performance of the test in the non-clinical setting;causing the machine learning mechanism, that uses the weighting, to execute on a computing device or at least one or more of the computing devices, wherein executing the machine learning mechanism includes performing actions to determine an accuracy of the test; wherein determining the accuracy of the test is based at least in part on two or more of the food data, the test data, second test data, or non-biomarker test data, wherein the accuracy of the test is related to how closely an individual follows a test protocol for the test; andcausing, based at least on the determined accuracy of the test, at least one of the following to be performed: confirming the test data by verifying at least a time when one or more foods are consumed based at least in part on a measurement of a blood sugar change received from a continuous glucose monitor (CGM);confirming the test data by verifying at least a digital image of an At Home Blood Test;calculating the value of a second biomarker from biomarker data associated with a first biomarker, wherein the first biomarker and the second biomarker are different biomarkers;combining the value of two or more different biomarker measurements to increase the accuracy of the test;adjusting at least a portion of the test data at least partly in response to determining that the test was performed at a different time than an indicated time based at least in part on a measurement of a blood sugar change received from the CGM; oradjusting a weighting of one or more of the food data or the test data when utilized in combination with other data to train or make predictions by a subsequent machine learning mechanism, wherein the weighting reflects an accuracy of the one or more of the food data or the test data versus the accuracy of the other data.","20","15/987699","2018-05-23","2019-0362648","2019-11-28","11348479","2022-05-31","ZOE LIMITED","George Hadjigeorgiou | Jonathan Thomas Wolf | Richard James Davies","","","","G09B-0019/0092","G09B-0019/0092 | A61B-0005/0022 | A61B-0005/14532 | A61B-0005/4866 | A61B-0005/7264 | G06K-0007/1413 | G06K-0007/1417 | G06N-0020/00 | G09B-0007/00 | A61B-0010/0038 | A63B-0024/0062 | A63B-2220/40 | A63B-2230/06","G16H-050/20","G16H-050/20 | G09B-019/00 | G06N-020/00 | A61B-005/00 | A61B-005/145 | G06K-007/14 | G09B-007/00 | A61B-010/00 | A63B-024/00","","","","","","4922022005225"
"US","US","P","B2","Automated extraction, inference and normalization of structured attributes for product data","Methods, systems, and apparatus, including computer programs encoded on computer storage media, for automated extraction, inference and normalization of structured attributes for a Product Category Normalizer to access product records from external data sources. The Product Category Normalizer defines a product-description taxonomy of product categories represented by a classification tree. The Product Category Normalizer employs product attribute data and machine learning techniques to analyze the product data of an input product record. Based on the product data of the input product record, the Product Category Normalizer extracts and infers appropriate product data for a relevant product category in the classification tree for the item described by the input product record. The Product Category Normalizer normalizes the product data of the input product record. The Product Category Normalizer provides an output normalized product record related to a product category and product attributes of the classification tree.","1. A computer-implemented method, comprising: defining a product-description taxonomy represented according to a classification tree by clustering tags based on product data from product records in a product catalog, the classification tree comprising at least one product category path defined by product category nodes at successively deeper tree levels, each product category node associated with at least one product attribute-key paired with one or more key-values;defining key data based on the product data, the key data comprising: at least one product attribute-key (master attribute-key), a plurality of allowed key-values, and a representative key-value matched to a list of equivalent key-values;defining heuristic data based on the product data and the key data, the heuristic data comprising: a mapping between a master attribute-key and a list of equivalent attribute-keys, at least one key-value exclusive to a particular master attribute-key and a frequent text phrase as a regex key-value that corresponds to a master attribute-key; andapplying one or more machine learning (ML) models, the key data and the heuristic data to an input product record to generate an output normalized product record categorizing the input product record according to the classification tree'ss product-description taxonomy, wherein the product data of the plurality of input product records is based on multiple, incongruent third-party product-description taxonomies.","22","16/740209","2020-01-10","2020-0151201","2020-05-14","11341170","2022-05-24","HEARST MAGAZINE MEDIA, INC.","Govind Chandrasekhar | Ramanan Balakrishnan | Praveen Sekar | Yashaswini D R | Shiva Sankar Selva Kumar Valsala Kumari | Vijay Mohanbhai Bambhaniya | Srinivasan Kidambi Sridharan | Vinoth Gopinathan | Varun Sivamani","","","","G06F-0016/285","G06F-0016/285 | G06F-0016/2246 | G06N-0005/003 | G06N-0005/04 | G06N-0020/00 | G06Q-0010/087","A61N-001/00","A61N-001/00 | G06F-016/28 | G06N-005/04 | G06F-016/22 | G06N-005/00 | G06N-020/00 | G06Q-010/08","","","","","","4922021004505"
"US","US","P","B1","Compatibility method for individuals","A compatibility method for determining the likelihood that two individuals will be compatible wherein the method of the present invention implements a testing protocol that utilizes music. The present invention operably integrates with a music library of each user prior to implementing a testing protocol. The testing protocol incorporates a plurality of questions wherein the questions are provided to a user with an answer set for a user to select an answer therefrom. Subsequent selecting an answer, the software application of the present invention will cross-reference keywords in the answer with keywords in the lyrics of the songs within the music library of the user. The software application will then play an alternate song based on the keyword correlation of the aforementioned wherein the song initiates play prior to presenting the user with an additional question. The protocol continues with the software application determining an emotional quotient value.","1. A method for determining compatibility of at least two individuals in order to potentially establish a relationship therebetween comprising the steps of: providing a software application, said software application being stored on a computing device wherein the computing device is accessible via conventional communication protocols;downloading the software application, wherein users download the software application onto a remote computing device;registering with the software application, wherein the users will complete the registration process of the software application;accessing a music library of the users, wherein the software application will access the music library of the users;scanning the lyrics of songs in the music library, wherein the software application of the present invention scans the songs and identifies keywords therein;presenting at least one test question, wherein the at least one test question is presented in conjunction with answers from which to choose and comprises the testing protocol, wherein the at least one test question is further presented while simultaneously playing at least one song from the music library;answering the at least one test question, wherein the user will select from one of the answers provided with the at least one test question;establishing an emotional quotient value for the user, wherein the emotional quotient value is based on keywords identified in the selected answer and keywords in the music library;identifying a potential match, wherein the software application will identify another individual that could be a match based on equivalent emotional quotient values.","20","16/556314","2019-08-30","","","11341201","2022-05-24","Darrell Boulby","Darrell Boulby","","","","G06F-0016/9536","G06F-0016/9536 | A61B-0005/02438 | A61B-0005/165 | G06F-0016/685 | G06F-0016/9537 | G06Q-0010/02 | H04L-0051/20 | H04N-0007/141","G06F-017/30","G06F-017/30 | G06F-016/9536 | A61B-005/16 | G06F-016/683 | G06Q-010/02 | H04L-051/222 | H04N-007/14 | G06F-016/9537 | A61B-005/024","","","","","","4922021004536"
"US","US","P","B2","Information processing apparatus, information processing method, and program","In one example embodiment, an information processing apparatus, for an observed image associated with an observation target object (e.g., a section of biological tissue), associates and stores position information and observation magnification information. In this embodiment, the information processing apparatus causes a display device to: (i) display an image associated with the observation target object; (ii) indicate the first positional information of the first observed image; and (iii) indicate the first observation magnification information of the first observed image.","1. A method of displaying a microscopic image, the method comprising: causing, by a processor, a display device to display a first area of the microscopic image; andcausing, by the processor, the display device to display a first part of the first area in a first color,wherein a position of the first part of the first area corresponds to a position of a displayed area of the microscopic image,wherein the first color includes one or more colors generated by the processor, which associates the first color with a magnification level to add the first color in the first part of the first area, andwherein the first color is representative of the magnification level of the displayed area of the microscopic image when it displayed.","11","16/527876","2019-07-31","2019-0356859","2019-11-21","11342063","2022-05-24","SONY CORPORATION","Yoichi Mizutani | Shigeatsu Yoshioka | Yoshihiro Wakita | Masashi Kimoto | Naoki Tagami","2009-269495","JP","2009-11-27","G16H-0030/20","G16H-0030/20 | A61B-0005/0059 | A61B-0090/20 | G06F-0003/14 | G06T-0007/0012 | G06T-0011/00 | G06T-0011/60 | G06T-2207/10056 | G06T-2207/30024","G06T-007/00","G06T-007/00 | G16H-030/20 | G06T-011/00 | G06T-011/60 | G06F-003/14 | A61B-090/20 | A61B-005/00","","","","","","4922021005395"
"US","US","P","B2","Interactive exercise machine data architecture","An interactive exercise system includes an exercise machine having a display able to present videos, a three-dimensional camera system, and local processing system able to provide pose estimation based on data from the three-dimensional camera system. The system also includes a communication module for connection to a cloud processing system, with the cloud processing system supporting exercise related analytics including those based on pose estimation provided by the local processing system.","1. An interactive exercise system comprising; an exercise machine having a display able to present videos, a three-dimensional camera system, and local processing system able to provide pose estimation based on data from the three-dimensional camera system;wherein one or both of (a) the local processing system and (b) a cloud processing system in data communication with the local processing system is programmed to: generate a kinematic model of a user represented in the data from the three-dimensional camera system;process the kinematic model using a model constructed by one of a machine learning or heuristic method to obtain results; andprovide live feedback on the display according to the results;wherein the exercise machine includes a mechanical support system;a display module held by the mechanical support system;a mirror element attached to at least partially cover the display module; andat least one movable arm connected to the mechanical support system that includes at least one force-controlled component engageable by a user.","15","16/534601","2019-08-07","2020-0047054","2020-02-13","11331538","2022-05-17","INTERACTIVE STRENGTH, INC.","Trent Ward | Gregor Angus Berkowitz | Martha Deery | Yves Albert Behar","","","","A63B-0024/0087","A63B-0024/0087 | A61B-0005/0205 | A61B-0005/1114 | A61B-0005/1126 | A61B-0005/486 | A63B-0021/0058 | A63B-0021/153 | A63B-0021/4035 | A63B-0024/0006 | A63B-0024/0021 | A63B-0024/0062 | A63B-0024/0075 | A63B-0071/0054 | A63B-0071/0622 | A63F-0013/213 | A63F-0013/28 | G06F-0003/011 | G06F-0003/016 | G06F-0003/0304 | G06F-0003/048 | G06K-0009/00342 | G06K-0009/00369 | A63B-2024/0012 | A63B-2024/0015 | A63B-2024/0025 | A63B-2024/0068 | A63B-2024/0093 | A63B-2071/0072 | A63B-2071/065 | A63B-2071/0647 | A63B-2071/0658 | A63B-2220/51 | A63B-2220/806 | A63B-2220/833 | A63B-2225/12 | A63B-2225/15 | A63B-2225/20 | A63B-2225/50 | A63B-2230/06 | A63B-2230/42","A63B-024/00","A63B-024/00 | G06F-003/01 | G06F-003/03 | G06K-009/00 | A63F-013/213 | A61B-005/11 | A61B-005/00 | A61B-005/0205 | A63B-021/00 | A63B-071/06 | A63F-013/28 | G06F-003/048 | A63B-021/005 | A63B-071/00","","","","","","4922020001520"
"US","US","P","B1","Medical imaging with functional architecture tracking","Accessed from memory are i) a first brain image, ii) a second brain image, iii) a difference map, and iv) a connectivity element. A GUI may include: concurrently displaying each of these elements. User input to the GUI is received. A location of the user input is determined. The operations also include responsive to determining that the location of the user input is within the display of the first brain image, modifications are made to the display of the i) second brain image, ii) the difference map, and iii) the connectivity element to highlight elements of the display that correspond to an element of the first brain image that corresponds to the location of the user input.","1. A system for generating composite-connectomes from collections of single-connectomes, the system comprising: one or more processors;computer memory storing instructions that, when executed by the processors, cause the processors to perform operations comprising:accessing, from a computer-accessible memory, i) a first brain image, ii) a second brain image, iii) a difference map, and iv) a connectivity element having properties representing differences in connectivity between the first brain image and the second brain image and rendering a graphical user interface (GUI), comprising:concurrently displaying i) the first brain image, ii) the second brain image, iii) the difference map, and iv) the connectivity element, wherein at least some of the corresponding elements in the displays of the i) the first brain image, ii) the second brain image, iii) the difference map, and iv) the connectivity element have different relative pixel locations, and wherein the GUI includes a sets of bounding boxes, such that bounding boxes of the set are located on corresponding elements in the display;receiving user input to the GUI;determining a location of the user input; andresponsive to determining that the location of the user input is within the display of the connectivity element, modifying the display of the i) first brain image, ii) second brain image, and iii) the difference map to highlight elements of the display that correspond to an element of the connectivity element that corresponds to the location of the user input.","17","17/527720","2021-11-16","","","11334976","2022-05-17","OMNISCIENT NEUROTECHNOLOGY PTY LIMITED","Michael Edward Sughrue | Stephane Philippe Doyen | Peter James Nicholas | Xinling Jiang","","","","G06T-0005/50","G06T-0005/50 | G06F-0003/04845 | G06T-0007/0012 | G06T-2200/24 | G06T-2207/20092 | G06T-2207/20221 | G06T-2207/30016","G06K-009/00","G06K-009/00 | A61B-005/05 | G06T-005/50 | G06T-007/00 | G06F-003/04845","","","","","","4922020004937"
"US","US","P","B1","Health status system, platform, and method","A health status platform includes a receiving component that receives a test result a test of a biological sample collected from a human patient. The test result includes an indication of a presence of an infectious disease in the patient, and an identification and a verification of the patient. The platform includes a certificate component that issues a certificate of origin of the biological sample; and a data merging component that cooperates with a venue access manager that controls access to a venue. The data merging component implements a distributed ledger system that stores encrypted test results of the patient and the identification and verification of the patient, and an end-to-end encryption system that receives an encrypted venue access request from a venue access manager, decrypts the access request, determines if an access request is valid, and if valid, provides an encrypted certificate of origin to the venue access manager.","1. A health safety kiosk, comprising: a health test sample station;a display and user interface component;a communications component;an application biometric key (ABK) component comprising machine instructions for operating the health safety kiosk; anda processor accessing the ABK component to:store one or more user profiles in memory, each of the one or more profiles representing a unique type of biometric sample data;receive a representation of biometric sample data obtained from a user, and store the representation in one of the one or more profiles in the memory in read-only format, the representation uniquely identifying the user;receive test sample data from a test sample of the user, the test sample provided by the user at the test station;analyze the test sample of the user to produce a test sample result, the test sample result indicating a presence in the user of an infectious disease;store the test sample data and the test sample result in the memory in read-only format;receive a request from an external device to provide the representation and the test sample data to the external device, the request identifying the external device; andcontrol the health safety kiosk to provide the test sample result and the representation to the external device.","17","17/176088","2021-02-15","","","11335440","2022-05-17","TensorX, Inc.","N. Edward White | G. Edward Powell | Van L. Marshall | Mark T. Lane | John M. Clerici","","","","G16H-0010/40","G16H-0010/40 | A61B-0005/1171 | A61B-0005/1172 | A61B-0005/4842 | G06F-0016/2365 | G07C-0009/37 | G07C-0009/38 | G16H-0010/60 | G16H-0050/20 | G16H-0050/30 | G16H-0050/50 | H04L-0009/3268 | H04L-0063/0428 | G06K-0009/00711 | G06Q-0050/265 | G16H-0050/80 | H04L-2209/38","G16H-010/40","G16H-010/40 | G16H-050/20 | G07C-009/37 | G07C-009/38 | H04L-009/32 | G06F-016/23 | A61B-005/1172 | G16H-010/60 | G16H-050/80 | G06Q-050/26 | G16H-050/50 | H04L-029/06 | A61B-005/1171 | A61B-005/00 | G16H-050/30 | G06K-009/00","","","","","","4922020005399"
"US","US","P","B2","Method for real time analyzing stress using deep neural network algorithm","The present invention relates to a stress analysis method including: acquiring bio-signals from a test subject; calculating a probability of each of a plurality of stress level values by processing the bio-signals using a deep neural network algorithm; estimating a stress level value with the maximum probability of the plurality of stress level values as a stress level value of the test subject; determining usefulness of the estimated stress level value; and outputting the estimated stress level value determined to be useful through the determination of usefulness, as the final stress level.","1. A method of controlling a mobile device, the method comprising: receiving, via a wireless interface, raw bio-signals of a test subject from a sensor;inputting the raw bio-signals into a convolution layer of a deep neural network algorithm having one or more filters to extract features from the raw bio-signals and generate an output dimension;inputting the output dimension to a pooling layer of the deep neural network algorithm to generate a reduced dimension;inputting the reduced dimension to a dense layer of the deep neural network algorithm having one or more neural layers each having a weight and connected neurons, and generating a probability of each of a plurality of stress level values based on processing the reduced dimension using the weight and the connected neurons;estimating a stress level value with a maximum probability of the plurality of stress level values as a stress level value of the test subject;determining that the estimated stress level value is useful based on a set threshold value; anddisplaying, via a display of the mobile device, the estimated stress level value determined to be useful as an output stress level,wherein the output stress level is a score or percentage indicating an amount of stress experienced by the test subject,wherein the determining that the estimated stress level value is useful includes: determining that the estimated stress level value is useless when the maximum probability of the plurality of stress level values is less than the set threshold value,in response to determining that the estimated stress level value is useless, requesting that the test subject input a recognized stress level,in response to receiving the recognized stress level value input by the test subject, applying the recognized stress level value to the deep neural network algorithm,updating the deep neural network algorithm to generate an updated deep neural network algorithm, andperforming stress analysis through the updated deep neural network algorithm.","18","16/456139","2019-06-28","2020-0054262","2020-02-20","11317840","2022-05-03","KOREA INSTITUTE OF SCIENCE AND TECHNOLOGY","Inchan Youn | Kuiwon Choi | Hyung Min Kim | Suh-Yeon Dong | Hyunmyung Cho","10-2018-0095468","KR","2018-08-16","A61B-0005/165","A61B-0005/165 | A61B-0005/4035 | A61B-0005/746 | G06F-0017/18 | G06N-0003/084 | G06N-0020/00","A61B-005/16","A61B-005/16 | A61B-005/00 | G06F-017/18 | G06N-003/08 | G06N-020/00","","","","","","4922018001022"
"US","US","P","B2","System and method for detecting invisible human emotion in a retail environment","A system for detecting invisible human emotion in a retail environment is provided. The system comprises a camera and an image processing unit. The camera is configured in a retail environment to capture an image sequence of a person before and during when a price of a product or service becomes visible. The image processing unit is trained to determine a set of bitplanes of a plurality of images in the captured image sequence that represent the hemoglobin concentration (HC) changes of the person, and to detect the person's invisible emotional states based on HC changes. The image processing unit is trained using a training set comprising a set of subjects for which emotional state is known.","1. A system for determining probability for a state of human emotion among a set of identifiable states of human emotion from a digital image sequence of a person in a retail environment, the system comprising: a computer-readable memory comprising the digital image sequence, the digital image sequence being of light re-emitted from the skin of the person before and during viewing of a product; anda processing unit comprising one or more processors in communication with the computer-readable memory, the image processing unit executable to: determine, using a first machine learning model trained with a hemoglobin concentration (HC) training set, HC changes of the person using bit values from each bitplane of images in the captured image sequence, the HC training set comprising bit values from each bitplane of images captured from one or more people while such people experience a known state of emotion; anddetermine a measure of probability, using a second machine learning model trained with a state training set, for the emotional state of the person against each of the set of identifiable states of human emotion, the state training set obtained by receiving bit values from each bitplane of images representing HC changes determined by the first machine learning model.","15","16/875322","2020-05-15","2020-0319706","2020-10-08","11320902","2022-05-03","NURALOGIX CORPORATION","Kang Lee | Pu Zheng","","","","G06F-0003/013","G06F-0003/013 | A61B-0003/113 | A61B-0005/0077 | A61B-0005/145 | A61B-0005/1455 | A61B-0005/16 | A61B-0005/165 | G06K-0009/4652 | G06K-0009/78 | G06N-0003/0445 | G06N-0003/08 | G06Q-0030/0201 | G06Q-0030/0238 | G06T-0007/20 | H04N-0007/188 | A61B-0005/08 | A61B-0005/318 | A61B-2503/12 | G06F-2203/011 | G06K-0009/00228 | G06K-0009/00302 | G06T-2207/20081","G06F-003/01","G06F-003/01 | A61B-005/16 | A61B-003/113 | A61B-005/00 | G06K-009/78 | G06K-009/46 | G06N-003/04 | G06T-007/20 | G06Q-030/02 | A61B-005/145 | A61B-005/1455 | G06N-003/08 | H04N-007/18 | A61B-005/08 | G06K-009/00 | A61B-005/318","","","","","","4922018004066"
"US","US","P","B2","Electronic methods and systems for processing information related to intake of supplements by a user","Embodiments provide systems and methods for processing information related to nutrient intake of a user associated with a user device. The method performed by a server system includes receiving a request to register the user with the server system through a health application installed on the user device. The method includes extracting user information from one or more sources and creating a user profile based on the user information. The method includes determining at least one nutrient for recommending to the user. The at least one nutrient is determined based on the user information. Further, the method includes comparing two or more products including the at least one nutrient. The method further includes facilitating a display of the comparison of at least a quantity of each ingredient in the two or more products on the health application installed on the user device.","1. A method for managing micronutrient intake of a user associated with a user device, the method comprising: receiving, by a server system, a request to register the user with the server system through a health application installed on the user device;extracting, by the server system, user information from a plurality of sources, wherein extracting the user information comprises retrieving information related to the user from the plurality of sources and external databases, wherein the plurality of sources comprises the user device, one or more wearable devices associated with the user, and one or more nutrient trackers, and wherein the user information comprises information of: content of each micronutrient present in the user'ss body over a period of time, tracked diet, and other health metrics of the user;creating, by the server system, a user profile based on the user information, wherein the user profile comprises information related to micronutrient consumption, dosage values, energy level based on amount of consumption of the micronutrients, consumption activity, one or more products possessed by the user, at least one micronutrient goal, and a micronutrition plan;creating, by the server system, a unique identification of the user, wherein the unique identification maps the user to the user profile, and wherein the unique identification is a quick response code;determining, by the server system, at least one micronutrient for recommending to the user, based on the user information;comparing, by the server system, two or more products on an ingredient level comprising the at least one micronutrient by scanning a barcode corresponding to each product and capturing a product label of each product for comparison;facilitating, by the server system, a display of the comparison of at least a quantity of each ingredient comprised in the two or more products on the health application installed on the user device;determining, by the server system, a micronutrients goal and a plan to achieve the micronutrients goal for the user based on the user information, wherein the micronutrients goal comprises intake dosage required for each micronutrient of the at least one micronutrient determined based on the user information;sending, by the server system, at least one reminder to the user device to consume one or more products to achieve the micronutrients goal according to the plan;receiving, by the server system, a first signal indicating a consumption of a dosage of the one or more products, wherein the first signal includes at least a tactile input, a voice input, and a gesture input;updating, by the server system, a quantity of each product of the one or more products in a digital inventory of the health application on the consumption of dosage of the one or more products;tracking, by the server system, the contents of each supplement and recording a feedback on the products based on the user consumption of the products; andautomatically ordering, by the server system, one or more products based on the user consumption of the products.","19","16/932759","2020-07-18","2022-0020472","2022-01-20","11322241","2022-05-03","NONSTOP EVOLVING CORP.","Sami Choura","","","","G16H-0020/60","G16H-0020/60 | G06F-0016/2379 | G06Q-0010/087 | G06Q-0010/109 | G06Q-0030/0215 | G06Q-0030/0605 | G06Q-0030/0627 | G06Q-0030/0629 | G06Q-0030/0631 | G06Q-0050/01 | G16H-0010/60 | G16H-0040/67 | G16H-0050/20 | A61B-0005/117 | A61B-0005/4866","G16H-020/60","G16H-020/60 | G16H-050/20 | G06Q-030/06 | G16H-040/67 | G06Q-050/00 | G06Q-010/10 | G06Q-030/02 | G06Q-010/08 | G06F-016/23 | G16H-010/60 | A61B-005/00 | A61B-005/117","","","","","","4922018005399"
"US","US","P","B1","Intelligent medical care path systems and methods","Further system and methods associated there with can use a combination of big data, machine learning, and/or regression equations to make living care paths based on sensitivities, probability, and/or statistics, which increases the chances of a living care path being successful. System, in some embodiments, can also provide a visual representation of the treatment options and statistics to the patient and HCP. As configured, system can empower patients, making them more informed about their condition, expectations of recovery, and more confident in their HCP's recommended treatment measure.","1. A method for executing a patient'ss living care path based on continuous monitoring of a patient comprising: providing a non-transitory computer-readable medium storing instructions that, when executed on a computing device, cause the computing device to perform a method for executing a patient'ss living care path, wherein the computing device is a patient specific hub apparatus comprised of: an at least one data input device, a display; and a housing defining a cavity comprised of a medicine storage apparatus, a processor operably coupled to a memory and a power source, wherein the memory stores computer executable instructions and a processor that executes the instructions to cause the patient hub apparatus to perform the steps of:continuously monitoring the patient by receiving patient data from a medical monitoring device to form a plurality of medical data sets;aggregating the plurality of medical data sets using a clustering algorithm into one or more phenotypic groups to assign the patient to an at least one phenotypic group;first processing the plurality of medical data set of the at least one assigned phenotypic group to predict one or more patient outcomes;wherein said first processing comprises the steps of:plotting the data inputs per patient within their phenotypic group into tables based on the total number of respective outcome possibilities to create a phenotypic group outcome data table plot;creating a bootstrapped data table plot by randomly selecting patient data groups;forming a new data table plot; andexecuting a decision tree creation from each of these bootstrapped data tables selecting an output of one or more patient outcomes;second processing the plurality of medical data set of the assigned phenotypic group to determine an optimal time-based set of measures for increasing the probability of attaining an at least one living care path goal;wherein said second processing comprises the steps of:plotting the data inputs per patient within their phenotypic group into tables based on the total number of respective goals to create a phenotypic group goal data table plot;selecting an output of the probability of attaining an at least one living care path goal;combining an output of said one or more patient outcomes with an output of said optimal time-based set of measures to provide the living care path;graphically displaying the living care path, anddispending medicine from said medicine storage apparatus of said patient hub in response to continuous monitoring of said patient.","4","17/079501","2020-10-25","","","11322250","2022-05-03","TNACITY BLUE OCEAN, INC.","Scott K Laster | Brian D. Childress","","","","G16H-0040/20","G16H-0040/20 | A61B-0005/0022 | A61B-0005/0205 | A61B-0005/02055 | A61B-0005/14532 | A61B-0005/14551 | A61B-0005/369 | A61B-0005/4806 | A61B-0005/4845 | A61B-0005/4848 | A61B-0005/681 | A61B-0005/686 | A61B-0005/7275 | G06Q-0010/06316 | G06Q-0010/10 | G06Q-0050/01 | G16H-0010/60 | G16H-0015/00 | G16H-0020/13 | G16H-0040/67 | G16H-0050/20 | G16H-0050/70 | G16H-0080/00 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0245 | A61B-0005/0533 | A61B-0005/082 | A61B-0005/091 | G06F-0040/174 | G06Q-0040/08 | G06Q-0050/26","G16H-020/17","G16H-020/17 | G16H-040/20 | G16H-050/70 | G16H-050/20 | G16H-015/00 | G06Q-050/00 | G06Q-010/06 | G06Q-010/10 | G16H-020/13 | G16H-040/67 | G16H-080/00 | A61B-005/00 | A61B-005/0205 | A61B-005/1455 | A61B-005/145 | G16H-010/60 | A61B-005/369 | G06F-040/174 | G06Q-040/08 | G06Q-050/26 | A61B-005/021 | A61B-005/024 | A61B-005/0245 | A61B-005/08 | A61B-005/0533 | A61B-005/091","","","","","","4922018005408"
"US","US","P","B1","Methods and systems for predicting printed label's life","Various embodiments illustrated herein disclose a method comprising receiving, by a processor, one or more patient characteristics associated with a first patient. The one or more patient characteristics comprises at least a type of sanitization, and/or a frequency of sanitization usage. Further, the method includes receiving one or more image characteristics associated with an image of a patient bracelet worn by the first patient. The method further includes training a machine learning (ML) model defining a relation between the one or more patient characteristics and the one or more image characteristics. The ML model is utilized to predict a count of days until the patient bracelet, associated with a second patient, deems unusable. Additionally, the method includes generating an instruction to a printing apparatus to print a new patient bracelet for the second patient based on the count of days being less than a predetermined number of days threshold.","1. A method comprising: receiving, by a processor, one or more patient characteristics associated with a first patient, wherein the one or more patient characteristics comprises at least a type of sanitization, and/or a frequency of sanitization usage;receiving, by the processor, one or more image characteristics associated with an image of a patient bracelet worn by the first patient; andtraining, by the processor, a machine learning (ML) model defining a relation between the one or more patient characteristics and the one or more image characteristics, wherein the ML model is utilized to predict a count of days until the patient bracelet, associated with a second patient, deems unusable; andgenerating, by the processor, an instruction to a printing apparatus to print a new patient bracelet for the second patient based on the count of days being less than a predetermined number of days threshold.","20","17/090224","2020-11-05","2022-0133148","2022-05-05","11311192","2022-04-26","HAND HELD PRODUCTS, INC.","Praveen Allaka","","","","A61B-0005/0022","A61B-0005/0022 | A61B-0005/6824 | A61B-0005/7267 | G06K-0007/1408 | G06K-0007/1473 | G06K-0009/6256","A61B-005/00","A61B-005/00 | G06K-009/62 | G06K-007/14","","","","","","4922017000930"
"US","US","P","B2","Electronic device, and method for analyzing face information in electronic device","A method for analyzing face information in an electronic device is provided. The method includes detecting at least one face region from an image that is being captured by a camera module, zooming in the at least one detected face region, and analyzing the at least one detected and zoomed in face region according to at least one analysis item.,","1. An electronic device comprising: a camera;a display; andat least one processor configured to: control the camera to obtain an image,identify an analysis item among a plurality of analysis items for a skin analysis,based on the identified analysis item, detect at least one face region among a plurality of face regions of a face in the obtained image, the detected at least one face region being associated with the identified analysis item,analyze a skin condition corresponding to the identified analysis item in the detected at least one face region, andcontrol the display to display a result of analyzing the skin condition corresponding to the identified analysis item.","15","16/848489","2020-04-14","2020-0242334","2020-07-30","11311195","2022-04-26","Samsung Electronics Co., Ltd.","Joo-Young Son | Jin-Ho Kim | Woo-Sung Kang | Yun-Jung Kim | Hong-Il Kim | Jae-Won Son | Won-Suk Chang | In-Ho Choi | Dae-Young Hyun | Tae-Hwa Hong","10-2014-0152239 | 10-2015-0092549","KR | KR","2014-11-04 | 2015-06-29","A61B-0005/0077","A61B-0005/0077 | A61B-0005/1176 | A61B-0005/442 | A61B-0005/444 | A61B-0005/7275 | G06F-0003/017 | G06K-0009/0061 | G06K-0009/00248 | G06K-0009/00281 | G06T-0007/90 | G16H-0015/00 | G16H-0030/20 | G16H-0050/20 | H04N-0005/23218 | H04N-0005/23293 | H04N-0005/23296 | A61B-0005/443 | A61B-0005/6898 | A61B-0005/743 | A61B-2576/00 | A61B-2576/02 | G06F-0001/1626 | G06F-0003/04883 | G06T-2207/30201 | G16H-0030/40","A61B-005/00","A61B-005/00 | G06K-009/00 | G06F-003/01 | H04N-005/232 | G16H-015/00 | G16H-050/20 | A61B-005/1171 | G06T-007/90 | G16H-030/20 | G06F-001/16 | G06F-003/0488 | G16H-030/40 | G06F-003/04883","","","","","","4922017000933"
"US","US","P","B2","Information processing apparatus, information processing method, non-transitory computer-readable medium, and information processing system for displaying biological signal measurements","An information processing apparatus includes an acquiring unit, a determining unit, and a changing unit. The acquiring unit is configured to acquire determination information for determining a display layout of a screen for displaying information related to one or more biological signals. The determining unit is configured to determine a display layout corresponding to the determination information acquired by the acquiring unit. The changing unit is configured to change a display layout of the screen in accordance with the display layout determined by the determining unit.","1. An information processing apparatus comprising: processing circuitry configured to,receive a plurality of biological signals associated with a patient from a plurality of measurement devices, the plurality of measurement devices including measurement devices of at least two different measurement device types;acquire determination information for displaying information related to one or more biological signals of the plurality of biological signals, the determination information including at least patient information corresponding to the patient and role information corresponding to a user, the role information indicating a medical specialty of the user;determine an analysis screen display layout corresponding to the acquired determination information;change a display layout of the analysis screen in accordance with the determined display layout, the changing the display layout of the analysis screen including displaying a subset of the plurality of biological signals based on the acquired determination information and the measurement device types of the corresponding measurement devices;receive an estimation user input associated with a selected annotation on at least one biological signal from the plurality of biological signals displayed on the analysis screen; andestimate at least one signal source corresponding to the annotation in response to the received estimation user input.","20","16/290095","2019-03-01","2019-0274640","2019-09-12","11311249","2022-04-26","RICOH COMPANY, LTD.","Shinya Mukasa | Hideaki Yamagata | Noriyuki Tomita | Aritaka Hagiwara | Yutaka Yagiura | Daisuke Sakai","2018-044701 | 2018-231110","JP | JP","2018-03-12 | 2018-12-10","A61B-0005/7445","A61B-0005/7445 | A61B-0005/245 | A61B-0005/7425 | G06F-0003/04842 | G06F-0003/04845 | G06F-0003/04886 | G06F-0009/451","A61B-005/00","A61B-005/00 | G06F-003/0488 | G06F-003/04886 | A61B-005/245 | G06F-009/451 | G06F-003/04842 | G06F-003/04845","","","","","","4922017000984"
"US","US","P","B2","Dynamic dosing systems for phototherapy and associated devices, systems, and methods","Dynamic dosing systems for phototherapy and associated devices, systems, and methods are disclosed herein. In some embodiments, a dynamic dosing phototherapy system can include a self-service phototherapy kiosk (""SPK"") configured to emit UV radiation, a user interface communicatively coupled to the SPK, and a dynamic dosing system communicatively coupled to the user interface and the SPK. The dynamic dosing system can determine initial, user-specific parameters specific to define a first individual phototherapy protocol, and then determine adjustments to the initial parameters and the first individual phototherapy protocol based on user inputs related to erythema response to a previous phototherapy treatment session. The SPK can deliver UV radiation to a user in accordance with the first individual phototherapy protocol and/or an adjusted, second phototherapy protocol that takes into account the user's erythema response.","1. A dynamic dosing phototherapy system, comprising: a self-service phototherapy kiosk (""SPK"") comprising a housing defining an at least partially enclosed chamber and a UV radiation assembly configured to emit phototherapeutic UV radiation toward a phototherapy zone within the chamber, the chamber being sized to receive a human body;a user interface communicatively coupled to the SPK and configured to display a plurality of graphical user interfaces (GUIs) for receiving user inputs from a user related to characteristics affecting photosensitivity, erythema response to a screening dose, and erythema response to a previous phototherapy treatment session;a dynamic dosing system communicatively coupled to the user interface and the SPK, the dynamic dosing system comprising a computer-readable storage medium storing instructions that, when executed by a computing system, cause the computing system to perform operations comprising? determining, based at least in part on the user inputs received from the user via the user interface, one or more parameters specific to the user, wherein the one or more parameters include at least one of a user skin type category, individual minimal erythemal dose (MED), or an individual treatment frequency, and wherein the one or more parameters are used to define a first individual phototherapy protocol to deliver a first individual treatment dose and a screening protocol the screening protocol, including: communicating the screening dose of UV radiation to the SPK such that the SPK delivers the screening dose of UV radiation equivalent to 70% or less of the first individual treatment dose;receiving the user inputs related to erythema response to the screening dose;determining, based on the user inputs related to erythema response to the screening dose, whether the first individual phototherapy protocol is suitable for the user; and when an erythemal response occurred in response to the screening dose, reducing the first individual treatment dose to reflect the erythemal response;when the first individual phototherapy protocol is suitable for the user communicating the first individual phototherapy protocol to the SPK such that the SPK delivers UV radiation in accordance with the first individual phototherapy protocol in a next dose delivered;determining, based on the user inputs related to erythema response to the previous phototherapy treatment session, adjustments to the one or more parameters specific to the user to define a second individual phototherapy protocol; andcommunicating the second individual phototherapy protocol to the SPK such that the SPK delivers UV radiation in accordance with the second individual phototherapy protocol.","32","16/954075","2018-12-13","2020-0376292","2020-12-03","11311744","2022-04-26","BENESOL, INC.","William Alexander Moffat | Sen Wen | Linda Cox Arnsdorf | Eben Lynn Falconer Calhoun | Chieh-en Wu","","","","A61N-0005/0616","A61N-0005/0616 | A61B-0005/445 | G06F-0003/048 | G06N-0020/00 | G16H-0020/13 | G16H-0020/40 | A61N-2005/064 | A61N-2005/0627 | A61N-2005/0628 | A61N-2005/0632 | A61N-2005/0661","A61N-005/06","A61N-005/06 | G06F-003/048 | G16H-020/40 | G16H-020/13 | G06N-020/00 | A61B-005/00","","","","","","4922017001473"
"US","US","P","B2","Head-mounted display apparatus and power saving control program for head-mounted display apparatus","A head-mounted display apparatus includes an image display unit configured to display an image, a sound output unit configured to output a sound, a mounting state determination unit configured to determine a mounting state of the head-mounted display apparatus, and an image sound output control unit configured to turn ON/OFF display, and turn ON/OFF an output of a sound, based on the mounting state. The image sound output control unit is configured to turn ON display of an image and an output of a sound when the mounting state is the first state, turn OFF display of an image and turn ON an output of a sound when the mounting state is not the first state and is the second state, and turn OFF display of an image and an output of a sound when the mounting state is not the first state and is not the second state.","1. A head-mounted display apparatus, comprising: an image display unit configured to display an image;a sound output unit configured to output a sound;a front frame configured to support the image display unit;a contact sensor disposed in the front frame;a camera disposed in the front frame;a mounting state determination unit configured to determine a mounting state of the head-mounted display apparatus; andan image sound output control unit configured to turn ON/OFF display of an image by the image display unit, and turn ON/OFF an output of a sound by the sound output unit, based on the mounting state, whereinthe mounting state determination unit determines whether the mounting state is a first state where the head-mounted display apparatus is mounted in a position in which an image is visually recognizable, and determines whether the mounting state is a second state where the head-mounted display apparatus is mounted on a head,the mounting state determination unit acquires a first image of an outside scenery by using the camera in response to detecting a contact of a user with the contact sensor and then acquires a second image of the outside scenery by using the camera in response to detecting the user is no longer in contact with the contact sensor, and determines whether the mounting state is the first state or is not the first state by comparing the first image with the second image, wherein the mounting state determination unit determines that the mounting state is the first state in response to determining a visually recognizable change between the first image and the second image in a case where the head-mounted display is initially turned ON, andthe image sound output control unit is configured toturn ON display of an image and an output of a sound when the mounting state is the first state,turn OFF display of an image and turn ON an output of a sound when the mounting state is not the first state and is the second state, andturn OFF display of an image and an output of a sound when the mounting state is not the first state and is not the second state.","7","17/161671","2021-01-29","2021-0240250","2021-08-05","11314314","2022-04-26","SEIKO EPSON CORPORATION","Keiichi Okano | Shinichi Kobayashi","2020-013567","JP","2020-01-30","G06F-0001/3265","G06F-0001/3265 | A61B-0005/6803 | G01C-0019/00 | G01D-0005/24 | G01P-0015/18 | G02B-0027/0172 | G02B-0027/0176 | G06F-0001/3287 | G06F-0003/165 | G06T-0007/97 | G09G-0003/3208 | A61B-0005/02438 | G02B-2027/0138 | G02B-2027/0178 | G09G-2330/022 | G09G-2330/023 | G09G-2330/026 | G09G-2330/027","G06F-001/3234","G06F-001/3234 | G02B-027/01 | G06T-007/00 | G06F-003/16 | G01P-015/18 | G01C-019/00 | G01D-005/24 | G06F-001/3287 | A61B-005/00 | G09G-003/3208 | A61B-005/024","","","","","","4922017004000"
"US","US","P","B2","Typifying emotional indicators for digital messaging","The present disclosure provides computing systems and techniques for indicating an emotional and/or environmental state of a user in a digital messaging application. A computing device can determine an emotional and/or environmental state of a first user responsive to reading or responding to a message and can convey the determined emotional and/or environmental state to a second computing device, to be transiently presented by the second computing device.","1. An apparatus, comprising: a processor; anda memory coupled to the processor, the memory comprising instructions that when executed by the processor cause the processor to: receive, from a messaging device, state data related to a message received at the message device, wherein the state data was determined based on characteristics of how a user interacted with the messaging device and characteristics of the messaging device in response to the message received at the messaging device;generate a state indication based in part on the state data, using a state prediction model, the state indication including: an indication of an emotional state of the user of the messaging device, andan indication of an environmental state of the user of the messaging device;generate an information element including the state indication; andsend the information element to a digital message application of the message device, the information element including the state indication to the messaging device, wherein a representation of the state indication may be presented for a transient period.","20","16/992241","2020-08-13","2020-0372221","2020-11-26","11314943","2022-04-26","CAPITAL ONE SERVICES, LLC","Jeremy Phillips | Andrew Beane","","","","G06F-0040/30","G06F-0040/30 | A61B-0005/0077 | A61B-0005/01 | A61B-0005/021 | A61B-0005/165 | A61B-0005/6898 | A61B-0005/7264 | G06F-0003/0484 | G06F-0003/04817 | G06F-0040/253 | G06K-0009/00302 | G08B-0021/18","G06F-040/30","G06F-040/30 | G06K-009/00 | G06F-003/0484 | G06F-003/0481 | G08B-021/18 | A61B-005/00 | A61B-005/01 | A61B-005/021 | A61B-005/16 | G06F-040/253 | G06F-003/04817","","","","","","4922017004623"
"US","US","P","B2","Information alerts method, apparatus and device","A preset alert trigger event associated with a wealth management application operated on an electronic device is detected. In response to detecting the preset alert trigger event, the electronic device obtains a physiological characteristic parameter characterizing an emotion of a target user. The electronic device determines that a preset normal emotion fluctuation condition is not satisfied according to the physiological characteristic parameter. In response to determining that the normal emotion fluctuation condition is not satisfied, a risk alert on a wealth management action of the target user performed in the wealth management application is output.","1. A computer-implemented information alert method, comprising: detecting a preset alert trigger event associated with a wealth management application operated on an electronic device, wherein the preset alert trigger event comprises at least one of: receiving an instruction from a target user to perform a wealth management action associated with a wealth management product displayed in the wealth management application, anda characteristic indicator of the wealth management product output to a display is not within a preset indicator range, the characteristic indicator of the wealth management product characterizing a performance of one or more assets of the wealth management product;in response to detecting the preset alert trigger event, periodically obtaining, by the electronic device, a physiological characteristic parameter from a physiological sensor;determining, by the electronic device, that a preset normal emotion fluctuation condition is not satisfied according to the physiological characteristic parameter;in response to determining that the preset normal emotion fluctuation condition is not satisfied, determining, by the electronic device, a particular preset risk type from a plurality of preset risk types, each associated with the wealth management action;outputting, by the electronic device, a risk alert message associated with the particular preset risk type; after outputting the risk alert message, determining, by the electronic device, that the preset normal emotion fluctuation condition is satisfied according to the physiological characteristic parameter;in response to determining that the preset normal emotion fluctuation condition is satisfied, cancelling the outputting of the risk alert message and outputting a normal emotion alert message associated with the preset normal emotion fluctuation condition.","20","16/806693","2020-03-02","2020-0202445","2020-06-25","11315187","2022-04-26","ADVANCED NEW TECHNOLOGIES CO., LTD.","Yue Zang","2017-11482381","CN","2017-12-29","G06Q-0040/06","G06Q-0040/06 | A61B-0005/02055 | A61B-0005/1176 | A61B-0005/165 | A61B-0005/369 | A61B-0005/4884 | G06F-0003/015 | G06K-0009/00302 | G16H-0010/60 | G16H-0040/67 | G16H-0050/30 | G16H-0050/70 | H04M-0001/72454 | H04W-0004/38 | A61B-0005/021 | A61B-0005/024 | H04M-2250/12","G06Q-040/06","G06Q-040/06 | G16H-040/67 | G16H-010/60 | G16H-050/30 | G16H-050/70 | H04W-004/38 | A61B-005/0205 | A61B-005/1171 | A61B-005/16 | A61B-005/00 | G06F-003/01 | G06K-009/00 | A61B-005/369 | H04M-001/72454 | A61B-005/021 | A61B-005/024","","","","","","4922017004862"
"US","US","P","B2","Clinical infrastructure with features for the prevention of egress of private information","DICOM data is automatically prepared for transit outside of the clinical-data infrastructure, by examining a plurality of metadata fields in the corresponding metadata in the DICOM data; identifying a first subset of the metadata fields as containing private information; identifying a second subset of the metadata fields as private-information free; accessing at least some of the plurality of layers of the DICOM data; and transforming the accessed layers into a single transmission-image, the transmission-image being in a format i) other than DICOM and ii) that stores the second subset of the metadata fields as transmission-metadata in a scheme that is non-redundant for a given transmission-image.","1. A computer-readable non-transitory storage media for prevention of egress of private information from a clinical data-infrastructure, the computer-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising: receiving, at clinical-data infrastructure, Digital Imaging and Communication in Medicine (DICOM) data to be sent to a cloud-service provider, the DICOM data comprising a plurality of images and for each image corresponding metadata, the cloud-service provider being outside of clinical-data infrastructure, the clinical-data infrastructure comprising a clinical-data client and a gap server on a same local network, the clinical-data infrastructure configured to pass traffic, going out of the local network to the cloud service provider, through the gap server;preparing, automatically, the DICOM data for transit outside of the clinical-data infrastructure, by: examining a plurality of metadata fields in the corresponding metadata in the DICOM data;identifying a first subset of the metadata fields as containing private information;identifying a second subset of the metadata fields as private-information free;accessing at least two of the plurality of images of the DICOM data; andtransforming the accessed images into a single transmission-image, the transmission-image being in a format i) other than DICOM and ii) that stores the second subset of the metadata fields as transmission-metadata in a scheme that is non-redundant for a given transmission-image; andinstantiating the gap server configured to: encrypt the transmission-image and the transmission-metadata into an encrypted transmission-image and encrypted transmission-metadata, both the encrypted transmission-image and the encrypted transmission-metadata being free of the private information of the first subset of the metadata fields of the DICOM data, wherein the private information does not leave the clinical-data infrastructure;transmit to the cloud-service provider and with a unique identifier, an encrypted transmission-image made from the transmission-image and an encrypted transmission-metadata made from the transmission metadata without transmitting the private information outside the clinical data infrastructure;receive, at the gap server, a response-message from the cloud-service including the unique identifier; anddetermine that the unique identifier is stored in a private-information datastore indexed with the private information, the private-information datastore also storing additional-private information indexed with other identifiers; andbased on the unique identifier, provide the private information and the response-message to a client on the clinical-data infrastructure.","20","17/342420","2021-06-08","2021-0392118","2021-12-16","11315676","2022-04-26","OMNISCIENT NEUROTECHNOLOGY PTY LIMITED","Michael Edward Sughrue | Stephane Philippe Doyen","","","","G16H-0030/20","G16H-0030/20 | A61B-0005/0013 | A61B-0005/0022 | A61B-0005/4064 | G06F-0009/451 | G06F-0009/45558 | G06T-0007/0012 | G06T-0011/00 | G16H-0010/60 | G16H-0030/40 | H04L-0009/32 | H04L-0063/0428 | G06T-2207/20092 | G06T-2207/30016","H04L-009/32","H04L-009/32 | G16H-030/20 | G06T-007/00 | G06T-011/00 | G06F-009/455 | A61B-005/00 | G16H-010/60 | G16H-030/40 | G06F-009/451 | H04L-029/06","","","","","","4922017005346"
"US","US","P","B2","Implantable medical device with gyroscope","An implantable medical device (IMD) that includes a housing, a first electrode secured relative to the housing, a second electrode secured relative to the housing, and a gyroscope secured relative to the housing. The IMD may include circuitry in the housing in communication with the first electrode, the second electrode, and the gyroscope. The circuitry may be configured to determine and store a plurality of torsion data measurements, from which a representation of a twist profile may be determined.","1. A leadless cardiac pacemaker (LCP) configured to sense cardiac activity and to pace a patient'ss heart, the LCP comprising: a housing;an electrode secured relative to the housing and exposed to an environment outside of the housing;a gyroscope disposed within the housing;a controller disposed within the housing in communication with the electrode and the gyroscope, the controller configured to: pace the patient'ss heart using the electrode;determine a twisting profile of the patient'ss heart over one or more cardiac cycles based at least in part on data obtained from the gyroscope;determine a correlation between the determined twisting profile and one or more additional cardiac parameters, wherein the one or more additional cardiac parameters include one or more of a dP/dt, a heart sound, a blood flow, a respiration, a heart chamber volume, a pressure-volume (PV) loop, a volume-volume (VV) loop, a QRS signal width and a cardiac shorting;determine a status of the patient'ss heart based at least in part on the determined twisting profile and the determined correlation between the determined twisting profile and one or more of the additional cardiac parameters; andcommunicate the status of the patient'ss heart to an external device.","18","16/535890","2019-08-08","2020-0001092","2020-01-02","11305125","2022-04-19","CARDIAC PACEMAKERS, INC.","Bin Mi | Pramodsingh Hirasingh Thakur | Jeffrey E. Stahmann | Keith R. Maile | Qi An | Brendan Early Koop | Yinghong Yu | Viktoria A. Averina | Michael J. Kane | Krzysztof Z. Siejko","","","","A61N-0001/36578","A61N-0001/36578 | A61B-0005/00 | A61B-0005/02028 | A61B-0005/1121 | A61N-0001/3756 | G01C-0019/38 | G06F-0003/0346 | A61B-0005/0205 | A61B-0005/0215 | A61B-0005/283 | A61B-0005/366 | A61B-0005/4839 | A61B-2562/0219 | A61N-0001/37252","A61N-001/365","A61N-001/365 | G06F-003/0346 | G01C-019/38 | A61B-005/00 | A61B-005/02 | A61B-005/11 | A61N-001/375 | A61N-001/372 | A61B-005/0205 | A61B-005/0215 | A61B-005/283 | A61B-005/366","","","","","","4922016001489"
"US","US","P","B2","Touchscreen with biosensor","Particular embodiments described herein provide for an electronic device that can be configured to include a touchscreen that includes one or more biosensors. The touchscreen can include drive lines, sense lines, a photoconductive material, one or more biosensing areas, a touchscreen engine, and a biosensor engine. In a touchscreen mode, the touchscreen engine can be configured to couple with the sense lines and determine a place on the touchscreen where a user touched. In a biosensing mode, the biosensor engine can be configured to use the drive lines or sense lines and determine biometrics of the user.","1. An electronic device comprising: a touchscreen, wherein the touchscreen includes: drive lines;sense lines;one or more biosensing areas; anda photoconductive material in the one or more biosensing area, wherein the photoconductive material includes graphene and at least a portion of the drive lines and the sense lines are embedded in the photoconductive material;a touchscreen engine, wherein when a touchscreen mode is activated, the touchscreen engine is configured to couple with the sense lines and determine a place on the touchscreen where a user touched the touchscreen with a finger; anda biosensor engine, wherein when a biosensing mode is activated, the biosensor engine is configured to determine biometrics of the user when the user touches at least one of the one or more biosensing areas.","17","16/729347","2019-12-28","2020-0133434","2020-04-30","11307720","2022-04-19","INTEL CORPORATION","Sean Jude William Lawrence","201941039072","IN","2019-09-27","G06F-0003/0441","G06F-0003/0441 | A61B-0005/02427 | G06K-0009/00013 | G06K-2009/0006","G06F-003/044","G06F-003/044 | A61B-005/024 | G06K-009/00","","","","","","4922016004067"
"US","US","P","B2","Surgical device guidance and monitoring devices, systems, and methods","Provided herein are systems, devices, assemblies, and methods for localization of one or more tags in a patient.","1. A device comprising: a) an attachment component comprising at least one location emitter, wherein the attachment component is configured to be attached to a hand-held medical device with a device tip;b) a display component: i) attached to, or ii) integral with, the attachment component, wherein the display component comprises a display screen; andc) a processor configured to receive information from a plurality of witness stations and generate position data of a tag and the hand-held medical device,wherein said display screen is configured to display the position data, andwherein said tag comprises an antenna and is physically separate from, and not physically linked to, any of: said attachment component, said display component, and said hand-held medical device; andwherein the position data displayed by the display screen includes: i) a tag indicator which corresponds to the physical location of said tag, andii) at least one of the following additional indictors: A) a total distance indicator which indicates the distance of the device tip to the tag,B) a medical device indicator which corresponds to the location of the medical device with respect to the tag,C) a tag-tip vector indicator which provides a representation of the two-dimensional distance, and two-dimensional location, of the device tip to the tag; andD) a depth indicator which provides an indication of how high above, or below, the device tip is with respect to the tag.","20","16/199583","2018-11-26","2019-0090779","2019-03-28","11298044","2022-04-12","ELUCENT MEDICAL, INC.","Daniel W. van der Weide | Noah van der Weide | Eric N. Rudie | David Miel | Lee G. Wilke | Fred T. Lee, Jr.","","","","A61B-0005/062","A61B-0005/062 | A61B-0005/0068 | A61B-0005/06 | A61B-0005/064 | A61B-0005/066 | A61B-0005/743 | A61B-0034/20 | G06K-0007/10386 | A61B-0005/6886 | A61B-0006/4241 | A61B-0018/04 | A61B-0018/08 | A61B-0090/39 | A61B-2034/2051 | A61B-2090/061 | A61B-2505/05 | H01Q-0001/2208 | H01Q-0001/2225","A61B-005/06","A61B-005/06 | A61B-034/20 | A61B-005/00 | G06K-007/10 | H01Q-001/22 | A61B-090/00 | A61B-006/00 | A61B-018/08 | A61B-018/04","","","","","","4922015000997"
"US","US","P","B1","Machine learning analytics in real time for health services","Methods and systems detect a random state change in a subject in real time. Eye state changes may be identified in encephalogram brain signals, or honeybee dance patterns may be classified. Multivariate signals including state change information are received via a plurality of channels. The signals are sampled and may be filtered to remove DC components. Statistical characteristics of the signals are monitored. When the statistical characteristics exceed a threshold during a critical time interval, a potential change of state is detected. The critical time segment of the signals may be filtered to generate respective state change artifact signals. The state change artifact signals are decomposed by MEMD, and intrinsic mode functions are generated. Features are extracted from the intrinsic mode functions. These steps may be repeated while the extracted features are provided to a logistic regression classifier that is used to predict a state of the subject.","1. A method for detecting a random state change in a physical subject, the method comprising: receiving, using one or more electronic processors, a plurality of multivariate signals via a respective plurality of physical channels, wherein the plurality of multivariate signals comprise information about the state change in the physical subject, wherein the plurality of multivariate signals are sampled at a specified sampling rate;monitoring, using the one or more electronic processors, values of statistical characteristics of the plurality of multivariate signals;detecting, using the one or more electronic processors, a potential change of state in the physical subject when the monitored statistical characteristics values of at least one of the plurality of multivariate signals exceed a threshold;low-pass filtering, using the one or more electronic processors, a time interval of the plurality of multivariate signals that precedes the detected potential change of state in the physical subject to generate a respective plurality of state change artifact signals;simultaneously decomposing, using the one or more electronic processors, the plurality of state change artifact signals by multivariate empirical mode decomposition and generating a plurality of intrinsic mode functions;extracting, using the one or more electronic processors, features from one or more of the plurality of intrinsic mode functions;providing, using the one or more electronic processors, the extracted features to a logistic regression classifier;repeating the steps of monitoring, detecting, low-pass filtering, decomposing, extracting, and providing the extracted features to the logistic regression classifier; andpredicting, using the one or more electronic processors, a state of the physical subject using the logistic regression classifier.","20","16/161975","2018-10-16","","","11298071","2022-04-12","UNIVERSITY OF SOUTH FLORIDA","Abolfazl Saghafi | Chris P. Tsokos","","","","A61B-0005/4076","A61B-0005/4076 | A61B-0005/24 | A61B-0005/2415 | A61B-0005/316 | A61B-0005/725 | G06F-0017/18 | G06N-0007/00","A61B-005/00","A61B-005/00 | A61B-005/24 | A61B-005/372 | G06F-017/18 | G06N-007/00 | A61B-005/316","","","","","","4922015001024"
"US","US","P","B2","System and method for optimal sensor placement","A controller includes a memory that stores instructions and a processor that executes the instructions. The instructions cause the controller to execute a process that includes receiving sensor data from a first sensor and a second sensor. The sensor data includes a time-series observation representing a first activity and a second activity. The controller generates models for each activity involving progressions through states indicated by the sensor data from each sensor. The controller receives from each sensor additional sensor data including a time-series observation representing the first activity and the second activity. The controller determines likelihoods that the models generated a portion of the additional sensor data and calculates a pair-wise distance between each sensor-specific determined likelihood to obtain calculated distances. The calculated distances for each sensor are grouped and a relevance of each sensor to each activity is determined by executing a regression model using the grouped calculated distances.","1. A controller for determining an arrangement of sensors for monitoring physiology of a subject, comprising: a memory that stores instructions; anda processor that executes the instructions,wherein, when executed by the processor, the instructions cause the controller to execute a process comprising:receiving, from a first sensor of at least two sensors, a first sensor data comprising at least one time-series observation representing at least a first activity and a second activity;receiving, from a second sensor of the at least two sensors, a second sensor data comprising at least one time-series observation representing the first activity and the second activity;generating, by the processor, a first model for the first activity involving a first progression through a plurality of states indicated by at least a portion of the first sensor data;generating, by the processor, a second model for the second activity involving a second progression through a plurality of states indicated by at least a portion of the first sensor data;generating, by the processor, a third model for the first activity involving a third progression through a plurality of states indicated by at least a portion of the second sensor data;generating, by the processor, a fourth model for the second activity involving a fourth progression through a plurality of states indicated by at least a portion of the second sensor data;receiving, from the first sensor, a third sensor data comprising at least one time-series observation representing at least the first activity and the second activity;receiving, from the second sensor, a fourth sensor data comprising at least one time-series observation representing at least the first activity and the second activity;determining, using the processor, a likelihood that the first model generated at least a portion of the third sensor data, a likelihood that the second model generated at least a portion of the third sensor data, a likelihood that the third model generated at least a portion of the fourth sensor data, and a likelihood that the fourth model generated at least a portion of the fourth sensor data;calculating, using the processor, a pair-wise distance between each sensor-specific determined likelihood to obtain calculated distances;grouping, using the processor, the calculated distances for the likelihoods involving the first sensor, and grouping, using the processor, the calculated distances for the likelihoods involving the second sensor, to obtain grouped calculated distances; anddetermining, using the processor, a first relevance of the first sensor and a second relevance of the second sensor for capturing the first activity and the second activity by executing a regression model using the grouped calculated distances.","20","16/263636","2019-01-31","2019-0244126","2019-08-08","11301768","2022-04-12","KONINKLIJKE PHILIPS N.V.","Ali Akbar Ahmad Samadani","","","","G06N-0007/005","G06N-0007/005 | A61B-0005/0002 | A61B-0005/6887 | G06F-0017/18 | G16H-0040/63 | G16H-0040/67 | H04L-0067/12","G06N-007/00","G06N-007/00 | A61B-005/00 | G06F-017/18 | G16H-040/63 | G16H-040/67 | H04L-029/08 | H04L-067/12","","","","","","4922015004692"
"US","US","P","B2","Methods and systems for performing image analytics using graphical reporting associated with clinical images","Methods and systems for performing image analytics using graphical reporting associated with clinical images. One system includes at least one data source and a server. The server includes an electronic processor and an interface for communicating with the data source. The electronic processor is configured to receive training information from the at least one data source over the interface. The training information includes a plurality of images and graphical reporting associated with each of the plurality of images. Each graphical reporting includes a graphical marker designating a portion of one of the plurality of images and diagnostic information associated with the portion of the one of the plurality of images. The electronic processor is also configured to perform machine learning to develop a model using the training information. The model is used to automatically analyze an image.","1. A system for performing image analytics using graphical reporting associated with clinical images, the system comprising: at least one data source; anda server including an electronic processor and an interface for communicating with the data source the electronic processor configured to receive training information from the at least one data source over the interface, the training information including a plurality of images and graphical reporting associated with each of the plurality of images, each graphical reporting including a graphical marker designating a portion of one of the plurality of images and diagnostic information associated with the portion of the one of the plurality of images, andperform machine learning to develop a model using the training information by obtaining a clinical result associated with a first image included in the plurality of images,performing a comparison of the clinical result and the diagnostic information associated with the first image,assigning a weight to the diagnostic information based on the comparison, anddeveloping the model using the training information based on the weight, wherein the model is used to automatically analyze a new image.","3","15/179409","2016-06-10","2016-0364862","2016-12-15","11301991","2022-04-12","INTERNATIONAL BUSINESS MACHINES CORPORATION","Murray A. Reicher | Jon T. DeVries | Michael W. Ferro, Jr. | Marwan Sati","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0005/7267 | A61B-0010/02 | A61B-0034/10 | G06F-0003/0482 | G06F-0003/0488 | G06F-0040/169 | G06F-0040/205 | G06K-0009/627 | G06K-0009/6227 | G06K-0009/6232 | G06K-0009/6256 | G06K-0009/6262 | G06K-0009/6269 | G06N-0020/00 | G06T-0007/0014 | G06T-0011/008 | G06V-0030/194 | G16H-0010/20 | G16H-0015/00 | G16H-0020/10 | G16H-0030/20 | G16H-0050/20 | G16H-0050/50 | G16Z-0099/00 | A61B-0005/0077 | A61B-0005/015 | A61B-0005/055 | A61B-0005/318 | A61B-0005/4312 | A61B-0005/441 | A61B-2034/108 | A61B-2090/373 | A61B-2090/374 | A61B-2090/378 | A61B-2090/3762 | G06T-2200/24 | G06T-2207/10088 | G06T-2207/20081 | G06T-2207/30004 | G06T-2207/30008 | G06T-2207/30016 | G06T-2207/30052 | G06T-2207/30061 | G06T-2207/30068 | G06T-2207/30088 | G06V-2201/031","G06K-009/00","G06K-009/00 | G06T-007/00 | G16H-050/50 | G16H-015/00 | G16H-050/20 | G06N-020/00 | G06K-009/62 | G06F-040/169 | G06F-040/205 | G16Z-099/00 | G06V-030/194 | A61B-005/00 | G06F-003/0488 | A61B-034/10 | G06F-003/0482 | A61B-010/02 | G06T-011/00 | G16H-030/20 | G16H-010/20 | G16H-020/10 | A61B-005/01 | A61B-005/055 | A61B-005/318 | A61B-090/00","","","","","","4922015004912"
"US","US","P","B2","System and method for identifying features of a friction ridge signature based on information representing a topography of friction ridges","One or more features of a friction ridge signature of a subject may be identified based on information representing a three-dimensional topography of friction ridges of the subject. Information representing the three-dimensional topography of the friction ridges of the subject may be received. One or more level-three features of the friction ridge signature of the subject may be identified based on the information representing the three-dimensional topography of the friction ridges of the subject. The one or more level-three features may include one or more topographical ridge peaks, topographical ridge notches, topographical ridge passes, pores, and/or other information.","1. A system configured to identify features of a friction ridge signature of a subject based on information representing a three-dimensional topography of friction ridges of the subject, the system comprising: a processor configured to receive information representing the three-dimensional topography of the friction ridges of the subject, wherein the information representing the three-dimensional topography of the friction ridges of the subject includes a three-dimensional point cloud;wherein the processor is configured by machine-readable instructions to identify one or more level-three features of the friction ridge signature of the subject based on the information representing the three-dimensional topography of the friction ridges of the subject;wherein at least one of the level-three features is a three-dimensional structure comprising one or more topographical ridge passes; andwherein each of the one or more topographical ridge passes is a path across one of the friction ridges that includes one or more of: a region in the three-dimensional point cloud having an elevation level that is higher than valleys nearest the one of the friction ridges and lower than peaks nearest the one of the friction ridges or a region in the three-dimensional point cloud having a series of points that are higher than the valleys nearest the one of the friction ridges and lower than the peaks nearest the one of the friction ridges.","18","16/989394","2020-08-10","2020-0372656","2020-11-26","11302013","2022-04-12","IDENTIFICATION INTERNATIONAL, INC.","Richard K. Fenrich | Bryan D. Dickerson","","","","G06T-0007/13","G06T-0007/13 | A61B-0005/0077 | A61B-0005/1079 | A61B-0005/1172 | A61B-0005/7239 | B42D-0025/313 | G06F-0021/32 | G06T-0001/0007 | G06T-0007/0012 | G06T-0007/64 | G06V-0040/1347 | G06V-0040/1359 | A61B-0005/01 | A61B-2562/028 | A61B-2562/0247 | G06T-2200/04 | G06T-2207/30088","G06F-021/32","G06F-021/32 | A61B-005/107 | G06V-040/12 | G06T-007/00 | G06T-007/13 | A61B-005/1172 | A61B-005/00 | B42D-025/313 | G06T-007/64 | G06T-001/00 | A61B-005/01","","","","","","4922015004934"
"US","US","P","B2","Human-like emulation enterprise system and method","An enterprise system and method for maintaining and transitioning humans to a human-like self-reliant entity is presented. Said system including at least one a biological, biomechatronic, and mechatronic entity with a biological or artificial neural network to at least one transform or maintain. Embodiments are provided to assist in the transition of human between a biological state to a bio-mechatronic and mechatronic entity. Said entity's biological, biomechatronic, and mechatronic subsystems are configured to communicate and interact with one another in order for said enterprise system to manage, configure, maintain, and sustain said entity throughout the entity's life-cycle. Subsystem embodiments and components supported by the enterprise system are presented.","1. An enterprise method comprising: providing a business architecture for managing, transitioning, constructing, and maintaining, and sustaining a subscribing biological, biomechatronic, and mechatronic system by collecting data from a personal digital assistant comprising a computer with a cognitive memory system and computer subsystems operated in real time to dynamically correlate neural network activity of at least one biological neural network and artificial neural network data related to surrounding environment data akin to a human as part of the enterprise system to provide data to populate and enable construction of the a human-like self-reliant entity; said personal-digital assistant including:operating a 360 degree field-of-regard audio sensing, recording, processing, transmission, and amplifier subsystem; said audio subsystem including a three-dimensional audio sensing module with a plurality of small microphones facing outward from a housing that includes an acoustical direction module; said sensed audio signatures operable upon by a said acoustical direction module to detect the relative azimuth, range, and elevation, and predict the identity of entities in and nature of the surrounding environment and produce said detected data; said acoustical direction system communicating said data to host computer cognitive memory for retrieval and correlation processing using at least one artificial neural network; the audio subsystem operable to play audio files received from the audio subsystem and host computer to replicate typical audio files in at least one monaural, binaural, stereo, or three-dimensional sound effect format by processing the sound and amplification of the sound using at least one of stereo speakers, surround-sound speakers, speaker-arrays, or headphones; said personal assistant monitoring user audio and interactively providing audio to the user as the user moves about the environment surrounding the apparatus;operating a 360 degree field-of-view image sensing, recording, processing, transmission, and display subsystem; said image subsystem system including a panoramic camera system; said panoramic camera system providing at least one circular or, spherical field-of-view coverage about said apparatus; the image subsystem transmitting at least some portion of the panoramic image to the host computer or samples out from a region-of-interest sensing module that contains a database of predesignated patterns which the image sensor or image sensor identifies and samples out to send to the host computer; said ROI image from at least one the panoramic sensor or image and transmits the ROI image to the host computer for additional processing; said image subsystem communicating said panoramic and ROI imagery data to host computer cognitive memory for retrieval and correlation processing using at least one artificial neural network; the image subsystem operable to receive imagery captured by the imagery subsystem or host computer and operate on said imagery to drive at least one imagery in the form of graphics, text, or video content to produce at least one monoscopic, binocular, stereoscopic, three-dimensional, or holographic content for an associated display system; said personal assistant monitoring imagery of the user and interactively displaying imagery to the user as the user moves about the environment surrounding the apparatus;operating a host computer subsystem with a cognitive memory with an artificial neural network with backpropagation integrated into the housing of the apparatus; the artificial neural network with backpropagation operating in near-real time on audio and imagery data and information provided by the 360 degree panoramic audio and image subsystems to learn user or subscriber perceptions, relationships, preferences, physiological reactions, and nature of the user to the surrounding environment based on audio and visual information derived; said host computer including data derived from audio and image sensor information, and derived metadata of user perceptions, preferences, physiological reactions, relationships and nature into non-volatile memory; said audio visual subsystem acoustical direction system and identification system and panoramic imagery and ROI imagery communicating said data to said host computer'ss cognitive memory for correlation processing using at least a correlation engine with at least one comparator, transducer, translator, an artificial neural network, or a combination thereof; said host computer with at least one artificial neural network hardware, firmware or software operating on sensed data to at least one construct, update, and operate on an already constructed relational database in constructive memory derived from observation by the 360 degree audio and image subsystems observing the user in the surrounding environment; said relational database including data gathered from other various sources such as biometric sensor and the internet; then operating on said constructed relational database to assist the user in functioning in near-real time within the local surrounding environment or on a remote environment the user is interacting in conjunction with said host computer connected telecommunication system and network; the host computing subsystem including interactive virtual assistant functionality, natural-language user interface, and smart assistant display functionality for interactively providing panoramic sensing and feedback to at least one the user, the host computer, peripheral devices, or a remote user or agent and audio-visual presentation of local, live, stored, and remote content transmitted to and from a remote source on a telecommunications system and network in communicating relationship to the host computer system; said host computer having at least one the software, firmware, or hardware to present said the recorded or live 360 spherical field-of-view panoramic image on at least one flat, spherical, or circular display, conduct multi-point teleconferencing or videoconferencing, telepathy, or display graphic and textual data; said host computer system in a communicative relationship with an audio-visual system providing the content to the user based on rules established by the user or an administrator of the host computer; said host computer system with cognitive memory including internet functionality and compatibility; and said host computer including an electrical system to provide electricity to power electronic components of the computer and associated audio-visual subsystems;including a support housing with a mounting structure and sensor assembly to secure the apparatus on at least one the body of a user, eyeglasses, clothing, prosthetic device, headgear, head mounted display and as a dismounted apparatus; said support apparatus optionally designed in at least one single housing or in modular separate housings; singularly housed support apparatus components communicatively connected by the circuitry and separately housed support apparatus components communicatively connected by wireless transceivers or a wireless network; said combined and separated embodiments of the apparatus including an electrical power source; said housing including the personal panoramic audio-visual assistant with artificial intelligence; said housing comprising at least one tubular shape, spherical shape, or combination thereof with curved or flattened surfaces; said housing including a base structure situated on or attached to an object in the surrounding environment; said housing being positional with said 360 degree panoramic camera, display, and audio system not having moving parts; said audio microphones and audio amplifiers, camera objective lenses and display surface facing outward the periphery situated for interaction with the user; said apparatus including an on/off button on the exterior of the apparatus; and ports to include headphone, an electrical recharging, and other input and output and access ports located on the periphery of the housing and accessible to the user; said housing including at least one plug-in electrical power or battery power supply; said electrical power connected to the electronic components to drive the apparatus'ss display, camera, and host computer system; said housing including an internal space within the apparatus to house the host computer; said host computer having at least one the functionality of and operating as a personal electronic device, such as a smartphone, or a port for plugging the personal electronic device so that the apparatus comprising said personal electronic device'ss functionality; at least one the host computer, personal electronic device, or a combination thereof providing command and control of the apparatus; said display and objective lenses of the camera being secured by fastening or adhesive to the periphery of the housing; display composition being of at least one electroluminescent display type and that may be a rotating or stationary display system, or a holographic projection system; said display optionally including touchscreen functionality; said display having at least one continuous display or a plurality of displays viewable from all directions about the apparatus situated on or fastened to an object in the surround environment in which the apparatus is situated; said camera and display for optimal usage held by the support armature and situated on the support armature away from the object it is situated upon so that the viewable surface of the display and camera objective lenses are located in a non-interference field-of-view position on the periphery of the housing and for optimal usage said audio microphones and audio amplifiers are located in a non-interference field-of-regard location to facilitate user or onlooker interaction with said interactive apparatus in an optimal manner; andoperating said 360 degree field-of-regard audio sensing, recording, processing, tra... ","20","16/601010","2019-10-14","2020-0218767","2020-07-09","11287847","2022-03-29","VIRTUAL VIDEO BY RITCHEY, LLC | VIRTUAL VIDEO REALITY BY RITCHEY, LIMITED LIABILITY CORPORATION","Kurtis John Ritchey | Kenneth Ira Ritchey","","","","G06F-0001/163","G06F-0001/163 | A61B-0005/0022 | A61B-0005/0059 | A61B-0005/055 | A61B-0005/1114 | A61B-0005/245 | A61B-0005/369 | A61B-0005/4064 | A61B-0005/6803 | A61B-0005/686 | A61B-0005/7246 | A61B-0005/7264 | A61B-0006/501 | G02B-0027/017 | G03B-0037/00 | G03H-0001/2294 | G05D-0001/0016 | G05D-0001/0038 | G06F-0001/1626 | G06F-0001/1686 | G06F-0003/013 | G06F-0003/015 | G06F-0003/017 | G06F-0003/038 | G06F-0003/147 | G06F-0016/90 | G06F-0016/90335 | G06N-0020/00 | G06T-0011/60 | G09G-0003/003 | G09G-0005/026 | G09G-0005/14 | G10L-0015/22 | G16H-0040/63 | H04N-0005/2259 | H04N-0005/23203 | H04N-0005/247 | H04N-0005/335 | H04N-0007/18 | H04N-0007/185 | A61B-0005/0024 | A61B-0005/745 | A61B-2560/0242 | G01R-0033/4806 | G02B-0013/06 | G02B-0015/10 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0187 | G06F-2203/0381 | G09G-2370/16 | G09G-2380/08 | H04N-0005/23238","G06F-001/16","G06F-001/16 | G06F-016/903 | H04N-007/18 | G06F-003/038 | H04N-005/335 | G06N-020/00 | G03B-037/00 | H04N-005/225 | A61B-005/00 | A61B-005/055 | A61B-005/11 | G06F-003/01 | G16H-040/63 | G03H-001/22 | G09G-005/02 | G09G-005/14 | G06F-003/147 | G09G-003/00 | G06F-016/90 | A61B-006/00 | G02B-027/01 | G05D-001/00 | G06T-011/60 | G10L-015/22 | H04N-005/232 | H04N-005/247 | A61B-005/245 | A61B-005/369 | G02B-013/06 | G02B-015/10 | G01R-033/48","","","","","","4922013003924"
"US","US","P","B2","Systems and methods for providing media content recommendations","Systems and associated methods are described for providing content recommendations. The system accesses a plurality of recommendation algorithms and assigns a plurality of weight values to each prediction algorithm. Then, the system generates a set of candidate weight combinations, such that each candidate combination includes a weight value assigned to each prediction algorithm. Then requests for content items are received over a predetermined period of time. For each combination, the system generates a set of recommended content items and an evaluation metric that is based on matches with requests. Afterwards, the system replaces a candidate combination that resulted in a generation of a lowest evaluation metric. The aforementioned steps are repeated until the evaluation metrics stop improving. Then display identifiers are displayed for a set of recommended content items generated for a candidate combination with the highest evaluation metric.","1. A method for providing content recommendations, the method comprising: (a) accessing a plurality of prediction algorithms;(b) assigning a plurality of weight values to each prediction algorithm;(c) generating a set of candidate weight combinations, wherein each candidate combination comprises a weight value assigned to each prediction algorithm;(d) receiving requests for content items over a predetermined period of time;(e) for each particular respective candidate combination: generating a set of recommended content items based on the plurality of the prediction algorithms and the weight values of the particular candidate combination;generating evaluation metrics based on a match between the requests for content items and the set of recommended content items generated for the particular candidate combination;(f) replacing a candidate combination that resulted in a generation of a lowest evaluation metric;(g) repeating steps (d)-(f) until the evaluation metrics stop improving, wherein determining that the evaluation metrics stopped improving comprises: maintaining a historical high evaluation metric;whenever a new evaluation metric is generated, comparing the new evaluation metric to the historical high evaluation metric; anddetermining that the evaluation metrics stopped improving when new evaluation metrics fail to exceed the historical high evaluation metric during a predetermined number of repetitions of the steps (d)-(f); and(h) generating for display identifiers for a set of recommended content items generated for a candidate combination with the highest evaluation metric.","18","16/370101","2019-03-29","2020-0311562","2020-10-01","11288582","2022-03-29","ROVI GUIDES, INC.","Kyle Miller | Bryan S. Scappini | James W. Lent","","","","G06N-0005/02","G06N-0005/02 | G06N-0003/12 | G06Q-0030/0202 | H04N-0021/251","G06F-030/23","G06F-030/23 | G06Q-020/40 | G06Q-050/18 | A61F-002/28 | G06N-005/02 | G06Q-030/02 | G06N-003/12 | H04N-021/25","","","","","","4922013004658"
"US","US","P","B2","Method and apparatus to account for transponder tagged objects used during clinical procedures employing a shielded receptacle with antenna","Medical procedure related objects (e.g., instruments, supplies) tagged with transponders (e.g., RFID transponders, dumb transponders) are accounted for in a medical or clinical environment via an accounting system using a number of antennas and interrogators/readers. A first set of antennas and RFID interrogator(s) interrogate portions of the environment for RFID tagged objects, for example proximate a start and an end of a procedure. Shielded packaging and/or shielded receptacles shield tagged objects, preventing interrogation except for those objects in unshielded portions of the environment. A shielded receptacle may include an antenna to interrogate the contents thereof in a relatively noise-free environment. A data store may maintain information including a current status or count of each instrument or supply, for instance as checked in or checked out. A handheld antenna and/or second set of antennas interrogates a body of a patient for retained instruments or supplies tagged with dumb transponders.","1. A method of operating a system to track items in a clinical environment, the method comprising: during a first period, causing at least one room antenna to emit at least one interrogation signal, the at least one antenna positioned and oriented to provide coverage of any unshielded portions of the clinical environment;during the first period, detecting any response signals to the at least one interrogation signal, the response signals returned from any wireless transponders in the unshielded portions of the clinical environment;identifying, by at least one processor, each of a number of wireless transponders in the unshielded portions of the clinical environment based on the response signals detected during the first period;adding a number of item entries to an inventory stored to at least one non transitory processor-readable medium based at least in part on the response signals detected during the first period, each of the item entries in the inventory representative of a respective item prepared for use during a clinical procedure;during a second period, causing the at least one room antenna to emit at least one interrogation signal;during the second period, detecting any response signals to the at least one interrogation signal, the response signals including at least one response signal returned from at least one wireless transponder that was removed from a shielded package between the first and the second periods;identifying, by the at least one processor, each of a number of wireless transponders in the unshielded portions of the clinical environment based on the response signals detected during the second period;updating the inventory based on the response signals detected during the second period, including adding at least one item entry to the inventory that corresponds to the at least one wireless transponder that was removed from a shielded package between the first and the second periods;during a third period proximate an end of a clinical procedure, causing at least one receptacle antenna to emit at least one interrogation signal within an interior of a shielded receptacle;during the third period, detecting any response signals to the at least one interrogation signal in the interior of the shielded receptacle, where the interior of the shielded receptacle is completely shielded from at least one of radio or microwave frequency energy emitted externally from the at least one shielded receptacle during the third period;identifying, by the at least one processor, each of a number of wireless transponders in the shielded receptacle based on the response signals to the at least one interrogation signal in the interior of the shielded receptacle detected during the third period; andupdating the inventory based on the response signals detected during the third period.","7","17/013141","2020-09-04","2021-0057086","2021-02-25","11289190","2022-03-29","COVIDIEN LP","Bryan Hansen | Jeffrey L. Jensen","","","","G16H-0040/20","G16H-0040/20 | A61B-0042/10 | A61B-0046/00 | A61B-0050/13 | A61B-0050/33 | A61B-0090/98 | A61F-0015/00 | G06K-0007/10356 | G06K-0007/10396 | G06K-0019/0723 | A61B-0050/30 | A61B-2017/00442 | A61B-2050/005 | A61B-2050/0056 | G08B-0021/0275 | G08B-0021/24 | H01Q-0001/2216","G06F-017/00","G06F-017/00 | G06Q-030/00 | G06Q-090/00 | G16H-040/20 | A61B-090/98 | A61F-015/00 | G06K-019/07 | A61B-046/00 | G06K-007/10 | A61B-050/33 | A61B-042/10 | A61B-050/13 | G08B-021/02 | G08B-021/24 | H01Q-001/22 | A61B-017/00 | A61B-050/30 | A61B-050/00","","","","","","4922013005262"
"US","US","P","B2","User-stress based notification system","A system for filtering device information to be provided to a user by a digital device or system according to physiological information collected from the user. The physiological information may be used to determine the user's present cognitive stress, wherein the device information may be prioritized, withheld, delayed, or deleted if the present cognitive stress exceeds a predetermined threshold. The device information may be further evaluated with contextual information such as different aspects of the device information or non-physiological user information (e.g. location, time of the day). The system may manage device information with minimal or less user interaction than user defined rule systems.","1. A system for managing information received for presentation to a user, the system comprising: a processor; anda non-transitory machine-readable medium including instructions for controlling operation of the system, which when executed by the processor, cause the processor to perform operations comprising:collecting, from a sensor, physiological information from the user corresponding to a present state of the user;determining a present stress level of the user from the collected physiological information corresponding to the present state of the user;determining a priority of the information received for presentation to the user, wherein the priority is based on a rarity factor indicating that the information is rarely provided to the system;determining which stress threshold of a plurality of stress thresholds is exceeded by the present stress level;adjusting the plurality of stress thresholds based on past occurrences of whether the user keeps or discards information presented to the user at a particular stress level;applying a filter to the information according to the present stress level of the user and the priority to obtain filtered information; andpresenting the filtered information to the user according to which stress threshold of the plurality of stress thresholds is exceeded by the present stress level, wherein filtering the information comprises at least one of forwarding, prioritizing, withholding, delaying, or skipping presentation of the information to the user.","14","15/868752","2018-01-11","2019-0045020","2019-02-07","11290553","2022-03-29","INTEL CORPORATION","Boaz Ein-Gil | Omri Mendels | Alex Rapoport","","","","H04L-0067/26","H04L-0067/26 | A61B-0005/165 | G06F-0016/9535 | G06Q-0010/10 | G06Q-0010/109 | H04L-0067/306 | A61B-0003/112 | A61B-0005/021 | A61B-0005/0816","H04L-029/08","H04L-029/08 | H04L-067/55 | A61B-005/16 | G06F-016/9535 | G06Q-010/10 | H04L-067/306 | A61B-005/08 | A61B-003/11 | A61B-005/021","","","","","","4922013006614"
"US","US","P","B2","Methods and apparatus to adjust content presented to an individual","Methods, apparatus, systems and articles of manufacture to adjust content presented to an individual are disclosed. An example system includes a first modality sensor to measure a first response of an individual to first content during a first time frame and a second modality sensor to measure a second response of the individual to the first content during the first time frame. The first modality sensor is to measure a third response of the individual to first content during a second time frame, and the second modality sensor is to measure a fourth response of the individual to the first content during the second time frame. The example system also includes a mental classifier executing instructions to determine a first mental classification of the individual based on a first comparison of the first response to a first threshold and a second comparison of the second response to a second threshold. The mental classifier also is to determine a second mental classification of the individual based on a third comparison of the third response to a third threshold and a fourth comparison of the fourth response to a fourth threshold. In addition, the mental classified is to determine a mental state of the individual based on a degree of similarity between the first mental classification and the second mental classification. The example system also includes a content modifier to at least one of modify the first content to include second content or replace the first content with second content based on the mental state.","1. A system comprising: a sensor to measure biometric responses of an individual to media content; anda processor to: determine a first baseline associated with physiology of the individual;determine a first threshold based on the first baseline;perform a first comparison of first biometric responses from the individual measured by the sensor during a first time frame of the media content with the first threshold;when the first biometric responses do not satisfy the first threshold, adjust the first baseline to a second baseline;determine a second threshold based on the second baseline;perform a second comparison of second biometric responses from the individual measured by the sensor during a second time frame of the media content with the second threshold;determine a mental classification of the individual based on the second comparison; andat least one of modify or replace the media content during the second time frame based on the mental classification.","20","17/013099","2020-09-04","2020-0404370","2020-12-24","11290779","2022-03-29","NIELSEN CONSUMER LLC","Carl D. Marci | Brian Levine | Brendan Murray","","","","H04N-0021/44218","H04N-0021/44218 | A61B-0003/112 | A61B-0003/113 | A61B-0005/0064 | A61B-0005/0533 | A61B-0005/165 | A61B-0005/7282 | G06F-0003/01 | G06K-0009/00221 | G06K-0009/00362 | G06Q-0030/0201 | H04N-0021/4223 | H04N-0021/42201 | H04N-0021/4415 | H04N-0021/44204 | H04N-0021/44213 | H04N-0021/458 | H04N-0021/812 | A61B-0005/7278 | A61B-2503/12","H04N-021/442","H04N-021/442 | A61B-005/0533 | A61B-003/11 | A61B-003/113 | A61B-005/16 | H04N-021/81 | G06Q-030/02 | H04N-021/4415 | G06K-009/00 | H04N-021/458 | G06F-003/01 | H04N-021/422 | H04N-021/4223 | A61B-005/00","","","","","","4922013006835"
"US","US","P","B2","Methods and systems for using sound data to analyze health condition and welfare states in collections of farm animals","Systems and methods are described for selecting a sound type of interest from a first (e.g., master/global) machine learning library comprising information derived from reference audio stream data acquired from a plurality of farm animal operation reference sound monitoring events, including from a first farm animal operation monitoring event of a first farm animal operation, wherein the sound type of interest is associated with a condition state of interest of a first collection of farm animals. Further, information associated with the selected sound type of interest can be included a second machine learning library, wherein the second machine learning library is operational on an edge computing device located in proximity to a second farm animal operation. Audio stream data can be acquired from the second farm animal operation in a second farm animal operation monitoring event, and processed using the second machine learning library information to determine whether the sound type of interest is present in the acquired audio stream data, thereby generating information associated with the presence or absence of the condition during the second farm animal operation monitoring event.","1. A method to monitor sounds from a poultry animal operation, comprising: selecting at least one sound type of interest from a first machine learning library comprising information derived from reference audio stream data acquired from a plurality of poultry animal operation sound monitoring events for poultry animal operations comprising collections of either chickens or turkeys, including from a first poultry animal operation monitoring event for a first poultry animal operation, wherein the selected at least one sound type of interest is associated with at least one poultry animal condition state of interest derived from one or more reference audio streams including the first collection of poultry animals;including information associated with the selected at least one sound type of interest in a second machine learning library, the information comprising at least one condition state filter associated with the selected at least one sound type of interest, the at least one condition state filter deployed with a confidence or accuracy of at least 80%, wherein the second machine learning library is operational on an edge computing device located in or proximate to a second poultry animal operation comprising a second collection of poultry animals, wherein the second collection of poultry animals comprises either chickens or turkeys;acquiring audio stream data from the second poultry animal operation in a second poultry animal operation monitoring event; andprocessing the acquired audio stream data from the second poultry animal operation monitoring event with information in the second machine learning library to determine whether the selected at least one sound type of interest is present in the acquired audio stream data from the second poultry animal operation monitoring event, the processing identifying an origin of an audio stream feature of the acquired audio stream data based upon the at least one condition state filter, thereby generating information associated with a presence or absence during the second poultry animal operation monitoring event of the at least one poultry animal condition state of interest in poultry animals in the second collection of poultry animals.","18","17/337750","2021-06-03","2021-0289755","2021-09-23","11278007","2022-03-22","AGLOGICA HOLDING, INC.","Marcel Joseph Sarzen | Joseph Marcel Sarzen | Christopher G. Rosati","","","","A01K-0029/005","A01K-0029/005 | A61B-0005/48 | A61B-0005/742 | G05B-0019/406 | G06N-0005/04 | G06N-0020/00 | G10L-0025/66 | A61B-2503/40 | G05B-2219/45113 | H04L-0067/10 | H04L-0067/12 | H04R-0001/406 | H04R-0003/005","A01K-029/00","A01K-029/00 | G10L-025/66 | G06N-020/00 | G06N-005/04 | A61B-005/00 | G05B-019/406 | H04L-029/08 | H04R-001/40 | H04R-003/00 | H04L-067/12 | H04L-067/10","","","","","","4922012000715"
"US","US","P","B2","Implantable electrocorticogram brain-computer interface system for restoring extremity movement","A fully-implantable brain-computer interface cyber-physical system capable of acquiring and analyzing electrocorticogram (""ECoG"") signals, recorded directly from the subdural space of the brain, to enable direct brain control of a prosthetic (e.g., a robotic gait exoskeleton or a functional electrical stimulation (""FES"") system) is disclosed. The present system comprises a plurality of electrodes, for acquiring the ECoG signals, and a digital signal processor (""DSP"") for deriving a plurality of real-time commands from the ECoG signals. These real-time commands may then be wirelessly transmitted to the prosthetic for execution. Further, to avoid wireless data transmission of the ECoG signals from the plurality of electrodes to the DSP, which would expose the brain, skull, and scalp tissue to potentially harmful radio frequencies, a subcutaneous tunneling cable operatively couples the plurality of electrodes and the DSP.","1. A system for direct brain control of a prosthetic, the system comprising: a. a neural signal acquisition unit (102), capable of being implanted in the skull of a patient, capable of acquiring a plurality of brain signals and processing them into a data stream, wherein the power of the neural signal acquisition unit (102) is sufficiently low to prevent heating of local tissues, wherein the neural signal acquisition unit (102) comprises an ultra-low power amplifier array and serializer integrated circuit (""IC"") (108) capable of low-noise amplification and serialization of the data stream;b. a subcutaneous tunneling cable (110), connecting the neural signal acquisition unit (102) and a chest wall unit (101), wherein the subcutaneous tunneling cable (110) comprises one wire for delivering the data stream from the neural signal acquisition unit (102) to the chest wall unit (101);c. the chest wall unit (101) incorporating low-power analog-to-digital converter (ADC), digital signal processor (1)SP, low-power wireless transceiver, wireless charging unit, and memory, embedded on a centimeter-scale system and implantable in the body of a patent, capable of multitude of operations including digitization and decoding of serialized neural signals coming from the neural signal acquisition unit (102), receiving the data stream from the signal acquisition unit, processing the data stream into a set of real time commands and transmitting the command to a remote prosthetic; andd. a battery (11.1), integrated into the chest wall unit (101), capable of providing power to the chest wall unit (101) and the neural signal acquisition unit (102), wherein the battery (111) is wirelessly rechargeable;wherein a total power consumption of the system is less than 30 mW.","17","16/376831","2019-04-05","2019-0231204","2019-08-01","11278226","2022-03-22","THE REGENTS OF THE UNIVERSITY OF CALIFORNIA | UNIVERSITY OF SOUTHERN CALIFORNIA","Payam Heydari | Zoran Nenadic | An Do | Charles Liu","","","","A61B-0005/24","A61B-0005/24 | A61B-0005/30 | A61B-0005/6868 | G06F-0003/00 | G06F-0003/01 | G06F-0003/015 | A61B-0005/369 | A61B-0005/4851 | A61B-0005/7225 | A61F-0002/72 | A61H-2201/165 | A61N-0001/0534","A61F-002/72","A61F-002/72 | A61B-005/24 | G06F-003/00 | G06F-003/01 | A61B-005/30 | A61B-005/00 | A61B-005/369 | A61N-001/05","","","","","","4922012000932"
"US","US","P","B2","Systems and methods for automatic treatment planning and optimization","Systems and methods for the automatic generation and optimization of radiation therapy treatment plans, and systems and methods for the automatic generation and optimization of an adapted plan in an adaptive radiation therapy workflow.","1. An automatic treatment planning system, comprising: a user interface; anda processing device configured to: receive, via the user interface, clinical data of a first data source and clinical data of a second data source, the second data source being of a different type than the first data source; andexecute a sequence of programmed instructions embodied on a computer-readable storage medium to generate one or more treatment plan candidates based on the received clinical data;the processing device comprising a treatment planning module configured to: derive a first set of objectives for the clinical data of the first data source;derive a second set of objectives for the clinical data of the second data source;combine the first set of objectives and the second set of objectives using a weighting factor; andautomatically generate one or more treatment plan candidates based on the weighted combination of the first set of objectives and the second set of objectives;the processing device being further configured to send the treatment plan candidates to a controller of a treatment device to operate the treatment device according to a selected treatment plan candidate.","13","16/564924","2019-09-09","2021-0069527","2021-03-11","11278737","2022-03-22","VARIAN MEDICAL SYSTEMS INTERNATIONAL AG","Jarkko Peltola | Pauli Suhonen | Christopher Boylan | Stephen Thompson | Esa Kuusela","","","","A61N-0005/1031","A61N-0005/1031 | A61N-0005/1075 | G06F-0017/10 | A61N-2005/1074","A61N-005/10","A61N-005/10 | G06N-007/00 | G06F-017/10","","","","","","4922012001440"
"US","US","P","B2","Driver monitoring and response system","An evaluation engine has two or more modules to assist a driver of a vehicle. A driver drowsiness module analyzes monitored features of the driver to recognize two or more levels of drowsiness of the driver of the vehicle. The driver drowsiness module evaluates drowsiness of the driver based on observed body language and facial analysis of the driver. The driver drowsiness module is configured to analyze live multi-modal sensor inputs from sensors against at least one of i) a trained artificial intelligence model and ii) a rules based model while the driver is driving the vehicle to produce an output comprising a driver drowsiness-level estimation. A driver assistance module provides one or more positive assistance mechanisms to the driver to return the driver to be at or above the designated level of drowsiness.","1. An evaluation engine having two or more modules to monitor a driver of a vehicle, comprising: a driver drowsiness module that is configured to analyze monitored features of the driver to be capable to recognize two or more levels of drowsiness of the driver of the vehicle;a facial analysis module that is configured to be capable to perform at least two of i) face tracking, ii) eye movement and iii) eye blink tracking on the driver of the vehicle to assist in detecting the levels of drowsiness of the driver of the vehicle, where an output analysis of the facial analysis module is supplied to the driver drowsiness module;a sensor interface that is located among the two or more modules, including the facial analysis module and the driver drowsiness module, and one or more sensors, where the sensor interface is configured to receive input from the one or more sensors located in the vehicle including i) one or more cameras, and ii) a motion sensing device coupled with a speech user interface, to monitor the driver of the vehicle;where the driver drowsiness module is configured to utilize the output of the facial analysis module to evaluate drowsiness of the driver based on at least one of observed body language and facial analysis of the driver, to detect and classify one or more levels of drowsiness of the driver of the vehicle when those states occur for the driver;a driver assistance module that is configured to attempt to maintain the driver in a level selected from a group consisting of i) in a non-drowsiness level, ii) at or below a first level of drowsiness of the driver, and iii) any combination of both, based on an output from the driver drowsiness module; and, when the driver is not at least at or below the first level of drowsiness of the driver, then the driver assistance module is configured to provide one or more positive assistance mechanisms back to the driver to attempt to change the driver'ss level to the level of i) where the driver is the non-drowsiness level, ii) where the driver'ss level of drowsiness is lowered to a lower level of drowsiness, and iii) any combination of both, where the evaluation engine with its two or more modules is adaptive on providing a level of assistance that the driver needs in order to get back to any of i) the non-drowsiness level, and/or ii) one of the levels of drowsiness of the driver, based on feedback collected from the driver from the sensor interface, and an assessment by the driver assistance module of what effect a first level of assistance provided to the driver had on a current level of drowsiness of the driver.","20","16/472760","2017-12-19","2021-0129748","2021-05-06","11279279","2022-03-22","SRI INTERNATIONAL","Amir Tamrakar | Girish Acharya | Makoto Okabe | John James Byrnes","","","","B60Q-0009/00","B60Q-0009/00 | A61B-0005/0077 | A61B-0005/1103 | A61B-0005/1114 | A61B-0005/1116 | A61B-0005/1128 | A61B-0005/163 | A61B-0005/18 | A61B-0005/7267 | A61B-0005/741 | A61B-0005/749 | G06F-0003/167 | G06K-0009/00845 | A61B-2560/0443","B60Q-001/00","B60Q-001/00 | B60Q-009/00 | A61B-005/16 | A61B-005/00 | A61B-005/11 | A61B-005/18 | G06F-003/16 | G06K-009/00","","","","","","4922012001979"
"US","US","P","B2","Timeclock control system and method","A timeclock control system includes a client electronic device configured to administer an alertness test to a user. A timeclock controller is coupled to and configured to be actuated by the client electronic device.","1. A timeclock control system comprising: a client electronic device configured to administer an alertness test to a user on a graphical user interface of the client electronic device to determine an alertness test result for the user, wherein the client electronic device is configured to: render a plurality of objects and a disrupter configured to distract the user on the graphical user interface for use within the alertness test being administered to the user and solicit a response from the user concerning whether at least a pair of the plurality of objects are identical to each other, wherein the disrupter is configured to rotate over the plurality of objects and, while rotating, temporarily obscure at least a portion of one or more objects of the plurality of objects rendered on the graphical user interface during the alertness test; anda timeclock controller, coupled to and configured to be actuated by the client electronic device to allow the user to log into work based upon, at least in part, the alertness test result for the user.","12","15/625853","2017-06-16","2017-0360352","2017-12-21","11282024","2022-03-22","PREDICTIVE SAFETY SRP, INC.","Henry M. Bowles | Marcus T. Wichmann | Darren B. Chamberlin","","","","G06Q-0010/06398","G06Q-0010/06398 | A61B-0005/0002 | A61B-0005/0022 | A61B-0005/0077 | A61B-0005/0205 | A61B-0005/1112 | A61B-0005/14542 | A61B-0005/16 | A61B-0005/162 | A61B-0005/165 | A61B-0005/18 | A61B-0005/4064 | A61B-0005/4088 | A61B-0005/6898 | G06F-0003/0482 | G06F-0003/0484 | G06F-0003/04812 | G06F-0021/36 | G06Q-0010/1091 | G07C-0009/20 | G07C-0009/28 | G07C-0009/30 | G08B-0021/02 | G08B-0021/06 | G09B-0005/065 | G09B-0007/00 | G09B-0007/10 | G16H-0010/20 | G16H-0015/00 | H04L-0063/10 | H04W-0012/08 | A61B-0005/024 | A61B-0005/14532 | A61B-0005/4857 | A61B-2503/20 | A61B-2560/0247 | A61B-2562/0204 | H04W-0012/63 | H04W-0080/12 | H04W-0084/042","G06Q-010/10","G06Q-010/10 | G06Q-010/06 | G06F-003/0481 | G06F-003/0484 | G08B-021/06 | H04W-012/08 | G07C-009/20 | G07C-009/28 | G07C-009/30 | G06F-021/36 | H04L-029/06 | A61B-005/18 | G06F-003/0482 | G09B-005/06 | G09B-007/10 | A61B-005/00 | A61B-005/11 | A61B-005/16 | G09B-007/00 | G08B-021/02 | G06F-003/04812 | G16H-015/00 | A61B-005/0205 | A61B-005/145 | G16H-010/20 | H04W-012/63 | H04W-080/12 | H04W-084/04 | A61B-005/024","","","","","","4922012004703"
"US","US","P","B2","Information processing device, information processing method, and fluorescence image capturing system","[Problem] To predict a remaining time during which a fluorescence image is observable. [Solution] An information processing device according to the present disclosure includes a remaining time estimation unit that estimates, on the basis of a luminance limit value for observation of a fluorescence image and a change in luminance of a fluorescence image, a remaining time until the luminance of the fluorescence image reaches the luminance limit value. This configuration enables the time at which the luminance of a fluorescence image reaches the luminance limit value to be estimated according to a change in luminance of the fluorescence image, and it is possible to predict a remaining time during which a fluorescence image is observable.","1. An information processing device comprising circuitry configured to: estimate, on a basis of a current luminance level of a fluorescence image, a luminance limit value for observation of the fluorescence image, and a rate of decay in luminance of the fluorescence image, a remaining time until the luminance of the fluorescence image reaches the luminance limit value;superimpose the fluorescence image and a visible light image on each other; andreduce a superimposition rate of the visible light image to the fluorescence image when the remaining time is smaller than a predetermined value,wherein the luminance limit value corresponds to a minimum level at which the fluorescence image remains observable over noise in the fluorescence image.","18","16/761800","2018-10-18","2021-0177263","2021-06-17","11272844","2022-03-15","SONY CORPORATION","Takami Mizukura | Minori Takahashi | Kenji Takahashi | Kentaro Fukazawa","2017-218188","JP","2017-11-13","A61B-0005/0071","A61B-0005/0071 | G06T-0007/0012 | H04N-0005/2256 | A61B-0005/742 | A61B-2562/0242 | G06F-0003/14 | G06T-2207/10024 | G06T-2207/10064 | G06T-2207/20104 | G06T-2207/20212","A61B-005/00","A61B-005/00 | G06F-003/14 | H04N-005/225 | G06T-007/00","","","","","","4922011000736"
"US","US","P","B2","System and method of utilizing three-dimensional overlays with medical procedures","The disclosure provides a system that may: render, based at least on first positions of locations of iris structures of an eye, a first two-dimensional overlay image associated with a three-dimensional image overlay image; display, via a first display, the first two-dimensional overlay image; render, based at least on the first positions and at least on a horizontal offset, a second two-dimensional overlay image associated with the three-dimensional overlay image; display, via a second display, the second two-dimensional overlay image; render, based at least on second positions, a third two-dimensional overlay image associated with the three-dimensional overlay image; display, via the first display, the third two-dimensional overlay image; render, based at least on second positions of locations of the iris structures and at least on the horizontal offset, a fourth two-dimensional overlay image associated with the three-dimensional overlay image; and display, via the second display, the fourth two-dimensional overlay image.","1. A medical system, comprising: at least one processor;a binocular digital microscope having a first ocular having a first display and a second ocular having a second display, wherein the binocular digital microscope is communicatively coupled to the processor; anda memory medium that is coupled to the at least one processor and that includes instructions, when executed by the at least one processor, cause the system to: receive a first image of an eye of a patient;determine locations of multiple iris structures of the eye of the patient from the first image of the eye of the patient;determine first positions of the locations of the multiple iris structures;render, based at least on the first positions of the locations of the multiple iris structures, a first two-dimensional overlay image associated with a three-dimensional overlay image;display, via the first display of the plurality of displays, the first two-dimensional overlay image associated with the three-dimensional overlay image;render, based at least on the first positions of the locations of the multiple iris structures and based at least on a horizontal offset, a second two-dimensional overlay image associated with the three-dimensional overlay image, wherein the horizontal offset is an interocular distance between the first ocular and the second ocular;display, via the second display of the plurality of displays, the second two-dimensional overlay image associated with the three-dimensional overlay image;receive a second image of the eye of the patient;determine second positions of the locations of the multiple iris structures from the second image of the eye of the patient;render, based at least on the second positions of the locations of the multiple iris structures, a third two-dimensional overlay image associated with the three-dimensional overlay image;display, via the first display, the third two-dimensional overlay image associated with the three-dimensional overlay image;render, based at least on the second positions of the locations of the multiple iris structures and based at least on the horizontal offset, a fourth two-dimensional overlay image associated with the three-dimensional overlay image; anddisplay, via the second display, the fourth two-dimensional overlay image associated with the three-dimensional overlay image, wherein the display of the third two-dimensional image and the display of the fourth two-dimensional image separated by the horizontal offset create a binocular disparity and a depth perception when viewed through the first ocular and the second ocular.","14","17/078026","2020-10-22","2021-0121246","2021-04-29","11272990","2022-03-15","ALCON INC.","Aleksander Gudalo","","","","A61B-0034/25","A61B-0034/25 | A61B-0005/7445 | A61B-0090/20 | G02B-0027/0101 | G06F-0003/1423 | G06T-0007/0004 | G06T-0007/70 | G06T-0019/006 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0141 | G06K-0009/00604 | G06T-2207/10056 | G06T-2207/30041 | G06T-2210/41","A61B-034/00","A61B-034/00 | A61B-005/00 | A61B-090/20 | G02B-027/01 | G06F-003/14 | G06T-019/00 | G06T-007/00 | G06T-007/70 | G06K-009/00","","","","","","4922011000880"
"US","US","P","B2","Content marshaling using biometric data","Methods, systems, and computer program products for content marshaling using biometric data are provided herein. A computer-implemented method includes estimating a cognitive state of a user based at least in part on analyzing one or more sets of biometric data pertaining to the user; dynamically compiling multi-modal content for the user based at least in part on (i) the estimated cognitive state of the user, (ii) one or more user parameters, and (iii) content availability information; and outputting the compiled content to the user via one or more devices.","1. A computer-implemented method, the method comprising: estimating a cognitive state of a user based at least in part on analyzing sets of biometric data pertaining to the user, wherein the sets of biometric data comprise (i) eye gaze data derived from one or more cameras focused on a the user, (ii) brain signal data derived from electro-encephalography brainwave data, and (iii) body movement data derived from an accelerometer sensor-enabled smart wearable worn by the user;dynamically compiling multi-modal content for the user based at least in part on: (i) the estimated cognitive state of the user,(ii) content availability information, and(iii) multiple user parameters consisting: user engagement information comprising an average session length metric,a diligence score metric based at least in part on (a) login frequency, (b) average session length and (c) length of content video,a reflective score metric based at least in part on a number of rewind events,an impatience score metric based at least in part on a number of forward seek events,a completion amount metric,a user-specific model comprising multiple content type difficulty preferences, cognitive load information, user device resource information, user budget information, temporal information, user mood information, user mastery level information with respect to multiple types of content, anduser pedagogy information comprising identification of known and unknown concepts for the user, and identification of user preferences with respect to depth-first searching and breadth-first searching content exploration; andoutputting the compiled content to the user via one or more devices;wherein the method is carried out by at least one computing device.","10","16/209076","2018-12-04","2020-0175057","2020-06-04","11275778","2022-03-15","INTERNATIONAL BUSINESS MACHINES CORPORATION","Smitkumar Narotambhai Marvaniya | Malolan Chetlur | Bikram Sengupta | Renuka Sindhgatta | Mukesh Kumar Mohania","","","","G06F-0016/436","G06F-0016/436 | A61B-0005/165 | G06F-0003/013 | G06K-0009/00026 | G06K-0009/00302 | G06K-0009/00335","G06F-016/00","G06F-016/00 | G06F-016/435 | A61B-005/16 | G06K-009/00 | G06F-003/01","","","","","","4922011003647"
"US","US","P","B2","Augmented reality viewing and tagging for medical procedures","Technology is described for augmenting medical imaging for use in a medical procedure. The method can include the operation of receiving an image of patient anatomy captured by a visual image camera during the medical procedure. An acquired medical image associated with the patient anatomy can then be retrieved. Another operation can be associating the acquired medical image to the patient anatomy. An augmentation tag associated with a location in one layer of the acquired medical image can be retrieved. A further operation can be projecting the acquired medical image and the augmentation tag using an augmented reality headset to form a single graphical view as an overlay to the patient anatomy in either 2D, 3D or holographic form.","1. A method for augmenting medical imaging of a patient, the medical imaging displayed using an augmented reality headset worn by a medical professional, the method comprising: retrieving an acquired medical image associated with patient anatomy from data storage, the acquired medical image comprising imaging acquired of one or more anatomical structures of the patient anatomy;aligning the acquired medical image with a portion of the patient anatomy viewable through augmented reality headset, wherein the one or more anatomical structures of the medical imaging are aligned with the patient anatomy;retrieving an augmentation tag from data storage, the augmentation tag associated with a location of the acquired medical image, wherein the augmentation tag identifies at least one anatomical structure of the acquired medical image found at the location; andprojecting the acquired medical image and the augmentation tag using the augmented reality headset to form a single graphical view as an overlay to the patient anatomy viewable through a lens of the augmented reality headset.","19","17/201983","2021-03-15","2021-0298866","2021-09-30","11266480","2022-03-08","NOVARAD CORPORATION","Wendell Arlen Gibby | Steven Todd Cvetko","","","","A61B-0090/36","A61B-0090/36 | A61B-0001/00009 | A61B-0001/04 | A61B-0005/066 | A61B-0005/1072 | A61B-0005/1075 | A61B-0090/361 | A61B-0090/90 | A61B-0090/94 | A61B-0090/96 | A61B-0090/98 | G02B-0027/017 | G06F-0003/011 | G06F-0003/012 | G06F-0003/017 | G06T-0007/74 | G06T-0011/00 | G06T-0019/006 | G16H-0030/20 | G16H-0030/40 | G16H-0050/50 | A61B-2017/00207 | A61B-2017/00216 | A61B-2034/101 | A61B-2090/363 | A61B-2090/365 | A61B-2090/372 | A61B-2090/373 | A61B-2090/374 | A61B-2090/376 | A61B-2090/378 | A61B-2090/3762 | A61B-2090/502 | G06T-0019/00 | G06T-2207/10016 | G06T-2207/10068 | G06T-2207/30204 | G06T-2210/41","A61B-090/00","A61B-090/00 | G16H-030/20 | A61B-005/06 | A61B-005/107 | A61B-090/96 | A61B-090/98 | G02B-027/01 | G06T-007/73 | G06T-019/00 | G06F-003/01 | A61B-001/04 | A61B-001/00 | A61B-090/90 | A61B-090/94 | G06T-011/00 | G16H-030/40 | G16H-050/50 | A61B-090/50 | A61B-034/10 | A61B-017/00","","","","","","4922010001078"
"US","US","P","B2","Route guidance and obstacle avoidance system","A route guidance and obstacle avoidance system for the visually impaired includes a main body assembly worn around the body of the user and a headset assembly worn around the head of the user. The system incorporates obstacle avoidance technology and navigation technology to generate a 3D audio output to a user so as to generate an audible virtual environment reflective of the physical environment of the user. The assemblies include line of sight sensors, a central processing unit, a navigation module, and a camera assembly. Obstacles are detected and classified with a location and distance, and are concurrently processed with real time navigation that is adjusted to instruct not only how to get to a location but also how to avoid obstacles.","1. A route guidance and obstacle avoidance system for a visually impaired user, comprising: a main body assembly including a line of sight sensor and a central processing unit, the line of sight sensor configured to detect obstacles in front of the user, the main body assembly being secured to a front portion of the user;a navigation module in the main body assembly configured to locate the user and report navigation information to the central processing unit, the navigation module used to calculate waypoints for a selected destination; anda headset assembly including a line of sight sensor and a pair of headphones, the headset assembly being worn in communication with a head of the user, the line of sight sensor of the headset assembly being used to track the viewing direction of the head in relation to the main body assembly, the line of sight being measured in azimuth and elevation;a virtual space generator configured to receive data communication from the line of sight sensors, video data, and the navigation information from the navigation module to calculate a virtual 3D space relative to an orientation of the headset assembly and line of sight of the user, at least one of a digital compass and digital tilt sensor used with the virtual space generator to alleviate IMU drift with the virtual space generator;a synthesizer configured to receive data from the virtual space generator, the synthesizer configured to generate a 3D audio output including 3D audio markers for objects selectively located at particular locations within the virtual 3D space, the synthesizer generating a 3D audible output to the user through the pair of headphones, the 3D audible output being a real time audible representation of a physical environment of the user;wherein the synthesizer generates a 2D audio beacon for next waypoints along a route to the selected destination, the 3D audio beacon being located within the 3D audio output and used to navigate the user in the 3D audio environment.","20","16/361870","2019-03-22","2019-0290492","2019-09-26","11266530","2022-03-08","Jennifer Hendrix","Jennifer Hendrix","","","","A61F-0009/08","A61F-0009/08 | G06F-0003/012 | G06K-0009/00671 | G06T-0007/70","A61F-009/08","A61F-009/08 | G06T-007/70 | G06K-009/00 | G06F-003/01","","","","","","4922010001127"
"US","US","P","B2","Mitigating risk behaviors","In an approach to predicting physiological and behavioral states utilizing models representing relationships between driver health states and vehicle dynamics data, one or more computer processors capture one or more vehicle motion parameters. The one or more computer processors to capture one or more physiological parameters; identify contextual data associated with the one or more captured vehicle motion parameters and the one or more captured physiological parameters; predict one or more driving behavior parameters by utilizing one or more physical models fed with the one or more vehicle motion parameters and the identified contextual data; predict one or more driver health parameters by utilizing a model trained with the one or more captured physiological parameters and the identified contextual data; generate a risk assessment based on the one or more predicted driving behavior parameters and the one or more predicted driver health parameters.","1. A computer-implemented method comprising: capturing, by one or more computer processors, one or more vehicle motion parameters;capturing, by one or more computer processors, one or more physiological parameters;identifying, by one or more computer processors, contextual data associated with the one or more captured vehicle motion parameters and the one or more captured physiological parameters;predicting, by one or more computer processors, one or more driving behavior parameters by utilizing one or more physical models fed with the one or more vehicle motion parameters and the identified contextual data;predicting, by one or more computer processors, one or more driver health parameters by utilizing a model trained with the one or more captured physiological parameters and the identified contextual data; andgenerating, by one or more computer processors, a risk assessment based on the one or more predicted driving behavior parameters and the one or more predicted driver health parameters.","20","16/599188","2019-10-11","2021-0107501","2021-04-15","11267482","2022-03-08","INTERNATIONAL BUSINESS MACHINES CORPORATION","Julien Monteil | Yassine Lassoued | Sergio Cabrero Barros | Rodrigo Hernan Ordonez-Hurtado | Martin Mevissen | Sergiy Zhuk | Nigel Hinds | Bo Wen | Jeffrey Rogers","","","","B60W-0050/087","B60W-0050/087 | A61B-0003/165 | A61B-0005/01 | A61B-0005/02055 | A61B-0005/14532 | A61B-0005/163 | A61B-0005/18 | A61B-0005/369 | A61B-0005/389 | A61B-0005/6893 | A61B-0005/7275 | B60W-0040/09 | G06N-0005/02 | G06N-0020/00 | G06Q-0040/08 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0533 | A61B-0005/0816 | B60W-2520/10 | B60W-2520/105 | B60W-2520/14 | B60W-2520/16 | B60W-2520/18 | B60W-2530/10 | B60W-2540/22 | B60W-2540/30 | B60W-2552/00 | B60W-2554/801 | B60W-2555/20 | B60W-2555/60","G01M-017/00","G01M-017/00 | B60W-050/08 | G06N-005/02 | B60W-040/09 | G06Q-040/08 | A61B-005/0205 | A61B-005/01 | A61B-003/16 | A61B-005/16 | A61B-005/145 | A61B-005/18 | A61B-005/00 | G06N-020/00 | A61B-005/369 | A61B-005/389 | A61B-005/08 | A61B-005/0533 | A61B-005/024 | A61B-005/021","","","","","","4922010002073"
"US","US","P","B2","Headset playback acoustic dosimetry","In-ear sound pressure level, SPL, is determined that is caused by output audio being converted into sound by a headset worn by a user. The in-ear SPL is converted into a sound sample having units that are suitable for evaluating sound noise exposure. These operations are repeated to produce a sequence of sound samples during playback. This sequence of sound samples is then written to a secure database. Access to the database is authorized by the user. Other aspects are also described and claimed.","1. A digital signal processing method for headset playback acoustic dosimetry at a processor of an audio source device, the method comprising: a) determining in-ear sound pressure level, SPL, that is caused by output audio being converted into sound by a headset worn by a user, wherein the headset is a peripheral device of the audio source device;b) converting the in-ear SPL into a sound sample having units for sound noise exposure;repeating a) and b) a plurality of times to produce a time sequence of sound samples;writing the time sequence of sound samples to a secure data storage stored on a memory of the audio source device access to which is authorized by the user; andwriting metadata for the time sequence of sound samples to the data secure data storage wherein the metadata identifies a model of the headset and an application program from which the output audio originated.","17","16/872084","2020-05-11","2020-0379717","2020-12-03","11268848","2022-03-08","APPLE INC.","Jakub Mazur | Tyrone T. Chen | Hang Zhang","","","","G01H-0003/14","G01H-0003/14 | A61B-0005/12 | A61B-0005/6817 | G01H-0011/06 | G06F-0003/165 | G06F-0016/61 | G06K-0009/4604 | H03G-0011/008 | H04R-2460/15","G06F-017/00","G06F-017/00 | G01H-003/14 | H03G-011/00 | A61B-005/12 | A61B-005/00 | G06K-009/46 | G06F-016/61 | G01H-011/06 | G06F-003/16","","","","","","4922010003425"
"US","US","P","B2","System and method for holographic image-guided non-vascular percutaneous procedures","Holographic image-guidance can be used to track an interventional device during a non-vascular percutaneous procedure. The holographic image guidance can be provided by a head-mounted device by transforming tracking data and body image data to a common coordinate system and creating a holographic display relative to a patient's body to track the interventional device during the non-vascular percutaneous procedure. The holographic display can also include graphics to provide guidance for the physical interventional device as it travels through the patient's anatomy.","1. A method comprising: receiving, by a head-mounted device comprising a processor, tracking data for a physical interventional device in a tracking coordinate system, wherein the physical interventional device is used during a percutaneous non-vascular medical procedure;transforming, by the head-mounted device, the tracking data for the physical interventional device in the tracking coordinate system into a headset coordinate system;accessing, by the head-mounted device, image data from a pre-operative image of a patient'ss anatomy comprising a physical operative site in an imaging coordinate system;transforming, by the head mounted device, the image data in the imaging coordinate system into the headset coordinate system;registering, by the head-mounted device, a 3D holographic representation of the interventional device based on the tracking data for the physical interventional device in the headset coordinate system to 3D anatomical holographic projections of the patient'ss anatomy based on the imaging data in the headset coordinate system;displaying, by the head mounted device, the 3D anatomical holographic projections providing a visualization of a holographic version of the patient'ss anatomy including a physical operative site within the patient'ss anatomy; andnavigating, by the head mounted device, the 3D holographic representation of the interventional device in the 3D anatomical holographic projections based on the tracking data for the interventional device in the headset coordinate system.","20","17/092409","2020-11-09","2021-0081035","2021-03-18","11269401","2022-03-08","THE CLEVELAND CLINIC FOUNDATION","Karl West | Jeffrey H. Yanof","","","","G06F-0003/011","G06F-0003/011 | A61B-0005/062 | A61B-0005/6852 | A61B-0005/745 | A61B-0005/7405 | A61B-0034/20 | A61B-0034/25 | A61B-0090/361 | A61B-0090/37 | A61M-0005/00 | G06F-0003/012 | G06K-0009/00671 | G06K-0009/3216 | G06T-0003/0006 | G06T-0003/20 | G06T-0007/344 | G06T-0017/20 | A61B-0005/064 | A61B-2034/102 | A61B-2034/107 | A61B-2034/2051 | A61B-2034/2057 | A61B-2034/2068 | A61B-2090/363 | A61B-2090/365 | A61B-2090/366 | A61B-2090/367 | A61B-2090/368 | A61B-2090/372 | A61B-2090/3762 | A61B-2090/3983 | A61B-2090/502 | A61M-2205/507 | G02B-2027/0174 | G06K-2009/3225 | G06K-2209/05 | G06T-0019/006","G06F-003/01","G06F-003/01 | A61B-005/06 | A61B-005/00 | G06T-017/20 | G06T-003/00 | G06T-003/20 | A61B-034/20 | G06T-007/33 | A61B-090/00 | G06K-009/32 | G06K-009/00 | A61M-005/00 | A61B-034/00 | A61B-034/10 | A61B-090/50 | G02B-027/01 | G06T-019/00","","","","","","4922010003973"
"US","US","P","B2","Thermally enriched multi-modal and multi-channel biometric authentication","Embodiments of the present invention provide an improvement to conventional biometric authentication systems and techniques by providing an innovative system, method and computer program product for thermal enrichment of biometric authentication data to generate a high-confidence verification of user identity. A collaborative system for receiving data and continuously analyzing the data to determine emerging patterns is provided. The invention provides for the enrichment of biometric authentication data with thermal imaging data in order to discern between authentic data samples in contrast to inanimate copies or models. Furthermore, the invention is designed to detect and analyze liveliness of data samples as a means of authentication.","1. A system providing the enrichment of biometric data with thermal imaging data for authentication, the system comprising: a module containing a memory storage device, a communication device, and a processor, with computer-readable program code stored thereon, wherein executing the computer-readable code is configured to cause the processor to: receive image data and thermal data of an object from one or more devices and channels;combine the received image data and thermal data to produce thermal image data;normalize the thermal image data based on environmental, device, and channel characteristics;enrich the thermal image data with multimodal biometric data to produce an enriched biometric thermal image;process the enriched biometric thermal image using a trained neural network model, wherein the processing comprises identification of isothermal curves and normalized thermal gradient coloring on the enriched biometric thermal image;perform an authentication check on the enriched biometric thermal image using the trained neural network model to determine if the isothermal curves and normalized thermal gradient coloring matches known characteristics for a purported identity of the object; andbased on the authentication check, verify or deny the purported identity of the object.","20","16/706512","2019-12-06","2021-0173910","2021-06-10","11269983","2022-03-08","BANK OF AMERICA CORPORATION","Eren Kursun","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/7267 | G01J-0005/48 | G06K-0009/00892 | G06K-0009/00906 | G06T-0007/0014 | H04L-0063/0861 | G06T-2207/10048 | G06T-2207/20084","G06F-021/32","G06F-021/32 | G06T-007/00 | G01J-005/48 | G06K-009/00 | A61B-005/00 | H04L-029/06","","","","","","4922010004552"
"US","US","P","B2","Physical activity coaching platform with dynamically changing workout content","A computer implemented coaching platform is described that utilizes contextual data associated with a user and/or his environment in order to provide dynamically changing content while the user is undergoing physical activity. Related apparatus, systems, techniques and articles are also described.","1. An electronic coaching platform comprising: an electronic content authoring tool comprising non-transitory machine-readable instructions that are executable on a computer, the content authoring tool being configured to: electronically receive a routine definition of a routine of an activity, the routine definition having one or more segments, each segment including at least one of a segment parameter, a segment goal, and a segment prompt library, the segment parameter including at least one of a duration, a distance, and an intensity defining the routine, wherein the segment goal includes a measurement determining whether a user is successfully performing the routine, and wherein the measurement includes one or more of a heart rate, a pace and an intensity associated with the routine,retrieve, based on a received selection by the user, at least part of the routine definition to download to an electronic device associated with the user,format the routine definition in a non-transitory machine-readable format for electronic transmission via a communications network from the computer,store each formatted routine definition in a memory,receive biofeedback data from one or more sensors associated with the user and connected with the electronic device, the biofeedback data providing at least one contextual attribute associated with the user while performing the routine to analyze the at least one contextual attribute based on the segment goal of the routine definition to determine a state of the user during each segment of the routinecalculate, while the user is performing at least a part of the routine, a routine score based on the biofeedback data and based on whether the at least one contextual attribute associated with the user satisfies the measurement, the routine score indicating how the user is able to follow and achieve segment goals defined by the routine definition, andadjust the segment parameter, based on the routine score and upon determination that the at least one contextual attribute does not satisfy the measurement and based on historical workouts of the user; anda workout store in digital communication with the memory of the content authoring tool to receive one or more routine definitions defined by the content authoring tool in the non-transitory machine-readable format, the workout store configured to: store the one or more routine definitions and metadata associated with the one or more routine definitions in a database,organize the stored one or more routine definitions and metadata for access by a workout engine hosted on the electronic device associated with the user, the workout engine being configured to select one or more prompts from the segment prompt library based on the state of the user or the user'ss performance of the routine and the routine score, the one or more prompts including sensory feedback information associated with the routine, the sensory feedback information being formatted as a combination of visual feedback, auditory feedback, and tactile feedback.","20","16/865681","2020-05-04","2020-0265734","2020-08-20","11270598","2022-03-08","PEAR SPORTS LLC","Kari Kristian Rauhala | Simon Sollberger | Joseph Rzepiejewski | Eric Franchomme | Robert G. Allison","","","","G09B-0005/04","G09B-0005/04 | G06Q-0010/0639 | G06Q-0030/0251 | G06Q-0030/0269 | G16H-0020/30 | G16H-0020/40 | A61B-0005/024 | A61B-0005/742 | A63B-0024/0075 | G09B-0019/003","G09B-005/00","G09B-005/00 | G09B-005/04 | G06Q-010/06 | G06Q-030/02 | G16H-020/40 | G16H-020/30 | A61B-005/024 | A61B-005/00 | A63B-024/00 | G09B-019/00","","","","","","4922010005165"
"US","US","P","B2","In-home remote monitoring systems and methods for predicting health status decline","The present application relates generally to in-home monitoring and an early health crisis alarm system for elderly individuals and patients with chronic diseases. In one aspect, using artificial intelligence and signals from electricity usage, water usage, ballistocardiography (BGC), and ultra-wideband radar, the system and methods may be used to identify and track daily human activities and physical status in the home. Anomalies to patterns can be determined by identifying disruptions in previously established patterns.","1. A system for producing an alarm for a prediction of chronic liver disease (CLD) for a patient, the system comprising: an ultra-wideband (UWB) radar sensor;a water censor configured to measure a water flow rate;a dashboard configured to produce an audio or visual alarm;at least one processor;and at least one memory including computer program code;the at least one memory and the computer program code configured to, with the at least one processor, cause the system at least to:receive a measurement from the UWB radar sensor;determine a respiratory rate of the patient from the measurement received from the UWB radar sensor;receive a measurement from the water sensor;determine a toileting activity of the patient based on the received measurement from the water sensor; andtrigger, at the dashboard, the audio or visual alarm of a prediction of exacerbation of CLD if there is a decrease in the toileting activity concurrent with an increase in the respiratory rate.","23","16/691696","2019-11-22","2021-0057101","2021-02-25","11270799","2022-03-08","VINYA INTELLIGENCE INC.","Michael E. deSa | Johnie Rose, II","","","","G16H-0050/20","G16H-0050/20 | A61B-0005/0022 | A61B-0005/0205 | A61B-0005/05 | A61B-0005/112 | A61B-0005/1102 | A61B-0005/1115 | A61B-0005/1116 | A61B-0005/1118 | A61B-0005/1126 | A61B-0005/1128 | A61B-0005/4812 | A61B-0005/4833 | A61B-0005/7264 | A61B-0005/7275 | A61B-0005/742 | A61B-0005/744 | A61B-0005/746 | G06N-0005/02 | G06N-0020/00 | G06Q-0050/06 | G08B-0007/06 | G08B-0021/043 | G08B-0021/0423 | G08B-0021/0453 | G08B-0021/0484 | G08B-0021/0492 | G16H-0010/60 | G16H-0020/10 | G16H-0020/30 | G16H-0040/63 | G16H-0040/67 | G16H-0050/30 | G16H-0050/70 | G16H-0070/60 | A61B-0005/024 | A61B-0005/0816 | A61B-2503/08 | G01R-0015/18 | G16H-0040/20","G16H-050/30","G16H-050/30 | A61B-005/00 | A61B-005/11 | G08B-021/04 | G16H-050/20 | G16H-040/63 | G16H-070/60 | G16H-020/30 | G16H-040/67 | G16H-010/60 | G16H-050/70 | G16H-020/10 | G06N-020/00 | A61B-005/0205 | A61B-005/05 | G06N-005/02 | G06Q-050/06 | G08B-007/06 | G16H-040/20 | A61B-005/024 | A61B-005/08 | G01R-015/18","","","","","","4922010005364"
"US","US","P","B2","Video generation method and apparatus","An image generation system includes a region of interest identifying unit operable to identify a region of interest within a piece of content, the piece of content comprising one or more objects, and an image generation unit operable to generate an image for display comprising one or more of the one or more objects such that objects at a different visual depth to the region of interest are present in the generated image at a lower quality.","1. An image generation system for generating 3D images, the system comprising: a region of interest identifying unit operable to identify a region of interest within a piece of content, the piece of content comprising one or more objects and the region of interest includes at least one of the one or more objects; andan image generation unit operable to generate an image representing the piece of content for display comprising one or more of the one or more objects and the at least one of the one or more objects, such that:(i) objects among the one or more of the one or more objects that are at a different visual depth to the region of interest, that includes the at least one of the one or more objects, are present in the generated image at least one of at a lower image quality and at a lower rendering quality, and(ii) a quality of the one or more of the one or more objects is varied in dependence upon a relationship between: (a) the one or more of the one or more objects, and (b) at least one of the one or more objects that is the region of interest, wherein the relationship is other than a spatial difference in visual depth,wherein the image generation unit is operable select one of a plurality of meshes specifically associated with the one or more of the one or more objects, each of the plurality of meshes being of a different quality, such that application of the selected one of the plurality of meshes to the one or more of the one or more objects in rendering the one or more of the one or more objects achieves the varying of the quality of the one or more of the one or more objects in dependence upon the relationship.","11","16/636441","2018-07-13","2020-0175751","2020-06-04","11262590","2022-03-01","SONY INTERACTIVE ENTERTAINMENT INC.","Patrick John Connor","2017012690 | 2017016621","GB | GB","2017-08-08 | 2017-10-11","G02B-0027/0179","G02B-0027/0179 | A61B-0005/02438 | A61B-0005/0531 | A61B-0005/6803 | G02B-0027/0093 | G02B-0027/0172 | G06F-0003/013 | G06F-0003/015 | G06K-0009/3233 | G06T-0015/04 | G06T-0015/205 | H04N-0013/25 | H04N-0013/268 | H04N-0013/275 | G02B-2027/0178 | G02B-2027/0187","G06F-003/01","G06F-003/01 | G06K-009/32 | G06T-015/04 | G06T-015/20 | G02B-027/01 | A61B-005/024 | A61B-005/0531 | A61B-005/00 | G02B-027/00 | H04N-013/268 | H04N-013/25 | H04N-013/275","","","","","","4922009003772"
"US","US","P","B2","Vision protection method and systems thereof","A vision protection method is provided to ensure a viewer to rest his/her eyes after viewing on an electronic device for a certain period, wherein the eyesight protection method includes the steps of detecting at least one of eye activities of the viewer and working parameter of the electronic device in a working mode of the electronic device during the viewer is working on a current work displaying by the electronic device; switching the working mode of the electronic device to a resting mode when an abnormal eye activity of the viewer is detected; and switching the electronic device from the resting mode back to the working mode to resume the display of the current work of the electronic device. Therefore, the viewer is enforced to rest his/her eyes after every certain period.","1. A system for eyesight protection of a viewer of an electronic device, comprising: a display;a camera mounted relative to the display for imaging a viewer in front of the display;a control module coupled to the display and the camera to: (a) detect a viewer in front of the display;(b) initiate a working mode wherein: i) the display presents working information to the viewer;ii) the control module triggers a timer counting down a predetermined working time period for the working mode;iii) the control module monitors one or more parameters of an eye of the viewer using the camera, the one or more parameters comprising monitoring blinks of the eye of the viewer;iv) the control module analyzes images from the camera to detect a sitting posture of the viewer; andv) the control module monitors ambient light intensity of the electronic device;(c) during the working mode, if the control module determines that the blinks indicate potential dry eye problems, the control module detects an improper sitting posture of the viewer, or the ambient light intensity is detected below a preset ambient light threshold, a warning is presented on the display.","8","17/035521","2020-09-28","2021-0145279","2021-05-20","11253152","2022-02-22","EYES4LIVES, INC.","Roger Wu","","","","A61B-0003/18","A61B-0003/18 | A61B-0003/112 | A61B-0003/113 | A61B-0003/12 | A61B-0003/14 | A61B-0005/1075 | A61B-0005/1103 | A61B-0005/1116 | A61B-0005/1128 | A61B-0005/486 | A61B-0005/489 | A61H-0005/00 | A61M-0021/02 | G06F-0003/013 | G06K-0009/00597 | G06Q-0010/109 | G09G-0005/00 | A61B-0005/1176 | A61B-2503/20 | A61B-2560/0266 | A61B-2560/0271 | A61H-2201/5007 | A61H-2201/5043 | A61H-2201/5064 | A61H-2201/5092 | A61M-2021/005 | A61M-2021/0027 | A61M-2205/056 | A61M-2205/3303 | A61M-2230/62 | A63F-2300/8094 | G06F-2203/011 | G09G-2354/00 | G09G-2356/00 | G09G-2360/144 | G09G-2360/145","A61B-003/18","A61B-003/18 | G06Q-010/10 | G06K-009/00 | A61B-005/107 | A61B-005/11 | A61B-005/00 | A61B-003/11 | A61B-003/14 | A61B-003/12 | A61B-003/113 | A61H-005/00 | G09G-005/00 | A61M-021/02 | G06F-003/01 | A61B-005/1171 | A61M-021/00","","","","","","4922008000947"
"US","US","P","B2","Media content selection based on physiological attributes","A media-playback device includes: a media-output device that plays media content items; a physiological measurement device programmed to measure at least one physiological measurement of a user of the media-output device; and a physiological control engine configured to: identify a current physiological measurement for the user; and cause the media-output device to modify playback of the media content items based upon the current physiological measurement.","1. A method for playing media content items, the method comprising: receiving a current cadence of a user, the current cadence associated with a level of exertion of the user during performance of a repetitive-motion activity;receiving a future cadence of the user, the future cadence involving a change in the level of exertion of the user during performance of the repetitive-motion activity, wherein the future cadence of the user is acquired from a workout plan that involves one or more changes in the cadence of the user, and wherein the workout plan requires the user to maintain the current cadence for a first predetermined interval, and to change the current cadence to the future cadence for a second predetermined interval; andadjusting a tempo used to select media content items for playback, the tempo being adjusted based on the future cadence.","14","16/911018","2020-06-24","2020-0319847","2020-10-08","11256471","2022-02-22","SPOTIFY AB","Owen Smith | Sten Garmark | Gustav Soderstrom","","","","G06F-0003/165","G06F-0003/165 | A61B-0005/0022 | A61B-0005/01 | A61B-0005/024 | A61B-0005/02438 | A61B-0005/0816 | A61B-0005/0833 | A61B-0005/1112 | A61B-0005/4266 | A61B-0005/6898 | A63B-0071/00 | A63B-0071/0622 | A63B-0071/0686 | G01S-0019/19 | G06F-0003/011 | G06F-0003/015 | G16H-0040/67 | A61B-0005/02055 | A61B-0005/681 | A61B-0005/6831 | A63B-2071/0625 | A63B-2220/12 | A63B-2230/062","G06F-017/00","G06F-017/00 | G06F-003/16 | G06F-003/01 | A61B-005/024 | A63B-071/00 | G01S-019/19 | A61B-005/00 | A61B-005/08 | A61B-005/11 | G16H-040/67 | A61B-005/01 | A61B-005/083 | A63B-071/06 | A61B-005/0205","","","","","","4922008004240"
"US","US","P","B2","Medical image processing apparatus, medical image processing system, and medical image processing method","A medical image processing apparatus according to an embodiment includes a processing circuit. The processing circuit specifies teaching data used for generation of a learned model. The processing circuit performs image analysis with respect to pieces of collected medical image data. The processing circuit extracts medical image data having an attribute common to the teaching data of the learned model from the pieces of medical image data based on an analysis result of the image analysis, as a candidate of the teaching data of the learned model.","1. A medical image processing apparatus comprising: processing circuitry configured tospecify teaching data having been used for generation of a learned model,perform image analysis with respect to pieces of collected medical image data,extract plural pieces of medical image data of an image type common to the teaching data from the pieces of collected medical image data based on the analysis result of the image analysis; andextract medical image data having an attribute common to the teaching data of the learned model from the pieces of extracted medical image data based on an analysis result of the image analysis, as a candidate of the teaching data of the learned model, includingextract medical image data having an attribute common to the teaching data from the pieces of extracted medical image data, based on at least one of image quality, a degree of similarity to an original image of the teaching data, and restrictions to be used for the learned model.","11","16/598540","2019-10-10","2020-0118265","2020-04-16","11257211","2022-02-22","CANON MEDICAL SYSTEMS CORPORATION","Takuma Igarashi","2018-191547","JP","2018-10-10","G06T-0007/0012","G06T-0007/0012 | A61B-0005/055 | A61B-0005/7207 | G16H-0030/20 | G16H-0030/40 | A61B-0006/466 | A61B-0008/483 | A61B-0008/5215","G06K-009/00","G06K-009/00 | G06Q-010/00 | G06T-007/00 | A61B-005/055 | G16H-030/20 | A61B-005/00 | G16H-030/40 | A61B-006/00 | A61B-008/08","","","","","","4922008004976"
"US","US","P","B2","System and method for insulin pump medical device including a slider assembly wherein images on display allow for highlighting and magnifying images","A medical system includes an input assembly for receiving one or more user inputs. The input assembly includes at least one slider assembly for providing an input signal. Processing logic receives the input signal from the input assembly and provides a first output signal and a second output signal. A display assembly is configured to receive, at least in part, the first output signal from the processing logic and render information viewable by the user. The second output signal is provided to one or more medical system components. The information rendered on the display assembly may be manipulatable by the user and at least a portion of the information rendered may be magnified.","1. A medical system comprising: a first medical device comprising: a reservoir for holding a fluid; a pump assembly for pumping fluid out of the reservoir for delivery; a feedback system for indicating the dosage of fluid delivered; a display assembly for rendering information; an input assembly comprising at least one slider assembly, for manipulating the information rendered on the display assembly and for providing an input signal in response to an input; and a computer program product residing on a computer readable medium having a plurality of instructions stored thereon which, when executed by a processor, cause the processor to perform operations comprising: selecting information on the display assembly, and simultaneously highlighting and scrolling the selected information on the display assembly whereby the rate of scrolling is variable based on the input signal corresponding to a user input to the slider assembly; and wherein the slider assembly changes the rate of scrolling of the highlighted information based on where a user contacts the slider assembly.","14","17/012165","2020-09-04","2020-0397982","2020-12-24","11246978","2022-02-15","DEKA PRODUCTS LIMITED PARTNERSHIP","Kevin L. Grant | Douglas J. Young | Matthew C. Harris","","","","A61M-0005/14244","A61M-0005/14244 | A61M-0005/1723 | G06F-0003/03547 | G06F-0003/0482 | G06F-0003/04847 | G06K-0009/2054 | G06T-0003/40 | G16H-0010/60 | G16H-0020/17 | G16H-0040/67 | A61M-2005/14208 | A61M-2205/502 | A61M-2205/505 | A61M-2205/52 | A61M-2205/58 | A61M-2230/201 | G06F-0003/03548 | G06F-0003/044 | G06F-2203/0339","A61M-005/142","A61M-005/142 | G06F-003/0482 | G16H-040/67 | G06F-003/0354 | G06F-003/0484 | G06K-009/20 | G06T-003/40 | A61M-005/172 | G16H-020/17 | G16H-010/60 | G06F-003/044","","","","","","4922007001365"
"US","US","P","B2","Craniaofacial emotional response influencer for audio and visual media","An emotional enhancement system for music and film that uses audio and video input to select or generate a stimulation pattern which stimulates the user's face using lights and electrical stimulation directed to certain points on the user's face and head which are believed to induce certain emotions. Using the system, a user's emotional experience of the music or film can be enhanced by inducing emotions in the user coordinated with portions of the music or film. In some embodiments, biometric sensors or cameras and facial tracking software may be used to monitor the emotions displayed by a user while listening to music or watching films, and adjust the stimulation accordingly.","1. A system for influencing emotional response to audio and visual media using craniofacial stimulation, comprising: a user database comprising user preferences regarding preferred emotions during playback of media input;a media metadata library comprising metadata for media input;a stimulation pattern library comprising stimulation patterns;a digital camera configured to record images of the user'ss face; anda stimulation headset comprising: one or more stimulation transducers being arranged to correspond to one or more target areas about a face and head of a user when the headset is worn, wherein the one or more stimulation transducers are configured to apply stimulation to the one or more target areas; andan emotion influencing engine comprising a first plurality of programming instructions stored in a memory of, and operating on a processor of, a computing device, wherein the first plurality of programming instructions, when operating on the processor, cause the computing device to: receive media input, the media input comprising audio, video, or a combination of audio and video;retrieve the metadata for the media input from the media metadata library, the metadata comprising a time of an expected emotion within the media input and a pre-programmed emotion for the time of the expected emotion;retrieve the user preference comprising a preferred emotion during playback of the media input;receive the indication of the emotion from the facial recognition software application;at the time of the expected emotion within the media content, determine whether the indication of emotion corresponds to the user preference for the media content;based on the media input, the metadata, the user preference, and the indication of emotion, perform one of the following: retrieve the stimulation pattern from the library of stimulation patternsoperate one or more stimulation transducers on a set of headphones in accordance with the stimulation pattern;determine whether the stimulation pattern induces the expected emotion by comparing the expected emotion to the indication of the emotion;update the library of stimulation patterns based on the determination; andthe facial recognition software application comprising a second plurality of programming instructions stored in the memory of, and operating on the processor of, the computing device, wherein the second plurality of programming instructions, when operating on the processor, cause the computing device to: receive images from the digital camera;calculate an indication of an emotion displayed on the user'ss face from analyzing the image using the machine learning algorithm; andsend the indication of emotion to a machine learning algorithm.","8","16/568418","2019-09-12","2020-0188629","2020-06-18","11247021","2022-02-15","TREV LAS, LLC","Abby D. Levenberg","","","","A61M-0021/00","A61M-0021/00 | A61B-0005/0077 | A61B-0005/165 | A61N-0001/3603 | G06F-0003/015 | G06K-0009/00302 | A61M-2021/005 | A61M-2021/0022 | A61M-2021/0027 | A61M-2205/3303 | A61N-0001/36025 | A61N-0005/0618 | G06K-2009/00328","A61M-021/00","A61M-021/00 | A61B-005/00 | A61B-005/16 | A61N-001/36 | G06K-009/00 | G06F-003/01 | A61N-005/06","","","","","","4922007001408"
"US","US","P","B2","Gaze-driven augmented reality","Augmented reality (AR) systems, methods, and instrumentalities are disclosed. A user's gaze point may be estimated and may be used to search for and present information, e.g., information relating to areas on which the user is focusing. The user's gaze point may be used to facilitate or enable modes of interactivity and/or user interfaces that may be controlled by the direction of view of the user. Biometric techniques may be used to estimate an emotional state of the user. This estimated emotional state may be used to be the information that is presented to the user.","1. A method for providing augmented reality (AR) to a user, comprising: estimating a gaze point of the user to define a first region of interest (ROI);adaptively adjusting the size of the first ROI based upon one or more of a gaze focused for a time on a portion of the first ROI exceeding a threshold or a number of objects of interest in the first ROI exceeding a threshold, wherein the adjusted ROI contains one or more physical objects currently in proximity to the user;extracting images from the adjusted ROI and creating a set of descriptors which describe the one or more physical objects without input from the user;estimating an emotional state of the user;determining a subset of descriptors by comparing the set of descriptors to the estimated emotional state of the user and giving greater weight to descriptors associated with an emotional state closest to the estimated emotional state of the user, wherein representative emotional states are mapped to descriptors of objects, such that objects consistent with the estimated emotional state of the user are prioritized;without further input from the user, searching for information regarding the one or more physical objects in the adjusted ROI using the subset of descriptors;retrieving the searched information prioritized by the subset of descriptors; anddisplaying the retrieved information.","21","15/924956","2018-03-19","2018-0211112","2018-07-26","11250263","2022-02-15","InterDigital Patent Holdings, Inc.","Eduardo Asbun | Yuriy Reznik | Ariela Zeira | Gregory S. Sternberg | Ralph Neff","","","","G06K-0009/00671","G06K-0009/00671 | A61B-0005/0022 | A61B-0005/1112 | A61B-0005/165 | A61B-0005/742 | G02B-0027/017 | G06F-0003/013 | G06K-0009/00302 | G06K-0009/00604 | G16H-0040/67 | H04W-0004/30 | A61B-0005/0533 | A61B-0005/369 | G06F-0003/015","A61B-005/00","A61B-005/00 | A61B-005/0476 | A61B-005/053 | A61B-005/11 | A61B-005/16 | G02B-027/01 | G06K-009/00 | G06F-003/01 | H04W-004/30 | G16H-040/67 | A61B-005/0533 | A61B-005/369","","","","","","4922007004622"
"US","US","P","B2","System for displaying medical monitoring data","A first medical device can receive a physiological parameter value from a second medical device. The second physiological parameter value may be formatted according to a protocol not used by the first medical device such that the first medical device is not able to process the second physiological parameter value to produce a displayable output value. The first medical device can pass the physiological parameter data from the first medical device to a separate translation module and receive translated parameter data from the translation module at the first medical device. The translated parameter data can be processed for display by the first medical device. The first medical device can output a value from the translated parameter data for display on the first medical device or an auxiliary device.","1. A method of displaying medical data, the method comprising: under the control of a first medical device comprising digital logic circuitry, receiving a physiological signal from a physiological sensor;determining, based on the physiological signal, a first physiological parameter value associated with a patient;outputting, in a first section of a first portion of a display, the first physiological parameter value for display together with a corresponding waveform of a first physiological parameter;receiving a second physiological parameter value associated with the patient;outputting, in a second portion of the display, the second physiological parameter value for display without a corresponding waveform of a second physiological parameter;in response to receiving a user input dragging the second physiological parameter value from the second portion of the display to the first portion of the display, causing the second physiological parameter value together with a corresponding waveform of the second physiological parameter to be displayed in a second section of the first portion of the display that is adjacent to the first section of the first portion of the display;in response to determining an alarm condition associated with the second physiological parameter, causing highlighting of the second section of the first portion of the display surrounding the displayed second physiological parameter value and the corresponding waveform of the second physiological parameter;outputting, in the first portion of the display, indications of respective alarm limits associated with each displayed physiological parameter including the first physiological parameter, wherein the indications of the alarm limits are displayed adjacent to the respective associated displayed values of the physiological parameters, and wherein the second physiological parameter value is displayed in the second portion of the display without indications of corresponding alarm limits; andfurther in response to receiving the user input dragging and dropping the second physiological parameter value from the second portion of the display to the first portion of the display, causing indications of alarm limits associated with the second physiological parameter to be displayed in the second section of the first portion of the display and adjacent to the second physiological parameter value.","23","16/670051","2019-10-31","2020-0060629","2020-02-27","11241199","2022-02-08","MASIMO CORPORATION","Peter Scott Housel | Bilal Muhsin | Ammar Al-Ali | Massi Joe E. Kiani","","","","A61B-0005/742","A61B-0005/742 | A61B-0005/0002 | A61B-0005/002 | A61B-0005/021 | A61B-0005/02055 | A61B-0005/0816 | A61B-0005/14551 | A61B-0005/743 | A61M-0016/0051 | A61M-0016/021 | G06F-0021/84 | G16H-0040/63 | H04Q-0009/00 | A61B-0005/4821 | A61B-0005/4836 | A61B-2560/0214 | A61B-2560/045 | A61B-2562/08 | A61B-2562/227 | A61M-0005/172 | A61M-2205/18 | A61M-2205/3368 | A61M-2205/3375 | A61M-2205/3561 | A61M-2205/3569 | A61M-2205/3584 | A61M-2205/3592 | A61M-2205/505 | A61M-2209/086 | A61M-2230/04 | A61M-2230/10 | A61M-2230/201 | A61M-2230/202 | A61M-2230/205 | A61M-2230/208 | A61M-2230/30 | A61M-2230/42 | A61M-2230/432 | A61M-2230/50","A61B-005/00","A61B-005/00 | A61B-005/1455 | H04Q-009/00 | A61B-005/08 | A61B-005/021 | A61B-005/0205 | A61M-016/00 | G06F-021/84 | G16H-040/63 | A61M-005/172","","","","","","4922006000758"
"US","US","P","B2","Techniques for determining fluid volumes using bioimpedance information","Techniques and apparatuses for determining fluid volumes of a patient are described. In one embodiment, for example, an apparatus may include at least one memory, and logic coupled to the at least one memory. The logic may be configured to receive baseline bioimpedance information for at least a portion of a human body at a baseline pressure, receive pressurized bioimpedance information of the portion of the human body at a pressurized pressure, the pressurized pressure greater than the baseline pressure and configured to substantially remove blood volume from the portion at the pressurized pressure, and determine at least one of interstitial fluid volume (VIT) or peripheral blood volume (BVP) based on the baseline bioimpedance information and the pressurized bioimpedance information. Other embodiments are described.","1. An apparatus, comprising: at least one memory; andlogic coupled to the at least one memory, the logic to: receive baseline bioimpedance information for at least a portion of a human body of a patient at a baseline pressure during a dialysis treatment of the patient performed by a dialysis system,receive pressurized bioimpedance information of the portion of the human body at a pressurized pressure, the pressurized pressure greater than the baseline pressure and configured to substantially remove blood volume from the portion at the pressurized pressure, the baseline bioimpedance information and the pressurized bioimpedance information generated at one of a low current frequency and a high current frequency,determine at least one of: interstitial fluid volume (VIT) based on the baseline bioimpedance information and the pressurized bioimpedance information generated at the low current frequency, orperipheral blood volume (BVp) based on the baseline bioimpedance information and the pressurized bioimpedance information generated at the high current frequency, andcontrol the administration of the dialysis treatment to the patient via setting one of an ultrafiltration rate (UFR) or an ultrafiltration volume (UFV) based on at least one of VIT or BVp.","20","16/031763","2018-07-10","2019-0015013","2019-01-17","11234609","2022-02-01","FRESENIUS MEDICAL CARE HOLDINGS, INC.","Fansan Zhu | Peter Kotanko | Nathan W. Levin","","","","A61B-0005/0537","A61B-0005/0537 | A61B-0005/0295 | A61B-0005/02233 | A61B-0005/053 | A61B-0005/1451 | A61B-0005/4881 | A61B-0005/6828 | A61B-0005/6843 | A61B-0005/7225 | A61M-0001/28 | G16H-0050/20 | A61B-0005/02 | A61M-2205/3303 | A61M-2230/65 | G06F-0017/18","A61B-005/0537","A61B-005/0537 | A61B-005/053 | A61B-005/145 | A61B-005/00 | G16H-050/20 | A61B-005/022 | A61B-005/0295 | A61M-001/28 | G06F-017/18 | A61B-005/02","","","","","","4922005000922"
"US","US","P","B2","Group application oriented transcranial brain atlas generation method, prediction method and prediction apparatus","The present invention discloses a transcranial brain atlas generation method, and discloses a group application oriented transcranial brain atlas prediction method and a corresponding transcranial brain atlas prediction apparatus. The transcranial brain atlas generation method includes the following steps: creating a cranial surface coordinate system at an individual level; establishing a transcranial mapping system used to connect a cranial location and a brain location; and constructing a transcranial brain atlas by using a two-step stochastic process in a Markov chain. According to the transcranial brain atlas provided in the present invention, invisible intracerebral atlas label information is projected onto a visible scalp, so that a researcher or a doctor may ""directly"" use these pieces of brain structure information and function atlas information, thereby greatly improving the function of the brain atlas during use of a transcranial brain mapping technology.","1. A transcranial brain atlas generation method, comprising the following steps: (1) creating a cranial surface coordinate system at an individual level, the step (1) comprises the following substeps:(11) identifying five cranial landmarks Nz, Iz, AL, AR, and Cz on a scalp surface;(12) defining an intersection curve between the scalp surface and a plane passing through Nz, Cz, and Iz as a cranial equator;(13) giving a point p on the scalp surface, wherein a longitude curve can be uniquely determined as an intersection curve between the scalp surface and a plane passing through AL, AR, and p, and p′ is an intersection point between the cranial equator and the longitude curve; and(14) uniquely determining any point p on an upper scalp by using a pair of non-negative real numbers (pe, pl): pe=LNZ-p′/Le,pe∈[01] pl=LAL-p/LAL-p-AR,pl∈[01] wherein LNz-p′ is a curve length from Nz to p′ along the cranial equator, and Le is a full length of the cranial equator; and LAL-p is a curve length from AL to p along the longitude curve whose full length is LAL-p-AR;(2) establishing a transcranial mapping system used to connect a cranial location and a brain location; and(3) constructing a transcranial brain atlas by using a two-step stochastic process in a Markov chain.","14","16/314650","2018-04-22","2021-0225004","2021-07-22","11234633","2022-02-01","BEIJING NORMAL UNIVERSITY","Chaozhe Zhu","2017-11268640 | 2017-11322471","CN | CN","2017-12-05 | 2017-12-12","A61B-0005/4064","A61B-0005/4064 | A61B-0005/05 | A61B-0008/08 | G06F-0003/03545 | G06T-0007/143 | G06T-0017/20 | G06T-0019/00 | G06T-0019/20 | A61B-2560/0223 | G06T-2207/10088 | G06T-2207/30016 | G06T-2210/41 | G06T-2210/56","A61B-005/00","A61B-005/00 | A61B-005/05 | G06F-003/0354 | G06T-017/20 | G06T-007/143 | G06T-019/20 | A61B-008/08 | G06T-019/00","","","","","","4922005000946"
"US","US","P","B2","Data storage on implantable magnetizable fabric","The disclosure is directed to a system, device and method for data storage on implantable magnetizable fabric. The system includes implantable magnetizable fabric coupled to a graft segment of a prosthesis for being delivered into a body of a subject. The system includes information written on the implantable magnetizable fabric. The system further includes a magnetic detection device capable of, after the prosthesis is delivered into the body of the subject, detecting the implantable magnetizable fabric and accessing at least a portion of the information.","1. An apparatus for storing data on a prosthesis, the apparatus comprising: implantable magnetizable fabric coupled to a graft segment of a prosthesis for being delivered into a body of a subject;information written on the implantable magnetizable fabric; andwherein: after the prosthesis is delivered into the body of the subject, the implantable magnetizable fabric is detectable by a magnetic detection device and at least a portion of the information is accessible by the magnetic detection device;the magnetic detection device comprises a data cable, a catheter, and a head;the head is configured to access a portion of the information;the data cable is disposed inside the catheter; andthe magnetic detection device is delivered inside the prosthesis to access at least the portion of the information.","12","16/541758","2019-08-15","2020-0054437","2020-02-20","11234806","2022-02-01","COOK MEDICAL TECHNOLOGIES LLC","Ralf Spindler","","","","A61F-0002/07","A61F-0002/07 | A61B-0005/4851 | G06F-0007/405 | G06K-0007/084 | H03M-0005/145","A61F-002/07","A61F-002/07 | A61B-005/00 | H03M-005/14 | G06K-007/08 | G06F-007/40","","","","","","4922005001117"
"US","US","P","B2","System and method for asynchronous brain control of one or more tasks","A closed-loop system for asynchronous brain control of at least one task includes a brain state decoder configured to decode neural signals of a user into control signals for controlling the at least one task, a task interface module configured to transmit the control signals to the at least one task, store status information including a series of messages regarding each of the at least one task, and select one message of the series of messages regarding the at least one task to transmit to the user, and a brain state encoder configured to map the one message received from the task interface module into brain state montages for transmission to the user.","1. A closed-loop system for asynchronous brain control of at least one task, the system comprising: a brain state decoder configured to decode neural signals of a user into control signals for controlling the at least one task;a task interface module configured to transmit the control signals to the at least one task, store status information comprising a plurality of messages regarding each of the at least one task, and select one message of the plurality of messages to transmit to the user; anda brain state encoder configured to map the one message received from the task interface module into brain state montages for transmission to the user.","20","17/112052","2020-12-04","2021-0240265","2021-08-05","11237634","2022-02-01","HRL LABORATORIES, LLC","Praveen Pilly | Jaehoon Choe | Michael Howard | Nicholas Ketz","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/291 | G06F-0009/4843 | G06F-0009/544","G06F-009/46","G06F-009/46 | G06F-003/01 | G06F-009/54 | G06F-009/48 | A61B-005/291","","","","","","4922005003919"
"US","US","P","B2","Tracking movement of objects and notifications therefor","Improved approaches for monitoring status of articles being shipped are disclosed. The monitoring can produce notifications to interested parties. The notifications typically contain status information pertaining to the articles being shipped. Alternatively, interested parties can gain access to status information pertaining to the articles being shipped via a website. According to one embodiment, the status information includes at least position (location) information and shipping conditions information.","1. An apparatus for tracking movement of one or more objects, said apparatus comprising: shared frequency synthesizing circuitry including a crystal oscillator;global positioning system circuitry operatively connected to the shared frequency synthesizing circuitry;communication RF circuitry operatively connected to the shared frequency synthesizing circuitry;a non-transitory storage device to store at least computer program code; andat least one processor to perform at least some of the stored computer program code, the at least one processor performing at least some of the stored computer program code to track movement of an object, the stored computer program code comprises: computer program code for receiving status information associated with the object at least during movement, the status information being provided by status electronic circuitry, the status electronic circuitry being provided at the object at least during its movement, the status information including at least position information and condition information, and the condition information being related to at least one condition of or around the object;computer program code for determining whether a notification condition pertaining to the object exists based on at least the status information;computer program code for producing a notification message when the notification condition is determined to exist; andcomputer program code for initiating electronically sending the notification message, after the notification message has been produced, to an interested user,wherein the notification condition is determined to exist at least when the condition information indicates that the at least one condition exceeds a predetermined threshold, andwherein the status electronic circuitry is configured to control an actuator based at least in part on the status information, the actuator being operatively coupled to the status electronic circuitry at least during movement of the object, and the actuator being operatively coupled to an air-cooling apparatus, with the air-cooling apparatus configured to be responsive to the actuator so as to cool an environment associated with and movable with the object.","54","16/830666","2020-03-26","2020-0226542","2020-07-16","11238398","2022-02-01","IPVENTURE, INC.","Chung Lau | C. Douglass Thomas | Peter P. Tong","","","","G06Q-0010/0833","G06Q-0010/0833 | A61B-0005/0002 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/02055 | A61B-0005/1112 | A61B-0005/1123 | A61B-0005/24 | A61B-0005/6801 | A61B-0005/7282 | A61B-0005/742 | G06F-0011/3013 | G06F-0011/3055 | G06F-0011/3058 | G06Q-0010/00 | G06Q-0010/0832 | G06Q-0010/107 | G16H-0010/60 | G16H-0020/10 | G16H-0040/67 | G16H-0050/20 | H04L-0067/04 | H04L-0067/18 | H04W-0004/02 | H04W-0004/029 | H04W-0004/20 | A61B-0005/02 | A61B-0005/1116 | A61B-0005/1118 | H04W-0004/027 | H04W-0064/00 | Y10S-0128/92","G06Q-010/08","G06Q-010/08 | G06Q-050/22 | G06Q-010/10 | G06F-011/30 | G16H-050/20 | G16H-040/67 | G16H-010/60 | G16H-020/10 | A61B-005/00 | A61B-005/11 | A61B-005/0205 | H04W-004/029 | G06Q-010/00 | H04L-029/08 | H04W-004/02 | H04W-004/20 | A61B-005/02 | A61B-005/04 | H04W-064/00 | A61B-005/24","","","","","","4922005004679"
"US","US","P","B2","Methods and systems for providing online monitoring of released criminals by law enforcement","The methods and systems are designed to utilize an integrated combination of just in time, just in place, and just on device actions connected to an image recognition process for monitoring criminals who are probation, offenders who are on parole, sex offenders, and witnesses under protection by law enforcement.","1. A system for registering and monitoring a criminal offender over a network, the system comprising: an app downloadable to a device comprising a camera, a GPS locator, a network interface, and a user interface, the app comprises: a process to initiate an offender to take and capture a picture of an offender'ss face with the camera;a process to capture a first set of GPS coordinates, a first time and a date of the picture of the offender'ss face;a process to initiate the offender to take and capture a picture of an identification card with the camera;a process to capture a second set of GPS coordinates, a second time and a date of the picture of the identification card;a process to capture identification data of the device;a process to send data comprising at least one of the picture of the offender'ss face, the picture of the identification card, the first set of GPS coordinates, the second set of GPS coordinates, the first time and date of the picture of the offender'ss face, the second time and date of the picture of the identification card, and the identification data to a location on the network; and?a process to receive and communicate information;an identification engine on a server at the location on the network, the identification engine comprising: an input configured to receive the data from the app;an image comparison algorithm configured to compare the picture of the offender'ss face, the picture of the identification card, then determine the likelihood that the offender and a person in the picture of the identification card are substantially the same; anda time comparison algorithm configured to determine if the first time and date of the picture of the offender'ss face, the second time and date of the picture of the identification card are substantially the same; anda registration token to be sent to the app if the offender and a person in the picture of the identification card are substantially the same, and if the first time and date of the picture of the offender'ss face, the second time and date of the picture of the identification card are substantially the same.","20","16/653667","2019-10-15","2020-0043319","2020-02-06","11238722","2022-02-01","OPTIMUM ID, LLC","Hemanshu Nigam | Michael Lang | James Drolshagen","","","","G08B-0025/001","G08B-0025/001 | A61B-0005/117 | G06F-0016/51 | G06F-0016/5838 | G06F-0016/5866 | G06K-0009/00228 | G06K-0009/00288 | G06K-0009/00892 | G06K-0009/00912 | G06K-0009/00899 | G06Q-0050/26","G08B-025/00","G08B-025/00 | G06F-016/51 | G06F-016/583 | A61B-005/117 | G06F-016/58 | G06K-009/00 | G06Q-050/26","","","","","","4922005005002"
"US","US","P","B2","Configurable concise datasets platform","A scalable configurable universal complete spectrum concise datasets platform is provided that utilizes measure points from sensor-observation-derived representations or concise datasets in the making of selected cyber determinations regarding or utilizing sensor observations or sensor observation subjects. The platform utilizes necessary resources and predetermined criteria in the making of selected cyber determinations, the platform utilizes measure points and personalized processes in the accurate or reliable locating of selected analytically rich aspects, characteristics or features from sensor-observation-derived representations, wherein appropriate informational representations are assigned to selected analytically rich aspects, characteristics, features, measure points or the sensor observation and stored in concise datasets where they may be utilized in real-time or thereafter in the making of selected cyber determinations regarding or utilizing sensor observations or sensor observation subjects. The platform is configurable for being utilized as a touchless user interface, a 100% accurate cyberspace identity test or a universal health metrics monitor.","1. A scalable configurable universal complete spectrum concise datasets platform of processes, procedures, methods, formulas, programming, sensors and computing devices, wherein said concise datasets platform selects, derives or utilizes data for or from concise datasets; wherein, said concise datasets are utilizable in the making of at least one selected cyber determination regarding or utilizing at least one sensor observation or at least one sensor observation subject, said concise datasets platform is comprised of:a set of resources that include (a) at least one computing device, (b) at least one sensor, (c) selected necessary programming, (d) selected information, (e) criteria selected from a spectrum of criteria that may be utilized by said platform, and (f) other necessary resources; wherein said at least one utilized computing device includes at least one tangible, non-transient memory device and at least one input device or at least one output device;wherein said concise datasets platform utilizes all or part of said set of resources for (i) the selecting or deriving of data that is to be included in at least one concise dataset, or (ii) the making of at least one selected cyber determination regarding or utilizing at least one sensor observation or at least one sensor observation subject;wherein said at least one concise dataset is utilizable in the making of said at least one selected cyber determination regarding or utilizing said at least one sensor observation or said at least one sensor observation subject;wherein said at least one concise dataset is comprised of selected sensor data or selected derived data;wherein said selected sensor data is comprised of at least one informational representation that was selected from at least one sensor observation dataset;wherein said derived data is comprised of at least one informational representation that was derived from the processing of (i) at least one informational representation that was selected from at least one sensor observation dataset, or (ii) at least one informational representation that was selected from derived data;wherein processes, procedures, methods or formulas that are utilized in the processing of said derived data are selected from a spectrum of processes, procedures, methods or formulas that are utilized in the deriving of data for or from concise datasets;wherein data from said selected sensor data or said selected derived data are utilizable by said platform in the making of said selected cyber determinations regarding or utilizing sensor observations or sensor observation subjects;wherein said concise datasets platform is configurable for utilizing at least one measure point in locating of at least one selected analytically rich aspect, characteristic or feature of or from at least one sensor-observation-derived representation of at least one sensor observation or at least one sensor observation subject;wherein said at least one measure point is utilizable by said platform in the making of said at least one selected cyber determination regarding or utilizing sensor observations or sensor observation subjects;wherein said at least one analytically rich aspect, characteristic or feature of or from said at least one sensor-observation-derived representation that is located through utilization of said at least one measure point, and said at least one measure point are each assigned at least one appropriate informational representation;wherein said at least one analytically rich aspect, characteristic or feature of or from said at least one sensor observation or said at least one sensor observation subject are selected from a spectrum of analytically rich aspects, characteristics or features of or from sensor observations or sensor observation subjects;wherein said at least one analytically rich aspect, characteristic or feature of or from said at least one measure point is selected from a spectrum of analytically rich aspects, characteristics or features of or from measure points;wherein said at least one cyber determination is selected from a spectrum of cyber determinations that may be made regarding or utilizing sensor observations, sensor observation subjects or measure points;wherein said spectrum of cyber determinations that may be made regarding or utilizing sensor observations, sensor observation subjects or measure points includes at least one cyber determination that identifies at least one tell that is utilizable in the accurate or reliable making of at least one selected cyber determination regarding or utilizing sensor observations, sensor observation subjects or measure points;wherein said at least one tell is from a spectrum of sensor-observable tells regarding or utilizing sensor observations, sensor observation subjects or measure points;wherein said at least one selected cyber determination is utilizable for at least one purpose selected from a spectrum of purposes for which cyber determinations regarding or utilizing sensor observations, sensor observation subjects or measure points may be utilized;wherein said concise datasets platform is configurable for making said at least one selected cyber determination in real time, or at one or more times thereafter;wherein said concise datasets platform is configurable for making at least one member selected from the group consisting of (i) one-time single event cyber determinations regarding or utilizing sensor observations, sensor observation subjects or measure points, (ii) intermittently provided cyber determinations regarding or utilizing sensor observations, sensor observation subjects or measure points, or (iii) constantly provided cyber determinations regarding or utilizing sensor observations, sensor observation subjects or measure points;wherein said selected information is from a spectrum of information that may be utilized by said concise datasets platform in the making of said selected cyber determinations;wherein said spectrum of information includes information from at least one sensor observations;wherein said selected information is from one or more points in time, or from one or more periods of time;wherein said at least one sensor observation is made by at least one sensor selected from a spectrum of sensors that are utilized in the making of selected cyber determinations regarding or utilizing sensor observations or sensor observation subjects;wherein said concise datasets platform is configurable and all or part of its resource may be configured for utilization in at least one configuration;wherein said concise datasets platform is scalable in regard to included or utilized concise datasets platform resources to fall at any one point in a range of from a minimum to a maximum, wherein at the minimum said platform is scaled to include or utilize only the resources that are needed in the making of a least complex selected cyber determination, in regard to included or utilized platform resources, and wherein at the maximum said platform is scaled to include or utilize all platform resources;wherein said at least one sensor observation or said at least one sensor observation subject are selected from a spectrum of sensor observations or sensor observation subjects; andwherein said concise datasets platform further comprises utilizing, in any sequence, at least one part of at least one operation selected from the group consisting of (a) first series observation operations, wherein said concise datasets platform is configured for utilizing at least one first series sensor observation, wherein at least one first series sensor observation or at least one subject of said at least one first series sensor observation has at least one previously determined analytically rich aspect, characteristic or feature, said concise datasets platform recognizes said at least one previously determined aspect, characteristic or feature, said concise datasets platform assigns at least one appropriate informational representation regarding said at least one recognized aspect, characteristic or feature of or from said at least one sensor observation or said at least one sensor observation subject, said at least one assigned informational representation is utilizable by said concise datasets platform in the making of at least one selected cyber determination regarding or utilizing said at least one sensor observation or said at least one sensor observation subject, said concise datasets platform includes at least one assigned informational representation of or from said at least one first series observation in at least one first series observation concise dataset,(b) second series observation operations, wherein said concise datasets platform is configured for utilizing at least one second series sensor observation, wherein at least one second series sensor observation or at least one subject of said at least one second series observation has at least one selected yet-to-be-determined analytically rich aspect, characteristic or feature, said concise datasets platform recognizes said at least one yet-to-be-determined analytically rich aspect, characteristic or feature, said concise datasets platform assigns at least one appropriate informational representation regarding said at least one yet-to-be-determined analytically rich aspect, characteristic or feature of or from said at least one sensor observation or said at least one sensor observation subject, said at least one assigned informational representation is utilizable by said concise datasets platform in the making of at least one selected cyber determination regarding or utilizing said sensor observations or sensor observation subjects, said concise datasets platform includes at least one assigned informational representation of or from said at least one second series observation in at least one second series observation concise dataset,(c) measure point operations, wherein said concise datasets platform utilizes at least ... ","26","17/165191","2021-02-02","2021-0158972","2021-05-27","11238992","2022-02-01","Jeffry David Aronson","Jeffry David Aronson","","","","G16H-0050/70","G16H-0050/70 | A61B-0005/0823 | A61B-0005/1101 | A61B-0005/1172 | A61B-0005/1176 | A61B-0005/4222 | A61B-0005/4803 | G06F-0016/2379 | G06F-0016/24558 | G06Q-0050/265 | G16H-0010/60 | G16H-0015/00 | G16H-0040/20 | G16H-0040/67 | G16H-0050/20 | G16H-0070/20 | G06F-0003/011 | G06K-0009/00067 | G06K-0009/00268 | G06K-0009/00362","G16H-050/70","G16H-050/70 | G06F-016/2455 | G16H-040/67 | G16H-070/20 | G16H-015/00 | G16H-050/20 | G16H-010/60 | G06Q-050/26 | G16H-040/20 | A61B-005/1172 | A61B-005/1171 | A61B-005/08 | A61B-005/11 | A61B-005/00 | G06F-016/23 | G06F-003/01 | G06K-009/00","","","","","","4922005005264"
"US","US","P","B2","Platform for secure communications with medical device","A communication platform at least partially implements secure communications between a medical device and a trusted authority (TA) service provider. The secure communications prevent access to the secure communications by the communication platform while permitting access to the secure communications at the medical device and/or at the trusted authority service provider.","1. A trusted authority service provider comprising: a communication element;an authority delegation function programmed to a controller;a master performance manager programmed to the controller; andthe controller to: implement, via the communication element, first secure communications relative to an implantable medical device through a non-IMD-dedicated communication platform, wherein the first secure communications prevent access to the first secure communications by the non-IMD-dedicated communication platform and permit secure access to the first secure communications at the implantable medical device and the trusted authority service provider,implement, via the communication element, second secure communications relative to an IMD application associated with the communication platform,implement, via the authority delegation function, authorizing the communication platform to communicate directly with the implantable medical device, andform, via the mater performance manager, at least a portion of a first one of the first secure communications to change a performance of the implantable medical device in response to a first one of the second secure communications received from the IMD application requesting a change in performance of the medical device.","19","15/751056","2016-08-11","2020-0086128","2020-03-19","11229394","2022-01-25","INSPIRE MEDICAL SYSTEMS, INC.","John Rondoni | Dave Dieken","","","","A61B-0005/361","A61B-0005/361 | A61B-0005/4836 | A61N-0001/37254 | A61N-0001/37282 | G06F-0021/606 | G06F-0021/6245 | H04W-0012/084","A61N-001/372","A61N-001/372 | G06F-021/60 | G06F-021/62 | A61B-005/361 | A61B-005/00 | H04W-012/084","","","","","","4922004000830"
"US","US","P","B2","Display device for medical apparatus","A display device for a medical apparatus including a projection surface arranged on the medical apparatus and configured to present a predetermined display content in a way visible to a user of the medical apparatus, and a projection device arranged on the medical apparatus and configured to project the predetermined display content from a rear side of the projection surface onto the projection surface. The projection surface and the projection device are configured as a head-up display unit for visual field presentation of the display content, and the medical apparatus may be an apparatus for carrying out extracorporeal blood treatment including the display device.","1. A display device of a medical apparatus, comprising: a projection surface mounted to the medical apparatus and configured to present a predetermined display content to a user of the medical apparatus on a front side of the projection surface; anda projection device mounted on the medical apparatus and configured to project the predetermined display content from a rear side of the projection surface onto the projection surface;wherein a predetermined portion of the projection surface is provided for displaying at least one predetermined operating condition of the medical apparatus, and in at least a first and a second operating condition at least one of a predetermined first portion or a predetermined second portion of the projection surface is controllably configured to display the first and second operating conditions in different colors; andwherein the display device further comprises at least one of: at least one interactive element which is arranged on the projection surface and is configured to detect an interaction of the user in a touch-sensitive manner, orat least one manually-operable key, actuator or switch integrated in the projection surface.","10","15/644164","2017-07-07","2018-0015215","2018-01-18","11229731","2022-01-25","B. BRAUN AVITUM AG","Meike Peters | Sebastian Brogger | Bjorn Broker | Bruno Stenzel","10-2016-112886","DE","2016-07-13","A61M-0001/367","A61M-0001/367 | A61M-0001/14 | G03B-0021/62 | G06F-0003/0412 | G06K-0009/00335 | G16H-0040/63 | H04N-0009/3173 | A61M-2205/3306 | A61M-2205/505 | A61M-2205/584 | G03B-0021/00 | G16H-0020/17","A61M-001/36","A61M-001/36 | G06F-003/041 | G16H-020/17 | G16H-040/63 | G03B-021/62 | A61M-001/14 | G06K-009/00 | H04N-009/31 | G03B-021/00","","","","","","4922004001164"
"US","US","P","B2","Extended reality system","Systems and methods are disclosed for recommending products or services by receiving a three-dimensional (3D) model of one or more products; performing motion tracking and understanding an environment with points or planes and estimating light or color in the environment; and projecting the product in the environment.","1. A method for recommending products, comprising: receiving a three-dimensional (3D) model of one or more products;performing motion tracking and understanding an environment with points or planes and estimating light or color in the environment;optimizing points and color features extracted from each image of the environment;correlating different manufacturer'ss product sizes and creating correspondences among different manufacturer products;determining best fitting product based on dimensions of the 3D model plus one or more predetermined gaps;recommending the best fitting product by looking up the correspondences among different manufacturer products; andprojecting the product in the environment.","20","16/736512","2020-01-07","2020-0143558","2020-05-07","11232580","2022-01-25","Bao Tran | Ha Tran","Bao Tran | Ha Tran","","","","G06T-0007/62","G06T-0007/62 | A43B-0003/0005 | A43B-0017/00 | A43D-0001/025 | A45D-0044/005 | B33Y-0080/00 | G06K-0009/00208 | G06K-0009/00369 | G06K-0009/00664 | G06K-0009/2018 | G06K-0009/22 | G06K-0009/4671 | G06K-0009/6206 | G06Q-0030/0631 | G06Q-0030/0643 | G06T-0007/20 | G06T-0007/90 | G06T-0019/20 | H04N-0005/272 | A43B-0003/0015 | A43B-0007/04 | A43B-0007/24 | A43D-2200/60 | A45D-2044/007 | A61F-0002/0059 | A61F-0002/12 | G06T-2207/10028 | G06T-2207/10048 | G06T-2207/30201 | G06T-2219/2012 | G06T-2219/2016 | H04N-2005/2726","G06Q-030/06","G06Q-030/06 | G06T-007/62 | A43D-001/02 | A45D-044/00 | G06T-019/20 | A43B-003/00 | A43B-017/00 | B33Y-080/00 | G06K-009/20 | G06K-009/00 | G06K-009/62 | G06K-009/22 | G06K-009/46 | G06T-007/90 | G06T-007/20 | H04N-005/272 | A61F-002/12 | A61F-002/00 | A43B-007/04 | A43B-007/24","","","","","","4922004003998"
"US","US","P","B2","System and method using optical tags to conduct secure transactions and authentications","A method for limiting exposure to infectious disease by controlling access to a controlled access venue to admit individuals who have tested non-infectious for a particular highly infectious disease or have received a vaccination against the same comprises receiving, at a central website operator server, consumer user identification information from consumer users and storing that information in a consumer user database associated with the server. A biometric identifier from the consumer users is transmitted to the consumer user database and each associated with its respective associated consumer user to form an enrollment record. Identity of a presented individual is collected at a medical certification point and an authority record is created in an accessible authority database. At said medical certification point, an antibody test or vaccination is administered and the same is noted in the authority record. A biometric is collected from a venue presented consumer at a presentation venue, compared to authority records for a match, and the result provided to the venue.","1. A method for authentication applicable, for example, to controlling access to a space at a controlled access venue, wherein the operator of said controlled access venue wishes to knit access to said venue to individuals who have tested non-infectious for a particular highly infectious disease or have received a vaccination against said particular highly infectious disease by qualifying individuals for entry into said venue, comprising: (a) receiving a set of consumer user identification information from each of a plurality of consumer users and storing said sets of consumer user identification information in a consumer user database, each of said sets of consumer user identification information being associated with an associated one of said plurality of consumer users;(b) receiving a biometric identifier from each of said plurality of consumer users, each of said biometric identifiers being associated with an associated one of said plurality of consumer users, and storing said biometric identifiers in said consumer user database with associative information associating each biometric identifier with its associated consumer user to form an enrollment record;(c) verifying the identity of a presented individual at a medical certification authority, by:(i) receiving presented consumer user identification information from the consumer user whose identity is to be verified;(ii) acquiring from a consumer user a locally generated biometric identifier of the type stored in said consumer user database directly from the individual whose identity is to be verified, and associating it with said received presented consumer user information to form a presented set of consumer user identification information;(iii) comparing said presented set of consumer user identification information to the sets of consumer user identification information stored in said consumer user database to determine whether there is a match between consumer user identification information stored in said database and said presented set of consumer user identification information; and(iv) reading the enrollment record associated with said presented individual and accessing medical records to determine whether said presented individual has tested non-infectious for said particular highly infectious disease or otherwise tested as being non-infectious for a particular highly infectious disease, or has received a vaccination against said particular highly infectious disease indicating that the individual is qualified for entry into said venue;(d) generating a comparison verification signal in response to the determination that the presented set of consumer user identification information and the locally generated biometric identifier match a set of consumer user identification information including its associated biometric identifier and that the individual is qualified for entry into said venue, said comparison verification signal generation, orperforming a certification procedure with respect to said presented individual which generates a certification result indicating that the presented individual is non-infectious, and supplementing said enrollment record with such test information and qualifying said individual for entry into said venue;(e) storing said certification result;(f) providing to a particular portable communications device associated with the presented consumer user or to the controlled access venue, in response to a request originated from said particular portable communications device of the consumer user an identification signal to be processed at said venue as is a verification that the presented consumer user has been verified as having been verified as eligible for entry into the controlled access venue; and(g) providing said particular consumer user access to said controlled access venue in response to said identification signal to allow access to said presented consumer user.","15","16/870979","2020-05-10","2021-0350648","2021-11-11","11232663","2022-01-25","Lalit Lodha | Joshua Huntington","Lalit Lodha | Joshua Huntington","","","","G07C-0009/257","G07C-0009/257 | G06Q-0020/40145 | G07C-0009/27","G07C-009/27","G07C-009/27 | G16H-010/40 | A61B-005/1171 | A61B-005/1172 | G06F-021/32 | A61B-005/117 | G06K-009/00 | G07C-009/25 | G06Q-020/40","","","","","","4922004004079"
"US","US","P","B1","Vein thromboembolism (VTE) risk assessment system","A vein thromboembolism (VTE) risk assessment system that includes a casing having a shape adapted to secure a plurality of components with the casing. The casing includes a microphonic sensor, a Photoplethysmography (PPG) sensor, an Inertial Measurement Unit (IMU) sensor, a diaphragm, and a microcontroller. The microphonic sensor captures VTE audio signals indicative of the VTE risk of the user. The PPG sensor measures blood volume changes in a skin area in response to venous hemodynamic changes in a lower limb. The IMU sensor captures seismic signals indicative of the VTE risk of the user. The diaphragm enhances auscultation signals. The microcontroller transmits data to a computing device.","1. A vein thromboembolism (VTE) risk assessment system, the VTE risk assessment system comprising: a handheld electronic device (HEI) with a casing having a shape adapted to secure a plurality of components with said casing comprising:a microphonic sensor to capture audio signals from a user'ss body indicative of a VTE risk of the user;a Photoplethysmography (PPG) sensor configured to measure blood volume changes in a skin area in response to venous hemodynamic changes in a lower limb;an Inertial Measurement Unit (IMU) sensor to capture seismic signals indicative of the VTE risk of the user;a diaphragm to enhance auscultation signals; anda microcontroller to transmit data received from the microphonic sensor, the PPG sensor, and the IMU sensor to a computing device, wherein the computing device is configured to: receive, in one or more temporal windows, a representation of data from one or more of the following: the IMU sensor, the PPG sensor signals, and the microphonic sensor signals;detect features from at least one or more portions of the received representations of data that fall within each of the one or more temporal windows;identify patterns in the detected features based on one or more of the following models: a classification model and a regression model; andusing the identified patterns, calculate, a probability of whether the identified patterns corresponds to the VTE risk of the user.","30","17/099772","2020-11-16","","","11232866","2022-01-25","ACORAI AB","Filip Ludwig Peters","","","","G16H-0050/30","G16H-0050/30 | A61B-0005/0024 | A61B-0005/0205 | A61B-0005/02416 | A61B-0005/6898 | A61B-0005/72 | A61B-0005/7264 | A61B-0005/7275 | G06F-0003/015 | G06K-0009/00536 | G06T-0007/0012 | G16H-0040/67 | G16H-0080/00 | A61B-0007/04 | G06T-2207/30101","A61B-005/00","A61B-005/00 | G16H-050/30 | G16H-040/67 | G16H-080/00 | G06F-003/01 | G06T-007/00 | G06K-009/00 | A61B-005/024 | A61B-005/0205 | A61B-007/04","","","","","","4922004004279"
"US","US","P","B1","Machine learning-based surgical instrument characterization","Data is received that is generated by at least one sensor forming part of a surgical instrument. The sensor(s) on the surgical instrument can characterize use of the surgical instrument in relation to a patient. A force profile segmentation model can construct a force profile using the received data. The force profile includes a plurality of force patterns. The force profile segmentation model includes at least one first machine learning trained using historical surgical instrument usage data. In addition, a plurality of features are extracted from the received data. Thereafter, one or more attributes characterizing use of the surgical instrument are determined by a force profile pattern recognition model using the constructed force profile and the extracted features. The force profile pattern recognition model includes at least one second machine learning model. Data characterizing the determination can be provided (e.g., displayed to a surgeon, etc.).","1. A computer-implemented method comprising: receiving data generated by at least one sensor forming part of a surgical instrument, the at least one sensor characterizing use of the surgical instrument in relation to a patient;constructing, by a force profile segmentation model using the received data, a force profile comprising a plurality of force patterns, the force profile segmentation model comprising at least one first machine learning trained using historical surgical instrument usage data;extracting a plurality of features from the received data;determining, by a force profile pattern recognition model using the constructed force profile and the extracted features, one or more attributes characterizing use of the surgical instrument, the force profile pattern recognition model comprising at least one second machine learning model; andproviding data characterizing the determination.","30","17/318975","2021-05-12","","","11232868","2022-01-25","ORBSURGICAL LTD.","Garnette Sutherland | Amir Baghdadi | Rahul Singh | Hamidreza Hoshyarmanesh | Sanju Lama","","","","G16H-0050/70","G16H-0050/70 | A61B-0017/28 | A61B-0034/30 | A61B-0090/37 | A61B-0090/98 | G06K-0019/0723 | G06N-0020/00 | G16H-0015/00 | G16H-0020/40 | G16H-0040/20 | G16H-0040/40 | G16H-0040/67 | G16H-0050/20 | G16H-0070/20 | H04L-0063/0421 | A61B-2017/00057 | A61B-2017/00075 | A61B-2090/064 | A61B-2090/372 | G06F-0003/14","G16H-050/70","G16H-050/70 | G16H-040/67 | G16H-050/20 | G16H-070/20 | G16H-040/40 | G16H-040/20 | G06K-019/07 | G16H-015/00 | G16H-020/40 | H04L-029/06 | A61B-034/30 | A61B-017/28 | A61B-090/00 | A61B-090/98 | G06N-020/00 | G06F-003/14 | A61B-017/00","","","","","","4922004004281"
"US","US","P","B2","Repetitive human activities abnormal motion detection","Abnormal motions are detected in sensor data collected with respect to performance of repetitive human activities. An autoencoder network model is trained based on a set of standard activity. Repetitive activity is extracted from sensor data. A first score is generated indicative of distance of a repetition of the repetitive activity from the standard activity. The repetitive activity is used to retrain the autoencoder network model, using weights of the autoencoder network model as initial values, the weights being based on the training of the autoencoder network model using the set of standard activity. A second score is generated indicative of whether the repetition is an outlier as compared to other repetitions of the repetitive activity. A final score is generated based on a weighting of the first score and the second score.","1. A method for detection of abnormal motions in sensor data collected with respect to performance of repetitive human activities, comprising: training an autoencoder network model based on a set of standard activity;extracting repetitive activity from sensor data;using the autoencoder network model, generating a first score indicative of distance of a repetition of the repetitive activity from the standard activity;using the repetitive activity to retrain the autoencoder network model, using weights of the autoencoder network model as initial values, the weights being based on the training of the autoencoder network model using the set of standard activity;using the autoencoder network model as retrained, generating a second score indicative of whether the repetition is an outlier as compared to other repetitions of the repetitive activity; andgenerating a final score based on a weighting of the first score and the second score.","19","16/717285","2019-12-17","2021-0177307","2021-06-17","11224359","2022-01-18","ROBERT BOSCH GMBH","Huan Song | Liu Ren | Lincan Zou","","","","A61B-0005/1118","A61B-0005/1118 | G06K-0009/00335 | G06F-0003/017","A61B-005/11","A61B-005/11 | G06K-009/00 | G06F-003/01","","","","","","4922003000796"
"US","US","P","B2","Personalized eye openness estimation","Methods, systems, and devices for personalized (e.g., user specific) eye openness estimation are described. A network model (e.g., a convolutional neural network) may be trained using a set of synthetic eye openness image data (e.g., synthetic face images with known degrees or percentages of eye openness) and a set of real eye openness image data (e.g., facial images of real persons that are annotated as either open eyed or closed eyed). A device may estimate, using the network model, a multi-stage eye openness level (e.g., a percentage or degree to which an eye is open) of a user based on captured real time eye openness image data. The degree of eye openness estimated by the network model may then be compared to an eye size of the user (e.g., a user specific maximum eye size), and a user specific eye openness level may be estimated based on the comparison.","1. A method for detecting a degree to which an eye is open, comprising: capturing real time eye openness image data using a sensor;estimating a degree of eye openness based at least in part on the real time eye openness image data and a set of synthetic eye openness image data comprising known levels of eye openness;identifying a maximum eye openness level of a user during an enrollment procedure, wherein the maximum eye openness level is based on a personalized eye size of the user;estimating a user specific eye openness level based at least in part on the estimated degree of eye openness and the maximum eye openness level of the user; andtriggering an action based at least in part on the estimated user specific eye openness level.","20","16/239352","2019-01-03","2020-0218878","2020-07-09","11227156","2022-01-18","QUALCOMM INCORPORATED","Eyasu Zemene Mequanint | Shuai Zhang | Yingyong Qi | Ning Bi","","","","G06K-0009/0061","G06K-0009/0061 | A61B-0005/1103 | A61B-0005/1128 | A61B-0005/1176 | G06F-0021/32 | G06K-0009/00604 | G06K-0009/00845 | G06T-0007/00 | G08B-0021/182","G06K-009/00","G06K-009/00 | G06T-007/00 | A61B-005/11 | G06F-021/32 | G08B-021/18 | A61B-005/1171","","","","","","4922003003571"
"US","US","P","B1","Multilayer information dynamics for activity and behavior detection","Described is a system for activity and behavior detection in a target system. Raw data extracted from various heterogeneous sources of the target system is fused across spatial and temporal scales into a multi-graph representation. Information flows of the multi-graph representation are analyzed using a set of multi-layer information dynamic measures. Based on the set of multi-layer information dynamic measures, at least one of an economic and social indicator of emerging activity of interest in the target system is derived. The indicator is then used for prediction of future activity of interest in the target system.","1. A system for detecting activities and behaviors in a target system, the system comprising: one or more processors and a non-transitory computer-readable medium having executable instructions encoded thereon such that when executed, the one or more processors perform operations of: fusing raw data extracted from various heterogeneous sources of the target system across spatial and temporal scales into a multi-graph representation;quantifying flow complexity of the multi-graph representation using a flow entropy measure which captures flow event density normalized by flow distances;determining flow transfer dynamics on the multi-graph representation using a flow transfer entropy measure;identifying at least one of an economic and a social indicator of an activity of interest in the target system based on the flow entropy measure and the flow transfer entropy measure;detecting the activity of interest in the target system based on the at least one of the economic and the social indicator;outputting an alert of the detected activity of interest, wherein the alert is codified with at least one of an alert identification, an alert-time-stamp, and a location; andbased on the alert, directing one or more sensors toward the activity of interest to capture imagery of the activity of interest.","27","15/497202","2017-04-25","","","11227162","2022-01-18","HRL LABORATORIES, LLC","Tsai-Ching Lu | Kang-Yu Ni | Ryan M. Uhlenbrock","","","","G06K-0009/00771","G06K-0009/00771 | G06K-0009/62 | G06K-0009/6292 | G06N-0020/00 | A61B-0005/103 | G06F-0015/00 | G06Q-0030/02 | G06Q-0099/00 | G08B-0013/19615","G06K-009/00","G06K-009/00 | G06K-009/62 | G06N-020/00 | G08B-013/196 | A61B-005/103 | G06F-015/00 | G06Q-099/00 | G06Q-030/02","","","","","","4922003003577"
"US","US","P","B2","Media content tracking of users' gazing at screens","A method includes receiving a user identifier and instructing display systems to display media content based on the user identifier. Each display system has a corresponding screen. The method also includes receiving image data from an imaging system configured to have a field of view arranged to capture images of a user. The method further includes determining gaze characteristics of the user including a gaze target of the user. The method further includes determining whether the gaze target corresponds to one of the screens. When the gaze target corresponds to one of the screens, the method includes determining a time period of gaze engagement with the corresponding screen. The method also includes storing at least one of the gaze characteristics and the media content or an identifier of the media content displayed on the screen corresponding to the gaze target.","1. A method comprising: receiving, at data processing hardware, user identifiers associated with a plurality of users located in a display environment, each user identifier comprising a uniform resource locater (URL) indicating a respective genre of media content relating to the associated user;instructing, by the data processing hardware, each of a plurality of display systems within the display environment to concurrently display genres of media content based on the URLs associated with the plurality of users, each of the display systems having a corresponding screen concurrently viewable by the plurality of users located in the display environment;receiving, at the data processing hardware, image data from an imaging system configured to have a field of view arranged to capture images of the plurality of users while the plurality of users view the corresponding screens of the plurality of display systems;determining, by the data processing hardware, collective group gaze characteristics for the plurality of users based on the image data, the collective group gaze characteristics comprising gaze targets for each of the plurality of users;for each user of the plurality of users, determining, by the data processing hardware, (i) a time period of gaze engagement with a corresponding screen based on the respective gaze target of the respective user and (ii) the respective genre of media content being displayed on the corresponding screen during the time period of gaze engagement; andfor at least one genre of media content associated with one of the plurality of users, generating, by the data processing hardware, a collective gaze engagement metric indicating a collective time period of gaze engagement based on an aggregate of the time periods of gaze engagement by the plurality of users with the at least one genre of media content.","20","16/843943","2020-04-09","2020-0242659","2020-07-30","11227307","2022-01-18","KELLOGG COMPANY","Gustav Hoffman | Ganapa Sashidhara Murthy","","","","G06Q-0030/0246","G06Q-0030/0246 | A61B-0005/0077 | A61B-0005/1114 | A61B-0005/163 | A61B-0005/7264 | G06F-0003/013 | G06F-0003/0304 | G06F-0003/1446 | G06K-0009/0061 | G06K-0009/00268 | G06K-0009/00604 | G06Q-0030/02 | H04N-0021/41415 | H04N-0021/4223 | H04N-0021/441 | H04N-0021/44204 | H04N-0021/44218 | H04N-0021/44222 | H04N-0021/4532 | A61B-2503/12 | A61B-2576/00","G09G-005/00","G09G-005/00 | G06Q-030/02 | G06K-009/00 | G06F-003/14 | H04N-021/414 | H04N-021/441 | H04N-021/45 | A61B-005/00 | H04N-021/4223 | G06F-003/03 | H04N-021/442 | G06F-003/01 | A61B-005/11 | A61B-005/16","","","","","","4922003003721"
"US","US","P","B2","Method and system for user interaction through user posture recognition","An user interaction system for performing interaction with a user based on recognition of a sitting posture of the user includes a posture recognition seat including a posture recognition sensor for recognition the sitting posture of the user, and at least one communication module for communication with an external device, a posture mimicking animated device including a communication module for performing communication with the communication module of the posture recognition seat, wherein the posture mimicking animated device visually mimics the sitting posture of the user or visually or audibly guide the sitting posture of the user, in accordance with state information about the sitting posture of the user determined based on a sensing result obtained by the posture recognition sensor, and a mobile application installed in a communication device possessed by the user, wherein the mobile application receives synchronized information related to the sitting posture of the user via communication with the posture recognition seat, and generates user interaction information corresponding to the received information and displays the user interaction information on an application screen.","1. A user interaction system for performing interaction with a user based on recognition of a sitting posture of the user, the system comprising: a posture recognition seat including a posture recognition sensor for recognition of the sitting posture of the user, and at least one communication module for communication with an external device;a posture mimicking animated device including a communication module for performing communication with the communication module of the posture recognition seat, wherein the posture mimicking animated device visually mimics the sitting posture of the user or visually or audibly guide the sitting posture of the user, in accordance with state information about the sitting posture of the user determined based on a sensing result obtained by the posture recognition sensor; anda mobile application installed in a communication device possessed by the user, wherein the mobile application receives synchronized information related to the sitting posture of the user via communication with the posture recognition seat, and generates user interaction information corresponding to the received information and displays the user interaction information on an application screen,wherein the posture recognition seat includes:the posture recognition sensor including a plurality of pressure sensors spaced apart from each other so that an entirety of a seated surface of the seat is covered with the plurality of pressure sensors;a posture recognition module configured to determine a current sitting posture of the user based on a plurality of sensed values acquired by the plurality of pressure sensors to obtain state information about the determined sitting posture; andthe at least one communication module configured to transmit the state information obtained from the posture recognition module to each of the posture mimicking animated device and the mobile application,wherein the posture recognition module is configured to compare the obtained plurality of sensed values with a posture table storing, therein, information pre-classified based on the state of the sitting posture of the user to obtain the state information about the sitting posture of the user,wherein the state information includes posture values that indicate a correct-posture sitting state, a left-inclined sitting state, a right-inclined sitting state, a front-inclined sitting state, a rear-inclined sitting state, and a legs-crossing sitting state,wherein the posture mimicking animated device includes a lower body; an upper body coupled to a top of the lower body; legs respectively coupled to both sides of the lower body; a supporter on which the lower body is seated; and a posture mimicking controller,wherein the upper body is coupled to the lower body via a motor and, thus, under motor control of the posture mimicking controller, performs a rolling rotation relative to the lower body so that the left-inclined sitting state and the right-inclined sitting state are implemented, and performs a pitching rotation relative to the lower body so that the front-inclined sitting state and the rear-inclined sitting state are implemented,wherein the legs extend toward an outer surface of the supporter and are suspended so as not to touch a ground on which the supporter is placed, and is coupled to the lower body via a motor to perform a rolling rotation relative to the lower body under the motor control of the posture mimicking controller such that the legs-crossing sitting state is implemented,wherein the posture mimicking animated device includes: face-shaped portions made of a light-transmitting material and shaping a face and disposed on an upper front side of the upper body; andcolor light emitting modules disposed inside the upper body in positions corresponding to the face-shaped portions, respectively,wherein under control of light emission of the color light emitting modules by the posture mimicking controller, a predetermined color corresponding to the state information on the sitting posture is rendered through the face-shaped portions, such that the state information about the sitting posture is visually recognized by the user,wherein the mobile application is configured to display a two-dimensional or three-dimensional animation object having the same shape as a shape of the posture mimicking animated device on the application screen,wherein the mobile application is configured to allow the animation object visualizing the posture mimicking animated device to be processed to express a visual effect corresponding to mimicking motion and color lighting of the posture mimicking animated device based on the synchronized information received from the posture recognition seat,wherein the mobile application is configured to display additional information related to the sitting posture of the user on the application screen,wherein the additional information includes at least one of a sitting time duration, a temporal percentage of each posture for the sitting time duration, a period-specific statistics about the temporal percentage, or user'ss health state information based on the sitting posture,wherein when information on whether the user has sat on the seat is not received from the posture recognition seat at an appointment time set as a time when the user should sit thereon or a corresponding registration schedule of the user,the posture mimicking animated device or the mobile application is configured to display a predetermined visual effect for indicating that the appointment or the schedule is not conducted through the animation object or the posture mimicking animated device.","3","16/998416","2020-08-20","2020-0405221","2020-12-31","11219409","2022-01-11","MORETHINGS CO., LTD.","Jae Kyung Kwak | Kwang Dek An | Ji Won Oh | Hwan Il Park | Jeong Min Han","10-2018-0020389","KR","2018-02-21","A61B-0005/486","A61B-0005/486 | A61B-0005/4561 | A61B-0005/6891 | A61B-0005/742 | G06F-0003/011 | G06Q-0010/1095 | G06T-0013/40 | G06T-0013/80 | G08B-0005/22 | G09B-0019/00 | G16H-0020/30 | G16H-0040/67 | H04M-0001/72427 | A61B-2562/0247 | A61B-2562/04 | H04W-0004/80","A61B-005/00","A61B-005/00 | G16H-020/30 | G16H-040/67 | H04M-001/72427 | G06F-003/01 | G06Q-010/10 | G06T-013/40 | G06T-013/80 | G08B-005/22 | G09B-019/00 | H04W-004/80","","","","","","4922002000862"
"US","US","P","B2","Method of generating a patient-specific bone shell","The exemplary embodiments of the present disclosure are described and illustrated below to encompass methods and devices for designing patient specific prosthetic cutting jigs and, more specifically, to devices and methods for segmenting bone of the knee and the resulting cutting guides themselves. Moreover, the present disclosure relates to systems and methods for manufacturing customized surgical devices, more specifically, the present disclosure relates to automated systems and methods of arthroplasty cutting guides, systems and methods for image segmentation in generating computer models of knee joint.","1. A method for generating a bone model from radiographic images comprising: obtaining a calibration target with at least one radio opaque member having a geometry with a recognizable orientation;imaging a bone with the calibration target in a known pose;recognizing the geometry by image processing of the imaging;determining pose parameters from the geometry and the known pose, where determining the pose parameters includes associating the geometry of each radio opaque member of the at least one radio opaque member with a corresponding three dimensional radio opaque member of at least one three dimensional radio opaque member; andgenerating and outputting a bone contour from the imaging and the pose parameters.","20","16/054478","2018-08-03","2019-0015213","2019-01-17","11219526","2022-01-11","ZIMMER, INC.","Mohamed Rashwan Mahfouz","","","","A61F-0002/30942","A61F-0002/30942 | A61B-0008/4245 | A61B-0034/10 | A61F-0002/3094 | A61F-0002/3601 | A61F-0002/389 | A61F-0002/3859 | A61F-0002/4003 | G06F-0017/18 | G06T-0019/00 | G16H-0050/50 | G16Z-0099/00 | A61B-2034/102 | A61B-2034/108 | A61F-0002/38 | A61F-2002/30943 | A61F-2002/30948 | A61F-2002/3863 | A61F-2002/3895 | G06K-0009/00 | G06T-2210/41","A61F-002/30","A61F-002/30 | G06K-009/00 | A61F-002/38 | A61B-008/00 | G06T-019/00 | G16H-050/50 | G16Z-099/00 | G06F-017/18 | A61B-034/10 | A61F-002/36 | A61F-002/40","","","","","","4922002000979"
"US","US","P","B2","Analyte sensor and apparatus for insertion of the sensor","An apparatus for insertion of a medical device in the skin of a subject is provided.","1. An insertion assembly, comprising: (a) an on-body unit, comprising: a housing comprising a top surface and a bottom surface, wherein the top surface comprises an opening of the top surface, wherein the bottom surface comprises an opening of the bottom surface, and wherein a longitudinal axis extends through the opening of the top surface and the opening of the bottom surface;a glucose sensor comprising at least two electrodes; andsensor electronics disposed within the housing and coupled with the glucose sensor; and(b) an inserter, comprising: a proximal end, a distal end, and an interior; anda sharp, wherein the on-body unit and the sharp are entirely disposed in the interior of the inserter,wherein at least a portion of the glucose sensor is disposed in the sharp,wherein the sharp extends through the opening of the top surface and the opening of the bottom surface along the longitudinal axis when the on-body unit is in a first position,wherein the inserter is configured to advance the on-body unit and the sharp from the first position to a second position such that the sharp pierces skin of a user and the housing of the on-body unit is secured to the skin of the user in the second position,wherein the distal end of the inserter is configured to be positioned on the skin of the user before advancement of the on-body unit and the sharp,wherein the inserter is further configured to automatically retract the sharp from within the user and entirely into the interior of the inserter and leave a part of the glucose sensor in the skin of the user, andwherein the distal end of the inserter is further configured to be removed from the skin of the user after automatic retraction of the sharp from within the user.","30","17/221169","2021-04-02","2021-0219887","2021-07-22","11213229","2022-01-04","ABBOTT DIABETES CARE INC.","Phillip Yee | Christopher A. Thomas | Udo Hoss | Lei He | Michael R. Love","","","","A61B-0005/14865","A61B-0005/14865 | A61B-0005/002 | A61B-0005/0004 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/02055 | A61B-0005/14503 | A61B-0005/14532 | A61B-0005/6849 | A61B-0005/6898 | A61B-0005/72 | A61B-0005/742 | A61B-0005/7405 | A61B-0005/7425 | A61B-0005/7455 | A61M-0005/158 | A61M-0005/1723 | C12Q-0001/001 | C12Q-0001/006 | G06K-0007/10366 | H04L-0067/12 | A61B-2560/0214 | A61B-2560/0412 | A61B-2560/0475 | A61B-2560/0487 | A61B-2562/0295 | A61M-0005/3286 | A61M-2005/1726 | A61M-2205/33 | A61M-2205/3584 | A61M-2205/502 | A61M-2230/201","A61B-005/00","A61B-005/00 | A61B-005/1486 | A61B-005/0205 | G06K-007/10 | A61B-005/145 | A61M-005/158 | C12Q-001/00 | H04L-029/08 | A61M-005/172 | A61M-005/32","","","","","","4922001000948"
"US","US","P","B2","Detection device and detection method","The present disclosure relates to the field of brainwave technology, and provides a detection device and a detection method. The detection device includes: a brainwave acquisition circuit configured to collect a brainwave signal of a user in the case that a distance between the user and a to-be-detected object is smaller than a threshold; and a processing circuit configured to process the collected brainwave signal to acquire brainwave information, and acquire a detection result of the to-be-detected object in accordance with the brainwave information. The brainwave information includes at least one of olfactory information and gustatory information of the user.","1. A detection device, comprising: a brainwave acquisition circuit configured to collect a brainwave signal of a user;a processing circuit configured to process the brainwave signal to acquire brainwave information, and acquire a detection result of a to-be-detected object by the detection device in accordance with the brainwave information; andan image collection circuit in communication with the processing circuit and configured to collect image data about the to-be-detected object,wherein the brainwave information comprises at least one of olfactory information and gustatory information of the user; andwherein the processing circuit is further configured to process the image data collected by the image collection circuit to acquire visual information about the to-be-detected object, and acquire the detection result of the to-be-detected object in accordance with the visual information and the brainwave information.","17","15/779051","2017-10-26","2021-0165487","2021-06-03","11216068","2022-01-04","BOE TECHNOLOGY GROUP CO., LTD.","Wenchu Dong","2017-10329277","CN","2017-05-11","G06F-0003/015","G06F-0003/015 | A61B-0005/381 | A61B-0005/486 | A61B-0005/72 | A61B-0005/7267 | G06K-0009/00973 | G06K-0009/6256 | G06K-2009/00939","G06F-003/01","G06F-003/01 | A61B-005/381 | A61B-005/00 | G06K-009/00 | G06K-009/62","","","","","","4922001003762"
"US","US","P","B2","Systems and methods for automating validation and quantification of interview question responses","In an illustrative embodiment, systems and methods for automating candidate video assessments include receiving a submission from a candidate for an available position including baseline response video segments and question response video segments. The system can determine, from detected nonverbal features within the baseline response video segments, nonverbal baseline scores. For each of the interview questions, candidate response attributes can be detected including a response direction, a response speed, and nonverbal features. A nonverbal reaction score is calculated from the detected nonverbal features and the baseline scores. A response score can be calculated from the response direction and response speed, and a trustworthiness score is determined based on a correspondence between the response score and the nonverbal reaction score. A next interview question can be determined in real-time from a benchmarked version of the response score. Overall scores reflecting candidate trustworthiness can be presented within a user interface screen.","1. A system for automatically conducting an objective evaluation of nonverbal responses made by a candidate for an available position, the system comprising: processing circuitry; anda non-transitory computer readable memory coupled to the processing circuitry, the non-transitory computer readable memory storing machine-executable instructions, wherein the machine-executable instructions, when executed on the processing circuitry, cause the processing circuitry to receive, from a first remote computing device of a first party via a network, a video submission from the candidate for the available position, wherein the video submission includes a plurality of baseline response video segments,determine, based on one or more nonverbal features detected within one or more of the plurality of baseline response video segments, at least one baseline nonverbal reaction score for the candidate,conduct, in real-time, an automated interview session comprising a set of interview questions selected from a plurality of close-ended interview questions, wherein each question of the plurality of close-ended interview questions is configured to elicit a response from the candidate indicating one of a respective set of potential answers, wherein the automated interview session comprises a) presenting, to the candidate via the first remote computing device, a selected interview question of the plurality of close-ended interview questions,b) receiving, from the first remote computing device responsive to the selected interview question, a question response video segment,c) identifying, from the question response video segment, a given answer of the respective set of potential answers,d) detecting candidate response attributes from the question response video segment, wherein the candidate response attributes include one or more prosodic features and/or one or more facial expression features,e) calculating, from the candidate response attributes and based on the at least one baseline nonverbal reaction score, at least one response nonverbal reaction score, andf) repeating steps a) through e) one or more times by selecting, based at least in part on the given answer and the at least one response nonverbal reaction score for a present interview question of the plurality of close-ended interview questions, a next interview question of the plurality of close-ended interview questions as the selected interview question,analyze the given answers to the set of interview questions and the response nonverbal reaction scores corresponding to the set of interview questions to identify an open-ended question of a plurality of potential open-ended questions targeted to the candidate,receive, from the first remote computing device responsive to the open-ended question, an open-ended video segment,apply at least one natural language classifier to the open-ended video segment to map a verbal portion of the open-ended video segment to a set of personality aspects, andprepare, for review by a second party, candidate interview results comprising at least one candidate score.","18","17/160165","2021-01-27","2021-0233031","2021-07-29","11216784","2022-01-04","CUT-E ASSESSMENT GLOBAL HOLDINGS LIMITED","Achim Preuss | Richard Justenhoven | Mario Martella | Andrey Matveev | Nicholas Martin","","","","G06Q-0010/1053","G06Q-0010/1053 | A61B-0005/164 | A61B-0005/165 | A61B-0005/167 | G06K-0009/00221 | G06K-0009/00302 | G06K-0009/00315 | G06K-0009/00335 | G09B-0007/02 | G10L-0015/1807 | G10L-0025/63","G06K-009/00","G06K-009/00 | G06Q-010/10 | G09B-007/02 | G10L-015/18 | A61B-005/16 | G10L-025/63","","","","","","4922001004472"
"US","US","P","B2","Information processing apparatus, information processing method, computer-readable medium, and biological signal measurement system","An information processing apparatus includes a display control unit. The display control unit is configured to perform control to display a signal source in a superimposed manner on a plurality of biological tomographic images sliced in a predetermined direction, the signal source corresponding to a part of biological data indicating a temporal change of a biological signal, and initially display, in a display region of a screen of a display unit, a biological tomographic image on which a predetermined signal source is superimposed among the plurality of sliced biological tomographic images.","1. An information processing apparatus, comprising: a display control unit configured to perform control to display, in a region of an analysis screen displayed by a display device, one or more signal sources in a superimposed manner on a plurality of sliced biological tomographic images, of a brain of a human subject, that are sliced in a particular direction, each signal source corresponding to a part of biological data indicating a temporal change of a biological signal and representing a dipole estimation result determined based on the biological data, each signal source superimposed at a particular position on at least one sliced biological tomographic image to indicate a point position associated with the signal source in an image portion of the brain of the human subject that is shown in the at least one sliced biological tomographic image, anddisplay, in a display region of a display screen displayed by the display device, two or more sliced biological tomographic images of the plurality of sliced biological tomographic images, such that the two or more sliced biological tomographic images are arranged in a certain direction in the display region of the display screen,wherein the two or more sliced biological tomographic images include a particular sliced biological tomographic image, of the plurality of sliced biological tomographic images, that is displayed in the display region in accordance with a particular condition, and other sliced biological tomographic images of the two or more sliced biological tomographic images are arranged in the certain direction in relation to the particular sliced biological tomographic image in the display region of the display screen.","20","16/038227","2018-07-18","2018-0325483","2018-11-15","11207044","2021-12-28","RICOH COMPANY, LTD.","Michinari Shinohara | Yutaka Yagiura | Daisuke Sakai","2017-053716 | 2017-191739","JP | JP","2017-03-17 | 2017-09-29","A61B-0006/5211","A61B-0006/5211 | A61B-0005/00 | A61B-0005/24 | A61B-0005/318 | A61B-0005/369 | A61B-0005/389 | A61B-0006/032 | A61B-0006/463 | A61B-0006/468 | A61B-0006/56 | G06F-0003/015 | G06F-0003/0482 | G06F-0003/04817 | G06T-0007/11 | G06T-2207/10088 | G06T-2207/30016","A61B-006/00","A61B-006/00 | G06T-007/11 | G06F-003/0481 | G06F-003/0482 | A61B-005/00 | A61B-006/03 | G06F-003/01 | A61B-005/24 | A61B-005/318 | A61B-005/369 | A61B-005/389","","","","","","4921053000943"
"US","US","P","B2","3D navigation system and methods","A 3D navigation system and methods for enhancing feedback during a medical procedure, involving: an optical imaging system having an optical assembly comprising movable zoom optics and movable focus optics, a zoom actuator for positioning the zoom optics, a focus actuator for positioning the focus optics, a controller for controlling the zoom actuator and the focus actuator in response to received control input, at least one detector for capturing an image of at least one of a target and an obstacle, the at least one detector operable with the optical assembly, and a proprioception feature operable with the optical imaging system for generating a 3D perception, the proprioception feature comprising a communication feature for providing 3D information, the 3D information comprising real-time depth information in relation to real-time planar information within an interrogation volume.","1. A 3D navigation system for enhancing feedback during a medical procedure, the system comprising: an optical imaging system comprising: an optical assembly comprising movable zoom optics and movable focus optics;a zoom actuator for positioning the zoom optics;a focus actuator for positioning the focus optics;a controller for controlling the zoom actuator and the focus actuator in response to received control input;at least one detector for capturing an image of at least one of a target and an obstacle, the at least one detector operable with the optical assembly;a proprioception feature operable with the optical imaging system for generating and enhancing a 3D perception, the proprioception feature comprising a communication feature for providing 3D information, the communication feature comprising a tracked tool and a capture feature, the capture feature comprising a 3D infrared (IR) optical tracking stereo camera, the 3D information comprising real-time depth information in relation to real-time planar information in relation to an interrogation volume, the capture feature configured to acquire 3D image data by inwardly precessing in a spiral pattern and outwardly precessing in the spiral pattern within an interrogation volume, whereby acquired 3D image data is provided, and the capture feature having a sensory device for translating the acquired 3D image data into a usable form, whereby translated acquired image data is provided; anda display device for presenting the 3D information, based on the translated acquired 3D image data, the 3D information applicable to a particular context of use,the zoom optics and the focus optics respectively independently movable by the controller by way of the zoom actuator and the focus actuator, respectively, andthe optical imaging system configured to operate at a minimum working distance from at least one of the target and the obstacle, the working distance defined between an aperture of the optical assembly and at least one of the target and the obstacle,whereby feedback during the medical procedure is enhanceable.","20","16/346498","2016-10-31","2019-0254757","2019-08-22","11207139","2021-12-28","SYNAPTIVE MEDICAL INC.","Cameron Anthony Piron | Michael Frank Gunter Wood","","","","A61B-0034/20","A61B-0034/20 | A61B-0005/742 | A61B-0005/7405 | A61B-0005/7455 | A61B-0008/0841 | A61B-0034/00 | A61B-0034/30 | A61B-0034/76 | A61B-0090/20 | A61B-0090/30 | A61B-0090/361 | A61B-0090/37 | G02B-0021/0012 | G02B-0021/22 | G02B-0021/36 | G06F-0003/167 | A61B-0017/3421 | A61B-2017/00973 | A61B-2034/107 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2057 | A61B-2034/2063 | A61B-2034/2065 | A61B-2034/2068 | A61B-2034/2074 | A61B-2090/309 | A61B-2090/363 | A61B-2090/3614 | A61B-2090/371 | A61B-2090/378 | A61B-2090/508","A61B-005/00","A61B-005/00 | A61B-034/20 | A61B-090/20 | A61B-090/00 | A61B-008/08 | G02B-021/00 | G02B-021/22 | G02B-021/36 | G06F-003/16 | A61B-090/30 | A61B-034/30 | A61B-034/00 | A61B-034/10 | A61B-017/34 | A61B-017/00 | A61B-090/50","","","","","","4921053001038"
"US","US","P","B2","Vehicle and control method for the same","A vehicle is provided to include a bio-signal sensor that detects a bio-signal of a user, a display device that displays an image and a controller that determines at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal. The controller accumulates a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity and transmit a control signal to the display device to display the accumulated positivity index.","1. A vehicle, comprising: a bio-signal sensor configured to detect a bio-signal of a user;a display device configured to display an image; anda controller configured to determine at least one of a positivity of the user or change amount of the positivity of the user based on the detected bio-signal,accumulate a positivity index when at least one of the positivity or change amount of the positivity is equal to or greater than a predetermined positivity or a predetermined change amount of the positivity, andtransmit a control signal to the display device to display the accumulated positivity index,wherein the display device is configured to display a cymatics image which is transformed according to a frequency, and the controller is configured to determine at least one of a frequency or size of the cymatics image in proportion to the positivity index, and transmit a control signal to the display device to display the cymatics image having at least one of the determined frequency or size.","18","16/693239","2019-11-23","2020-0215970","2020-07-09","11203292","2021-12-21","HYUNDAI MOTOR COMPANY | KIA MOTORS CORPORATION","Jinmo Lee | Kaangdok Yee | Jung Keun You | Joongkwan Kim","10-2019-0001744","KR","2019-01-07","B60Q-0009/00","B60Q-0009/00 | A61B-0005/165 | A61B-0005/18 | A61B-0005/6893 | A61B-0005/743 | A61B-0005/744 | A61B-0005/7405 | A61B-0005/7455 | B60H-0001/00828 | B60H-0003/0007 | B60N-0002/90 | B60R-0001/00 | G06F-0003/011 | G06K-0009/00302 | G06K-0009/00832 | B60N-2002/981 | G06F-2203/011","B60Q-009/00","B60Q-009/00 | A61B-005/00 | A61B-005/16 | A61B-005/18 | B60N-002/90 | B60H-003/00 | B60H-001/00 | B60R-001/00 | G06F-003/01 | G06K-009/00","","","","","","4921052001412"
"US","US","P","B2","Auxiliary image display and manipulation on a computer display in a medical robotic system","A medical system may comprise a stereo display and an input device. The medical system may also comprise a processor configured to generate a three-dimensional image of an anatomical object and cause the three-dimensional image of the anatomical object and a two-dimensional window to be displayed. The processor may also be configured to cause a position and an orientation of the two-dimensional window relative to the three-dimensional image of the anatomical object to be changed on the stereo display by manipulation of the input device. The processor may also be configured to define a cut-plane to indicate a two-dimensional slice of the three-dimensional image of the anatomical object. The processor may also be configured to cause the two-dimensional slice of the three-dimensional image of the anatomical object to be displayed. An orientation of the displayed two-dimensional slice may be different than an orientation of the cut-plane with the three-dimensional image.","1. A medical system comprising: a stereo display;an input device; anda processor configured to: generate a three-dimensional image of an anatomical object from scanned images of the object;cause the three-dimensional image of the anatomical object and a two-dimensional window to be displayed on the stereo display;cause a position and an orientation of the two-dimensional window relative to the three-dimensional image of the anatomical object to be changed on the stereo display, according to manipulation of the input device after associating the two-dimensional window with the input device;define a cut-plane by an intersection of the two-dimensional window with the three-dimensional image of the anatomical object so as to indicate a two-dimensional slice of the three-dimensional image of the anatomical object; andcause the two-dimensional slice of the three-dimensional image of the anatomical object to be displayed on the stereo display, wherein an orientation of the displayed two-dimensional slice of the three-dimensional image of the anatomical object is different than an orientation of the cut-plane with the three-dimensional image of the anatomical object.","20","16/564734","2019-09-09","2019-0388169","2019-12-26","11197731","2021-12-14","INTUITIVE SURGICAL OPERATIONS, INC.","Brian David Hoffman | Rajesh Kumar | David Q. Larkin | Nitish Swarup | Guanghua G. Zhang","","","","A61B-0034/37","A61B-0034/37 | A61B-0001/00193 | A61B-0001/04 | A61B-0001/313 | A61B-0005/055 | A61B-0005/742 | A61B-0018/12 | A61B-0034/10 | A61B-0034/70 | A61B-0034/71 | A61B-0034/76 | A61B-0090/36 | G06F-0003/011 | G06F-0003/016 | G06F-0003/0346 | G06F-0003/0481 | G06F-0003/0486 | G06F-0003/04817 | G06F-0003/04842 | G06F-0003/04845 | G06F-0003/04847 | A61B-0018/1482 | A61B-0034/30 | A61B-0090/361 | A61B-0090/37 | A61B-2018/00577 | A61B-2018/00595 | A61B-2018/00982 | A61B-2018/00994 | A61B-2090/101 | A61B-2090/364 | A61B-2090/374 | A61B-2090/378 | A61B-2090/3782 | A61N-0007/022 | G06F-2203/014 | G06F-2203/04804 | G06F-2203/04806","A61B-034/37","A61B-034/37 | A61B-034/10 | A61B-034/00 | A61B-090/00 | G06F-003/01 | G06F-003/0484 | G06F-003/0481 | G06F-003/0346 | G06F-003/0486 | A61B-001/00 | A61B-001/04 | A61B-001/313 | A61B-005/055 | A61B-005/00 | A61B-018/12 | A61B-090/10 | A61B-018/14 | A61B-034/30 | A61N-007/02 | A61B-018/00","","","","","","4921051000910"
"US","US","P","B2","Brain-computer interface platform and process for classification of covert speech","A device and method are provided for real-time classification of covert speech. The device comprises a plurality of sensors for capturing real-time bio-signal data for brain monitoring in response to mental tasks delivered to a user, and a brain computer interface with memory storing instructions to configure a processor to perform a method of real-time classification of covert speech. The method comprises capturing real-time bio-signal data for brain monitoring in response to mental tasks delivered to a user, pre-processing the raw bio-signal data, extracting a vector of features from the raw bio-signal data, selecting features from the vector of features, building classification model to generate classified covert speech data using the selected features, and controlling a display device with visual elements based on the classified covert speech data.","1. A device for real-time classification of covert speech comprising: a plurality of sensors for capturing real-time raw bio-signal data for brain monitoring in response to covert speech mental tasks delivered to a user, the real-time raw bio-signal data comprising raw electroencephalography (EEG) signal data and raw functional near-infrared spectroscopy (fNIRS) signal data;a brain computer interface with memory storing instructions to configure a processor to: pre-process the raw bio-signal data;extract a vector of features from the EEG signal data and a vector of features from the fNIRS signal data using a spectral estimation method and a time frequency method;select features from the vector of features from the EEG signal data and from the vector of features from the fNIRS signal data using a feature selection method;build a classification model for the EEG signal data and a classification model for the fNIRS signal data to generate classified covert speech data using the selected features using at least one of: a machine learning classifier method; anda regularization parameter; andcontrol a display device with visual elements based on the classified covert speech data; wherein:to pre-process the raw EEG signal data the processor is further configured to: filter the EEG signals using a band-pass filter; andremove electrooculography (EOG) and eye blink artifacts;to pre-process the raw fNIRS signal data the processor is further configured to: filter the fNIRS signals using a low-pass filter; andremove, from each channel trial, a mean of baseline data prior to that trial;to extract the vector of features for the EEG signal data, the processor is further configured to: determine a discrete wavelet transform to one or more decomposition levels;to extract the vector of features for the fNIRS signal data, the processor is further configured to: determine a mean value of oxygenated hemoglobin concentration change;to select features from the vector of features for the EEG signal data, the processor is further configured to: perform runs of K-fold cross validation (CV) on data using regularized linear discriminant analysis (RLDA) for different values of qamma (Y);select the gamma (Y) resulting in a highest cross-validation accuracy obtained with regularization parameter YEEG; anddetermine a highest cross validation accuracy regularization parameter A*EEG;to select features from the vector of features for the fNIRS signal data, the processor is further configured to: perform runs of K-fold CV on data using RLDA for different values of gamma (Y);select the gamma (Y) resulting in a highest cross-validation accuracy obtained with regularization parameter YfNIRS; anddetermine a highest cross validation accuracy regularization parameter A*fNIRS;to build the classification model for the EEG signal data, the processor is further configured to: employ a RLDA algorithm using regularization parameter EEG; andto build the classification model for the fNRIS signal data, the processor is further configured to: employ the RLDA algorithm using regularization parameter YfNIRS.","19","16/153085","2018-10-05","2019-0107888","2019-04-11","11199904","2021-12-14","HOLLAND BLOORVIEW KIDS REHABILITATION HOSPITAL","Alborz Rezazadeh Sereshkeh | Thomas Tak Kin Chau","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/0042 | A61B-0005/1103 | A61B-0005/14553 | A61B-0005/316 | A61B-0005/374 | A61B-0005/378 | A61B-0005/398 | A61B-0005/725 | A61B-0005/726 | A61B-0005/7207 | A61B-0005/7264 | G06F-0003/167 | G06K-0009/00523 | A61B-0005/4803 | A61B-0005/7203 | A61B-2505/09","G06F-003/01","G06F-003/01 | A61B-005/00 | A61B-005/316 | A61B-005/374 | A61B-005/378 | A61B-005/398 | A61B-005/11 | A61B-005/1455 | G06F-003/16 | G06K-009/00","","","","","","4921051003068"
"US","US","P","B2","System and method for locating and determining substance use","In devices that determine levels of substance use, a substance level in a test sample provided to the testing device by a test subject is determined. A sequence of images are captured to include a portion of the face of the test subject providing the test sample and a display of the testing device. A value indicative of the substance level detected within the test sample is determined. A validity indicator indicating validity of the value indicative of the substance level is determined. A current location is determined. The value indicative of the substance level, the validity indicator, and the current location are sent to a remote server.","1. A software product comprising: stored instructions, stored on non-transitory computer-readable media, that, when executed by a digital processor of a communication device, perform steps for controlling a testing device separate and distinct from the communication device to determine a substance level in a test sample provided to the testing device by a test subject, the stored instructions comprising instructions for: generating, from the communication device, a prompt for the test subject to provide the test sample to the testing device;capturing, using a camera of the communication device, a sequence of images that include the test subject providing the test sample to the testing device, the camera being positioned by a physical positioning mechanism of the testing device to include a portion of a face of the test subject providing the test sample and a display of the testing device;detecting a position of the face within a field of view of the camera;instructing the test subject to reposition the camera when the position indicates that the image may be unsuitable for authenticating an identity of the test subject;receiving, within the communication device and from the testing device, a value indicative of the substance level detected within the test sample;determining a validity indicator defining validity of the value indicative of the substance level, the validity indicator including a liveness indication determinable by: detecting eye movement of the test subject within images of the image sequence; andcounting a first number of blinks made by the test subject while providing the test sample;the liveness indication being based upon the first number;determining, within the communication device, a current location; andsending, from the communication device, the value indicative of the substance level, the validity indicator, and the current location to a remote server.","20","16/379572","2019-04-09","2019-0311101","2019-10-10","11200304","2021-12-14","ROBERT F. NIENHOUSE 1997 DECLARATION OF TRUST","Robert Frank Nienhouse","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/1176 | A61B-0010/00 | G01N-0033/4972 | G06K-0009/00288 | G06K-0009/00912 | H04N-0005/23219 | H04N-0005/23222 | H04W-0064/00 | A61B-2010/0009 | A61B-2010/0087 | G06K-2009/00939","G06F-021/32","G06F-021/32 | H04W-064/00 | H04N-005/232 | G06K-009/00 | G01N-033/497 | A61B-005/1171 | A61B-010/00","","","","","","4921051003466"
"US","US","P","B2","Systems and methods for modeling neural architecture","Systems and methods are described herein for modeling neural architecture. Regions of interest of a brain of a subject can be identified based on image data characterizing the brain of the subject. the identified regions of interest can be mapped to a connectivity matrix. The connectivity matrix can be a weighted and undirected network. A multivariate transformation can be applied to the connectivity matrix to transform the connectivity matrix into a partial correlation matrix. The multivariate transformation can maintain a positive definite constraint for the connectivity matrix. The partial correlation matrix can be transformed into a neural model indicative of the connectivity matrix.","1. A computer-implemented method comprising: identifying, by one or more processors, regions of interest of a brain of a subject based on image data characterizing the brain of the subject, wherein the image data comprises one of magnetic resonance imaging (MRI) data, functional magnetic resonance imaging (fMRI) image data, diffusion weighted imaging (DWI) data, diffusion tensor imaging (DTI) data, electromyogram (EMG) data, electroencephalogram (EEG) data, positron emission tomography (PET) data, magnetoencephalography (MEG) data, Electrocorticography (ECoG), ultrasound imaging data, and a combination thereof;mapping, by the one or more processors, the regions of interest of the image data to a connectivity matrix, wherein the connectivity matrix is a weighted and directed or undirected network;transforming, by the one or more processors, the connectivity matrix into a biomarker parameter, wherein the biomarker parameter corresponds to a defined statistic;analyzing, by the one or more processors, the biomarker parameter according to a probability density function of a statistical cognitive model, wherein the probability density function maps a separate population of the biomarker parameter to a cognitive phenomenon; andgenerating, by the one or more processors, a probability of the cognitive phenomenon based on the probability density function of the statistical cognitive model, wherein the probability quantifies a severity of the cognitive phenomenon.","12","17/193911","2021-03-05","2021-0209761","2021-07-08","11200672","2021-12-14","UNIVERSITY OF SAN FRANCISCO | UNIVERSITY OF NORTH CAROLINA | OHIO STATE INNOVATION FOUNDATION | THE PENN STATE RESEARCH FOUNDATION","Skyler Cranmer | Bruce Desmarais | Shankar Bhamidi | James Wilson | Matthew Denny | Zhong-Lin Lu | Paul Stillman","","","","G06T-0007/0014","G06T-0007/0014 | A61B-0005/0263 | A61B-0005/369 | A61B-0005/4064 | A61B-0005/7267 | G06F-0017/16 | G06F-0017/18 | G06K-0009/6226 | G01R-0033/4806 | G01R-0033/5608 | G01R-0033/56341 | G06T-2207/30016","G06T-007/00","G06T-007/00 | A61B-005/026 | A61B-005/00 | A61B-005/369 | G06F-017/16 | G06F-017/18 | G06K-009/62 | G01R-033/563 | G01R-033/48 | G01R-033/56","","","","","","4921051003828"
"US","US","P","B2","Short imagery task (SIT) research method","The present disclosure relates to biologically and behaviorally based methods of measuring audience response to a short stimulus.","1. A system comprising: a memory including machine executable instructions; andat least one processor to execute the instructions to: control a stimulus presentation device to present a first stimulus based on activation of a sensor;obtain first biometric data from a first subject exposed to the first stimulus for a first period of the first stimulus;obtain second biometric data from a second subject exposed to the first stimulus for the first period of the first stimulus;at least one of event-stamp the first biometric data and the second biometric data with an event indicator for the first period of the first stimulus or time-stamp the first biometric data and the second biometric data with a time indicator for the first period of the first stimulus;synchronize the first biometric data and the second biometric data based on one or more of the event indicator or the time indicator;generate a first synchrony score for the first stimulus based on the synchronization of the first biometric data and the second biometric data;synchronize third biometric data and fourth biometric data, the third biometric data associated with the first subject exposed to the first stimulus for a second period of the first stimulus and the fourth biometric data associated with the second subject exposed to the first stimulus for the second period;generate a second synchrony score for the first stimulus based on the synchronization of the third biometric data and the fourth biometric data;detect a synchrony response pattern between the first synchrony score and the second synchrony score;generate a subject response score based on an average of (1) a value of a first self- report response associated with the first subject in response to the exposure of the first subject to the first stimulus and (2) a value of a second self-report response associated with the second subject in response to the exposure of the second subject the first stimulus;integrate the synchrony response pattern and the subject response score into a physical and nonphysical integrated score; andselect at least one of (1) at least a portion of the first stimulus or (2) a second stimulus to be presented based on the integrated score.","17","16/287440","2019-02-27","2019-0196582","2019-06-27","11200964","2021-12-14","NIELSEN CONSUMER LLC | CITIBANK, N.A","Caleb J. Siefert","","","","G16H-0010/20","G16H-0010/20 | A61B-0003/112 | A61B-0005/0205 | A61B-0005/162 | A61B-0005/165 | G06F-0003/013 | G06K-0009/00892 | G09B-0005/065 | G09B-0005/10 | G09B-0007/073 | A61B-0005/024 | A61B-0005/0531 | A61B-0005/0816","A61B-005/16","A61B-005/16 | G16H-010/20 | G09B-007/073 | G09B-005/10 | A61B-003/11 | A61B-005/0205 | G06F-003/01 | G06K-009/00 | G09B-005/06 | A61B-005/024 | A61B-005/0531 | A61B-005/08","","","","","","4921051004116"
"US","US","P","B2","Hearing assistance device with brain computer interface","The present disclosure relates to communication devices. Such devices may comprise input for receiving sound signal to be processed and presented to a user, and output for outputting the processed signal to a user perceivable as sound. Such processing may be performed by use of a processor for processing the sound signal in dependence of a setting or a set of setting to compensate a hearing loss profile. Further, the communication device may comprise a bio-signal acquisition and amplifier component in communication with a user interface for providing the bio-signals as input to the user interface, the user interface controlling the setting or set of setting for operation of the communication device.","1. A hearing aid device comprising: an input for receiving a sound signal to be processed and presented to a user, and an output for outputting a signal to a user perceivable as sound,a processor for processing the sound signal in dependence of a setting or a set of settings to compensate a hearing loss profile, anda bio-signal acquisition and amplifier component comprising at least one of the following for acquiring bio-signals: an ear EEG electrode configured to be inserted into an ear canal or on a skin-part of the head of a wearer,an implantable EEG electrode configured to be placed under the skin at the head and/or skull of a wearer, andan implantable EEG electrode configured to be placed on the ear canal,wherein the hearing aid device is configured to distinguish between feedback artefacts and howl-like sounds from the environment based on an analysis of EEG signals measured on the user indicating whether a howl-like sound currently being heard is an artefact or a wanted signal.","13","16/917319","2020-06-30","2020-0336847","2020-10-22","11185257","2021-11-30","OTICON A/S","Niels Henrik Pontoppidan | Thomas Lunner | Michael Syskind Pedersen | Lars Ivar Hauschultz | Povl Koch | Graham Naylor | Eline Borch Petersen","2013-172066","EP","2013-06-14","A61B-0005/121","A61B-0005/121 | A61B-0005/165 | A61B-0005/291 | A61B-0005/38 | A61B-0005/6817 | A61B-0005/746 | G06F-0003/013 | G06F-0003/015 | G06K-0009/00604 | H04R-0025/00 | H04R-0025/305 | H04R-0025/505 | H04R-0025/606 | A61B-0005/374 | A61B-0005/375 | A61B-0005/6868 | H04R-0025/40 | H04R-0025/453 | H04R-0025/552 | H04R-0025/70 | H04R-2225/41 | H04R-2225/43 | H04R-2225/61 | H04R-2225/67","H04R-025/00","H04R-025/00 | A61B-005/12 | G06K-009/00 | A61B-005/00 | G06F-003/01 | A61B-005/38 | A61B-005/291 | A61B-005/16 | A61B-005/374 | A61B-005/375","","","","","","4921049000875"
"US","US","P","B2","System and method for delivering sensory stimulation to a user based on a sleep architecture model","The present disclosure pertains to a system and method for providing sensory stimulation (e.g., tones and/or other sensory stimulation) during sleep. The delivery of the sensory stimulation is timed based on a combination of output from a trained time dependent sleep stage model and output from minimally obtrusive sleep monitoring devices (e.g. actigraphy devices, radar devices, video actigraphy devices, an under mattress sensor, etc.). The present disclosure describes determining whether a user is in deep sleep based on this information and delivering sensory stimulation responsive to the user being in deep sleep. In some embodiments, the system comprises one or more sensory stimulators, one or more hardware processors, and/or other components.","1. A system configured to deliver sensory stimulation to a user during deep sleep in a sleep session, the system comprising: one or more sensory stimulators configured to provide sensory stimulation to the user during the sleep session;one or more sensors configured to generate output signals conveying information related to brain activity of the user during the sleep session, the information related to brain activity of the user including information related to sleep depth of the user; andone or more hardware processors coupled to the one or more sensory stimulators, the one or more hardware processors configured by machine-readable instructions to:determine local sleep depths for the user during the sleep session based on the output signals;obtain historical sleep depth information for a population of users, the historical sleep depth information being related to brain activity of the population of users that indicates sleep depth over time during sleep sessions of the population of users;cause a prediction model to be trained based on the historical sleep depth information by providing the historical sleep depth information as input to the prediction model;cause the trained prediction model to output a time dependent predicted sleep stage for the user during the sleep session, the time dependent predicted sleep stage indicating whether the user is in deep enough sleep for stimulation;determine the time dependent predicted sleep stage during the sleep session of the user based on a local maximum in sleep depth for the user relative to average sleep depth for the population of users, and a determination of whether a probability of a particular sleep stage for the population of users breaches a corresponding probability threshold; andcause the one or more sensory stimulators to provide the sensory stimulation to the user based on the time dependent predicted sleep stage over time during the sleep session, the one or more sensory stimulators being caused to provide the sensory stimulation to the user responsive to the time dependent predicted sleep stage indicating the user is in deep enough sleep for stimulation.","15","16/278901","2019-02-19","2019-0254591","2019-08-22","11185281","2021-11-30","KONINKLIJKE PHILIPS N.V.","Gary Nelson Garcia Molina | Birpal Singh Sachdev | William Gaussa | David White","","","","A61B-0005/4812","A61B-0005/4812 | A61B-0005/00 | A61B-0005/369 | A61B-0005/4809 | A61M-0021/00 | G06F-0017/18 | G06N-0020/00 | G16H-0020/30 | G16H-0050/70","A61B-005/00","A61B-005/00 | G06N-020/00 | G06F-017/18 | G16H-020/30 | A61M-021/00 | G16H-050/70 | A61B-005/369","","","","","","4921049000899"
"US","US","P","B2","Emotion inference device and emotion inference system","An emotion inference device and an emotion inference system that are capable of inferring a user's emotion with higher precision. A motorcycle includes an individual personality that is configured on the basis of information on a user from a plurality of products associated with the user, connected to a communication network, and including the motorcycle, an automobile, a rice cooker, a vacuum cleaner, a television receiver, and a refrigerator, the individual personality forms a base personality, and the motorcycle includes an emotion detecting section that detects an emotion.","1. An emotion inference device that is connected, through a communication network, to a plurality of products associated with a user, and that comprises a processor and a memory, wherein the memory stores:information on the user received from the plurality of products through the communication network;an individual personality that is an algorithm to imitate, by machine learning the information, an emotion or emotion change, of the user, that copes with an external factor; anda base personality that is formed by the individual personality and that is the emotion of the user in a calm state,the processor functions as an emotion detecting section that detects the emotion of the user by comparing the emotion or emotion change, of the user, that copes with the external factor and that is formed by executing the individual personality, and the base personality, andthe processor functions as an action control section that controls an action to direct the emotion, of the user, based on the emotion of the user in case deviation between the emotion estimated by the emotion detecting section and the base personality exceeds a predetermined range.","7","16/345018","2017-09-29","2019-0276037","2019-09-12","11186290","2021-11-30","HONDA MOTOR CO., LTD.","Atsushi Ito | Yasumasa Matsui | Kohei Matsuura | Akiko Sakai | Kimihiro Yonekawa","2016-223096","JP","2016-11-16","B60W-0040/09","B60W-0040/09 | A61B-0005/167 | A61B-0005/18 | B60W-0010/06 | B60W-0040/08 | B60W-0050/0097 | G06F-0003/017 | G06K-0009/00302 | G06K-0009/00335 | G06N-0005/04 | G06T-0007/0014 | G08G-0001/16 | G09B-0019/16 | G10L-0025/63 | B60W-2040/0872 | B60W-2050/0029 | B60W-2420/42 | B60W-2420/54 | B60W-2540/043 | B60W-2540/21 | B60W-2540/22 | B60W-2556/10 | G06F-2203/011","B60W-040/09","B60W-040/09 | B60W-010/06 | G06K-009/00 | G06N-005/04 | G10L-025/63 | B60W-040/08 | B60W-050/00 | G08G-001/16 | G06T-007/00 | G06F-003/01 | A61B-005/18 | A61B-005/16 | G09B-019/16","","","","","","4921049001900"
"US","US","P","B2","Wearable computing device","A smart ring includes a curved housing having a U-shape interior storing components including: a curved battery approximately conforming to the curved housing, a semi-flexible PCB approximately conforming to the curved housing and having mounted thereon: a motion sensor for generating motion data from physical perturbations of the smart ring, a memory for storing executable instructions, a transceiver for sending data to a client computer, a temperature sensor, and a processor for receiving motion data and performing executable instructions in response thereto, and a potting material disposed in the interior, forming an interior wall of the smart ring, wherein the potting material encapsulates the components and is substantially transparent to visible light, infrared light, and/or ultraviolet light.","1. A wearable computing device configured to be worn around a finger of a wearer comprising: an external housing having an outer surface, an interior surface and sidewalls, wherein the interior surface and the sidewalls are characterized by approximately a C-shaped cross section and define an interior space, and wherein the interior space is configured to retain a plurality of components;wherein the plurality of components comprises:a curved rechargeable battery disposed within the interior space of the housing, wherein the rechargeable battery is curved to conform to the interior surface;a printed circuit board disposed within the interior space configured to approximately conform to the interior surface;one or more sensors disposed upon the printed circuit board, wherein the one or more sensors are selected from a group consisting of: an accelerometer, a gyroscope, and a motion sensor, wherein the one or more sensors are configured to sense physical perturbations and to output sensed data;a memory disposed upon the printed circuit board, the memory configured to store one or more executable instructions;a short-range communication module disposed upon the printed circuit board, the short-range communications module configured to communicate a first set of data to a client computing device via a first communications protocol;a first temperature sensor configured to provide first temperature data associated with a temperature of a wearer of the wearable computing device;a processor disposed upon the printed circuit board and coupled to the battery, the one or more sensors, the memory, the short-range communication module, and the first temperature sensor, wherein the processor is configured to receive the sensed data, is configured to perform the one or more executable instructions in response to the sensed data, and is configured to direct the short-range communication module to output data to the client computing device; anda potting material disposed in the interior space encapsulating the plurality of components, wherein the potting material forms an interior wall of the wearable computing device, wherein the potting material is substantially transparent to light selected from a group consisting of: visible light, infrared light, and ultraviolet light.","20","17/013348","2020-09-04","2020-0401183","2020-12-24","11188124","2021-11-30","MOTIV (ABC), LLC | PROXY, INC.","Curt C. von Badinski | Michael J. Strasser | Peter Twiss","","","","G06F-0001/163","G06F-0001/163 | A61B-0005/01 | A61B-0005/021 | A61B-0005/0205 | A61B-0005/02416 | A61B-0005/1118 | A61B-0005/1455 | A61B-0005/14532 | A61B-0005/332 | A61B-0005/349 | A61B-0005/681 | A61B-0005/6826 | G01P-0015/00 | G02B-0019/0052 | G02B-0019/0061 | G04G-0021/02 | G04G-0021/025 | G06F-0001/1635 | G06F-0003/014 | G06F-0003/017 | G06F-0003/14 | G06F-0021/32 | G06K-0009/00885 | G06K-0009/00892 | G06K-0009/6202 | G08B-0005/36 | G08B-0021/02 | G08C-0017/02 | H02J-0007/0044 | H02J-0007/0047 | H02J-0007/35 | H02S-0040/22 | H02S-0099/00 | A61B-2560/0214 | A61B-2560/0412 | A61B-2562/146 | A61B-2562/164 | A61B-2562/166 | G02B-0019/0042 | G06K-2009/00939 | G08C-2201/30 | Y02E-0010/52","G06F-001/16","G06F-001/16 | A61B-005/00 | H02J-007/35 | A61B-005/332 | A61B-005/349 | G01P-015/00 | G04G-021/02 | G06F-003/01 | G06K-009/00 | G08B-021/02 | G08C-017/02 | G06F-003/14 | G08B-005/36 | H02J-007/00 | H02S-040/22 | G02B-019/00 | H02S-099/00 | A61B-005/01 | A61B-005/021 | A61B-005/024 | A61B-005/11 | A61B-005/145 | A61B-005/1455 | G06F-021/32 | G06K-009/62 | A61B-005/0205","","","","","","4921049003721"
"US","US","P","B2","Frame optimization system and method","The disclosed embodiments include a system that has a processor for executing computer-executable instructions and a computer-readable storage media for storing the computer-executable instructions. These instructions, when executed by the processor, enable the system to receive prescription data of a patient for corrective lenses and image data associated with the face of the patient. The system is configured to determine lens attributes for the patient based upon the prescription data and also determine facial attributes of the patient from the image data. Based on the lens attributes and the facial attributes of the patient, the system determines at least one frame recommendation for the patient.","1. A system comprising: one or more of at least one camera and at least one image scanner;a display;a processor for executing computer-executable instructions;a computer-readable storage media having stored thereon computer-executable instructions for: receiving prescription data of a patient for corrective lenses input into the system from a user at a user interface or imported into the system by another system,receiving information relative to a lifestyle of the patient,controlling one or more of the at least one camera and the at least one image scanner by a facial contour image module to capture image data associated with a face of the patient from the one or more of the at least one camera and the at least one image scanner,determining lens attributes for the patient based upon the prescription data of the patient, the lens attributes being physical features of the lens,determining facial attributes using the facial image contour module to analyze features of the patient from the captured image data of the patient obtained by the one or more of the at least one camera and the at least one image scanner,determining a first subset of frames of a plurality of frames based on the determined lens attributes,determining a second subset of frames of the plurality of frames based on the determined facial attributes, the second subset of frames having frame dimensions satisfying ranges corresponding to measurements of the facial attributes,determining a third subset of frames of the plurality of frames based on the information relative to the lifestyle of the patient,filtering the plurality of frames to obtain at least one frame recommendation of at least one frame for the patient, the at least one frame being within the determined first subset of frames, the determined second subset of frames, and the determined third subset of frames, andoutputting the at least one frame recommendation for the patient to the display by one of: (i) displaying the at least one frame recommendation as a two-dimensional image on the display, (ii) displaying the at least one frame recommendation as a three-dimensional rotatable image on the display, (iii) displaying the at least one frame recommendation on an image of the patient, (iv) displaying the at least one frame recommendation on images of other people other than the patient that have similar facial attributes as the patient, and (v) outputting a fabricated frame of the at least one frame recommendation.","20","15/565915","2015-04-16","2018-0108436","2018-04-19","11189366","2021-11-30","ESSILOR INTERNATIONAL","John Lastinger | Gabriel Keita","","","","G16H-0010/60","G16H-0010/60 | A61B-0003/00 | A61B-0003/111 | A61B-0005/1072 | A61B-0005/1077 | A61B-0005/1079 | G06F-0016/9535 | G06K-0009/00275 | G06Q-0030/02 | G06Q-0040/08 | G16H-0020/00 | G16H-0020/13 | G16H-0030/40 | G16H-0040/60 | A61B-0003/10","G16H-010/60","G16H-010/60 | G06F-016/9535 | A61B-003/11 | A61B-005/107 | G06Q-030/02 | A61B-003/00 | G16H-040/60 | G16H-020/00 | G16H-030/40 | G16H-020/13 | G06K-009/00 | G06Q-040/08 | A61B-003/10","","","","","","4921049004952"
"US","US","P","B2","Characterizing states of subject","Among other things, a user of a browser is exposed simultaneously to three interfaces: A viewing interface for at least one image of a subject that is stored on a device on which the browser is running, a decision support interface that aids the user in determining the state of the subject based on the image, and a template interface that aids the user in capturing uniform descriptive information about the state of the subject. At least two of the viewing interface, the decision support interface, and the template interface operate cooperatively so that actions of the user with respect to one of the two interfaces causes changes in content exposed by the other of the two interfaces.","1. A computer based method comprising displaying simultaneously to a user in a display: (a) an image of a subject, (b) information about how to express a determination about a state of the subject based on the image of the subject, the information including a representative figure that corresponds to the image of the subject and is associated with predefined words that at least partially describe the state of the subject, and (c) one or more invocable elements for the user to invoke, each of the one or more invocable elements including a single word or a phrase of two or more words representing at least a portion of the determination about the state of the subject, in response to a selection of a particular one of the one or more invocable elements using a first dragging and dropping of the image of the subject onto the particular one of the one or more invocable elements, displaying additional text based on an attribute of the selected particular invocable element, and automatically displaying a tool based on the selected particular invocable element, the tool usable to make a measurement on the image of the subject, the measurement relevant to the attribute of the selected particular invocable element, storing an association between the image of the subject and a numerical result of the measurement, and subsequent to storing the association, in response to a second dragging and dropping by the user of the image of the subject from a second interface onto a first interface that is displayed simultaneously to the second interface in which the image of the subject is displayed, adding the numerical result of the measurement to a structured report expressing the determination about the state of the subject.","19","14/757721","2015-12-23","2016-0124581","2016-05-05","11189369","2021-11-30","LIFETRACK MEDICAL SYSTEMS PRIVATE LTD.","Eric Schulze | Brendan Philip Rees | Dennis Mejia","","","","G16H-0015/00","G16H-0015/00 | A61B-0005/0022 | A61B-0005/0035 | A61B-0005/7246 | A61B-0005/7282 | A61B-0005/742 | A61B-0005/748 | A61B-0005/7425 | A61B-0005/7475 | G06F-0003/0481 | G06F-0003/0484 | G06F-0003/0486 | G06F-0040/106 | G06F-0040/123 | G06F-0040/134 | G06F-0040/169 | G06T-0007/0014 | G06T-0011/60 | G16H-0030/20 | G16H-0030/40 | G16H-0050/20 | A61B-0005/0002 | A61B-0005/0013 | A61B-0005/055 | A61B-0005/7264 | A61B-0005/743 | A61B-0005/7435 | A61B-0005/7485 | A61B-2560/0475 | A61B-2576/02","G06F-003/0486","G06F-003/0486 | G16H-015/00 | G06F-003/0484 | A61B-005/00 | G16H-050/20 | G06F-040/106 | G06F-040/123 | G06F-040/134 | G06F-040/169 | G16H-030/20 | G16H-030/40 | G06F-003/0481 | G06T-007/00 | G06T-011/60 | A61B-005/055","","","","","","4921049004955"
"US","US","P","B2","Instrument and method for monitoring an analyte concentration","A handheld display device for processing continuous sensor data has a communication interface, a graphical user interface having a gesture recognition component, a processor and a memory. The processor continuously receives time dependent sensor data, carbohydrate data, event data, and insulin data. The event data is indicative of a physical state of the subject. The processor determines a sensor data scaling factor, a carbohydrate data scaling factor and an insulin data scaling factor. The processor controls the graphical user interface to render a plot having single time, analyte concentration, carbohydrate amount, and insulin delivery amount axes. The analyte concentration axis is rendered on a first side of the plot and carbohydrate amount and insulin delivery amount axes are rendered on an opposite side of the plot, according to the scaling factors. The graphical user interface can be controlled with single and/or double finger gestures to magnify, shrink or shift axes.","1. A handheld display device for processing sensor data continuously measured by an implantable analyte sensor, comprising: a communication interface configured to receive sensor data from the analyte sensor via a wired or wireless communication channel;a graphical user interface having a gesture recognition component, wherein the graphical user interface is configured for receiving double finger gestures;a processor and a memory, the processor configured to:(a) continuously receive the sensor data via the communication interface, wherein the sensor data is indicative of an analyte concentration in bodily fluid and is time dependent;(b) receive time dependent carbohydrate data via the graphical user interface or via a carbohydrate data transfer interface, wherein the carbohydrate data is indicative of carbohydrate intake by the subject;(c) receive time dependent event data via the graphical user interface or via an event data transfer interface, wherein the event data is indicative of a physical state of the subject;(d) receive time dependent insulin data via the graphical user interface or via an insulin data transfer interface, wherein the insulin data is indicative of an insulin delivery to the subject;(e) determine a sensor data scaling factor, a carbohydrate data scaling factor and an insulin data scaling factor, wherein the scaling factors are based on an adjustable display time frame and the corresponding data received in the adjustable display time frame;(f) render a plot on the graphical user interface, the plot having a single time axis, an analyte concentration axis, a carbohydrate amount axis, and an insulin delivery amount axis, wherein: i. the analyte concentration axis is rendered on a first side of the plot according to the sensor data scaling factor,ii. the insulin delivery amount axis and the carbohydrate amount axis are rendered on a second side opposite to the first side of the plot according to the carbohydrate data scaling factor and the insulin data scaling factor, respectively,iii. the sensor data is rendered continually,iv. the carbohydrate data is rendered continually and/or as a carbohydrate discrete marker, andv. the insulin data is rendered continually and/or as an insulin discrete marker;(g) receive the double finger motion from the gesture recognition component, wherein the double finger gestures comprise a double finger motion parallel to a selected axis selected from the analyte concentration axis, the carbohydrate amount axis, and the insulin delivery amount axis, wherein the double finger motion is descriptive of an increase or decrease of distance between two fingers measured along the selected axis;(h) control the graphical user interface to magnify or shrink only the selected axis using the increase or decrease of the distance between the two fingers measured along the selected axis;(i) receive the single finger motion from the gesture recognition component; and(j) control the graphical user interface to shift the selected axis using the single finger motion.","15","16/143078","2018-09-26","2019-0022314","2019-01-24","11189373","2021-11-30","ROCHE DIABETES CARE, INC.","Wilfried Schmidt | Bernd Steiger","2016-163425","EP","2016-03-31","G16H-0020/17","G16H-0020/17 | A61B-0005/14532 | A61M-0005/1723 | G06F-0003/04845 | G06F-0003/04883 | G16H-0040/63 | G16H-0040/67 | A61M-2205/3569 | A61M-2205/50 | A61M-2205/505 | A61M-2205/52 | A61M-2205/8206 | A61M-2230/201 | G06F-2203/04808 | G16H-0015/00","G06F-003/0484","G06F-003/0484 | G16H-020/17 | G16H-040/63 | G16H-040/67 | A61B-005/145 | A61M-005/172 | G06F-003/0488 | G16H-015/00","","","","","","4921049004959"
"US","US","P","B2","Workstation for neurobiological disorder health professionals","A workstation for neurobiological disorder health professionals that aids professionals tracking, evaluating and archiving clinical and educational progress through a web application using 360-degree video. The invention allows for organized communication between internal and external resources of an institution to tackle patient/student needs that merit immediate attention from specialists either in an individual or group setting. For additional expertise in acute cases, the institution can get access to consult with specialists around the world through the invention's directory of professionals. The invention allows clinical supervisors to train staff and ensure consistency in therapeutic interventions and evaluations, as well as supervisors in a school setting for consistency in educational programming. The invention also accelerates staff training time through targeted supervision.","1. A workstation for educators and clinical professionals, comprising: a workspace module comprising a plurality of cubicles;a tube located in the middle of said workspace module comprising an immersive camera;an Internet access point;a plurality of computers;wherein each computer of said plurality of computers is connected to a secure cloud server comprising: at least one processor;a storage module;a memory module;a video capturing module; andprogram instructions stored on said storage module for execution by said processor, said stored program instructions comprising: program instructions for making video calls;program instructions for recording video;program instructions for recording video calls;program instructions for tagging video;program instructions for creating at least one user profile;program instructions for associating recorded video with said at least one user profile;program instructions for associating said immersive camera to at least one of said plurality of cubicles;wherein tagging video comprises adding a plurality of tags to said video in a live manner while a video call or video recording is in progress, and said plurality of tags is directed at providing information related to the video and unrelated to structuring data;wherein said secure cloud server further comprises a machine learning software module configured to collect video tag and patient or student data, extract and analyze comments and recommendations from the video tag and patient or student data, and provide treatment recommendations based on said video tag and patient or student data;wherein the recorded video is individually uploaded to the cloud server and stitched together as a 360-degree video; andwherein both the individually uploaded recorded video and the 360-degree video are stored separately, allowing a user to access video for either one patient/student or for a complete group of patients/students.","17","16/412483","2019-05-15","2020-0366867","2020-11-19","11190732","2021-11-30","GAMA LLC","Gabriel Jose Marcano Martir | Sheila Eileen Fridman Hurd","","","","H04N-0007/152","H04N-0007/152 | A61B-0005/1113 | A61B-0005/168 | A61B-0005/4076 | G06N-0020/00 | G09B-0005/065 | H04N-0005/23238 | H04N-0007/155 | A61B-2503/06 | A61B-2505/07 | A61B-2505/09 | G06Q-0050/2057 | H04R-0001/1041 | H04R-2420/07","H04N-007/15","H04N-007/15 | G06N-020/00 | H04N-005/232 | G09B-005/06 | A61B-005/11 | A61B-005/00 | A61B-005/16 | G06Q-050/20 | H04R-001/10","","","","","","4921049006300"
"US","US","P","B2","Analyte sensor transmitter unit configuration for a data monitoring and management system","Method and system for providing analyte sensor alignment and retention mechanism for improved connectivity with a transmitter unit for electrical connection, and further including transmitter unit contact pins with metal components to improve electrical conductivity with the analyte sensor in an analyte monitoring and management system is provided.","1. An apparatus comprising: a sensor subassembly comprising a glucose sensor and a sensor seal, wherein the glucose sensor comprises a first portion configured to be electrically coupled with a transmitter and a second portion configured to be inserted through a skin surface and in fluid contact with a bodily fluid of a user; anda transmitter mount, comprising: an opening configured to detachably couple with the transmitter;a base having an aperture through which the second portion of the glucose sensor is configured to pass before being inserted through the skin surface, anda locking mechanism comprising two protrusions made of a molded plastic material, the two protrusions configured to engage the sensor subassembly and secure the sensor subassembly to the base of the transmitter mount in a position for electrical coupling with the transmitter,wherein movement of the sensor subassembly is impeded when the locking mechanism is engaged with the sensor subassembly,wherein the transmitter mount is configured to be adhered to the skin surface of the user by an adhesive layer, andwherein the two protrusions of the locking mechanism are configured to engage the sensor subassembly before the transmitter is coupled with the transmitter mount and the first portion of the glucose sensor is coupled with the transmitter.","15","17/209758","2021-03-23","2021-0204844","2021-07-08","11179072","2021-11-23","ABBOTT DIABETES CARE INC.","John C. Mazza | Andrew H. Naegeli | Gary Ashley Stafford","","","","A61B-0005/14532","A61B-0005/14532 | A61B-0005/0002 | A61B-0005/1473 | A61B-0005/6833 | A61M-0005/1723 | H04L-0067/12","A61B-005/00","A61B-005/00 | A61B-005/145 | A61B-005/1473 | H04L-029/08 | A61M-005/172","","","","","","4921048000925"
"US","US","P","B2","Position-tracking-enabling connector for an ear-nose-throat (ENT) tool","A medical instrument includes a shaft, one or more position sensors, a connector and interrogation circuitry. The shaft is configured for insertion into a body of a patient. The one or more position sensors are fitted at a distal end of the shaft. The connector is configured to receive a mating connector. The interrogation circuitry is configured to detect whether the mating connector is connected to the connector, and, if not connected, to prevent tracking a position of the distal end in the body using the one or more position sensors.","1. A method for using a medical instrument that comprises a shaft for insertion into a body of a patient, one or more position sensors fitted at a distal end of the shaft, and a connector, the method comprising: (a) placing the medical instrument into a connected state in which a mating connector is connected to the connector and the medical instrument is coupled to a processor; and(b) placing the medical instrument into a disconnected state in which the mating connector is disconnected from the connector and the medical instrument is coupled to the processor, wherein the act of placing the medical instrument into the disconnected state results in the one or more position sensors being communicatively isolated from the processor.","10","15/795169","2017-10-26","2019-0125450","2019-05-02","11179203","2021-11-23","BIOSENSE WEBSTER (ISRAEL) LTD.","Assaf Govari | Vadim Gliner | Alon Boumendil","","","","A61B-0034/20","A61B-0034/20 | A61B-0005/062 | A61B-0017/24 | A61B-0090/98 | G06F-0021/10 | A61B-0005/065 | A61B-2017/00199 | A61B-2017/00482 | A61B-2034/2051 | A61B-2034/2059 | A61B-2090/0803 | A61B-2090/0814 | A61B-2560/028 | A61B-2562/08 | A61B-2562/227 | H04B-0005/0062","A61B-034/20","A61B-034/20 | A61B-017/24 | A61B-090/98 | A61B-005/06 | G06F-021/10 | A61B-090/00 | A61B-017/00 | H04B-005/00","","","","","","4921048001056"
"US","US","P","B1","Streaming analytics of human body movement data","Large amounts of human body movement data may be collected, possibly via streaming data, from one or more sensors worn by a user. The data may be analyzed along with other classification data to generate feedback for the user or for other interested people (e.g., a trainer, a coach, a team member, health professional, etc.). The analysis may utilize one or more machine learning (ML) algorithms that use training data to create one or more ML models. When a user is evaluated after receiving feedback, accuracy of the feedback may be evaluated and fed back to the ML model to continue training the ML model(s).","1. A system comprising: a wearable patch that is affixable to a user, the wearable patch including at least an accelerometer and a transmitter to transmit movement signals generated by the accelerometer based at least in part on movement of the user and corresponding movement of the wearable patch;a motion classification device to determine, from a predefined set of possible motion description data that describes different types of possible movement of the user, based at least in part on the movement signals representing the movement of the user and the corresponding movement of the wearable patch, and without receiving additional data indicating an activity of the user, motion description data classifying a specific type of movement exhibited by the user, the specific type of movement being determined based at least in part on a repeatable series of movements or previously identified patterns extracted from the movement signals;an analytics device to: receive the movement signals and the motion description data;generate a historical profile for the user of the wearable patch based at least in part on the movement signals and the motion description data;create a metric threshold based at least in part on the historical profile to indicate when subsequent movement data indicates at least one of fatigue of the user or a potential injury of the user; andanalyze the first movement signals and the motion description data associated with the movement signals to determine user information including at least one of a first likelihood of fatigue of the user, a second likelihood of injury of the user, or a predicted type of injury associated with the user; andan output device to receive the user information from the analytics device, the output device to cause display of the user information in response to at least one of the first likelihood of fatigue or the second likelihood of injury reaching or exceeding the metric threshold.","20","16/055345","2018-08-06","","","11172818","2021-11-16","AMAZON TECHNOLOGIES, INC.","Marvin Theimer | Richard Shawn Bice","","","","A61B-0005/0002","A61B-0005/0002 | G06F-0003/011 | G06K-0009/00342 | G06N-0020/00 | G06T-0007/20","G06F-003/01","G06F-003/01 | A61B-005/00 | G06T-007/20 | G06K-009/00 | G06N-020/00","","","","","","4921047000840"
"US","US","P","B2","System and method to monitor, guide, and evaluate breathing, utilizing posture and diaphragm sensor signals","Device, system and method to monitor user breathing patterns utilizing posture and diaphragm (breathing) sensor signals. The user worn device comprises a housing attached to a retractable belt that is worn around the user's trunk. The housing contains both posture and breathing sensors. The device monitors the output signals of these sensors and measures the state of both the user's posture and diaphragm (e.g. changes in the belt's length or force on the belt as a function of user breathing) to analyze breathing signals. The system's processor receives, processes, and transmits sensor signal data, and can also calibrate and interpret these signals utilizing various algorithms. In a preferred embodiment, the posture sensor is an accelerometer, and the retractable belt winds around a spring tensioned spool in the device's housing. The software can produce posture adjusted user respiration data, and can also be used for breath training and other purposes.","1. A device for tracking respiration and posture from a location on a human user, said device comprising: a single case housing comprising an outer surface with a recess, and an interior, and disposed within said interior of said single case housing, a computer processor, and an electronic angle sensor configured to measure user standing or sitting posture across a range of different standing or sitting angles of posture position;said recess comprising a housing fastener;said interior of said single case housing further comprising a non-elastic retracting belt, and a spring-driven retracting mechanism;wherein said device is a unitized device, said retracting mechanism further comprises a single spring tensioned spool, and said belt winds around said single spring tensioned spool;wherein said device further comprises an electronic breathing sensor comprising at least one of a housing fastener mounted flex sensor and a housing fastener mounted electronic strain gauge configured to produce an electronic breathing sensor signal measuring force on said retracting belt but not rotation of said single spring tensioned spool;said retracting belt comprising a first belt end that is connected to said retracting mechanism, and a second belt end configured with a belt fastener configured to reversibly attach to said housing fastener;said housing and said retracting belt configured so that when said retracting belt is worn around said user'ss trunk, and said belt fastener is attached to said housing fastener, said retracting belt experiences force in response to user respiration, and said breathing sensor produces a breathing sensor signal reporting on said respiration;said electronic angle sensor configured so that when said retracting belt is worn around said user'ss trunk, and said belt fastener is attached to said housing fastener, said angle sensor produces an angle sensor signal reporting on an angle of said user'ss range of standing or sitting posture angles;said processor configured to process said breathing sensor signal and said angle sensor signal, and to produce an output reporting on at least one of:a) angle sensor data reporting on at least one angle of said user'ss range of standing or sitting posture angles, and breathing sensor data reporting on said user'ss respiration; andb) posture adjusted user respiration data.","28","15/972172","2018-05-06","2018-0256074","2018-09-13","11172850","2021-11-16","Prana Tech LLC","Andre Maxim Persidsky | Robin Alexander Ahlund","","","","A61B-0005/113","A61B-0005/113 | A61B-0005/1116 | A61B-0005/6823 | G06K-0009/00335 | A61B-0005/486 | A61B-2562/0219 | G06F-0003/017","A61B-005/00","A61B-005/00 | A61B-005/113 | A61B-005/11 | G06K-009/00 | G06F-003/01","","","","","","4921047000871"
"US","US","P","B2","Augmented reality therapeutic movement display and gesture analyzer","Systems and methods for displaying augmented reality clinical movements may use an augmented reality device to display aspects of a clinical movement. The systems and methods may use a motion capture device to capture the clinical movement. A method may include analyzing information about the clinical movement to determine a path of motion representative of at least a portion of the clinical movement. The method may automatically define a path region or a virtual target in an augmented reality environment overlaid on a real environment. The method may display the path region or the virtual target on an augmented reality display.","1. A method for evaluating clinical movements, the method comprising: generating a plurality of path of motion targets based on a therapist clinical movement, the therapist clinical movement captured by a therapist using a movement capture apparatus;generating a three-dimensional path of motion based on a selected motion target of the plurality of motion targets;displaying a visual representation of the three-dimensional path of motion;receiving patient movement data from a patient motion sensor, the patient movement data indicative of a movement of a patient along the three-dimensional path of motion;determining, based on analysis of the patient movement data about the movement of the patient, whether the movement was within the three-dimensional path of motion; andgenerating a feedback notification indicating whether the movement was within the three-dimensional path of motion.","23","16/815466","2020-03-11","2020-0279112","2020-09-03","11176376","2021-11-16","Zimmer US, Inc.","Richard Wells | Timothy R. Price | Ted Spooner | Dave Van Andel | Travis Dittmer | John Kotwick | Jason Leighton","","","","G06K-0009/00671","G06K-0009/00671 | A61B-0003/0033 | A61B-0005/0002 | A61B-0005/015 | A61B-0005/1116 | A61B-0005/1118 | A61B-0005/1128 | A61B-0005/1495 | A61B-0005/486 | A61B-0005/746 | A61B-0005/7455 | G02B-0027/0093 | G02B-0027/017 | G02B-0027/0176 | G06F-0003/017 | G06F-0003/0487 | G06K-0009/00342 | G06T-0019/006 | G16H-0020/30 | G16H-0020/40 | G16H-0040/63 | G16H-0040/67 | A61B-0005/1112 | A61B-2505/09 | A61B-2562/0219 | A61B-2562/0247 | G06F-0003/011 | G06Q-0050/22","G09G-005/00","G09G-005/00 | G06K-009/00 | G16H-020/30 | A61B-005/00 | A61B-005/01 | A61B-005/1495 | G16H-040/63 | A61B-005/11 | G16H-020/40 | G16H-040/67 | G02B-027/00 | G02B-027/01 | G06T-019/00 | A61B-003/00 | G06F-003/01 | G06F-003/0487 | G06Q-050/22","","","","","","4921047004377"
"US","US","P","B2","Method and apparatus for acquiring a spatial map of auditory perception of a subject","This method for acquiring a spatial map of auditory perception of a subject comprises a plurality of successive test sequences, each test sequence comprising steps of: a) calibration (1002) of the subject's position, by displaying instructions to the subject, using a head-mounted visual display system worn by the subject, in order to acquire a reference position of the subject, the subject's position being measured using a video motion capture system by measuring the spatial coordinates of a first optical marker worn by the subject, b) choosing (1004) spatial coordinates of a target location of a sound source, said target cation being located around the subject, c) emitting (1006) a predefined sound, using a sound source placed at said target location, d) in response to acquisition instructions generated by the subject using an acquisition interface, acquiring (1008) an estimated location of said sound source, by using the video motion capture system to measure the spatial coordinates of a second optical marker held and pointed by the subject towards a perceived location of the sound source.","1. A method for acquiring a spatial map of auditory perception of a subject, wherein said method comprises a plurality of successive test sequences, each test sequence comprising: a) calibrating the subject'ss position, by displaying instructions to the subject, using a head-mounted visual display system worn by the subject, in order to acquire a reference position of the subject, the subject'ss position being measured using a motion capture system by measuring the spatial coordinates of a first location marker worn by the subject, said head-mounted display system preventing the subject from seeing his surroundings;b) choosing spatial coordinates of a target location of a real sound source, said target location being located around the subject,c) emitting a predefined sound, using the real sound source placed at said target location,d) in response to acquisition instructions generated by the subject using an acquisition interface, acquiring an estimated location of said sound source, by using the motion capture system to measure the spatial coordinates of a second location marker held and pointed by the subject towards a perceived location of the sound source, and measuring an orientation of the subject'ss gaze, using an eye-tracking device integrated with the head-mounted visual display system.","15","16/303948","2017-05-26","2020-0320768","2020-10-08","11176727","2021-11-16","INSTITUT NATIONAL DE LA SANTE ET DE LA RECHERCHE MEDICALE (INSERM) | UNIVERSITE CLAUDE BERNARD LYON 1 | UNIVERSITE JEAN MONNET SAINT ETIENNE | CENTRE NATIONAL DE LA RECHERCHE SCIENTIFIQUE","Romeo Salemme | Alessandro Farne | Valerie Gaveau | Anael Belle | Eric Koun | Francesco Pavani","2016-305621","EP","2016-05-27","G06T-0015/00","G06T-0015/00 | A61B-0005/0077 | A61B-0005/1114 | A61B-0005/1127 | A61B-0005/123 | A61B-0005/162 | A61B-0005/163 | A61B-0005/6803 | A61B-0005/7475 | G06F-0003/013 | G06T-0007/20 | G06T-0007/70 | H04S-0007/30 | A61B-2560/0223 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/30201 | G06T-2207/30204","G06T-015/00","G06T-015/00 | G06T-007/70 | A61B-005/16 | A61B-005/00 | A61B-005/11 | A61B-005/12 | G06F-003/01 | G06T-007/20 | H04S-007/00","","","","","","4921047004726"
"US","US","P","B2","Field of view (FOV) throttling of virtual reality (VR) content in a head mounted display","A method for reducing discomfort when viewing virtual reality (VR) content for use in head mounted displays (HMDs). The method includes accessing a model that identifies a plurality of learned patterns associated with the generation of corresponding baseline VR content that is likely to cause discomfort. The method includes processing a first application to generate data associated with simulated user interactions with first VR content of the first application. The method includes comparing the data to the model to identify a pattern in the data matching at least one of the learned patterns, such that the identified pattern is likely to cause discomfort. The method includes identifying a zone in the first application corresponding to identified pattern. The method includes applying a discomfort reduction filter effect within the zone for purposes of reducing potential discomfort in a user.","1. A method comprising: executing an application to generate a virtual reality (VR) content for display in association with user interaction by a user, wherein the VR content provides a view into a virtual environment;determining that the VR content includes movement of the view in the virtual environment that will cause orientation discomfort to the user;determining that a rate of head movement of the user corresponding to the movement of the view exceeds a threshold; andapplying a discomfort reduction filter effect to the view by reducing a size of a field of view (FOV) for the view during a portion of the movement of the view when the rate of head movement of the user exceeds the threshold,wherein there is a non-linear reduction in the size of the FOV as the rate of head movement of the user increases.","20","16/379732","2019-04-09","2019-0236836","2019-08-01","11176731","2021-11-16","SONY INTERACTIVE ENTERTAINMENT INC.","Dominic S. Mallinson","","","","G06T-0015/20","G06T-0015/20 | A61B-0005/024 | A61B-0005/0531 | A61B-0005/392 | A61M-0021/00 | A63F-0013/53 | G02B-0027/0093 | G02B-0027/017 | G06F-0003/011 | G06F-0003/012 | G06N-0003/02 | G06N-0003/04 | G06N-0003/08 | G06N-0005/04 | G06T-0019/006 | G06T-0019/20 | A61M-2021/005 | A61M-2021/0044 | A61M-2205/507 | A61M-2230/06 | A61M-2230/10 | A61M-2230/14 | A61M-2230/63 | A61M-2230/65 | A63F-2300/303 | G02B-2027/014 | G06F-0003/013 | G06F-0003/015 | G06T-2200/04 | G06T-2200/24 | G06T-2207/20024 | G06T-2207/20092","G06T-015/20","G06T-015/20 | A61B-005/392 | A61M-021/00 | G02B-027/01 | G02B-027/00 | A61B-005/024 | A61B-005/0531 | G06F-003/01 | G06N-003/04 | G06N-003/08 | G06T-019/00 | A63F-013/53 | G06N-003/02 | G06N-005/04 | G06T-019/20","","","","","","4921047004730"
"US","US","P","B2","Systems and techniques for monitoring subjects","The present invention generally relates to systems and methods for monitoring and/or providing feedback for drugs or other pharmaceuticals taken by a subject. In one aspect, the present invention is directed to devices and methods for determining a species within the skin of a subject; and producing feedback to a subject based on the determination of the species. The feedback may be, for example, visual, audible, tactile, a change in temperature, etc. In some cases, information regarding the determination of the species may be transmitted to another entity, e.g., a health care provider, a computer, a relative, etc., which may then provide feedback to the subject in some fashion. In some cases, the feedback may be directly indicative of the species. However, the feedback may also be indirect in some embodiments.","1. A device for application to a skin of a subject for treating the subject with a drug, the device comprising: a substance transfer component comprising one or more microneedles;a deployment actuator configured and arranged to move from a first position to a second position and from the second position to the first position, wherein when the deployment actuator is in the first position, the one or more microneedles do not contact the skin of the subject, and wherein when the deployment actuator is in the second position, at least one of the microneedles pierce the skin of the subject;a self-contained vacuum chamber having an internal pressure less than atmospheric pressure before the device is applied to the skin of the subject;a sensing chamber, positioned within the device, for containing the blood withdrawn from the subject, wherein the sensing chamber is separate from the self-contained vacuum chamber, and wherein the self-contained vacuum chamber is positioned within the device to withdraw a quantity of blood from the subject to be contained by the sensing chamber after the microneedles pierces the skin of the subject;a sensor positioned in sensing communication with the sensing chamber, the sensor being configured and arranged to determine an analyte suspected of being present within the blood within the sensing chamber;a microprocessor positioned in electrical communication with the sensor; anda drug reservoir within the device for containing the drug deliverable to the subject, wherein the device is configured to deliver the drug from the reservoir to the skin of the subject after the deployment actuator has moved from the first position to the second position and from the second position to the first position.","6","15/290217","2016-10-11","2017-0215790","2017-08-03","11177029","2021-11-16","YOURBIO HEALTH, INC.","Douglas A. Levinson | Howard Bernstein","","","","G16H-0020/10","G16H-0020/10 | A61B-0005/1112 | A61B-0005/145 | A61B-0005/14503 | A61B-0005/14514 | A61B-0005/14546 | A61B-0005/154 | A61B-0005/157 | A61B-0005/150022 | A61B-0005/150099 | A61B-0005/15113 | A61B-0005/15125 | A61B-0005/15142 | A61B-0005/150175 | A61B-0005/150389 | A61B-0005/150412 | A61B-0005/150503 | A61B-0005/150755 | A61B-0005/150809 | A61B-0005/150824 | A61B-0005/150854 | A61B-0005/150862 | A61B-0005/150969 | A61B-0005/150984 | A61B-0005/4833 | A61B-0005/4839 | A61B-0005/4848 | A61B-0010/0045 | G06Q-0020/10 | G06Q-0030/02 | G06Q-0030/0207 | G16H-0020/17 | G16H-0040/63 | A61B-0005/6824 | A61B-2010/008 | A61B-2010/0009 | A61M-0005/1723 | A61M-0037/0015 | A61M-2037/0023 | Y02A-0090/10","G16H-020/10","G16H-020/10 | G16H-020/17 | A61B-010/00 | G06Q-020/10 | G06Q-030/02 | A61B-005/15 | A61B-005/154 | A61B-005/145 | A61B-005/157 | A61B-005/151 | G16H-040/63 | A61B-005/00 | A61B-005/11 | A61M-037/00 | A61M-005/172","","","","","","4921047005022"
"US","US","P","B2","Methods and apparatus for neuromodulation","A neuromodulator accurately measures?in real time and over a range of frequencies?the instantaneous phase and amplitude of a natural signal. For example, the natural signal may be an electrical signal produced by neural tissue, or a motion such as a muscle tremor. The neuromodulator generates signals that are precisely timed relative to the phase of the natural signal. For example, the neuromodulator may generate an exogenous signal that is phase-locked with the natural signal. Or, for example, the neuromodulator may generate an exogenous signal that comprises short bursts which occur only during a narrow phase range of each period of an oscillating natural signal. The neuromodulator corrects distortions due to Gibbs phenomenon. In some cases, the neuromodulator does so by applying a causal filter to a discrete Fourier transform in the frequency domain, prior to taking an inverse discrete Fourier transform.","1. A method comprising: (a) measuring, via a sensor of an apparatus, a physiological signal;(b) calculating, via one or more processors of the apparatus, a discrete signal that comprises samples of the physiological signal;(c) calculating a second signal, which second signal is a discrete Fourier transform of the discrete signal;(d) calculating a smoothed signal, by performing calculations that include (i) applying a causal filter to the second signal, which causal filter deforms a front-segment but not an end-segment of the second signal in such a way that the start point of the smoothed signal is equal in value to the endpoint of the smoothed signal, and (ii) removing negative frequency components;(e) calculating a portion of an analytic signal, which analytic signal is equal to an inverse discrete Fourier transform of the smoothed signal;(f) calculating, based on said portion of the analytic signal, instantaneous phase of the physiological signal;(g) outputting instructions, based on the instantaneous phase; and(h) outputting a neuromodulation signal, based on the instructions.","23","16/943491","2020-07-30","2021-0059528","2021-03-04","11166632","2021-11-09","ELEMIND TECHNOLOGIES, INC | MASSACHUSETTS INSTITUTE OF TECHNOLOGY","Nir Grossman | David Wang | Edward Boyden","","","","A61B-0005/0036","A61B-0005/0036 | A61B-0005/1101 | A61B-0005/24 | A61B-0005/369 | A61B-0005/7253 | A61B-0005/7257 | A61N-0005/0622 | A61N-0007/00 | G06F-0017/14 | A61N-2005/063 | A61N-2007/0021","A61B-005/00","A61B-005/00 | G06F-017/14 | A61B-005/24 | A61B-005/369 | A61B-005/11 | A61N-005/06 | A61N-007/00","","","","","","4921046000842"
"US","US","P","B2","System for identifying information represented by biological signals","This system for identifying information represented by biological signals is configured so as to detect biological signals (S501), analyze the detected biological signals and then output feature data (S502), determine the respective similarities between the feature data and a plurality of teaching data (S503), store the similarities per time in a time series (S504), and determine information represented by the biological signals on the basis of the plurality of similarities within a prescribed period among the stored similarities in the time series (S505).","1. A system for identifying information represented by a biological signal, the system comprising: detection means for detecting a biological signal;analysis means for analyzing the detected biological signal and outputting feature data;first determination means for determining output vector indicating degrees of similarity between the feature data and each of a plurality of teaching data;storage means for chronologically storing the output vectors for each time; andsecond determination means for determining information represented by the biological signal based on a plurality of output vectors within a predetermined period in the chronological output vectors stored in the storage means, wherein the second determination means:calculates computed values for each one of the plurality of teaching data based on corresponding chronological components of the plurality of output vectors within the predetermined period, wherein an end point of the predetermined period is a last time at which an output vector is stored in the storage means, wherein when a new subsequent output vector is stored, a start point of the predetermined period and the end point move according to the new subsequent output vector, and the output vectors which fall outside of the predetermined period are not used to calculate the computed values; anddetermines the information represented by the biological signal based on the computed values.","13","16/767710","2018-11-30","2020-0375529","2020-12-03","11166667","2021-11-09","MELTIN MMI CO., LTD.","Masahiro Kasuya | Tatsuya Seki","2017-231054","JP","2017-11-30","A61B-0005/4836","A61B-0005/4836 | A61B-0005/4205 | A61B-0005/6822 | A61B-0005/6823 | A61B-0005/6824 | A61B-0005/6828 | A61B-0005/7267 | A61F-0002/583 | A61F-0002/72 | A61G-0005/00 | A61H-0001/0237 | A61H-0001/0274 | A61H-0001/0292 | A63B-0023/16 | G06F-0003/015 | G16H-0040/67 | G16H-0050/20 | A61B-0005/389 | A61B-2018/00839 | A61F-2002/6614 | A61F-2002/704 | A61H-2230/08 | A61M-2230/08 | A63B-2230/08","A61F-002/72","A61F-002/72 | A61B-005/00 | G16H-040/67 | G16H-050/20 | A61F-002/58 | A61G-005/00 | A61H-001/02 | A63B-023/16 | G06F-003/01 | A61F-002/66 | A61F-002/70 | A61B-005/389 | A61B-018/00","","","","","","4921046000877"
"US","US","P","B2","System and method for monitoring patient in a magnetic resonance imaging environment","The present disclosure relates to systems and methods for controlling an MRI safe patient care/monitoring system. The system may include a an MRI safe infusion pump that contains a co-processor which acts as a central brain for all other patient care/monitoring devices in the MRI room. The co-processor is separated from the primary processor by a buffer in the infusion pump's memory, so that no actions by the co-processor can affect the primary processor. The co-processor controls the display software, alert software and alerts database. All other patient care/monitoring devices are connected to the MRI safe infusion pump in a daisy chain, so that they can share a single connection to outside the MRI room. Outside the MRI room the co-processor has access to the remote control & display, which contains the remote control GUI, and to the internet, through which it will access an external medical network, which contains an algorithm database. The algorithm database has data processing algorithms for a variety of patient care/monitoring devices. allowing simpler ""dumb"" devices to be connected and still have ""smart"" functionality.","1. An MRI safe patient care/monitoring system comprising: a co-processor in a MRI safe infusion pump that acts as a central brain for all patient monitoring/care devices, the co-processer being disposed in a daisy chain system; wherein the daisy chain system is disposed with in a housing of the infusion pump and includes memory including a buffer;at least one alert database storing alert data for the patient care/monitoring devices;an input/output device; anda display device;the patient care/monitoring devices being connected to the infusion pump in a daisy chain, which allows for all information from and control of those devices to be sent to an external controller in the MRI control room through a single communication channel;wherein the co-processor can also control additional devices that have less processing power, allowing simpler ""dumb"" devices to be connected and still have ""smart"" functionality.","15","16/062829","2016-12-30","2020-0261647","2020-08-20","11167083","2021-11-09","KONINKLIJKE PHILIPS N.V.","John Cronin | Michael D'Andrea","","","","A61M-0005/172","A61M-0005/172 | G05B-0019/042 | G16H-0040/20 | G16H-0040/67 | H04L-0067/125 | A61M-2205/18 | A61M-2205/3553 | A61M-2205/3584 | A61M-2205/50 | A61M-2205/502 | A61M-2205/52 | G05B-2219/21039 | G16Y-0010/60 | G16Y-0040/10 | G16Y-0040/30","A61B-005/055","A61B-005/055 | A61M-005/142 | A61M-005/172 | A61B-005/00 | A61M-005/168 | G16H-040/67 | G16H-040/20 | G05B-019/042 | H04L-029/08 | G16Y-040/10 | G16Y-040/30 | G16Y-010/60","","","","","","4921046001293"
"US","US","P","B2","Sparse component analysis method for structural modal identification when the number of sensors is incomplete","Structural health monitoring providing sparse component analysis method for structural modal identification with incomplete number of sensors. Transforming structural acceleration response data into time frequency domain by short time Fourier transform, detecting time frequency points contributed by only one-order mode where real and imaginary parts have the same direction, and taking the detection result as the initial result of single-source-points; refining the initial result of detection of single-source-point located near the peak of power spectral density, and clustering single-source-points to obtain a mode shape matrix; constructing generalized spectral matrixes using short time Fourier transform coefficients, conducting singular value decomposition on generalized spectral matrix at a single-source-point, taking the first singular value as an auto-spectrum of single-order mode, obtaining the frequency of each order by picking the peak of auto-spectrum, and extracting damping ratio of each order by transforming the auto-spectrum into a time domain through inverse Fourier transform. By means of this method, structural modal parameters are obtained when the number of sensors is incomplete, thereby increasing the identification accuracy of the sparse component analysis method.","1. A sparse component analysis method for structural modal identification when a number of sensors is incomplete, including estimating mode shape matrix and extracting frequencies and damping ratios, wherein the steps are as follows: estimating mode shape matrix: step 1: acquiring an acceleration response Y(t)=[y1(t), y2(t), . . . , yl(t)]T of a structure at a time t when the number of sensors is incomplete; transforming an acceleration response of a time domain into a time frequency domain using short time Fourier transform; then the expression of the response is changed to Y(t,ω)=[y1(t,ω), y2(t,ω), . . . , yl(t,ω)], where l represents the number of sensors, and ω represents circular frequency;step 2: acquiring and marking an initial result of detection of single-source-point; the fact that the real part and imaginary part of a time frequency coefficient have the same direction is taken as a basis of detection of single-source-point, using the following formula where Re{?} represents a real part of the extracted data, Im{?} represents the imaginary part of the extracted data, and Δβ represents the threshold of detection of single-source-point; and if the detected single-source-point location is marked as (tk, ωk), the value is: Y(tk,ωk)=[y1(tk,ωk),y2(tk,ωk), . . . ,yl(tk,ωk)]T; step 3: averaging logarithmic amplitudes of all sensor locations;conducing the same processing on time frequency coefficients of all sensor locations: if the time frequency coefficient of the jth sensor location is yj(t, ω), connecting time frequency coefficients yj(t, ωi) corresponding to all frequency sections ωi, i=1, 2, . . . , N in sequence to obtain sequences {tilde over (y)}j, where N represents the number of frequency points used in short time Fourier transform;averaging the logarithmic amplitudes of all sensor locations: calculating the logarithmic amplitude of each element in each of the sequences {tilde over (y)}j, j=1, 2, . . . , l using Ampj(τ)=20×log10(|{tilde over (y)}j(τ)|), where {tilde over (y)}j(τ) represents the τth element in the sequence {tilde over (y)}j, Ampj(τ) represents the τth element in the logarithmic amplitude of the jth sensor location; and averaging logarithmic amplitudes: to obtain a mean logarithmic amplitude; step 4: calculating trend items of the mean logarithmic amplitude sequence Ampmean using polynomial regression, and then removing the trend items, to obtain a sequence Am pmean; conducting statistical analysis on Am pmean; and calculating a number of samples falling into each statistical interval; when the number of accumulated samples reaches 90% of the total number of samples, setting a sample value of a corresponding statistical interval as a threshold, and marking a time frequency point set represented by samples below the threshold as Ω; removing points falling into the set Ω in the initial result Y(tk,ωk) of detection of single-source-point obtained in step 2, to obtain refined single-source-points Y({circumflex over (t)}k,{circumflex over (ω)}k);step 5: dividing the refined single-source-points Y({circumflex over (t)}k,{circumflex over (ω)}k) into categories using hierarchical clustering, and calculating the clustering center of each category, i.e. mode shape matrix;extracting frequency and damping ratio:step 6: constructing a generalized spectral matrix using the time frequency coefficients Y(t,ω) in step 1: where STFTyjyk(ti,ω)=yj(ti,ω)·y*k(ti,ω); ti, represents the ith time; superscript * represents the conjugation for determining the complex number; and E represents the expectation for extracting data; step 7: if the frequency index contained in the single-source-point location ({circumflex over (t)}k,{circumflex over (ω)}k) is {circumflex over (ω)}k, conducting singular value decomposition on the generalized spectral matrix Hyy at {circumflex over (ω)}k, to obtain a first singular value sequence s1 at each frequency;step 8: taking the value on the first singular value sequence s1 of each category of single-source-points obtained in step 5 as an auto-spectrum of each order of modes, obtaining the frequencies of each order by picking the peak frequency of s1, and extracting the damping ratios by transforming s1 into a time domain through inverse Fourier transform.","1","16/342952","2018-03-06","2020-0073908","2020-03-05","11170070","2021-11-09","DALIAN UNIVERSITY OF TECHNOLOGY","Tinghua Yi | Xiaojun Yao | Hongnan Li","","","","G06F-0017/14","G06F-0017/14 | A61B-0005/11 | A61B-0005/7257 | G06F-0007/556 | A61B-2562/0219 | H04L-0067/12","G06F-017/14","G06F-017/14 | A61B-005/11 | A61B-005/00 | G06F-007/556 | H04L-029/08","","","","","","4921046004258"
"US","US","P","B1","Software configuration for virtual skincare assessment and virtual cues","A software configuration generates, with a processor, a graphical user interface that renders a menu of a plurality of selection indicia. The plurality of selection indicia includes a skincare rejuvenation area indicium, a skincare assessment indicium, and a skincare interactive cue indicium. Furthermore, the software configuration receives, with the processor, a first user input corresponding to a skincare rejuvenation area associated with the skincare rejuvenation area indicium. Moreover, the software configuration receives, with the processor, a second user input corresponding to the skincare assessment indicium to initiate a skincare assessment based on the skincare rejuvenation area. Furthermore, the software configuration performs, with an image capture device, an image capture of the skincare rejuvenation area. Additionally, the software configuration performs, with the processor, an image analysis of the image capture with one or more previous image captures captured by the image capture device.","1. A computer program product comprising a non-transitory computer readable storage device having a computer readable program stored thereon, wherein the computer readable program when executed on a computer causes the computer to: generate, with a processor, a graphical user interface that renders a menu of a plurality of selection indicia, the plurality of selection indicia including a skincare rejuvenation area indicium, a skincare assessment indicium, and a skincare interactive cue indicium;receive, with the processor, a first user input corresponding to a skincare rejuvenation area associated with the skincare rejuvenation area indicium, wherein the processor prompts for the first user input corresponding to the skincare rejuvenation area at a predetermined time interval, and wherein the processor prevents receipt of the first user input corresponding to the skincare rejuvenation area until the predetermined time interval;receive, with the processor, a second user input corresponding to the skincare assessment indicium to initiate a skincare assessment based on the skincare rejuvenation area;perform, with an image capture device, an image capture of the skincare rejuvenation area;filter out, with the processor, one or more portions of the image capture that have improved over a predetermined improvement threshold to identify one or more remaining portions of the image capture, wherein the predetermined improvement threshold varies based on the received skincare rejuvenation area due to at least two changes in discoloration, opacity, density and pore size of corresponding pixels;perform, with the processor, an image analysis of the one or more remaining portions of the image capture with one or more previous image captures captured by the image capture device, wherein the performed image analysis of the one or more remaining portions of the image capture delivers a faster result as compared to an image analysis of all portions of the image capture;determine, with the processor, that one or more metrics associated with the skincare rejuvenation area lack improvement in excess of the predetermined improvement threshold for the skincare rejuvenation area; andgenerate, with the processor based on the determination, one or more virtual cues that indicate visual movements during usage by a user of a skincare treatment process wherein the virtual cues are overlaid over an image of the user displayed by the graphical user interface.","18","16/931182","2020-07-16","","","11160497","2021-11-02","ELYSE ENTERPRISES LLC","Hillary Hayman","","","","A61B-0005/4848","A61B-0005/4848 | A45D-0044/00 | A61B-0005/0077 | A61B-0005/442 | A61B-0005/486 | A61B-0005/7405 | A61B-0005/7435 | A61B-0005/7475 | A61H-0015/00 | G06F-0003/0482 | G06F-0003/04847 | G06K-0009/00335 | G06Q-0030/0631 | G06T-0007/0012 | G09B-0019/003 | G16H-0050/20 | G16H-0050/30 | A45D-2044/007 | A61B-2576/02 | A61H-2201/1695 | G06T-2207/10016 | G06T-2207/30088","A61B-005/00","A61B-005/00 | G06F-003/0484 | G06T-007/00 | G06K-009/00 | G16H-050/30 | G16H-050/20 | G06Q-030/06 | A61H-015/00 | A45D-044/00 | G09B-019/00 | G06F-003/0482","","","","","","4921045000942"
"US","US","P","B2","Visual aid display device and method of operating the same","A display device and an operating method thereof are provided. The display device may include: a display; a camera; a memory configured to store one or more instructions; and a processor configured to execute the instructions to obtain an image captured by the camera, transform the image based on visual condition information of a user, the visual condition information including information about a type of visual impairment of the user, and display the transformed image on the display.","1. A display device comprising: a display;a camera;a memory configured to store instructions; anda processor configured to execute the instructions to: provide a user interface for obtaining visual condition information comprising information about at least one of a type of visual impairment of a user and a degree of visual impairment according to the type of visual impairment,obtain an image captured by the camera,identify an object included in the image,identify an attribute of the object,determine first image transformation information comprising a first transformation value regarding enlargement of the object as image transformation information for transforming the image based on the attribute indicating the object is a first object type,determine second image transformation information comprising a second transform value regarding a location change of the object as the image transformation information for transforming the image based on the attribute indicating the object is a second object type,transform the image based on the object, the image transformation information and the visual condition information of the user, the visual condition information comprising information about the type of visual impairment of the user, anddisplay the transformed image on the display.","20","15/705439","2017-09-15","2018-0125716","2018-05-10","11160688","2021-11-02","SAMSUNG ELECTRONICS CO., LTD.","Jung-hoon Cho | Yong-nam Kim | Seung-chan Kim","10-2017-0025055","KR","2017-02-24","A61F-0009/08","A61F-0009/08 | G02B-0027/017 | G02B-0027/0172 | G06F-0003/011 | G06F-0003/048 | G06K-0009/36 | G06K-0009/605 | G06T-0005/001 | G06T-0011/00 | G06T-0011/60 | G06T-0019/006 | G09B-0021/008 | H04N-0013/344 | G02B-2027/0138 | G06K-2009/366","A61F-009/08","A61F-009/08 | G02B-027/01 | G06T-019/00 | G06K-009/36 | G09B-021/00 | G06F-003/048 | H04N-013/344 | G06F-003/01 | G06T-011/00 | G06K-009/60 | G06T-005/00 | G06T-011/60","","","","","","4921045001132"
"US","US","P","B2","Activity and workout updates","The present disclosure generally relates to navigating, viewing, and sharing activity and workout data and interacting with workout and/or activity applications. In some examples, scrolling of activity data is based on the content being displayed. In some examples, friends' activity data may be viewed. In some examples, a notification and workout data for a friend's completed workout is received and displayed. In some example, the activity data received from friends is viewed and managed. In some examples, workout data for a multi-segment workout is displayed in a three-dimensional stack on a map. In some examples, a workout application operates in a limited mode until a touch input is received with a characteristic intensity that is greater than a threshold intensity.","1. An electronic device, comprising: a display device;one or more processors;memory; andone or more programs, where in the one or more programs are stored in the memory and configured to be executed by the one or more processors, the one or more programs including instructions for: displaying a first user interface that includes a representation of a current time;subsequent to displaying the first user interface, receiving a notification corresponding to a completed workout of a remote user associated with an external electronic device, wherein the completed workout is a workout selected by the remote user from a plurality of workout types;displaying, via the display device, a first graphical element representing the notification, wherein the first graphical element at least partially overlaps the representation of the current time;receiving a first user input corresponding to selection of the first graphical element while displaying the first graphical element; andin response to the first user input: ceasing to display the first graphical element representing the notification; anddisplaying, via the display device, a workout summary interface for the completed workout of the remote user associated with the external electronic device.","30","16/378136","2019-04-08","2019-0232111","2019-08-01","11161010","2021-11-02","Apple Inc.","Aled Hywel Williams | David Chance Graham | Christopher Wilson","","","","A63B-0024/0062","A63B-0024/0062 | A61B-0005/00 | A61B-0005/11 | A61B-0005/1118 | A61B-0005/6898 | A63B-0024/0084 | A63B-0071/0622 | G06F-0003/0481 | G06F-0003/0485 | G06F-0003/04845 | G06F-0003/04883 | G06K-0009/00342 | G06T-0011/60 | G16H-0020/30 | G16H-0040/67 | A61B-0005/0022 | A63B-2024/0068 | A63B-2071/0655 | A63B-2071/0691 | A63B-2220/12 | A63B-2220/20 | A63B-2220/40 | A63B-2220/62 | A63B-2220/805 | A63B-2220/806 | A63B-2220/807 | A63B-2225/50 | A63B-2230/06 | A63B-2230/75 | G16H-0050/30","G06F-003/048","G06F-003/048 | A63B-024/00 | A61B-005/00 | A61B-005/11 | G06T-011/60 | G06F-003/0481 | G16H-040/67 | G16H-020/30 | A63B-071/06 | G06F-003/0484 | G06F-003/0485 | G06F-003/0488 | G06K-009/00 | G16H-050/30","","","","","","4921045001448"
"US","US","P","B2","Estimation device, living body count estimation device, estimation method, and recording medium","An estimation device includes: a living body information extraction unit that extracts living body information which is a component corresponding to one or more living bodies in a space; an eigenvector calculation unit that calculates one or more eigenvectors of a living body correlation matrix obtained from the living body information; a first position estimation unit that estimates, using the living body correlation matrix, positions of the one or more living bodies and at least one false image, according to a predetermined position estimation method; a second steering vector output unit that extracts, from first steering vectors stored in a storage, and outputs as second steering vectors, first steering vectors corresponding to the positions estimated; and a second position estimation unit that estimates at least one of the position and the number of the one or more living bodies, using the one or more eigenvectors and the second steering vectors.","1. An estimation device, comprising: a living body information extraction unit configured to extract, from a reception signal obtained by receiving a signal transmitted to a predetermined space, living body information which is a component corresponding to one or more living bodies present in the predetermined space;an eigenvector calculation unit configured to calculate one or more eigenvectors of a living body correlation matrix obtained from the living body information;a first position estimation unit configured to estimate, using the living body correlation matrix, positions including a position of each of the one or more living bodies and a position of at least one false image, according to a predetermined position estimation method;a second steering vector output unit configured to extract, from among a plurality of first steering vectors stored in advance in a storage, first steering vectors corresponding to the positions estimated by the first position estimation unit, and output the first steering vectors as second steering vectors; anda second position estimation unit configured to estimate at least one of the position of each of the one or more living bodies and a total number of the one or more living bodies, using the one or more eigenvectors and the second steering vectors.","12","16/389541","2019-04-19","2019-0339379","2019-11-07","11163057","2021-11-02","PANASONIC INTELLECTUAL PROPERTY MANAGEMENT CO., LTD.","Shoichi Iizuka | Takeshi Nakayama | Naoki Honma | Nobuyuki Shiraki","2018-088490 | 2019-005577","JP | JP","2018-05-02 | 2019-01-16","G01S-0013/88","G01S-0013/88 | G01S-0013/04 | G01S-0013/426 | H04L-0012/00 | A61B-0005/0245 | G06F-0003/00 | G06K-0009/00362 | G06K-0009/00771 | G06T-0007/246 | G06T-2207/10028 | G06T-2207/30196 | G06T-2207/30242","G06K-009/00","G06K-009/00 | G01S-013/88 | G01S-013/42 | G01S-013/04 | H04L-012/00 | G06T-007/246 | G06F-003/00 | A61B-005/0245","","","","","","4921045003478"
"US","US","P","B2","Decision-support application and system for medical differential-diagnosis and treatment using a question-answering system","A method, computer system, and computer program product for decision support is provided. The present invention may include receiving a problem case information and generating a query based on the problem case information. The present invention may also include generating a plurality of answers for the query using the question-answering module. The present invention may also include calculating numerical values for multiple evidence dimensions from evidence sources for each of the answers using the question-answering module and may further include calculating a corresponding confidence value for each of the answers based on the numerical value of each evidence dimension using the question-answering module. The present invention may also include outputting the generated answers, the corresponding confidence values, and the numerical values of each evidence dimension for one or more selected answers using the input/output module.","1. A computer-implemented method comprising: identifying, by a decision-support system, at least one semantic concept in a problem case information that is logically related to a problem case-specific domain, wherein the decision-support system comprises a computerized device having access to a plurality of evidence sources containing knowledge associated with the problem case-specific domain;generating, by the decision-support system, a query based on the identified at least one semantic concept in the problem case information;in response to the generated query, generating, by the decision-support system, a plurality of answers by identifying a corresponding text within an evidence source associated with the problem case-specific domain to support the plurality of generated answers;calculating, by the decision-support system, a confidence value corresponding to each answer of the plurality of generated answers based on a plurality of calculated evidence dimensions relevant to the problem case-specific domain;outputting, concurrently by the decision-support system to a decision-maker, the plurality of generated answers, the calculated confidence value corresponding to each answer of the plurality of generated answers, and at least one generated digital link to the evidence source for each of the plurality of generated answers so that the plurality of generated answers, the calculated confidence value corresponding to each generated answer, and the at least one generated digital link to the evidence source for each of the plurality of generated answers are usable by the decision-maker to perform an action regarding the problem case information;in response to determining that the calculated confidence value corresponding to each answer of the plurality of generated answers is affected by at least one relevant information that is not contained within the problem case information, identifying the at least one relevant information as at least one missing information; andtransmitting, by the decision-support system, a request to the decision-maker to add the at least one missing information to the problem case information.","20","16/693963","2019-11-25","2020-0089677","2020-03-19","11163763","2021-11-02","INTERNATIONAL BUSINESS MACHINES CORPORATION","Sugato Bagchi | David A. Ferrucci | Anthony T. Levas | Erik T. Mueller","","","","G06F-0016/24522","G06F-0016/24522 | A61B-0005/00 | A61B-0034/10 | G06F-0003/048 | G06F-0016/2428 | G06F-0016/2455 | G06F-0016/31 | G06F-0016/334 | G06F-0016/532 | G06F-0016/90335 | G06F-0016/93 | G06F-0040/134 | G06F-0040/169 | G06N-0005/027 | G16H-0010/20 | G16H-0010/60 | G16H-0015/00 | G16H-0040/20 | G16H-0050/20 | G16H-0050/70 | G16H-0070/00 | G06F-0040/211 | Y02A-0090/10","G06F-016/2452","G06F-016/2452 | G06F-003/048 | G06N-005/02 | G16H-050/70 | G16H-010/20 | G16H-050/20 | G06F-016/31 | G06F-016/93 | G06F-016/33 | G06F-016/532 | G06F-016/242 | G06F-016/2455 | G06F-016/903 | A61B-005/00 | G06F-040/134 | G06F-040/169 | A61B-034/10 | G16H-070/00 | G16H-010/60 | G16H-015/00 | G16H-040/20 | G06F-040/211","","","","","","4921045004178"
"US","US","P","B2","Machine learning system for predicting optimal interruptions based on biometric data collected using wearable devices","Method and apparatus for using machine learning to monitor biometric data to provide intelligent alerts are provided. At a first moment in time, first biometric data for a plurality of users are received from a plurality of sensor devices. A group metric is generated by processing the first biometric data using at least one trained machine learning model, and it is determined that the group metric does not satisfy one or more predefined criteria. At a second moment in time, second biometric data for the plurality of users is received from the plurality of sensor devices, and an updated group metric is generated by processing the second biometric data using the at least one trained machine learning model. Upon determining that the updated group metric satisfies the one or more predefined criteria, an indication is provided that the one or more predefined criteria have been satisfied.","1. A method, comprising: receiving, at a first moment in time, first biometric data for a plurality of users from a plurality of sensor devices;identifying, for each respective user of the plurality of users, a respective user category based on characteristics of the respective user;retrieving, for each respective user category, a respective trained machine learning model, wherein the respective trained machine learning model was trained specifically for users belonging to the respective user category;generating a group metric by processing the first biometric data using at least one trained machine learning model, wherein biometric data associated with each respective user is processed using the respective trained machine learning model corresponding to the respective user category identified based on characteristics of the respective user, comprising: processing a first subset of the first biometric data using a first trained machine learning model based on determining that each user associated with the first subset belongs to a first user category for which the first trained machine learning model was trained; andprocessing a second subset of the first biometric data using a second trained machine learning model based on determining that each user associated with the second subset belongs to a second user category for which the second trained machine learning model was trained;determining that the group metric does not satisfy one or more predefined criteria;receiving, at a second moment in time, second biometric data for the plurality of users from the plurality of sensor devices;generating an updated group metric by processing the second biometric data using the at least one trained machine learning model; andupon determining that the updated group metric satisfies the one or more predefined criteria, providing an indication that the one or more predefined criteria have been satisfied.","20","15/846570","2017-12-19","2019-0188604","2019-06-20","11157832","2021-10-26","INTERNATIONAL BUSINESS MACHINES CORPORATION","Amitava Kundu | Sujan Sarathi Ghosh | Abhijit Singh","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/1118 | A61B-0005/7267 | G06F-0016/337 | G06F-0021/32 | A61B-0005/7264 | G06F-0001/163","G06F-021/00","G06F-021/00 | G06N-020/00 | G06F-021/32 | G06F-016/335 | A61B-005/00 | A61B-005/11 | G06F-001/16","","","","","","4921044004435"
"US","US","P","B2","Personal authentication apparatus system and method","A target authentication device includes an electrode to detect an electrical signal associated with a user of the device. The electrical signal represents an authentication code for the device. An authentication receiver module is coupled to the electrode. The module receives the electrical signal from the electrode and determines whether the electrical signal matches a predetermined criterion to authenticate the identity of the user based on the electrical signal. An authentication module is also disclosed. The authentication module includes one electrode to couple an electrical signal associated with a user to a user of a target authentication device, the electrical signal represents an authentication code for the device. An authentication transmission module is coupled to the electrode. The authentication transmission module transmits the electrical signal from the electrode. A method of authenticating the identity of a user of a target authentication device also is disclosed.","1. An apparatus, comprising: a physiological sensing module for receiving an electrical signal from a body associated device,the electrical signal including a profile of biometric signals including heart rate, body temperature, and circadian rhythm detected by at least one electrode coupled to a body of a user;the body-associated device placed in electrical contact with the body of a user of a target authentication device, wherein the body-authentication device is configured with a non-physiological secret code;wherein the electric signal comprises the profile of biometric signals hashed by the body associated device with the non-physiological secret code to yield a signature;an authentication receiver module coupled to the at least one electrode, the authentication receiver module configured to receive the electrical signal from the at least one electrode and to authenticate an identity of the user when the received electrical signal matches a predetermined criterion;wherein the predetermined criterion includes a baseline biometric signature stored in a database comprising a running average of previously detected biometric signals including heart rate, body temperature, and circadian rhythm received by the physiological sensing module as a running series of signatures generated by the body associated device, each signature in the running series including a different profile of biometric signals including heart rate, body temperature, and circadian rhythm to be used to calculate the running average.","17","15/592423","2017-05-11","2018-0096547","2018-04-05","11158149","2021-10-26","OTSUKA PHARMACEUTICAL CO., LTD.","Timothy Robertson | George Savage | Benedict Costello | David O'Reilly","","","","G07C-0009/37","G07C-0009/37 | A61B-0005/0006 | A61B-0005/0028 | A61B-0005/053 | A61B-0005/117 | A61B-0005/332 | A61B-0005/6898 | G06F-0021/34 | G06K-0009/00885 | H04B-0013/005 | A61B-0005/02055 | A61B-0005/11 | A61B-0005/1112 | A61B-0005/145 | A61B-0005/14532 | A61B-0005/282 | A61B-0005/30 | A61B-2560/0209 | A61B-2560/0412 | A61B-2560/0468 | A61B-2562/0219 | G06K-2009/00939","A61B-005/00","A61B-005/00 | G07C-009/37 | G06F-021/34 | A61B-005/332 | A61B-005/053 | A61B-005/117 | G06K-009/00 | H04B-013/00 | A61B-005/30 | A61B-005/282 | A61B-005/0205 | A61B-005/11 | A61B-005/145","","","","","","4921044004752"
"US","US","P","B2","Vehicle sound output device, sound output control method, and non-transitory computer readable medium storing sound output control program","A vehicle sound output device including: an acquiring section configured to acquire wakefulness level information that indicates a degree of a wakefulness state of a driver of a vehicle; and a control section configured to, in a case in which the degree of the wakefulness state indicates a decreased level of wakefulness, control a sound output section to output a sound based on at least one of a predetermined frequency, a predetermined tone, or a predetermined content that stimulates a sympathetic nerve of the driver.","1. A vehicle sound output device comprising: an acquiring section configured to acquire wakefulness level information that indicates a degree of a wakefulness state of a driver of a vehicle;a database including a plurality of content based on monitored speech content of the driver, each content corresponding to a level of interest of the driver and to a degree of the wakefulness state of the driver; anda control section configured to, in a case in which the degree of the wakefulness state indicates a decreased level of wakefulness, control a sound output section to output a sound based on a predetermined content that stimulates a sympathetic nerve of the driver, the predetermined content being selected from the plurality of content based on the level of interest of the driver and the degree of the wakefulness state of the driver in order to cause an interest degree of the driver to increase as the degree of the wakefulness state decreases,wherein the sound output section outputs the sound as speech at a predetermined frequency that increases as the degree of the wakefulness state decreases, and at a predetermined tone that has a deviation degree that increases as the degree of the wakefulness state decreases, the predetermined tone includes one or more of a rhythm, tempo, and an intonation.","15","16/231677","2018-12-24","2019-0212969","2019-07-11","11150863","2021-10-19","TOYOTA JIDOSHA KABUSHIKI KAISHA","Hideki Kobayashi | Akihiro Muguruma | Yukiya Sugiyama | Shota Higashihara | Riho Matsuo | Naoki Yamamuro","2018-002919","JP","2018-01-11","G06F-0003/16","G06F-0003/16 | A61B-0005/0022 | A61B-0005/18 | A61B-0005/6893 | A61B-0005/7203 | A61B-0005/7405 | A61B-0005/746 | B60K-0028/06 | G08B-0021/06 | A61B-0005/0077 | A61B-2503/22","G06F-017/00","G06F-017/00 | G06F-003/16 | A61B-005/00 | A61B-005/18 | B60K-028/06 | G08B-021/06","","","","","","4921043004267"
"US","US","P","B2","Method for treating a surface","Method for treating a surface includes: automatically evaluating at least one digital image which includes the target surface; determining the nature of the target surface according to the evaluation of the at least one digital image; determining at least one available treatment implement according to the evaluation of the at least one image; determining the nature of the surface treatment according to the evaluation of the at least one image; automatically determining a use of the determined treatment implement in the determined treatment of the determined surface; and providing information analogous to the determined use of the treatment implement.","1. A method for treating a target surface, the method comprising steps of: a. using machine learning in evaluating audio data and at least one digital image;b. determining a practitioner according to the evaluation of the audio data;c. determining an interaction between an implement and a target surface according to the evaluation of the at least one digital image;d. using machine learning to determine a use of the determined implement in the determined treatment of the determined surface;e. altering performance characteristics of the implement or providing information analogous to the determined use.","19","16/906050","2020-06-19","2020-0320350","2020-10-08","11151421","2021-10-19","The Procter & Gamble Company","Jonathan Livingston Joyce | Faiz Feisal Sherman | Jennifer Theresa Werner","","","","G06K-0009/6262","G06K-0009/6262 | A45D-0044/005 | A61B-0005/44 | G06K-0009/4628 | G06K-0009/6218 | G06K-0009/6267 | G06N-0020/00 | G06Q-0050/01 | A61B-0005/0077 | A61B-0005/441 | A61B-0005/448 | A61B-0005/7264 | A61B-0005/7275 | A61B-0005/74","G06K-009/62","G06K-009/62 | A45D-044/00 | A61B-005/00 | G06K-009/46 | G06N-020/00 | G06Q-050/00","","","","","","4921043004819"
"US","US","P","B2","Autonomous vehicle control using heart rate collection based on video imagery","Video of one or more vehicle occupants is obtained and analyzed. Heart rate information is determined from the video. The heart rate information is used in cognitive state analysis. The heart rate information and resulting cognitive state analysis are correlated to stimuli, such as digital media, which is consumed or with which a vehicle occupant interacts. The heart rate information is used to infer cognitive states. The inferred cognitive states are used to output a mood measurement. The cognitive states are used to modify the behavior of a vehicle. The vehicle is an autonomous or semi-autonomous vehicle. Training is employed in the analysis. Machine learning is engaged to facilitate the training. Near-infrared image processing is used to obtain the video. The analysis is augmented by audio information obtained from the vehicle occupant.","1. A computer-implemented method for vehicle control comprising: obtaining video of a vehicle occupant, using one or more imaging devices within the vehicle;analyzing the video to determine heart rate information, wherein the analyzing includes: identifying a face of the vehicle occupant in a portion of the video;separating pixels from the video of the vehicle occupant, into at least a green pixel temporal intensity trace;training a statistical classifier, wherein the training is learned from a data set consisting of human blood volume pulse synchronized with face videos; andrecognizing a pulse, from the video of the vehicle occupant, using the statistical classifier, by learning patterns of variability in the mean of the pixel temporal intensity trace;correlating the heart rate information to a stimulus that the vehicle occupant is encountering;inferring cognitive states of the vehicle occupant using the heart rate information that was correlated; andmodifying behavior for the vehicle, based on the cognitive states that were inferred.","27","16/729730","2019-12-30","2020-0134672","2020-04-30","11151610","2021-10-19","AFFECTIVA, INC.","Rana el Kaliouby | Viprali Bhatkar | Niels Haering | Youssef Kashef | Ahmed Adel Osman","","","","G06Q-0030/0265","G06Q-0030/0265 | A61B-0005/02055 | G06K-0009/00234 | G06K-0009/00261 | G06K-0009/00281 | G06K-0009/00832 | G06Q-0030/0269 | G10L-0015/26","G06Q-030/02","G06Q-030/02 | G06K-009/00 | A61B-005/0205 | G10L-015/26","","","","","","4921043005004"
"US","US","P","B2","Medical data managing apparatus and medical data managing system","A medical data managing apparatus includes processing circuitry. The processing circuitry generates network access data based on patient information of a patient. The processing circuitry acquires the reference data from a data generating device via a communication network constructed based on the network access data, and checks the reference data against benchmark data included in the patient information. The processing circuitry acquires, when it is determined that the reference data and the benchmark data represent same patient as a result of the check, the multiple non-DICOM data from the data generating device via the constructed communication network. The processing circuitry classifies each of the multiple non-DICOM data into first non-DICOM data to be registered in a data archive apparatus or other second non-DICOM data, and displays the first non-DICOM data and the second non-DICOM data on a display in different display modes.","1. A medical data managing apparatus, connected via a communication network to (1) a data generating device owning reference data for verification and multiple non-DICOM (Digital Imaging and Communications in Medicine) data and (2) a separate data archive apparatus, the medical data managing apparatus comprising: a display; andprocessing circuitry configured to: generate network access data based on patient information of a patient,acquire the reference data from the data generating device via a communication network constructed based on the network access data, and check the reference data against benchmark data included in the patient information,acquire, when it is determined that the reference data and the benchmark data represent a same patient as a result of the check, the multiple non-DICOM data from the data generating device via the constructed communication network,classify each of the multiple non-DICOM data into first non-DICOM data to be registered in the separate data archive apparatus or second non-DICOM data not needed to be registered in the separate data archive apparatus, anddisplay on the display the first non-DICOM data and the second non-DICOM data in different display modes such that the first non-DICOM data is visually distinguishable from the second non-DICOM data.","21","15/869693","2018-01-12","2018-0204639","2018-07-19","11152104","2021-10-19","CANON MEDICAL SYSTEMS CORPORATION","Hayato Konishi | Yasuyuki Miyoshi | Keita Mitsumori | Yosuke Yanagida","2017-003968","JP","2017-01-13","G16H-0030/20","G16H-0030/20 | G06F-0021/32 | G06F-0021/6245 | G06K-0009/6267 | G16H-0010/60 | H04L-0067/02 | G06K-0009/00255 | G06K-0009/00288 | G06K-0009/6201 | H04L-0012/4641","A61B-005/00","A61B-005/00 | G16H-030/20 | G16H-010/60 | G06F-021/62 | G06K-009/62 | H04L-029/08 | G06F-021/32 | G06K-009/00 | H04L-012/46","","","","","","4921043005494"
"US","US","P","B2","Electronic device and method for managing body information by electronic device","An electronic device and method are disclosed. The electronic device includes a communication circuit, a memory storing identifiers for one or more external electronic devices defined as a group, and a processor. The processor implements the method, including receiving biometric information detected by an external biometric detection device via transmission from at least one external electronic device of the group, selecting from within the group a particular external electronic device based on the received biometric information and the information related to the particular external electronic device, and transmitting the received biometric information to the selected particular external electronic device.","1. An electronic device, comprising: a communication circuit;a memory storing identifiers for a plurality of external electronic devices included in a first group, including biometric information for a plurality of users associated respectively with the plurality of external electronic devices; anda processor configured to: receive biometric information detected by an external biometric detection device wherein the received biometric information does not include identification of a user to whom the received biometric information belongs,select a particular external electronic device from among the plurality of external electronic devices associated respectively with the plurality of users, based on the received biometric information and stored biometric information associated with the plurality of external electronic devices, wherein the particular external device belongs to a particular user, andtransmit the received biometric information to the selected particular external electronic device belonging to the particular user,wherein the processor is configured to select the particular external electronic device by calculating a plurality of similarity scores respectively for the plurality of external electronic devices based on comparing the received biometric information to the stored biometric information, the plurality of similarity scores representing respective deviations of the stored biometric information with the received biometric information, andselecting the particular external electronic device having a similarity score indicating a lowest deviation with the received biometric information from among the plurality of external electronic devices,wherein the similarity score comprises a distance-based similarity score, andthe particular external electronic device is selected as previously stored biometric information stored in the particular external electronic device has a lowest similarity score with the received biometric information from among the group,wherein the distance-based similarity score comprises an Euclidean distance-based similarity score.","6","15/844871","2017-12-18","2018-0176019","2018-06-21","11153090","2021-10-19","SAMSUNG ELECTRONICS CO., LTD.","Seon-Hyung Lee | Soon-Hwan Kwon","10-2016-0173846","KR","2016-12-19","H04L-0009/3231","H04L-0009/3231 | A61B-0005/117 | A61B-0005/4869 | G06K-0009/00892 | G16H-0015/00 | G16H-0050/70 | H04L-0067/2842 | H04L-0067/306 | H04W-0004/38","H04L-009/32","H04L-009/32 | G06K-009/00 | H04L-029/08 | A61B-005/117 | H04W-004/38 | A61B-005/00 | G16H-015/00 | G16H-050/70","","","","","","4921043006475"
"US","US","P","B2","Electronic device for recognition of mental behavioral attributes based on deep neural networks","An electronic device that handles recognition of mental behavioral, affect, emotional, mental states, mental health, or mood-based attributes based on deep neural networks (DNNs), stores a set of EEG signals and a set of bio-signals associated with a subject. The electronic device trains a plurality of first recognition models on a training set of EEG signals and a training set of bio-signals associated with different training subjects. The electronic device trains a second recognition model on a feature vector from output layers of the plurality of first recognition models. The electronic device estimates a plurality of dependency or relationship data by application of the trained plurality of first recognition models on the set of EEG signals and bio-signals. The electronic device identifies a mental behavioral attribute of the subject by application of the trained second recognition model on the plurality of signals and their relationship data.","1. An electronic device, comprising: a memory configured to store a set of electroencephalogram (EEG) signals, a set of different types of bio-signals, and a plurality of user parameters associated with a specific subject; andat least one processor configured to: train a plurality of first recognition models, that includes a first plurality of deep neural networks (DNNs), on a training set of EEG signals and a training set of different types of bio-signals associated with a set of training subjects, wherein a first DNN of the plurality of DNNs is trained based on a first training set of EEG signals of the training set of EEG signals,each EEG signal of the first training set of EEG signals corresponds to a first region of a brain of a respective training subject of the plurality of training subjects,a second DNN of the plurality of DNNs is trained based on a second training set of EEG signals of the training set of EEG signals,each EEG signal of the second training set of EEG signals corresponds to a second region of the brain of the respective training subject of the plurality of training subjects,the first region is different from the second region, andthe set of training subjects is different from the specific subject;train a second recognition model, that corresponds to a second plurality of deep neural networks (DNNs), on a feature vector from output layers of the plurality of first recognition models, wherein the second recognition model is trained based on an interconnectivity parameter and a dependency parameter between a first set of mental behavioral attributes of a set of mental behavioral attributes and a second set of mental behavioral attributes of the set of mental behavioral attributes,the interconnectivity parameter indicates a functional connectivity between a first region of a brain of the specific subject and a second region of the brain of the specific subject in a case in which the brain of the specific subject stays functional for at least two mental behavioral attributes of the set of mental behavioral attributes,the first region of the brain of the specific subject is different from the second region of the brain of the specific subject,the first region of the brain of the specific subject corresponds to an emotional state of the specific subject,the second region of the brain of the specific subject corresponds to a body movement of the specific subject, andthe dependency parameter indicates an effect of a first mental behavioral attribute of the set of mental behavioral attributes on a second mental behavioral attribute of the set of mental behavioral attributes;estimate a plurality of relationship data, which indicates one of the functional connectivity or a dependency between the first region of the brain of the specific subject and the second region of the brain of the specific subject,wherein the plurality of relationship data is estimated by application of the trained plurality of first recognition models on the set of EEG signals associated with the specific subject and the set of different types of bio-signals associated with the specific subject; andidentify a specific mental behavioral attribute of the specific subject from the set of mental behavioral attributes, by application of an output layer of the trained second recognition model on the estimated plurality of relationship data.","20","16/155180","2018-10-09","2020-0107766","2020-04-09","11141088","2021-10-12","SONY CORPORATION","Ming-Chang Liu | Ahmad Khodayari-Rostamabad","","","","A61B-0005/165","A61B-0005/165 | A61B-0005/316 | A61B-0005/369 | A61B-0005/6814 | A61B-0005/7267 | G06F-0003/015 | G06K-0009/00892 | G06K-0009/6227 | G06K-0009/6254 | G06N-0003/0454 | A61B-0005/0205","A61B-005/16","A61B-005/16 | A61B-005/00 | G06F-003/01 | G06K-009/00 | G06K-009/62 | G06N-003/04 | A61B-005/316 | A61B-005/369 | A61B-005/0205","","","","","","4921042000910"
"US","US","P","B2","Image processing apparatus and computer-readable storage medium storing instructions for specifying lesion portion and performing differentiation classification in response to judging that differentiation classification operation is engaged based on signal from endoscope","An image processing apparatus have a processor configured to: receive a plurality of images of a tissue including a lesion portion, the plurality of images being received from an endoscope; judge, based on a signal from the endoscope, whether or not to engage a differentiation classification operation; in response to judging that the differentiation classification operation is engaged, process the plurality of images to specify the lesion portion within one or more of the plurality of images; and perform differentiation classification on the one or more images to classify the lesion portion into at least one class of a plurality of classes; and in response to judging that the differentiation classification operation is not engaged, not process the plurality of images to specify the lesion portion within the one or more of the plurality of images.","1. An image processing apparatus comprising: a processor comprising hardware, wherein the processor is configured to: receive a plurality of images of a tissue including a lesion portion, the plurality of images being received from an endoscope operated by an operator;judge, based on a signal from the endoscope, whether or not to engage a differentiation classification operation to classify the lesion portion in one or more of the plurality of images;in response to judging that the differentiation classification operation is engaged, process the plurality of images to specify the lesion portion within the one or more of the plurality of images; andperform differentiation classification on the one or more images in which the lesion portion is specified to classify the lesion portion into at least one class of a plurality of classes; andin response to judging that the differentiation classification operation is not engaged, not process the plurality of images to specify the lesion portion within the one or more of the plurality of images, andwherein the processor is configured to: judge, based on the signal from the endoscope, whether or not a distal end of an insertion section of the endoscope is still;in response to judging that the distal end of the insertion section is still, judge that the differential classification operation is engaged; andin response to judging that the distal end of the insertion section is not still, judge that the differential classification operation is not engaged.","14","16/433483","2019-06-06","2019-0311476","2019-10-10","11145053","2021-10-12","OLYMPUS CORPORATION","Takehito Hayami | Yamato Kanda | Takashi Kono | Mitsutaka Kimura | Ryoji Takami","","","","G06T-0007/0012","G06T-0007/0012 | A61B-0001/00 | A61B-0001/00009 | A61B-0001/04 | A61B-0001/0638 | A61B-0001/31 | A61B-0005/489 | A61B-0005/7264 | G06F-0003/013 | G06K-0009/628 | G06T-0007/11 | G06T-0007/20 | G06T-0007/60 | G06T-0007/70 | H04N-0005/2256 | G06K-2209/05 | G06T-2207/10068 | G06T-2207/10152 | G06T-2207/30028 | G06T-2207/30092 | G06T-2207/30096 | G06T-2207/30101 | G06T-2207/30244 | H04N-2005/2255","G06T-007/00","G06T-007/00 | G06T-007/11 | G06T-007/70 | A61B-001/00 | A61B-001/06 | A61B-001/31 | A61B-005/00 | G06F-003/01 | G06K-009/62 | G06T-007/20 | G06T-007/60 | H04N-005/225 | A61B-001/04","","","","","","4921042004843"
"US","US","P","B2","Medical device and method for providing information for glycemic control","A medical device for providing information for glycemic control is provided, wherein the device comprises storage means arranged to store data, receiving means arranged to receive blood glucose value data and security data, data processing means arranged to execute a first processing function for modifying data retrieved from the storage means and to execute a second processing function for providing information for glycemic control based on the blood glucose value data and data retrieved from the storage means, validating means arranged to validate the received security data and to provide validation data corresponding to the validation of the received security data, and safety means arranged to control an execution of at least a predetermined function out of the first and second processing functions based on the validation data.","1. A medical system for the treatment of diabetes, the medical system comprising: an administration computing system configured to manage security data for one or more medical devices; andat least one medical device for providing information for glycemic control, namely a dose of insulin to be set, the at least one medical device comprising: a memory arranged to store data comprising profile parameters for different dose adjustment profiles each comprising a specific initial dose value, a specific time interval for increasing the dose, a specific dose increase step and a specific low blood glucose threshold value;a receiver to receive blood glucose value data and the security data; andat least one processor connected to the receiver and configured to: validate the received security data and provide validation data corresponding to the validation of the received security data;unlock at least one first processing function and at least one second processing function for execution based on the validation data, wherein execution of the at least one first processing function and the at least one second processing function is allowed based on a control signal generated where the received security data was successfully validated, wherein the at least one first processing function includes adjusting the profile parameters for a selected dose adjustment profile and the at least one second processing function includes stepwise adapting the dose of insulin based, at least, on the selected dose adjustment profile to determine the value for the dose of insulin to be administered based on the received blood glucose value data and data retrieved from the memory, wherein the at least one second processing function is unlocked by providing the security data once, and wherein the at least one first processing function must be unlocked each time it is executed;execute the unlocked at least one first processing function and the unlocked at least one second processing function; andtransmit information regarding the determined dose to a dose setter configured for administration of the determined dose of insulin to a user, wherein the delivery of the determined dose to be administered is automatically activated.","20","15/426240","2017-02-07","2017-0143901","2017-05-25","11135367","2021-10-05","SANOFI-AVENTIS DEUTSCHLAND GMBH","Andrew Tubb","2009-001560","EP","2009-02-04","A61M-0005/1723","A61M-0005/1723 | A61B-0005/14532 | A61M-0005/142 | A61M-0015/0065 | G01N-0033/49 | G06F-0021/34 | G16H-0020/17 | G16H-0070/60 | A61M-2205/6009","G01N-033/48","G01N-033/48 | A61M-005/172 | A61B-005/145 | A61M-005/142 | G16H-020/17 | G16H-070/60 | A61M-015/00 | G01N-033/49 | G06F-021/34","","","","","","4921041001373"
"US","US","P","B2","Iris recognition using eye-tracking system","An eye-tracking system (e.g., a virtual reality or augmented realty headset) can be used for eye tracking and for iris recognition. Illuminators used to illuminate eyes of a user during eye tracking can be selectively powered on and off in connection with capturing image information in order to obtain image information that suitably depicts an iris region of an eye of the user. This image information can be used to recognize the iris region and by so doing authenticate and/or identify the user.","1. A system comprising: a camera configured to capture images of at least one eye of a user;a plurality of illuminators configured to project light toward the at least one eye of the user; anda management device in communication with the camera and the plurality of illuminators, the management device configured to: cause, by a set of illuminators of the plurality of illuminators, illumination of the at least one eye, wherein the set of illuminators comprises a specific percentage of the plurality of illuminators;cause, by the camera, capture of image information associated with the at least one eye during the illumination of the at least one eye; anddetermine whether an iris of the at least one eye can be recognized based at least in part on the image information.","24","16/298585","2019-03-11","2019-0278987","2019-09-12","11138429","2021-10-05","Tobii AB","Henrik Eskilsson | Marten Skogo","","","","G06K-0009/00617","G06K-0009/00617 | G02B-0027/017 | G06F-0003/013 | G06K-0009/00604 | A61B-0005/117 | G02B-2027/014 | G02B-2027/0178 | G06K-0009/0061 | G07C-0009/37","G06K-009/00","G06K-009/00 | G06F-003/01 | G02B-027/01 | A61B-005/117 | G07C-009/37","","","","","","4921041004415"
"US","US","P","B2","Health monitoring eco-system with optimized power consumption","A power-optimized eco-system for tracking a user's health comprises: one or more wearable remote sensors, each wirelessly communicating only with one wearable central sensor; a portable device readily accessible to the user; and a cloud platform. Each sensor is configured to measure data indicative of one or more physiological parameters. The central sensor is configured to receive and subsequently process data measured by each remote sensor, to process data measured by the central sensor, and to generate corresponding instructions. The portable device comprises: a receiver wirelessly receiving the processed data and instructions from the central sensor; a processor running a mobile application handling the processed data and instructions; and a transmitter. The cloud platform is configured to: receive the processed data from the transmitter; analyze the received processed data; and transmit the results of the analysis to at least one of the portable device and an authorized healthcare entity.","1. An adaptive power-optimized eco-system for tracking health of a user, the eco-system comprising: an electrodermal activity sensor configured to be worn by the user;a blood glucose sensor configured to be worn by the user;each of said electrodermal sensor and said blood glucose sensor configured to communicate with a wearable central sensor, said central sensor configured to receive and subsequently process first electrodermal activity data measured by said electrodermal activity sensor and to generate corresponding instructions responsive to said electrodermal activity data, said corresponding instructions comprising an automatically triggered order, transmitted from said central sensor to said glucose sensor, to collect glucose data.","12","15/653346","2017-07-18","2017-0367599","2017-12-28","11132424","2021-09-28","LIFEPLUS, INC.","Alodeep Sanyal | Benjamin Mbouombouo | Sankha Bhattacharya | Nilanjan Banerjee | Indranil Sen-Gupta","","","","G06F-0019/3418","G06F-0019/3418 | A61B-0005/0002 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/02 | A61B-0005/04 | A61B-0005/0402 | A61B-0005/6801 | A61B-0005/681 | A61B-0005/6803 | A61B-0005/74 | A61B-0005/742 | A61B-0005/746 | G06F-0019/321 | G06N-0020/00 | G06Q-0010/06 | G16H-0040/63 | G16H-0040/67 | G16H-0050/20 | H04L-0029/06 | H04L-0065/40 | H04L-0067/04 | H04L-0067/10 | H04L-0067/12 | H04L-0067/22 | H04L-0067/30 | H04W-0004/38 | A61B-0005/01 | A61B-0005/02055 | A61B-0005/02416 | A61B-0005/0404 | A61B-0005/0488 | A61B-0005/0533 | A61B-0005/1114 | A61B-0005/6814 | A61B-0005/6816 | A61B-0005/6823 | A61B-0005/6826 | A61B-0005/6828 | A61B-0005/6829 | A61B-0005/6831 | A61B-0005/6838 | A61B-0005/6843 | A61B-0005/72 | A61B-0005/7267 | A61B-0005/7275 | A61B-2560/0209 | A61B-2560/0223 | G06F-0019/00 | H04L-0029/00 | H04L-0067/00 | H04L-0067/306 | H04M-0001/7253 | H04M-0001/72541 | Y02D-0070/00 | Y02D-0070/10 | Y02D-0070/12 | Y02D-0070/126 | Y02D-0070/1262 | Y02D-0070/14 | Y02D-0070/142 | Y02D-0070/144 | Y02D-0070/20 | Y02D-0070/26","G06F-019/00","G06F-019/00 | A61B-005/00 | H04L-029/08 | G16H-050/20 | G16H-040/67 | G16H-040/63 | H04W-004/38 | A61B-005/04 | H04M-001/725 | A61B-005/053 | A61B-005/01 | A61B-005/024 | A61B-005/11 | A61B-005/0404 | G06N-020/00 | G06Q-010/06 | H04L-029/06 | A61B-005/02 | A61B-005/0402 | A61B-005/0488 | A61B-005/0533 | A61B-005/0205 | H04L-029/00","","","","","","4921040003672"
"US","US","P","B2","Data labeling system and method operative with patient and clinician controller devices disposed in a remote care architecture","A system and method for facilitating remote care management involving a patient having an implantable medical device (IMD). Upon establishing a remote care session between a patient controller device and a clinician programmer, wherein the clinician and the patient are remotely located with respect to each other, input from the patient or the clinician may be received via a user interface control associated with a particular functionality or aspect of the remote care session, including audiovisual (AV) communications, remote therapy programming, and related context. Responsive to the user input, a dialog interface is effectuated at one of the patient controller device and/or the clinician programmer. A user characterization label is received via the dialog interface from the user, wherein the user characterization label is indicative of a subjective assessment of the particular functionality of the remote care session, which may be used in generating user-labeled data pertaining thereto.","1. A method for facilitating remote care management involving a patient having an implantable medical device (IMD), the method comprising: establishing a remote care session between a patient controller device associated with the patient and a clinician programmer device associated with a clinician, wherein the clinician and the patient are remotely located with respect to each other and the remote care session includes an audiovisual (AV) communication session controlled by one or more audio controls and one or more video controls provided at the patient controller device and by one or more audio controls and one or more video controls provided at the clinician programmer device, the remote care session further including a remote therapy session for providing one or more programming instructions to the patient'ss IMD via the patient controller device responsive to determining that the patient requires remote therapy;receiving user input via a user interface control provided with at least one of the patient controller device and the clinician programmer device, the user interface control associated with a particular functionality of at least one of the AV communication session and the remote therapy session;responsive to the user input, effectuating a dialog interface at one of the patient controller device and the clinician programmer device; andreceiving a user characterization label via the dialog interface, the user characterization label indicating a subjective assessment of the particular functionality of at least one of the AV communication session and the remote therapy session selected via the user interface control by the patient or the clinician,wherein the particular functionality relates to at least one of audio quality of the AV communication session, video quality of the AV communication session, patient motor response capture quality, and patient vocalization capture quality,wherein the dialog interface comprises at least one of an audio dialog window facilitated by voice recognition, a video dialog window for facilitating motion capture and facial recognition, a label tray having a plurality of predetermined labels represented by corresponding software buttons, and a pull-down menu dialog window for facilitating label selection,wherein the user characterization label comprises at least one of a voice label, a ranking label, a multi-category label, a binary category label, a graphic icon label, an emoji label, a gesture-based label and a sliding scale label, andwherein at least the remote therapy session is paused in response to determining that a particular user characterization label has occurred a predetermined number of times over a configurable period of time.","12","16/901368","2020-06-15","2020-0398063","2020-12-24","11133113","2021-09-28","ADVANCED NEUROMODULATION SYSTEMS, INC.","Scott DeBates | Douglas Alfred Lautner | Tucker Tomlinson | James Nagle","","","","G16H-0080/00","G16H-0080/00 | A61B-0005/0031 | A61B-0005/7435 | A61N-0001/37247 | A61N-0001/37282 | G16H-0010/60 | G16H-0020/30 | G16H-0040/67 | H04L-0043/04 | H04L-0065/1069 | H04L-0065/80 | H04N-0007/14","A61N-001/372","A61N-001/372 | G16H-080/00 | G16H-020/30 | G16H-040/67 | G16H-010/60 | H04L-012/26 | H04L-029/06 | H04N-007/14 | A61B-005/00","","","","","","4921040004356"
"US","US","P","B2","Extracting a mother wavelet function for detecting epilleptic seizure","A method for creating a mother wavelet function. The method includes preparing a plurality of vectors, extracting a kernel from the plurality of vectors, and extracting the mother wavelet function from the kernel. The kernel includes a mode value of a vector of the plurality of vectors.","1. A method for creating a mother wavelet function for detecting epileptic seizure, the method comprising: placing a plurality of electroencephalography (EEG) electrodes on a scalp of a subject;acquiring, utilizing the plurality of EEG electrodes, a plurality of signals comprising a plurality of samples via a plurality of EEG channels associated with the plurality of EEG electrodes;extracting, utilizing one or more processors, a plurality of vectors from the plurality of EEG signals;extracting, utilizing the one or more processors, a kernel from the plurality of vectors by: estimating a multimodal ensemble probability density function (PDF) for a vector of the plurality of vectors; andchoosing a sample of the vector with a maximum value in the multimodal ensemble PDF as a mode value of the vector;extracting, utilizing the one or more processors, the mother wavelet function from the kernel;extracting, utilizing the one or more processors, a plurality of features from the plurality of signals based on the mother wavelet function; andclassifying, utilizing the one or more processors, each of the plurality of signals in one of a seizure class or a non-seizure class based on the plurality of features.","14","15/992161","2018-05-29","2019-0000390","2019-01-03","11123018","2021-09-21","NAJAFABAD BRANCH, ISLAMIC AZAD UNIVERSITY | BEHNAM, MORTEZA | POURGHASSEM, HOSSEIN","Morteza Behnam | Hossein Pourghassem","","","","A61B-0005/726","A61B-0005/726 | A61B-0005/30 | A61B-0005/316 | A61B-0005/374 | A61B-0005/4094 | A61B-0005/7264 | G06F-0017/148 | G06K-0009/0053 | G06K-0009/0055 | G06K-0009/00516 | G06K-0009/00523 | G06K-0009/00536 | G06K-0009/4619 | G16H-0050/30 | G16H-0050/70 | A61B-0005/7267 | G16H-0050/20","A61B-005/00","A61B-005/00 | G06F-017/14 | G06K-009/46 | G16H-050/20 | G06K-009/00 | G16H-050/70 | A61B-005/30 | A61B-005/316 | A61B-005/374 | G16H-050/30","","","","","","4921039000942"
"US","US","P","B2","Method, apparatus and system for tailoring at least one subsequent communication to a user","There is provided a method, apparatus and system for method for tailoring at least one subsequent communication to a user. Data acquired on the user in connection with a plurality of communications to the user is processed to determine a value for one or more characteristics indicative of a state of the user in connection with each communication. At least one communication parameter is varied in respect of each communication. The values for the one or more characteristics indicative of the state of the user that are determined in connection with each communication are compared to identify one or more communication parameters that are most suitable for the user. One or more communication parameters are set for the at least one subsequent communication to the user according to the one or more identified communication parameters. The subsequent communication to the user is enabled based on the set one or more communication parameters.","1. A computer-implemented method for tailoring at least one subsequent communication to a user, the computer-implemented method comprising: processing data acquired on the user in connection with a plurality of communications to the user to determine a value for one or more characteristics indicative of a state of the user in connection with each communication, wherein at least one communication parameter is randomized at a first level of randomness in respect of each communication within a first subset of communications of the plurality of communications, and the at least one communication parameter is randomized at a second level of randomness in respect of each communication within a second subset of communications of the plurality of communications, the first and second levels of randomness being different;comparing the values for the one or more characteristics indicative of the state of the user that are determined in connection with each communication to identify one or more communication parameters that are most suitable for the user;setting one or more communication parameters for the at least one subsequent communication to the user according to the one or more identified communication parameters; andtransmitting, by a communications interface, a signal to a communication device to initiate the at least one subsequent communication, the signal configured to change or enable a change of one or more communication parameters of the communication device for the at least one subsequent communication based on the set one or more communication parameters, wherein the one or more communication parameters of the communication device comprise any one or more of: a time for the at least one subsequent communication to the user, a duration for the at least one subsequent communication to the user, a content for the at least one subsequent communication to the user, an affective level for the content of the at least one subsequent communication to the user, a difficulty level for the content of the at least one subsequent communication to the user, a cognitive level for the content of the at least one subsequent communication to the user, and a form of the at least one subsequent communication to the user.","16","16/323542","2017-08-14","2019-0167105","2019-06-06","11116403","2021-09-14","KONINKLIJKE PHILIPS N.V.","Paul Anthony Shrubsole | Murtaza Bulut | Christian Andreas Tiemann | Warner Rudolph Theophile Ten Kate | Chaitanya Dongre","2016-184349","EP","2016-08-16","A61B-0005/0022","A61B-0005/0022 | A61B-0005/165 | A61B-0005/167 | G06Q-0010/06 | G06Q-0010/10 | G06Q-0050/22 | G16H-0010/60 | G16H-0080/00","A61B-005/00","A61B-005/00 | G06Q-010/10 | G16H-010/60 | G16H-080/00 | G06Q-010/06 | G06Q-050/22 | A61B-005/16","","","","","","4921038000941"
"US","US","P","B2","Patch guide method and program","Disclosed is a patch guide method including acquiring a 3-dimensional scan model including the head of an object by using a depth camera, by a computer, acquiring a 3-dimensional brain MRI model of the object, matching the scan model and the brain MRI model to acquire a matched model, acquiring an image captured by photographing the head of the object by using the depth camera, and matching one location of the captured image and one location on the matched model.","1. A patch guide method comprising: acquiring a 3-dimensional scan model including the head of an object by using a depth camera, by a computer;acquiring a 3-dimensional brain MRI model of the object;matching the 3-dimensional scan model and the 3-dimensional brain MRI model to acquire a matched model;acquiring an image captured by photographing the head of the object by using the depth camera;matching one location in the captured image and one location on the matched model; anddetermining a location of at least one patch configured to be attached to the head of the object, by using a 3-dimensional brain map,wherein the acquiring of the 3-dimensional brain MRI model of the object includes: acquiring a brain MRI image of the object;acquiring physical characteristics of a plurality of areas included in the brain MRI image, the physical characteristics including isotropic electrical conductivities of the plurality of areas and/or anisotropic electrical conductivities of the plurality of areas; andgenerating the 3-dimensional brain map of the object based on the acquired physical characteristics of the plurality of areas included in the brain MRI image, andwherein the determining the location of the at least one patch includes: acquiring a target stimulus point, to which an electrical stimulus is to be applied in a brain of the object, by using the 3-dimensional brain map, wherein the electrical stimulus is applied by using an electric field induced in the brain of the object with a magnetic field generated by a treatment coil;acquiring information on spatial distribution of magnetic vector potentials of the treatment coil based on a shape of the treatment coil;acquiring an optimal stimulus condition, in which an intensity of the magnetic field applied to the target stimulus point is maximized by the treatment coil, based on the information on the spatial distribution;performing a simulation of a delivery process of the electrical stimulus to the target stimulus point with the optimal stimulus condition; anddetermining the location of the at least one patch by using a result of the simulation.","12","15/688005","2017-08-28","2019-0059732","2019-02-28","11116404","2021-09-14","NEUROPHET INC.","Dong Hyeon Kim | Jun Kil Been","10-2017-0108056","KR","2017-08-25","A61B-0005/0035","A61B-0005/0035 | A61B-0005/0042 | A61B-0005/0077 | A61B-0005/055 | G06F-0003/14 | G06F-0003/147 | G06K-0009/00214 | G06K-0009/00248 | G06T-0007/0012 | G06T-0007/11 | G06T-0007/33 | G06T-0007/55 | G06T-0015/205 | G06T-0017/20 | A61B-0005/369 | A61B-0005/7264 | G01R-0033/4806 | G01R-0033/4808 | G01R-0033/56341 | G06K-2209/05 | G06T-2207/10028 | G06T-2207/10088 | G06T-2207/20081 | G06T-2207/20084 | G06T-2207/30016","A61B-005/00","A61B-005/00 | A61B-005/055 | G06T-015/20 | G06K-009/00 | G06T-007/00 | G06T-007/55 | G06F-003/14 | G06T-017/20 | G06T-007/33 | G06F-003/147 | G06T-007/11 | G01R-033/48 | G01R-033/563 | A61B-005/369","","","","","","4921038000942"
"US","US","P","B2","Posture estimation method, posture estimation apparatus and computer readable storage medium","The present disclosure provides a posture estimation method, a posture estimation apparatus, and a computer readable storage medium. Here, the posture estimation method includes: calculating an angular velocity control amount at a current moment based on a measured acceleration value and an estimated acceleration value at the current moment; correcting the measured angular velocity value at the current moment according to the calculated angular velocity control amount to obtain the corrected angular velocity value at the current moment; and obtaining an estimated posture quaternion value at a next moment according to the corrected angular velocity value obtained by calculation and an estimated posture quaternion value at the current moment.","1. A method for estimating a posture estimation method of a three-dimensional target object, comprising: calculating an angular velocity control amount at a current moment based on a measured acceleration value and an estimated acceleration value at the current moment;correcting a measured angular velocity value at the current moment according to the calculated angular velocity control amount to obtain a corrected angular velocity value at the current moment; andobtaining an estimated posture quaternion value at a next moment according to the corrected angular velocity value obtained by calculation and an estimated posture quaternion value at the current moment, so as to estimate a posture of the three-dimensional target object at the next moment;wherein the calculating the angular velocity control amount at the current moment based on the measured acceleration value and the estimated acceleration value at the current moment further comprises: calculating an error between the measured acceleration value and the estimated acceleration value at the current moment; andcalculating the angular velocity control amount at the current moment by using a proportional-integral-differential control algorithm according to the calculated error;wherein the calculating the error between the measured acceleration value and the estimated acceleration value at the current moment further comprises: acquiring the measured acceleration value ac (xc, yc, zc) at the current moment T;acquiring the estimated posture quaternion value QT(q0, q1, q2, q3) at the current moment T, and calculating the estimated acceleration value ag at the current moment T according to the following formula (I): calculating the error ET between the measured acceleration value ac and the estimated acceleration value ag at the current moment T according to the following formula (II):","14","16/537241","2019-08-09","2020-0286245","2020-09-10","11120562","2021-09-14","BEIJING BOE OPTOELECTRONICS TECHNOLOGY CO., LTD. | BOE TECHNOLOGY GROUP CO., LTD.","Zehua Dong | Minglei Chu | Lili Chen | Hao Zhang | Jiankang Sun | Yukun Sun | Jinghua Miao | Zhifu Li | Xuefeng Wang | Hongzhen Xue","2019-10173038","CN","2019-03-07","G06T-0007/277","G06T-0007/277 | G06K-0009/00369 | G06T-0007/207 | G06T-0007/74 | G06T-0019/006 | A61B-0005/1116 | A61B-0005/1118 | G06F-0003/011 | G06T-0007/60 | G06T-2207/10028","A61B-005/11","A61B-005/11 | G01C-021/16 | G06F-001/16 | G06F-003/01 | G06K-009/00 | G06T-019/00 | G06T-007/207 | G06T-007/277 | G06T-007/60 | G06T-007/73","","","","","","4921038005073"
"US","US","P","B2","Smart makeup mirror device having a display and integrating with an artificial intelligence voice assistant","A smart makeup mirror device includes a main body, a display mirror unit rotatably mounted in the main body, and a speaker. The display mirror unit includes a display module, a makeup mirror, a support frame for fixing the makeup mirror, and a mood lamp mounted on a rear surface of the support frame. The support frame rotates with respect to the main body so that an angle with respect to the makeup mirror and the ground is changed, or rotate in a circumferential direction of the makeup mirror. The display module is oriented horizontally or vertically when the support frame is rotated in the circumferential direction of the makeup mirror. The mood lamp illuminates a mood light toward a floor surface when a rear surface of the display mirror unit is rotated at a specific angle with respect to the main body to face the floor surface.","1. A smart makeup mirror device having a display and integrating with an artificial intelligence voice assistant, comprising: a main body;a display mirror unit rotatably mounted in the main body; anda speaker mounted in the main body to output a sound,wherein the display mirror unit includes:a display module that outputs an image by implementing a preset display mode based on a voice signal or a touch signal of a user;a makeup mirror in which a portion of the makeup mirror is configured to reflect a light and another portion of the makeup mirror is configured to transmit an image output from the display module;a support frame for fixing the makeup mirror; anda mood lamp mounted on a rear surface of the support frame,wherein the support frame includes:a hinge support part configured to rotate with respect to the main body about a virtual first rotation axis extending in a direction parallel to ground; anda frame part configured to rotate with respect to the hinge support part about a virtual second rotation axis perpendicular to a mirror surface of the makeup mirror and the virtual first rotation axis,wherein the support frame is configured to rotate with respect to the main body so that an angle with respect to the makeup mirror and the ground is changed, or rotate in a circumferential direction of the makeup mirror, andthe display module is configured to be oriented horizontally or vertically when the support frame is rotated in the circumferential direction of the makeup mirror, andwherein the mood lamp is controlled to illuminate a mood light toward a floor surface when a rear surface of the display mirror unit is rotated at a specific angle with respect to the main body to face the floor surface.","13","17/173428","2021-02-11","2021-0244204","2021-08-12","11109694","2021-09-07","ICON AI INC.","Min Young Shin","10-2020-0016415","KR","2020-02-11","A47G-0001/02","A47G-0001/02 | A47G-0001/24 | A61B-0005/441 | A61N-0005/0618 | G06F-0003/167 | A47G-2001/1673 | A47G-2200/143 | A47G-2200/146 | A61N-2005/0651 | G06T-2207/30201","G02B-005/08","G02B-005/08 | A47G-001/02 | A61B-005/00 | A47G-001/24 | A61N-005/06 | G06F-003/16 | A47G-001/16","","","","","","4921037000839"
"US","US","P","B2","Adjusting alarms based on sleep onset latency","In some implementations, a mobile device can adjust an alarm setting based on the sleep onset latency duration detected for a user of the mobile device. For example, sleep onset latency can be the amount of time it takes for the user to fall asleep after the user attempts to go to sleep (e.g., goes to bed). The mobile device can determine when the user intends or attempts to go to sleep based on detected sleep ritual activities. Sleep ritual activities can include those activities a user performs in preparation for sleep. The mobile device can determine when the user is asleep based on detected sleep signals (e.g., biometric data, sounds, etc.). In some implementations, the mobile device can determine recurring patterns of long or short sleep onset latency and present suggestions that might help the user sleep better or feel more rested.","1. A method, comprising: receiving, by a computing device, indication of a desired sleep duration;calculating, by the computing device, a suggested time for a user to go to sleep based on a scheduled alarm time, an average intended sleep time, the desired sleep duration, and an average sleep onset latency; andpresenting, by the computing device, a message including the suggested sleep time on a display of the computing device prior to the suggested sleep time.","20","16/934983","2020-07-21","2020-0345298","2020-11-05","11109798","2021-09-07","Apple Inc.","Roy J. Raymann | Wren N. Dougherty | Divya Nag | Deborah M. Lambert | Stephanie Greer | Thomas R. Gruber","","","","A61B-0005/4809","A61B-0005/4809 | G06Q-0010/109 | G08B-0021/06 | H04L-0067/22 | H04M-0001/72451 | H04W-0004/70 | A61B-0005/01 | A61B-0005/02416 | A61B-0005/0816 | A61B-0005/11 | A61B-0005/4815 | A61B-0005/742 | A61B-2560/0242","H04M-001/725","H04M-001/725 | A61B-005/00 | H04W-004/70 | G06Q-010/10 | G08B-021/06 | H04M-001/72451 | H04L-029/08 | A61B-005/01 | A61B-005/024 | A61B-005/08 | A61B-005/11","","","","","","4921037000943"
"US","US","P","B2","Electrocardiogram (ECG) signal based authentication apparatus and method","An authentication apparatus includes one or more processors configured to temporally implement a neural network, used to extract a feature value from hidden nodes, that is connected to input nodes to which an electrocardiogram (ECG) signal is input so as to share a weight set with the input nodes, and to match the ECG signal and the extracted feature value to a user for registration.","1. An authentication apparatus comprising: a calculator configured to calculate a first output value corresponding to an input electrocardiogram (ECG) signal by applying the input ECG signal to a first neural network, and calculate second output values corresponding to reference ECG signals by applying the reference ECG signals to a second neural network,wherein the first and second neural networks are at least temporally implemented by one or more processors, the first output value is provided as output from the first neural network, and the second output values are provided as outputs from the second neural network;an extractor configured to extract a number of second output values from the second output values based on the first output value; anda determiner configured to determine whether to authenticate a user associated with the input ECG signal based on a ratio of a number of the extracted second output values that are associated with a registered user to a total number of the extracted second output values.","12","16/212946","2018-12-07","2019-0105000","2019-04-11","11109812","2021-09-07","Samsung Electronics Co., Ltd. | Korea Advanced Institute of Science and Technology","Chisung Bae | Jin Woo Shin | Sung-Soo Ahn | Sang Joon Kim","10-2016-0012179","KR","2016-02-01","A61B-0005/7264","A61B-0005/7264 | A61B-0005/117 | A61B-0005/349 | G06F-0021/32 | A61B-0005/6898 | G06K-2009/00939","A61B-005/117","A61B-005/117 | A61B-005/00 | G06F-021/32 | A61B-005/349 | G06K-009/00","","","","","","4921037000957"
"US","US","P","B2","Secure transdermal communication with implanted device","A system and method for communication between an IMD and an external reader includes bringing a portion of a patient's body into contact with a device-body contact surface of an external reader. The reader transmits a first transdermal carrier wave from the contact surface into the patient's body, where the first carrier wave includes a request for communication with the IMD. The transdermal carrier waves are electrical conductive waves, optical waves, or acoustic waves. Upon detection of the first carrier wave, the IMD transmits a second transdermal carrier wave including a request for an access key from the reader and the reader replies by transmitting a third transdermal carrier wave including the access key back to the IMD. If the access key is valid, the IMD transmits information by radio frequency (RF) in an RF communication mode or a fourth transdermal carrier wave including data from the IMD.","1. A method of communication between an implanted medical device (IMD) implanted in a patient and an external reader, comprising: bringing a portion of a patient'ss body into contact with a device-body contact surface of an external reader, wherein an IMD is implanted in the patient;the reader transmitting a first transdermal carrier wave from the contact surface into the patient'ss body, wherein the first carrier wave comprises a request for communication with the IMD;upon detection of the first carrier wave, the IMD transmitting a second transdermal carrier wave comprising a request for an access key from the reader, wherein the second transdermal carrier wave includes or is related to an IMD value stored in a memory location of the IMD;upon detection of the second transdermal carrier wave, the reader transmitting a third transdermal carrier wave comprising the access key back to the IMD, wherein the access key is based on the IMD value present in the memory location of the IMD;the IMD examining the access key for validity; andif the access key is valid, the IMD transmitting one of the group of: information by radio frequency (RF) in an RF communication mode, anda fourth transdermal carrier wave comprising data from the IMD.","20","16/239395","2019-01-03","2019-0201702","2019-07-04","11110281","2021-09-07","CARDIAC PACEMAKERS, INC.","Bin Mi | Jonathan Bennett Shute | Kenneth P. Hoyme | Grace Ann Wiechman | Michael Sheehan Seeberger | Andrew Bomett","","","","A61N-0001/37254","A61N-0001/37254 | A61B-0005/117 | A61B-0005/686 | A61M-0005/14276 | A61M-0005/1723 | A61N-0001/37217 | A61N-0001/37282 | H04L-0063/0492 | H04L-0063/061 | H04L-0063/0861 | H04W-0004/80 | H04W-0012/04 | H04W-0012/06 | A61B-0005/14532 | A61M-0005/14248 | A61M-2205/502 | A61N-0001/37235","A61N-001/00","A61N-001/00 | A61N-001/372 | A61B-005/00 | H04W-012/04 | H04L-029/06 | H04W-012/06 | H04W-004/80 | A61M-005/142 | A61M-005/172 | A61B-005/117 | A61B-005/145","","","","","","4921037001424"
"US","US","P","B2","Medical system capable of artificial intelligence and internet of things","A medical system capable of artificial intelligence and Internet of Things includes a conditioner, a control terminal device and a computation device. A patient may perform a physiological tissue stimulation treatment through the conditioner, which may adjust a stimulation parameter according to a feedback result of the stimulation, and transmits a signal of a feedback result indicative of an abnormal stimulation through the Internet of Things to the control terminal device, which has a disease analysis module built therein capable of further identifying an abnormal signal indicative of a disease and the physiological tissue for the feedback result indicative of the abnormal stimulation, so that a medical caring staff adjusts the stimulation parameter for the conditioner with respect to the abnormal signal. Moreover, the medical caring staff may interact with the computation device through the control terminal device to perform a big data analysis for optimization of the stimulation treatment.","1. A medical system capable of artificial intelligence and Internet of Things, including: a conditioner including:a stimulator used to perform stimulation processing on a physiological tissue to be stimulated;a detection unit for detecting a physiological signal of a physiological tissue to be detected;a digital controller for performing digital signal processing on the physiological signal detected by the detection unit and analyzing a state of the stimulation processing performed on the physiological tissue to be stimulated to obtain a feedback result after the stimulation processing, the digital controller including:a storage device storing conditioning parameter data for conditioning at least one physiological tissue, the control parameter data including: a first stimulation parameter value, a second stimulation parameter value and a predictable response signal;an open loop circuit causing the stimulator to stimulate the physiological tissue to be stimulated according to the first stimulation parameter value stored in the storage device;a closed loop circuit determining that: (a) when the stimulator performs the stimulation processing with the first stimulation parameter value and there is feeding back of the predictable response signal, the stimulator stimulates the physiological tissue to be stimulated with the first stimulation parameter value; the closed loop circuit determining that the stimulator performs the stimulation processing with the first stimulation parameter value but there is not feeding back of the predictable response signal, such that the stimulator stimulates the physiological tissue to be stimulated with the second stimulation parameter value when there is feeding back of the predictable response signal; (c) a feedback result with an abnormal message to be output if the closed loop circuit determines that the stimulator performs the stimulation processing with either the first or the second stimulation parameter value, the predictable response signal is not fed back; anda conditioning end wireless transmission unit for wireless transmission of the feedback result processed by the digital controller, or wireless reception of the conditioning parameter data to be processed by the digital controller; anda computation device including:a cloud database for storing response values from stimulation performed on a plurality of physiological tissues with a plurality of stimulation parameter values;a server module for accessing the cloud database, the server module providing a cloud user interface for inputting desired settings or storing an updated stimulation parameter value of the at least one physiological tissue, or updating the predictable response signal of the at least one physiological tissue, or for displaying the feedback result of at least one physiological tissue to be inquired and a stimulation parameter value used by the feedback result; anda control terminal device including:a near-end transmitter for performing data transmission processing with the conditioning end wireless transmission unit to perform an access action on the storage device;a far-end transmitter for performing data transmission with the server module;an intelligent processing module for processing data received by the near-end transmitter and the far-end transmitter, and controlling the near-end transmitter and the far-end transmitter to transmit data, as well as providing an end user interface, which displays the feedback result of the physiological tissue to be detected that is due to the physiological tissue to be stimulated being stimulated, received by the near-end transmitter, or sets up the updated stimulation parameter value with which the conditioner performs the stimulation processing, wherein the updated stimulation parameter value is transmitted to the digital controller through the near-end transmitter and the conditioning end wireless transmission unit for processing, wherein the digital controller that outputs the abnormal message uses a received updated stimulation parameter value for the stimulator to stimulate the physiological tissue to be stimulated, and the closed loop circuit determines whether there is feeding back of the predictable response signal when the stimulator stimulates the physiological tissue to be stimulated with the updated stimulation parameter value, such that the feedback result of the abnormal message is output if the predictable response signal is not fed back, which can let the intelligent processing module provide subsequent update processing on the conditioning parameter data according to the abnormal message.","14","16/195379","2018-11-19","2019-0156949","2019-05-23","11114207","2021-09-07","NATIONAL CHENG KUNG UNIVERSITY","Shuenn-Yuh Lee | Yu-Jin Lin","2017140179 | 2018135861","TW | TW","2017-11-20 | 2018-10-11","G16H-0050/20","G16H-0050/20 | A61B-0005/0022 | A61B-0005/024 | A61B-0005/369 | A61B-0005/389 | A61B-0005/4041 | A61N-0001/36031 | A61N-0001/36139 | A61N-0001/3782 | A61N-0001/37223 | G06N-0020/00 | G16H-0040/60 | H04L-0067/125 | A61N-0001/025 | A61N-0002/002 | A61N-0005/06 | A61N-2005/0626","G16H-050/20","G16H-050/20 | G16H-040/60 | A61N-001/36 | A61N-001/372 | A61N-001/02 | H04L-029/08 | G06N-020/00 | A61B-005/024 | A61B-005/00 | A61N-001/378 | A61B-005/369 | A61B-005/389 | A61N-002/00 | A61N-005/06","","","","","","4921037005315"
"US","US","P","B2","Eye imaging in head worn computing","Head-worn computers with eye-imaging systems include a camera system positioned in a head-worn computer, wherein the camera system is further positioned to capture eye-image light that originates as reflections from a user's eye, wherein the camera system is further positioned to capture eye-image light as a reflection from a partially reflective surface that is positioned in front of an image display in the head-worn computer, wherein image light, from the image display, is transmitted through the partially reflective surface. A processor is adapted to cause the camera system to capture the eye-image light. The processor is further adapted to cause a comparison of the captured eye-image light with a pre-stored eye image of a known user of the head-worn computer. In the event the comparison confirms the identity of the known user, the user is granted permission to view content to be presented in a display of the head-worn computer.","1. A system comprising: one or more cameras configured for capturing images of one or more eyes of a user of a wearable head device, the wearable head device comprising the one or more cameras; andone or more processors configured for monitoring the one or more eyes of the user for changes in eye performance based on the captured images, wherein the monitoring of the one or more eyes contributes to monitoring health of the user of the wearable head device,wherein a selection of a timing or a rate of capture of the capturing of images of the one or more eyes of the user of the wearable head device is based on an indication of a selected use scenario from among multiple use scenarios, the selected use scenario associated with the images of the one or more eyes of the user, andwherein the timing or the rate of capture of the capturing of images of the one or more eyes of the user of the wearable head device varies among the multiple use scenarios.","19","16/443773","2019-06-17","2020-0037873","2020-02-06","11103132","2021-08-31","Mentor Acquisition One, LLC","John N. Border | John Haddick | Joseph Bietry","","","","A61B-0003/14","A61B-0003/14 | A61B-0003/0008 | A61B-0003/0025 | A61B-0003/0041 | A61B-0003/113 | A61B-0003/12 | A61B-0005/117 | A61B-0005/1171 | G02B-0027/017 | G02B-0027/0172 | G02C-0011/10 | G06F-0003/013 | G06F-0021/10 | G06F-0021/32 | G06K-0009/0061 | G06K-0009/00604 | G06K-0009/00617 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0178 | G02B-2027/0187","A61B-003/14","A61B-003/14 | G02B-027/14 | A61B-003/113 | A61B-003/00 | G02C-011/00 | A61B-005/1171 | G02B-027/01 | A61B-005/117 | G06F-003/01 | G06K-009/00 | A61B-003/12 | G06F-021/10 | G06F-021/32","","","","","","4921036000877"
"US","US","P","B2","Eye tracking calibration for a surgical robotic system","A method for passively calibrating and verifying eye tracking in a surgical robotic system. The gaze of a user facing a user display of a user console of a surgical robotic system is tracked while the user is using a user interface device (UID). When a user interaction to the UID is detected, an expected gaze position on the display of the user console is determined without instructing the user to look at the expected gaze position. The latter becomes a reference gaze point of the user at the time of detected user action. The measured gaze point of the user is compared with an acceptable threshold of the reference gaze point, and in response to a determination of a mismatch, calibration parameters used by the tracking are adjusted according to the reference gaze point. Other aspects are also described and claimed.","1. A method for eye tracking in a surgical robotic system, comprising: tracking a gaze of a user facing a display of a user console of a surgical robotic system while the user is using a user interface device (UID) thereby producing a measured gaze point of the user;detecting that the user is interacting with the UID thereby producing a detected user interaction;in response to the detected user interaction, determining an expected gaze point on the display of the user console at the time of the detected user interaction, as a reference gaze point of the user;in response to the detected user interaction, determining whether the measured gaze point at the time of the detected user interaction is within an acceptable threshold of the reference gaze point without prompting the user; andin response to determining the measured gaze point is within the acceptable threshold of the reference gaze point, continuing said tracking the gaze of the user without performing an eye tracking calibration.","19","16/449170","2019-06-21","2020-0401219","2020-12-24","11106279","2021-08-31","VERB SURGICAL INC.","Anette Lia Freiin von Kapri | Denise Miller | Joan Savall","","","","G06F-0003/013","G06F-0003/013 | A61B-0034/25 | A61B-0034/30 | A61B-2017/00216 | G06K-0009/00604","A61B-034/00","A61B-034/00 | A61B-034/30 | G06F-003/01 | A61B-017/00 | G06K-009/00","","","","","","4921036004007"
"US","US","P","B2","Versatile data structure for workout session templates and workout sessions","A fitness tracking system and methods collecting fitness data for a user during a workout session are disclosed. The fitness tracking system utilizes a versatile data structure for robustly representing complex workout session templates and recording fitness data associated with individual workout sessions. The versatile data structure enables detailed fitness data to be recorded in association with complex workout session templates in a manner that enables detailed analysis and a less cumbersome user experience.","1. A method of collecting fitness data for a user during a workout session, the method comprising: storing, in a memory, a first data structure defining a workout session template formed by a first plurality of exercises, the first data structure having a first tree structure that depends upon and represents an organization of the first plurality of exercises in the workout session template;presenting, with a display screen, a graphical depiction of the workout session template to the user in response to receiving a request from the user to begin a workout;receiving, with a user interface, first inputs defining at least one change to the first data structure;generating, with the processor, a second data structure defining the workout session formed by a second plurality of exercises, the second data structure having a second tree structure that depends upon and represents an organization of the second plurality of exercises in the workout session, the second data structure being derived from and storing data of the first data structure and representing the change to the first data structure; andstoring, in the memory, the second data structure.","20","16/557211","2019-08-30","2021-0065869","2021-03-04","11107568","2021-08-31","MYFITNESSPAL, INC.","Ben Hamill | Dory Glauberman | James Lyke | Jacob Peace | Luis Valerio | Carl Youngblood","","","","G16H-0020/30","G16H-0020/30 | A61B-0005/1118 | A63B-0024/0062 | G06F-0003/0482 | A63B-2024/0065 | A63B-2024/0068","G06F-017/00","G06F-017/00 | G16H-020/30 | A61B-005/11 | G06F-003/0482 | A63B-024/00","","","","","","4921036005284"
"US","US","P","B2","Wearable athletic activity monitoring methods and systems","A method for monitoring an individual engaged in an athletic activity includes detecting movement of the individual at a first time, using a sensor module coupled to the individual, determining that the movement of the individual corresponds to a predetermined activation movement, entering an active state of the sensor module in response to the determination that the movement of the individual corresponds to the predetermined activation movement, and detecting movement of the individual at a second time, using the sensor module in the active state.","1. An athletic activity monitoring system, comprising: an article of sporting equipment; anda sensor module connected to the sporting equipment, the sensor module comprising: a sensor; anda processor configured to: determine an initial spatial orientation of the article based on data collected by the sensor;determine a change in a spatial orientation of the article based on data collected by the sensor; anddetermine an activity metric of the article based on the change in the spatial orientation of the article;wherein the initial spatial orientation and the change in the spatial orientation are each determined relative to a point of reference.","20","16/532142","2019-08-05","2019-0358490","2019-11-28","11097156","2021-08-24","ADIDAS AG","Aurel Coza | Christian Dibenedetto | Jeffrey Allen","","","","A63B-0024/0006","A63B-0024/0006 | A61B-0005/1112 | A61B-0005/162 | A63B-0071/0619 | G06F-0003/011 | G06K-0009/00342 | A61B-2503/10 | A63B-2220/40 | A63B-2220/62 | A63B-2220/836","A63B-024/00","A63B-024/00 | G06F-003/01 | A61B-005/11 | G06K-009/00 | A61B-005/16 | A63B-071/06","","","","","","4921035001488"
"US","US","P","B2","Systems and methods for guiding and measuring neck and shoulder protraction and retraction motions in virtual/augmented reality","A virtual/augmented reality-based system for guiding patients through rehabilitation exercises and providing real time feedback is disclosed. In various embodiments the system, methods, and computer program products relate to guiding and measuring neck and shoulder motion, specifically protraction and retraction motions. A virtual reality environment includes a fixed object having a guidance feature and a moveable object having a complementary shape to the guidance features. Whether the fixed object is in a complimentary orientation to the movable object is determined. When the fixed object is in a complimentary orientation with the movable shape, an indication is presented to the user to perform a motion. A plurality of sensors determines a plurality of measurements relating to the motion of the user. Whether the fixed object is in a complementary orientation with the movable object is determined based on the plurality of measurements.","1. A method comprising: providing a virtual environment to the user via the virtual or augmented reality system, the virtual environment comprising: a first 3D object fixed in space, the first 3D object having a first shape, anda second 3D object fixed to the user, the second 3D object having a second shape that is complementary to the first shape;determining whether the first shape is in a complimentary orientation to the second shape;when the first shape is in a complimentary orientation with the second shape, indicating to the user to perform a motion via the virtual or augmented reality system;determining by a plurality of sensors a plurality of measurements relating to the motion of the user; anddetermining whether the second 3D object is in the complementary orientation with the first 3D object based on the plurality of measurements.","20","16/506404","2019-07-09","2020-0012336","2020-01-09","11099632","2021-08-24","XR HEALTH IL LTD","Eran Orr | Miki Levy","","","","G06F-0003/011","G06F-0003/011 | A61B-0005/11 | A61B-0005/486 | G06T-0007/70 | G06T-0019/006 | G09B-0005/02 | A61B-2505/09 | A61B-2562/0219 | A61B-2562/0223","G06F-003/01","G06F-003/01 | G06T-019/00 | G06T-007/70 | G09B-005/02 | A61B-005/11 | A61B-005/00","","","","","","4921035003946"
"US","US","P","B2","System for controlling a medical device","A system for controlling a medical device, the system comprising: a first channel and a second channel, an interface for receiving display data and control data and sending data into the first and second channels, the first channel comprising an imaging device adapted to display the display data and the control data, an image sensor adapted to detect at least an image part of the image, and an extraction device adapted to extract verification data from the image data, the second channel having a control device adapted to receive inputs from a user for and to output the control data and a control command, and a comparison device adapted to compare the test data from the extraction device and the control data from the control device.","1. An electronic system to control a medical device comprising a first communications channel;a second communications channel,an interface coupled to and in communication with the first and the second communications channel, the interface receiving display data and control data, the interface sending the display data and control data into the first channel and to send the control data into the second channel,the first channel coupled to and in communication with: an imaging device displaying the display data and the control data as an image on the imaging device,an image sensor detecting at least an image part of the image which is displayed on the imaging device and corresponds to the control data as image data, andan extraction device a extracting verification data from the image data,the second channel coupled to and in communication with a control device, the control device receiving inputs from a user to control the medical device and, in the case of an input, to output the control data and a control command, otherwise to output the control data,and a comparison device including a processor to compare test data from the extraction device and the control data from the control device, to determine a comparison result and to output the comparison result, the comparison result indicating whether the system is functioning properly.","14","16/251645","2019-01-18","2019-0228858","2019-07-25","11101035","2021-08-24","KARL STORZ SE & CO. KG","Klaus Bruederle | Marco Zeller | Jan Rueppell | Harald Baumann","10-2018-101345","DE","2018-01-22","G16H-0040/63","G16H-0040/63 | A61B-0034/00 | A61B-0034/25 | A61B-0090/00 | G06F-0003/01 | G16H-0040/40 | G16H-0040/67 | A61B-0005/1171 | A61B-2017/00207 | A61B-2017/00216 | G06F-0021/31 | G06F-0021/44 | G06F-2221/2149 | G16H-0040/20","G16H-040/63","G16H-040/63 | G16H-040/67 | A61B-034/00 | A61B-090/00 | G06F-003/01 | G16H-040/40 | A61B-017/00 | A61B-005/1171 | G16H-040/20 | G06F-021/31 | G06F-021/44","","","","","","4921035005337"
"US","US","P","B2","Feedback for water consuming appliance","A communication system provides feedback data for at least one water consuming device. The communication system includes a data collection interface, a controller, and an output interface. The data collection interface is configured to receive user data from at least one collection device. The controller is configured to perform an analysis of the user data from the at least one collection device. The output interface is configured to provide feedback data based on the analysis of the user data to a water consuming device.","1. A communication system for providing feedback data for at least one water consuming device comprising: at least one collection device configured to capture user data corresponding to a user;a data collection interface communicably coupled to the at least one collection device;a controller comprising a processor and memory, wherein the controller is communicably coupled to the data collection interface and the output interface, and wherein the controller is configured to: receive, via the data collection interface from the at least one collection device, the user data describing a biological characteristic of the user,determine feedback data corresponding to water flow from the water device based on the biological characteristic of the user described by the user data from the at least one collection device, andprovide, via the output interface to the water consuming device, the feedback data for controlling the water flow from the water consuming device based on the user data.","18","15/706198","2017-09-15","2019-0087510","2019-03-21","11093554","2021-08-17","KOHLER CO.","Rafael Rexach | Nona Beining | Alyssa Wilterdink | Shawn Booth | Thomas E. Lilly | Doug Diemel, Jr. | Leslie Petch","","","","G06F-0016/904","G06F-0016/904 | A61B-0005/0077 | A61B-0005/01 | A61B-0005/024 | A61B-0005/1032 | A61B-0005/1072 | A61B-0005/1123 | A61B-0005/16 | A61B-0005/4875 | A61B-0005/6889 | A61B-0005/7282 | G01N-0033/18 | G05B-0015/02 | G05B-0019/042 | G06F-0003/013 | G06F-0003/017 | G06F-0016/9035 | H04L-0012/282 | H04L-0012/2823 | H04L-0067/12 | A61B-0005/021 | A61B-0005/02416 | A61B-0005/1113 | A61B-0005/1128 | A61B-0005/165 | G01F-0001/66 | G01F-0001/661 | G01F-0023/18 | G01F-0023/72 | G05B-2219/2231 | G05B-2219/2642 | G06F-0003/0488 | H04L-2012/285","G06F-017/30","G06F-017/30 | G06F-016/904 | A61B-005/01 | A61B-005/024 | A61B-005/107 | A61B-005/11 | A61B-005/16 | A61B-005/00 | A61B-005/103 | H04L-029/08 | H04L-012/28 | G01N-033/18 | G06F-003/01 | G05B-019/042 | G05B-015/02 | G06F-016/9035 | G01F-001/66 | G06F-003/0488 | G01F-023/18 | G01F-023/72 | A61B-005/021","","","","","","4921034004489"
"US","US","P","B2","Interactive avatar for social network services","An embodiment is an avatar or avatar environment to visualize data within an athletic performance system or service and/or a social network system or service, for example as part of the Internet. The avatar may further evolve or alter its appearance, animation, or other visual or audio characteristics in response to the data or other input. In particular, the avatar of an embodiment may respond to and provide visualization of athletic or sport performance data. According to one or more aspects, an avatar may be placed on other network sites and updated based on athletic performance data. The avatar may be awarded for goals achieved by a user. The awards or gifts may further include non-avatar related items such as apparel, gift cards and the like.","1. A method comprising: receiving, by a computing system, a request to initiate a competition between at least a first user and a second user;generating, by the computing system, a first electronic avatar corresponding to the first user;generating, by the computing system, a second electronic avatar corresponding to the second user;displaying, by the computing system, the first electronic avatar at a first location and the second electronic avatar at a second location in an interface of the computing system;receiving, by the computing system, athletic performance data for the first user from a first athletic monitoring system and athletic performance data for the second user from a second athletic monitoring system;modifying, by the computing system, at least one of the first and second locations of the first and second electronic avatars in the interface in accordance with the athletic performance data received from the first athletic monitoring system and from the second athletic monitoring system, wherein modifying the at least one of the first and second locations of the first and second electronic avatars includes providing a scrolling sub-window overlay onto a background of the interface relative to the first electronic avatar or the second electronic avatar; andgenerating, by the computing system, a first animation characteristic for at least one of a first animation characteristic of the first electronic avatar and a second animation characteristic of the second electronic avatar in the interface in accordance with a moving average of a plurality of most recent athletic performance intervals from the athletic performance data received from the first athletic monitoring system and the second athletic monitoring system,wherein the first animation characteristic and the second animation characteristic indicate a variable athletic performance metric of the first and second users over a duration of the competition, wherein the variable athletic performance metric is obtained by at least one sensor on the first user configured to sense physical activity of the first user and at least one sensor on the second user configured to sense physical activity of the second user.","17","16/122313","2018-09-05","2019-0005373","2019-01-03","11093815","2021-08-17","NIKE, INC.","Jason Nims | Roberto Tagliabue | Danielle Quatrochi","","","","G06N-0003/006","G06N-0003/006 | A61B-0005/0022 | A61B-0005/1118 | A61B-0005/744 | A63B-0024/0059 | G06F-0003/0485 | G06F-0016/958 | G06F-0021/6245 | G06Q-0010/10 | G06Q-0050/01 | H04L-0065/403 | H04L-0067/38 | A61B-0005/4866 | A63B-0024/0075 | A63B-0069/0028 | A63B-2024/0068 | A63B-2024/0096 | A63B-2071/0638 | A63B-2220/20 | A63B-2220/40 | A63B-2225/50 | A63B-2230/06 | A63B-2230/75 | A63F-2300/407 | A63F-2300/5553 | A63F-2300/572 | A63F-2300/609 | A63F-2300/69 | A63F-2300/8005","G06N-003/00","G06N-003/00 | G06Q-050/00 | A63B-024/00 | A61B-005/00 | A61B-005/11 | G06F-003/0485 | G06F-016/958 | G06Q-010/10 | G06F-021/62 | H04L-029/06 | A63B-069/00 | A63B-071/06","","","","","","4921034004750"
"US","US","P","B2","Biometric sensor","A biometric sensing apparatus is employed by a person in order to obtain biometric data regarding the person. Transmitting and receiving antennas are used in order to transmit and receive signals. Measurements of the received signals are correlated with biological activity in order to provide biometric data for the person.","1. A biometric sensing apparatus, comprising: a transmit antenna configured to transmit a signal comprising at least a first frequency in proximity to a subject;a plurality of receive antennas in proximity to the subject;a signal processor operatively connected to the plurality of receive antennas, the signal processor being configured to process signals received on each of the plurality of receive antennas during a plurality of integration periods, and for each of the plurality of integration periods and for each of the receive antennas to determine at least one measurement corresponding to the first frequency, the measurement corresponding to a magnitude and/or phase of the first frequency as received at the receive antenna during the corresponding integration period;wherein the at least one measurement determined for each of the plurality of integration periods and for each of the receive antennas provides data regarding a biometric of the subject.","20","16/984060","2020-08-03","2020-0363885","2020-11-19","11083390","2021-08-10","Tactual Labs Co.","David Holman","","","","A61B-0005/0507","A61B-0005/0507 | A61B-0005/681 | A61B-0005/6802 | G06F-0003/044 | G06F-0003/0416 | G06F-0003/04166 | A61B-0005/6806 | G06F-2203/04102 | G06F-2203/04104 | G06K-0009/00885 | G06K-2009/00939","A61B-005/0507","A61B-005/0507 | A61B-005/00 | G06F-003/041 | G06F-003/044 | G06K-009/00","","","","","","4921033001002"
"US","US","P","B2","Systems and methods for user monitoring","A monitoring system includes a communications circuit and an analysis circuit where the communications circuit is configured to receive an image of a user from a social media system and receive orthodontic treatment plan data associated with the user from an aligner fabrication computer system. The analysis circuit is configured to determine an actual oral condition of the user at a time when the image of the user was taken, determine an expected oral condition of the user at the time when the image of the user was taken based on the orthodontic treatment plan data, compare the actual oral condition and the expected oral condition, and generate an output based on the comparison. The communications system communications the output to at least one of the user device and the aligner fabrication computer system.","1. A monitoring system comprising: a communications circuit configured to receive an image of a user from a social media system, and to receive orthodontic treatment plan data associated with the user from an aligner fabrication computer system; andan analysis circuit configured to: determine an actual oral condition of the user at a first time, the first time corresponding to when the image of the user was taken;determine a first expected oral condition of the user at the first time based on the orthodontic treatment plan data;compare the actual oral condition with the first expected oral condition;compare the actual oral condition with (i) a second expected oral condition of the user at a second time based on the orthodontic treatment plan data, the second time occurring before the first time and after the user begins an orthodontic treatment plan, and (ii) a third expected oral condition of the user at a third time based on the orthodontic treatment plan data, the third time occurring after the first time and before the user finishes the orthodontic treatment plan; andgenerate an output based on the comparisons, the output indicative of a progress of the user in relation to the first expected oral condition at the first time when the image of the user was taken;wherein the communications circuit is further configured to communicate the output to at least one of a user device and the aligner fabrication computer system;wherein the orthodontic treatment plan data comprises a plurality of treatment plan models used to manufacture dental aligners for the user to reposition at least one tooth of the user; andwherein the plurality of treatment plan models comprise an initial tooth model, an intermediate tooth model, and a final tooth model, the initial tooth model representing an initial position of the teeth during an orthodontic treatment plan, the intermediate tooth model representing an intermediate position of the teeth during the orthodontic treatment plan, the final tooth model representing a final position of the teeth during the orthodontic treatment plan.","23","16/563429","2019-09-06","2021-0068745","2021-03-11","11083411","2021-08-10","SDC U.S. SMILEPAY SPV","Christopher Yancey | John Dargis | Alex Fenkell","","","","A61B-0005/4547","A61B-0005/4547 | A61C-0007/08 | G06Q-0050/01 | G06T-0007/0012 | G16H-0020/30 | G06T-2207/20224 | G06T-2207/30036","A61B-005/00","A61B-005/00 | G06T-007/00 | G16H-020/30 | G06Q-050/00 | A61C-007/08","","","","","","4921033001023"
"US","US","P","B2","Systems and methods for assisted surgical navigation","In at least one embodiment, a method of surgical navigation is provided. The method includes receiving an external three-dimensional model of a surgical site from the viewpoint of a headset, wherein the external three-dimensional model is derived from reflected light. The method further includes aligning the external three-dimensional model with an internal three-dimensional model of the surgical site from the viewpoint of the headset, wherein the internal three-dimensional model is derived from medical imaging, and generating an aligned view. The method further includes providing the aligned view to the headset, and updating the aligned view in real-time while the headset is moved or the surgical site is moved or modified during a surgical procedure.","1. A system to product a three-dimensional model of a surgical site, comprising: a projector mounted to a user'ss headset to project an array of non-visible light beams onto a surgical site;a camera mounted to the headset to collect light reflected from the surgical site back towards the headset to produce optical data regarding the surgical site;a laser pointer to light up reference landmarks on anatomical reference features; anda processor mounted to the headset, the processor configured to: construct a non-video-based external three-dimensional model based on the optical data,produce mapping points by associating annotations provided as the laser pointer lights up respective reference landmarks, and receive user instructions to highlight and inspect respective ones of the anatomical reference features based on the reference landmarks,wherein the three-dimensional model of the highlighted anatomical reference features is rotatable and divisible according to user instructions to inspect the anatomical reference features.","11","16/001055","2018-06-06","2018-0344412","2018-12-06","11083527","2021-08-10","GLOBUS MEDICAL, INC.","Justin Esterberg","","","","A61B-0034/20","A61B-0034/20 | A61B-0005/055 | A61B-0034/10 | G06F-0003/011 | A61B-0090/98 | A61B-2017/00207 | A61B-2034/107 | A61B-2034/2048 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2063 | A61B-2034/2065 | A61B-2034/2072 | A61B-2090/309 | A61B-2090/365 | A61B-2090/371 | A61B-2090/372 | A61B-2090/373 | A61B-2090/502 | A61B-2560/0487","A61B-034/10","A61B-034/10 | G06F-003/01 | A61B-034/20 | A61B-005/055 | A61B-017/00 | A61B-090/30 | A61B-090/50 | A61B-090/98 | A61B-090/00","","","","","","4921033001139"
"US","US","P","B2","System and method for aiding communication","System and method for aiding communication for subjects suffering from paralysis of muscles controlled by peripheral nervous system are disclosed. A method for aiding communication for said subjects includes capturing, from a plurality of sensors, sensor data generated based on an interaction of a subject with an interactive UI. The plurality of sensors includes one or more body-parts movement tracking sensors for tracking motion of body-parts of the subject and one or more physiological signal sensors for monitoring physiological signals generated from the subject during the interaction. A plurality of model parameters indicative of characteristics of the subject related to the interaction are determined based on the sensor data. The navigation at the interactive UI is controlled on the plurality of model parameters.","1. A processor-implemented method for aiding communication, the method comprising: capturing, from a plurality of sensors, sensor data generated based on an interaction of a subject with an interactive UI, via one or more hardware processors, the plurality of sensors comprising one or more body-parts movement tracking sensors for tracking motion of body-parts of the subject and one or more physiological signal sensors for monitoring physiological signals generated from the subject during the interaction;determining, based on the sensor data, a plurality of model parameters indicative of characteristics of the subject related to the interaction, via the one or more hardware processors, wherein the sensor data is collective sensor data from the one or more body-parts movement tracking sensors and the one or more physiological signal sensors, wherein the plurality of model parameters are determined by training a model, and wherein training the model comprises: monitoring dynamically, the sensor data generated based on the interaction of the subject with the interactive UI to determine a consistent change in the interaction, wherein determining the consistent change in the interaction comprises verifying, for a threshold number of times, change in the interaction of the subject with the interactive UI and a changed response of the subject due to progression of medical condition of the subject, to validate whether the changed response is consistent during the threshold number of times, andautomatically updating, on determination of the consistent change, one or more model parameters from the plurality of model parameters based on the sensor data, andcontrolling navigation at the interactive UI based on the plurality of model parameters, via the one or more hardware processors.","18","16/321286","2017-05-23","2019-0171348","2019-06-06","11086473","2021-08-10","TATA CONSULTANCY SERVICES LIMITED","Arpit Vishwakarma | Aniruddha Sinha | Ratnamala Manna | Debatri Chatterjee | Vedraj | Kingshuk Chakravarty | Rajat Kumar Das | Anagha Nikhil Mehrotra | Arpan Pal | Rahul Dasharath Gavas | Anwesha Khasnobish | Abhijit Das","201621025832","IN","2016-07-28","G06F-0003/04815","G06F-0003/04815 | A61B-0005/1114 | A61B-0005/369 | A61F-0004/00 | G06F-0003/00 | G06F-0003/011 | G06F-0003/012 | G06F-0003/013 | G06F-0003/015 | G06F-0003/017 | G06N-0020/00 | G09B-0005/06 | H04N-0007/185 | A61B-0005/398 | G02B-0027/0093","A61B-005/11","A61B-005/11 | A61B-005/369 | G02B-027/00 | A61B-005/398 | G06F-003/0481 | G06F-003/01 | G06F-003/00 | A61F-004/00 | H04N-007/18 | G09B-005/06 | G06N-020/00","","","","","","4921033004064"
"US","US","P","B2","Method and system to control a workflow and method and system for providing a set of task-specific control parameters","The invention relates to a system and method to control a workflow comprising at least one task to be performed by a person (P3), wherein information is provided about at least one certain object (20, 32, 36, 38) related to the at least one task of the workflow, eye data (24, 26) are captured of at least one eye of the person (P3), and in dependency of the eye data (24, 26) and the information about the at least one certain object (20, 32, 36, 38) it is checked whether at least one task condition consisting in whether the task had been performed and/or whether the task is allowed to be performed is fulfilled. The invention also relates to a system and method for providing a set of task-specific control parameters (CP).","1. A method comprising: determining a gaze direction of a user;determining a state of attention of the user;determining, based on the gaze direction, whether the user has inspected first information associated with a task;in response to determining that the user has inspected the first information associated with the task, determining, based on the state of attention, whether the user has attentively inspected the first information associated with the task; andin response to determining that the user has attentively inspected the first information associated with the task: enabling a device to allow the user to perform the task;detecting a first object associated with the task; anddisplaying second information in association with the first object in order to perform the task on the first object.","24","16/710725","2019-12-11","2020-0117895","2020-04-16","11087127","2021-08-10","APPLE INC.","Eberhard Schmidt | Tom Sengelaub","2015-180283 | PCT-EP2016-068818","EP | WO","2015-08-07 | 2016-08-05","G06K-0009/00597","G06K-0009/00597 | A61B-0003/113 | A61B-0005/1128 | A61B-0005/163 | A61B-0005/168 | G05B-0019/0426 | G06K-0009/00671 | G06K-0009/00771 | G06Q-0010/0633 | G06Q-0010/063114 | G06T-0007/70 | G05B-2219/24024","G06K-009/00","G06K-009/00 | A61B-005/16 | G05B-019/042 | G06T-007/70 | A61B-003/113 | A61B-005/11 | G06Q-010/06","","","","","","4921033004714"
"US","US","P","B2","Monitoring device for a piece of sports equipment","Golf clubs according to at least some example aspects of this disclosure may include a golf club head and a shaft configured to engage with the golf club head which includes a grip engaged with the shaft. Further, the golf club may include a monitoring device, which includes a sensor and a transmitter. Additionally, the monitoring device may be configured to determine data related to the characteristics of a golf swing. Further, the monitoring device may be configured to transmit the data related to the characteristics of a golf swing to a remote computer.","1. A sports equipment system comprising: a piece of sports equipment, wherein the piece of sports equipment is a bat, a tennis racquet, a hockey stick, or a lacrosse stick; andan electronic monitoring device attached to the piece of sports equipment, the electronic monitoring device including a first inertial measurement unit and a second inertial measurement unit, wherein each of the first inertial measurement unit and the second inertial measurement unit has at least one gyroscope and at least one accelerometer, wherein the electronic monitoring device is configured to detect motion of the piece of sports equipment,wherein the electronic monitoring device includes a processor configured to receive data from the first inertial measurement unit and the second inertial measurement unit, and upon receiving data from the first inertial measurement unit and the second inertial measurement unit indicating detected motion of the piece of sports equipment, the processor determines that the electronic monitoring device is engaged with a particular piece of sports equipment or a particular type of sports equipment.","18","16/679518","2019-11-11","2020-0070019","2020-03-05","11077343","2021-08-03","NIKE, INC.","Michael Wallans | Robert M. Boyd | Philip J. Hatton | Mario A. Lafortune | John T. Stites","","","","A63B-0053/0487","A63B-0053/0487 | A61B-0005/1122 | A61B-0005/6895 | A63B-0053/047 | A63B-0053/0466 | A63B-0053/10 | A63B-0053/14 | A63B-0060/16 | A63B-0060/46 | A63B-0060/50 | A63B-0069/3632 | A63B-0069/3685 | A63B-0071/0619 | A63B-0071/0622 | G01S-0019/19 | G06F-0001/1684 | G06F-0003/0346 | G06K-0009/00342 | G06Q-0050/01 | A61B-2503/10 | A61B-2505/09 | A63B-0049/035 | A63B-0049/08 | A63B-0053/04 | A63B-0053/0433 | A63B-0053/0437 | A63B-0059/20 | A63B-0059/50 | A63B-0059/70 | A63B-2071/0063 | A63B-2071/068 | A63B-2071/0647 | A63B-2102/18 | A63B-2102/24 | A63B-2209/00 | A63B-2220/12 | A63B-2220/16 | A63B-2220/30 | A63B-2220/40 | A63B-2220/53 | A63B-2220/803 | A63B-2220/833 | A63B-2225/15 | A63B-2225/20 | A63B-2225/50 | A63B-2225/54","A63B-053/04","A63B-053/04 | A63B-069/36 | A63B-071/06 | A63B-053/10 | A63B-053/14 | G06F-003/0346 | A63B-060/16 | A63B-060/50 | G01S-019/19 | G06K-009/00 | G06Q-050/00 | A61B-005/00 | A61B-005/11 | G06F-001/16 | A63B-060/46 | A63B-049/08 | A63B-102/24 | A63B-049/035 | A63B-059/70 | A63B-059/20 | A63B-102/18 | A63B-059/50 | A63B-071/00","","","","","","4921032001540"
"US","US","P","B2","System and method for virtual radiation therapy quality assurance","A method comprises receiving one or more plan parameters of a first radiation treatment plan for a first patient and one or more passing rate data for the first treatment plan; generating a predictive model for passing rate data from the plan parameters of the first radiation treatment plan and the passing rate data for the first treatment plan; receiving one or more plan parameters of a second radiation treatment plan for a second patient; and applying the predictive model to the plan parameters of the second radiation treatment to generate one or more predicted passing rate data for the plan parameters for the second patient.","1. A method comprising: receiving one or more plan parameters of a first radiation treatment plan for a first patient and one or more passing rate data for the first radiation treatment plan;generating a predictive model for passing rate data from the plan parameters of the first radiation treatment plan and the one or more passing rate data for the first radiation treatment plan;receiving one or more plan parameters of a second radiation treatment plan for a second patient; andapplying the predictive model to the plan parameters of the second radiation treatment plan to generate one or more predicted passing rate data for the plan parameters for the second radiation treatment plan for the second patient.","17","15/563411","2016-03-30","2018-0085168","2018-03-29","11081225","2021-08-03","THE TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA","Gilmer Valdes | Timothy Solberg | Ryan Scheuermann | Marc Bellerive | C. Y. Hung | Arthur Olszanski","","","","G16H-0020/40","G16H-0020/40 | A61B-0034/10 | G06F-0019/00 | G06Q-0010/04 | G06Q-0010/06395 | G16H-0050/20 | G16H-0070/20 | A61N-0005/1028","G06F-017/10","G06F-017/10 | G16H-020/40 | G06Q-010/04 | G06Q-010/06 | G16H-070/20 | A61B-034/10 | G16H-050/20 | G06F-019/00 | A61N-005/10","","","","","","4921032005384"
"US","US","P","B2","Surgical tracking and procedural map analysis tool","In some embodiments, methods and systems are provided for accessing a surgical dataset including surgical data collected during performance of a surgical procedure. The surgical data can include video data of the surgical procedure. Using the surgical data, a plurality of procedural states associated with the surgical procedure can be determined. For a procedural state of the plurality of procedural states, temporal information can be identified that identifies a part of the video data to be associated with the procedural state. For the procedural state of the plurality of procedural states, electronic data can be generated that characterizes the part of the video data and outputting the electronic data associated with the plurality of procedural states.","1. A method comprising: accessing a global surgical dataset comprising a plurality of surgical data structures including surgical data collected during performance of a surgical procedure, wherein each surgical data structure includes: a plurality of nodes, each of the plurality of nodes being associated with one of a plurality of discrete procedural states for the surgical procedure and with a set of procedural metadata; anda plurality of edges, each edge of the plurality of edges connecting two nodes of the plurality of nodes, wherein each edge of the plurality of edges is associated with a surgical action or surgical event;receiving, by one or more data processors, surgical data for the surgical procedure;detecting, by the one or more data processors, a component from a first portion of the surgical data;determining a surgical data structure from the plurality of surgical data structures based on the component from the first portion of the surgical data, wherein the component associated with the surgical data structure is determined by processing the surgical data;identifying a node from the plurality of nodes of the surgical data structure based on the component from the surgical data;retrieving, by the one or more data processors, from the surgical data structure, a set of procedural metadata associated with the node;selecting, by the one or more data processors, an edge from among a set of edges, wherein each edge of the set of edges connects the node to a corresponding next node;generating, based on the selected edge, electronic data to be associated with the first portion of the surgical data; andoutputting the electronic data, the output associating the electronic data with the first portion of the surgical data.","20","16/747915","2020-01-21","2020-0160063","2020-05-21","11081229","2021-08-03","DIGITAL SURGERY LIMITED","Omar Alvi | Andre Chow | James Kellerman | James Liu | Danail Stoyanov","","","","G16H-0030/20","G16H-0030/20 | A61B-0005/0013 | A61B-0034/25 | A61B-0090/361 | G06K-0009/00718 | G06K-0009/00765 | G11B-0027/34 | G16H-0015/00 | G16H-0020/40 | G16H-0070/20 | H04L-0067/12 | H04L-0067/2804 | H04N-0005/77 | H04N-0005/9201 | A61B-2034/252 | A61B-2034/254 | A61B-2034/256 | A61B-2034/258 | A61B-2090/372 | A61B-2090/373 | A61B-2090/502 | A61B-2560/0487 | A61B-2576/00 | G06K-2209/05 | G06K-2209/27 | G16H-0040/67 | G16H-0050/50","G06K-009/00","G06K-009/00 | G16H-030/20 | G16H-015/00 | A61B-034/00 | G16H-070/20 | G16H-020/40 | H04L-029/08 | A61B-090/00 | A61B-005/00 | G11B-027/34 | H04N-005/77 | H04N-005/92 | G16H-050/50 | A61B-090/50 | G16H-040/67","","","","","","4921032005388"
"US","US","P","B2","Biometric recognition method","A biometric recognition method comprising the step of calculating a similarity score between a candidate biometric vector and a reference biometric vector, at least one of the two biometric vectors being quantified.","1. A biometric recognition method comprising: during an enrollment stage, the steps of:capturing an image of a portion of an individual'ss body; andextracting from that image biometric characteristics in the form of a reference biometric vector; andduring a recognition stage, the steps of:capturing an image of a portion of the body of a candidate for recognition;extracting from that image biometric characteristics in the form of a candidate biometric vector;calculating a similarity score between the candidate biometric vector and the reference biometric vector, only one of the two biometric vectors being quantified; andvalidating or refusing recognition as a function of the similarity score.","14","16/510399","2019-07-12","2020-0019691","2020-01-16","11074330","2021-07-27","IDEMIA IDENTITY & SECURITY FRANCE","Jonathan Milgram | Stephane Gentric","2018-056529","FR","2018-07-13","G06F-0021/32","G06F-0021/32 | G06K-0009/00288 | A61B-0005/1176 | G06K-0009/00885 | G07C-0009/37","G06K-009/00","G06K-009/00 | G06F-021/32 | G07C-009/37 | A61B-005/1171","","","","","","4921031003655"
"US","US","P","B2","Comparative analysis of anatomical items","A computer-implemented medical visualization method includes identifying a three-dimensional model of an anatomical item of a particular mammal; displaying a moving animation of the three-dimensional model, the moving animation created from multiple frames from imaging of the anatomical item over a short time period to capture movement of the anatomical item; displaying one or more non-moving views of the three-dimensional model while the moving animation is being displayed; and in response to receiving inputs from a user, changing the displayed moving animation and the one or more non-moving views automatically in coordination with each other.","1. A computer-implemented medical visualization method, comprising: identifying a three-dimensional model of an anatomical item of a particular mammal and a four-dimensional animated model of the anatomical item, wherein the three-dimensional model is formed from imaging data capturing different points in a movement of the anatomical item taken at a first imaging session, and the four-dimensional animated model is formed from imaging data taken at a separate, second imaging session;displaying a first view of the three-dimensional model and a first view of the four-dimensional animated model concurrently on a computer-generated visual display;stepping through frames, based on user input, of the first view of the three-dimensional model from the first imaging session while the concurrently-displayed first view of the four-dimensional animated model from the second imaging session continues to cycle at an associated frame rate to get to a point in time of interest, wherein each frame of the first view of the three-dimensional model represents a different point in the movement of the anatomical item during the first imaging session;receiving a user input indicating a change in visualization of either the three-dimensional model or the four-dimensional animated model; andin response to receiving the user input, displaying a second view of the three-dimensional model from the first imaging session and a second view of the four-dimensional animated model from the second imaging session concurrently on the computer-generated visual display, the second views each reflecting the indicated change in visualization.","11","15/858412","2017-12-29","2018-0122150","2018-05-03","11074760","2021-07-27","Boston Scientific Scimed, Inc.","Benjamin Bidne | Gregory Ernest Ostenson | David M. Flynn | Kenneth Matthew Merdan","","","","G06T-0019/20","G06T-0019/20 | A61B-0034/10 | G06K-0009/4604 | G06T-0007/0012 | G06T-0013/20 | G06T-0015/00 | G06T-0017/00 | G06T-0019/006 | A61B-2017/00243 | A61B-2017/00778 | A61B-2034/102 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2055 | A61B-2090/368 | G06F-0003/01 | G06F-0003/011 | G06K-2009/4666 | G06T-2207/10076 | G06T-2207/30004 | G06T-2207/30048 | G06T-2207/30101 | G06T-2211/40 | G06T-2219/00","G06T-019/20","G06T-019/20 | G06T-019/00 | G06K-009/46 | G06T-007/00 | G06T-015/00 | G06T-017/00 | G06T-013/00 | A61B-034/10 | A61B-034/20 | A61B-090/00 | G06F-003/01 | A61B-017/00 | G06T-013/20","","","","","","4921031004080"
"US","US","P","B2","Assistance apparatus for assisting interpretation report creation and method for controlling the same","An assistance apparatus for assisting creation of an interpretation report obtains a set of regions of interest, which are determined as regions that were observed, of medical image data of a subject that is displayed as an interpretation target, and a set of described regions, which are regions that correspond to description of an interpretation report about the medical image data of a subject. The assistance apparatus determines consistency between the set of regions of interest and the set of described regions, and lets a display unit display a result of the determination.","1. An assistance apparatus for assisting creation of an interpretation report, comprising: a memory storing a program; andone or more processors which, by executing the program, function as:a first obtaining unit obtaining a set of gaze regions, which are determined as regions that have been observed by a user during interpretation, of medical image data of a subject that is displayed as an interpretation target;a second obtaining unit obtaining a set of described regions, which are regions that correspond to a sentence of description of an interpretation report which is a result of the interpretation of the medical image data by the user;a determination unit determining consistency between the set of gaze regions and the set of described regions; anda display control unit controlling a monitor to display presentation information based on a result of the consistency determined by the determination unit, whereinthe determination unit determines that there is a missing description in the interpretation report if there is a region that is included in the set of gaze regions but not in the set of described regions, and when the determination unit determines there is a missing description in the interpretation report, the display control unit controls the monitor to display an information of a region that is included in in the set of gaze regions but not included in the set of described regions,the determination unit determines that there is a checking failure with respect to the medical image data if there is a region that is included in the set of described regions but not in the set of gaze regions, and when the determination unit determines there is a checking failure with respect to the medical image data, the display control unit controls the monitor to display an information of a region that is included in the set of described regions but not included in the set of gaze regions, andthe display control unit controls the monitor to display a message containing text indicating a region corresponding to a missing description when the determination unit determines that there is a missing description, and the display control unit controls the monitor to display a message including an image of an anatomical area emphasizing a region corresponding to a checking failure when the determination unit determines that there is a checking failure.","12","15/503804","2015-08-31","2017-0243348","2017-08-24","11075003","2021-07-27","CANON KABUSHIKI KAISHA","Ryo Sakamoto | Masami Kawagishi | Gakuto Aoyama","2014-181591","JP","2014-09-05","G16H-0030/20","G16H-0030/20 | A61B-0005/748 | A61B-0006/032 | A61B-0006/465 | A61B-0006/467 | A61B-0006/468 | A61B-0006/469 | A61B-0008/13 | A61B-0008/465 | A61B-0008/468 | A61B-0008/469 | G06F-0003/013 | G06F-0003/0488 | G06K-0009/03 | G06Q-0010/10 | G06T-0007/0012 | G06T-0007/11 | G16H-0015/00 | G16H-0030/40 | G16H-0050/20 | A61B-0005/055 | A61B-0005/7435 | A61B-0008/00 | A61B-0008/14","G06K-009/00","G06K-009/00 | G16H-030/20 | A61B-006/03 | G06F-003/01 | G06F-003/0488 | A61B-005/00 | G06Q-010/10 | A61B-006/00 | A61B-008/00 | G16H-030/40 | G16H-015/00 | G16H-050/20 | G06K-009/03 | G06T-007/11 | A61B-008/13 | G06T-007/00 | A61B-005/055 | A61B-008/14","","","","","","4921031004322"
"US","US","P","B2","Real time authentication based on blood flow parameters","Blood flow of a user can be measured using a sensor. Sensor data based on the measuring of the blood flow can be generated. Based on the sensor data, at least a first physiological biomarker of the blood flow measured by the sensor and at least a first morphological characteristic of the blood flow measured by the sensor can be determined. The user can be authenticated based, at least in part, on the first physiological biomarker and the first morphological characteristic.","1. A method of authenticating a user using a user device comprising: measuring, by the user device, using a first sensor, blood characteristics of the user and generating first sensor data based on the measuring of the blood characteristics;determining, based on the first sensor data, at least a first physiological biomarker of the blood characteristics measured by the first sensor, wherein the first physiological biomarker indicates a first oxygen saturation;deriving acceleration plethysmography parameters of the first sensor data;deriving at least a first morphological characteristic of the blood characteristics measured by the first sensor by processing the acceleration plethysmography parameters of the first sensor data;receiving second sensor data generated by a second sensor performing remote oximetry sensing, wherein the second sensor is remote to the user device;determining, based on the second sensor data, at least a second physiological biomarker of a blood flow detected by the second sensor, wherein the second physiological biomarker indicates a second oxygen saturation;receiving third sensor data generated by a third sensor, wherein the third sensor is remote to the user device;deriving acceleration plethysmography parameters of the third sensor data and deriving at least a second morphological characteristic of blood characteristics detected by the third sensor by processing the acceleration plethysmography parameters of the third sensor data;comparing the at least the first oxygen saturation to at least the second oxygen saturation and comparing at least the first morphological characteristic to at least the second morphological characteristic;determining whether at least the first oxygen saturation matches at least the second oxygen saturation and determining whether at least the first morphological characteristic matches at least the second morphological characteristic; andresponsive to at least the first oxygen saturation matching at least the second oxygen saturation and at least the first morphological characteristic matching at least the second morphological characteristic, outputting an indicator indicating the user is authenticated.","15","15/654263","2017-07-19","2018-0020927","2018-01-25","11064893","2021-07-20","SAMSUNG ELECTRONICS CO., LTD.","Jawahar Jain | Vatche A. Attarian | Sajid Sadi | Pranav Mistry","","","","A61B-0005/021","A61B-0005/021 | A61B-0005/0064 | A61B-0005/0261 | A61B-0005/02416 | A61B-0005/117 | A61B-0005/1455 | A61B-0005/14551 | A61B-0005/349 | A61B-0005/6801 | A61B-0005/681 | A61B-0005/6897 | A61B-0005/6898 | A61B-0005/7246 | G06F-0021/32 | G06F-0021/445 | G06K-0009/00885 | G06K-0009/00892 | G07C-0009/37 | G16H-0040/63 | H04L-0009/32 | H04L-0009/3231 | H04L-0009/3273 | H04L-0063/0861 | H04L-0063/0869 | A61B-0005/1172 | G06K-2009/00939","A61B-005/117","A61B-005/117 | A61B-005/1455 | G06F-021/32 | H04L-009/32 | A61B-005/349 | G07C-009/37 | G06F-021/44 | A61B-005/024 | A61B-005/026 | G06K-009/00 | A61B-005/00 | A61B-005/021 | G16H-040/63 | H04L-029/06 | A61B-005/1172","","","","","","4921030000969"
"US","US","P","B2","Prediction of business outcomes by analyzing image interests of users","A method and a system for predicting business outcomes by analyzing image interests of users are provided. The method includes generation of predictor models based on sample data of tests users. The sample data includes historical data of the test users, images that are of interest to the test users, and answers provided by the test users to psychometric questions. The predictor models are then used to predict psychometric features and business outcomes for a target user based on target data of the target user. The target data includes images that are of interest to the target user, historical data of the target user, and answers provided by the target user to the psychometric questions.","1. A method for predicting business outcomes for users, the method comprising: retrieving, by a server, historical data of at least one test user, a first set of answers provided by the test user to a set of psychometric questions, and a first set of images that is of interest to the test user;analyzing, by the server, the first set of answers to derive one or more psychometric features of the test user;converting, by the server using an image processor, each of the first set of images to a defined resolution format;extracting from each of the converted first set of images, by the server, a first set of feature values for a set of image features, wherein the set of image features is independent of one or more objects associated with each of the converted first set of images and includes at least a color distribution, and wherein a first subset of the first set of feature values indicates the color distribution for a first set of colors present in each of the first set of images;generating, by the server, one or more predictor models based on an association between the historical data of the test user, the one or more psychometric features of the test user, and the set of image features, wherein the association between the one or more psychometric features of the test user and the set of image features is determined based on the first set of feature values;retrieving, by the server, a second set of images that is of interest to a target user, wherein each image of the second set of images is associated with a corresponding date and time marker indicating when an interest was shown in the corresponding image by the target user;converting, by the server using the image processor, each of the second set of images to the defined resolution format;extracting from each of the converted second set of images, by the server, a second set of feature values for the set of image features, wherein a second subset of the second set of feature values indicates the color distribution for a second set of colors present in each of the converted second set of images, wherein the second set of feature values are extracted from each converted image of the converted second set of images based on a chronological order associated with each image of the second set of images, and wherein the chronological order associated with each image of the second set of images is indicated by the corresponding date and time marker; andpredicting, by the server, one or more business outcomes for the target user based on at least the one or more predictor models and the second set of feature values.","19","16/213191","2018-12-07","2020-0184495","2020-06-11","11068917","2021-07-20","DOTIN INC.","Roman Samarev | Ganesh Iyer","","","","G06Q-0030/0202","G06Q-0030/0202 | A61B-0005/16 | G06F-0003/04842 | G06F-0016/24578 | G06F-0016/90324 | G06F-0040/40 | G06F-0040/56 | G06K-0009/46 | G06K-0009/4652 | G06K-0009/726 | G06N-0003/08 | G10L-0015/02 | G10L-0015/083","G06Q-030/00","G06Q-030/00 | G06Q-030/02 | A61B-005/16 | G06N-003/08 | G06K-009/46 | G06K-009/72 | G10L-015/02 | G10L-015/08 | G06F-040/56 | G06F-040/40 | G06F-016/2457 | G06F-003/0484 | G06F-016/9032","","","","","","4921030004963"
"US","US","P","B2","Enabling wearables to cognitively alter notifications and improve sleep cycles","A method, computer system, and computer program product for cognitively adjusting a notification alert delivery time are provided. The embodiment may include receiving a message notification from a sender. The embodiment may also include determining an importance of the received message notification based on a plurality of notification attributes and a plurality of person attributes that are each associated with the received message notification. The embodiment may further include, in response to determining to alert a user of the received message notification based on the determined importance, identifying a current user sleep stage. The embodiment may also include, in response to determining the current user sleep stage will minimally impact the user, transmitting the received message notification to a user device.","1. A method for cognitively adjusting a notification alert delivery time, the method comprising: determining an importance of the received message notification based on a plurality of notification attributes and a plurality of person attributes that are each associated with the received message notification;in response to determining to alert a user of the received message notification based on the determined importance, identifying a current user sleep stage; andin response to determining the current user sleep stage will not minimally impact the user, tuning one or more wearable technology device parameters for a next sleep stage, wherein tuning the one or more wearable technology device parameters for a next sleep stage further comprises: calculating a minimum amount of time left in a user sleep cycle, wherein the minimum amount of time left in the user sleep cycle is calculated as T=ANTDmin+n+tprev, and wherein T is the time left in the user sleep cycle, ANTDmin is a minimum allowable notification time delay of the current and remaining sleep stages, n is a total amount of time elapsed in the current sleep stage, and tprev is a sum of an elapsed time in each previously completed sleep stage.","17","16/797232","2020-02-21","2020-0195599","2020-06-18","11070507","2021-07-20","INTERNATIONAL BUSINESS MACHINES CORPORATION","Bharath Ganesh | Dhandapani Shanmugam | Tuhin Sharma | Jothi Subramani","","","","H04L-0051/24","H04L-0051/24 | A61B-0005/4812 | G06F-0001/163 | G06Q-0010/10 | G06Q-0010/107 | H04L-0051/26 | H04W-0068/005 | H04L-0051/16 | H04L-0067/12 | H04M-0001/72436 | H04M-0001/72454 | H04M-2242/26 | H04W-0068/02","H04L-012/58","H04L-012/58 | G06F-003/01 | G06F-001/16 | H04W-068/00 | A61B-005/00 | G06Q-010/10 | H04W-068/02 | H04L-029/08 | H04M-001/72436 | H04M-001/72454","","","","","","4921030006542"
"US","US","P","B2","Electrical stimulation facilitated symptom transference for empathic response","A method of simulating a tremor in a subject and evoking an empathetic response in the subject towards a patient suffering from tremors, the method comprising: synthesizing a tremor inducing signal, which may be done by capturing and analyzing EMG measures of electrical pulses derived from a sensed neuromuscular event associated with tremors experienced by a patient; and applying an electrical muscle stimulation to the subject using the synthesized pulses associated with the patient experiencing tremors.","1. A method of simulating a tremor in a subject and evoking an empathetic response in the subject towards a patient suffering from tremors, the method comprising: administering an electrical muscle stimulation in a synthesized pattern to the subject to cause tremors in the subject, wherein the synthesized pattern is based on electromyogram (EMG) muscle motion data from a sensed neuromuscular event associated with tremors experienced by the patient,wherein the electrical muscle stimulation in the synthesized pattern is administered to the subject using a plurality of electrical muscle stimulation electrodes incorporated into a garment that delivers a corresponding plurality of channels of stimulation to the plurality of electrical muscle stimulation electrodes at locations along the circumference of the subject'ss forearm.","6","16/192194","2018-11-15","2019-0142339","2019-05-16","11058352","2021-07-13","KVI BRAVE FUND I INC.","Anirudh Thommandram | Yan Fossat","","","","A61B-0005/4884","A61B-0005/4884 | A61B-0005/0077 | A61B-0005/1101 | A61B-0005/1128 | A61B-0005/389 | A61B-0005/4082 | A61N-0001/0452 | A61N-0001/36003 | A61B-2562/0219 | A61N-0001/0476 | A61N-0001/0484 | G06F-0003/011 | G06F-0003/014 | G06F-0003/015","A61B-005/00","A61B-005/00 | A61B-005/11 | A61N-001/04 | A61N-001/36 | A61B-005/389 | G06F-003/01","","","","","","4921029000973"
"US","US","P","B2","Use of augmented reality to assist navigation during medical procedures","Physicians performing invasive procedures require accuracy and precision when working with surgical tools. Surgical procedures are increasingly becoming minimally invasive, with physicians operating using cameras to view the surgery site and directing their tools through oculars or video displays. Ideally, the physician should be able to perform the invasive procedure while simultaneously observing both the real-time image of the patient and additional data critical for his medical decisions about the manipulation of the surgical tool and the next surgical step. The augmented reality navigation system of the present disclosure provides tool location visibility for invasive procedures through the use of location sensors included on a camera and/or on the tools used during a procedure. A location tracking system determines and monitors the locations of the tools and camera based on the characteristics of signals detected by the sensors and displays informational overlays on images obtained with a camera.","1. A method for providing an augmented reality display of a subject, the method comprising: obtaining an image from a camera;identifying a location of the camera based on signals, wherein signals are received from sensors of a location pad in proximity of the subject'ss head wherein the location pad is not attached to the patient'ss head and receiving signals with sensors of the location pad, the signals generated by one or more field generators coupled to the camera;identifying a field of view of the camera based on the location of the camera;determining pixel value information of pixels corresponding to anatomical features in the field of view of the image;identifying one of the anatomical features of the subject based on the pixel value information and the location of the camera;generating one or more overlay features corresponding to the one anatomical feature in the field of view, the overlay features selected from a group comprising an anatomical feature outline of a sinus ostium and a text configured to label the sinus ostium;identifying the sinus ostium based on detection of a darkened portion of the image;in response to the identified sinus ostium, generating the outline of the identified ostium and generating the text on a selected location of the image, said selected location based on a variance of an average color value below a predetermined threshold for a plurality of pixels in an area of the image; andcompositing the generated overlay features for the identified sinus ostium with the image from the camera to form a composited image;displaying the composited image on a display; andnavigating one or more tools within the sinus ostium based upon the composited image.","17","16/229629","2018-12-21","2019-0192232","2019-06-27","11058497","2021-07-13","BIOSENSE WEBSTER (ISRAEL) LTD. | ACCLARENT, INC.","Andres Claudio Altmann | Assaf Govari | Vadim Gliner | Itzhak Fang | Noam Rachli | Yoav Pinsky | Itamar Bustan | Jetmir Palushi | Zvi Dekel","","","","A61B-0034/20","A61B-0034/20 | A61B-0001/0005 | A61B-0001/00009 | A61B-0001/04 | A61B-0001/233 | A61B-0005/0077 | A61B-0005/064 | A61B-0005/066 | A61B-0005/068 | A61B-0005/743 | A61B-0017/24 | A61B-0034/25 | A61B-0090/36 | A61B-0090/361 | G06F-0003/011 | G06T-0007/74 | G06T-0011/00 | A61B-2034/2051 | A61B-2034/2057 | A61B-2034/2065 | A61B-2034/2068 | A61B-2034/2072 | A61B-2034/254 | A61B-2034/256 | A61B-2090/365 | A61B-2090/373 | A61B-2090/3983 | A61B-2505/05 | G06T-2207/30204","A61B-034/20","A61B-034/20 | A61B-017/24 | A61B-090/00 | A61B-001/00 | G06T-007/73 | A61B-034/00 | A61B-005/00 | G06T-011/00 | A61B-005/06 | G06F-003/01 | A61B-001/04 | A61B-001/233","","","","","","4921029001118"
"US","US","P","B2","Sensor-controlled display output for dialysis machines","A medical apparatus, such as a dialysis machine (e.g. a hemodialysis machine or a peritoneal dialysis machine), includes a plurality of components, one or more sensors corresponding to the components and configured to detect signals, a display and a control unit. The control unit is configured to: receive signals from the one or more sensors, determine, from the signals, a status of the medical apparatus, and determine control commands for the display based on the determined status for status-dependent control of the display. The described apparatus improves the human-machine interface in terms of set-up time, operating time and freedom from errors. Depending upon the determined status, different status-specific menus may be illustrated on the display in order to assist the user when operating the apparatus or advise the user about any errors or subsequent steps.","1. A medical apparatus, comprising: a plurality of components;a plurality of sensors configured to detect status information relating to the plurality of components;a display configured to provide a user interface; anda controller configured to: control the display to display a sequence of display outputs corresponding to a sequence of operating steps for the medical apparatus;determine whether the sequence of operating steps is being correctly performed based on detected status information; andin response to determining that an error has occurred, controlling the display to display corrective measures for correcting the error;wherein the controller is configured to switch the user interface between a first menu type corresponding to a step-by-step mode and a second menu type corresponding to a parameter-based mode, wherein in the step-by-step mode, a respective display illustration is provided for each respective step of the sequence of operating steps, and wherein in the parameter-based mode, machine parameters are directly adjustable via user input;wherein the controller is further configured to: detect, via at least one of the plurality of sensors, that a dialyzer is inserted;detect a dialyzer type of the dialyzer based on polling sensor data; andoutput a specific display for further operating steps based on the dialyzer being inserted and the detected dialyzer type.","23","16/407967","2019-05-09","2019-0262095","2019-08-29","11058512","2021-07-13","FRESENIUS MEDICAL CARE DEUTSCHLAND GMBH","Pia Nora Daniel","10-2015-122347","DE","2015-12-21","A61B-0090/37","A61B-0090/37 | A61M-0001/14 | A61M-0001/16 | A61M-0001/1652 | A61M-0001/3672 | G06F-0003/0482 | G06F-0003/04842 | G06F-0003/04847 | G06F-0003/14 | G06F-0009/453 | G16H-0020/17 | G16H-0040/63 | A61M-2205/3327 | A61M-2205/3592 | A61M-2205/502 | A61M-2205/505 | A61M-2205/583","A61B-090/00","A61B-090/00 | G06F-003/0484 | G06F-009/451 | A61M-001/16 | G16H-040/63 | G16H-020/17 | A61M-001/14 | A61M-001/36 | G06F-003/0482 | G06F-003/14","","","","","","4921029001133"
"US","US","P","B2","Infusion management","Methods, computer systems and computer readable media for receiving data from infusion pumps in a healthcare setting and displaying the data on a user device are provided. Centralized clinician views are provided to manage individual and multiple patient infusions. Embodiments provide near real-time graphical displays of infusion data to clinicians on separate user devices. In addition, near real-time graphical displays of patient physiologic data is displayed simultaneously to a clinician along with the infusion data.","1. A computing system comprising: a computer processor;a display device coupled to the computer processor; anda computer-storage media storing computer readable executable instructions that, when executed by the computing system using the computer processor, perform a method of generating an at least one alert comprising: receiving patient identifying information;receiving an at least one order comprising a list of prescribed infusion fluids being administered to a patient at an at least one infusion pump and a prescribed infusion rate of the prescribed infusion fluids;receiving from the at least one infusion pump a continuous data feed comprising a volume of the prescribed infusion fluids administered by the at least one infusion pump over a time duration;determining an inconsistency between the continuous data feed and the at least one order;determining that the at least one order is not a current version of the at least one order;generating the at least one alert based on the inconsistency, the alert indicating that the inconsistency exists and that the at least one order associated with the pump is not the current version of the at least one order;causing for display a plurality of patient-specific-information display areas wherein each of the plurality of patient-specific-information display areas presents the patient identifying information, the at least one order, and the at least one alert;receiving an input associated with a cursor hovering over at least one of the plurality of patient-specific-information display areas; andbased on the input, causing for simultaneous display on a graphical user interface (GUI) on a clinician computing device that is remote from the at least one infusion pump: the plurality of patient-specific-information display areas; anda hover-invoked display area comprising additional information related to the at least one alert, wherein the additional information is different than the plurality of patient-specific-information display areas.","13","15/210098","2016-07-14","2016-0317742","2016-11-03","11058816","2021-07-13","CERNER INNOVATION, INC.","Mary Gannon | Lisa Kelly | Stephanie Rogers | Amanda Buckley","","","","A61M-0005/172","A61M-0005/172 | G06F-0003/0482 | G06F-0003/04812 | G06F-0003/04817 | G06F-0019/00 | G06Q-0010/10 | G16H-0020/17 | G16H-0040/63 | A61M-2205/502 | A61M-2230/00 | A61M-2230/30","G06F-003/048","G06F-003/048 | G06F-019/00 | A61M-005/172 | G06Q-010/10 | G16H-040/63 | G16H-020/17 | G06F-003/0481 | G06F-003/0482","","","","","","4921029001436"
"US","US","P","B2","Biometric recognition method","A biometric recognition method, comprising the step of calculating a similarity score of a candidate biometric vector with a reference biometric vector. At least one of the biometric vectors is extracted using at least one neural network.","1. A biometric recognition method, comprising the following steps: capturing an image of a recognition candidate'ss face;extracting biometric characteristics from the image in a form of a candidate biometric vector;calculating a similarity score of the candidate biometric vector with a stored reference biometric vector;validating or refusing recognition based on the similarity score;wherein the reference biometric vector is obtained during a preliminary enlistment phase by:capturing an image of an enlistment candidate'ss face;extracting biometric characteristics from the image of the enlistment candidate'ss face in a form of a reference biometric vector;wherein the extraction is carried out using at least one neural network having an output layer comprising at most 128 neurons for at least one of the biometric vectors and the biometric vector obtained by the neural network undergoes a quantification operation before being stored in such a manner that the quantized biometric vector has a size of 64 bytes or 32 bytes;wherein the quantized biometric vector is stored in a form of a two-dimensional bar code displayable on a screen of a smartphone of the enlistment candidate, the two-dimensional bar code comprises at least one of the following data:a name of the enlistment candidate;a first name of the enlistment candidate;information relating to a journey to be made by the enlistment candidate;wherein the quantized biometric vector is associated with a signature before being transformed into the two-dimensional bar code and the signature is an encrypted value corresponding to the name of the enlistment candidate and the information relating to a journey to be made by the enlistment candidate; andwherein the recognition candidate commands the smartphone to display the two-dimensional bar code and present the two-dimensional bar code to a bar code reader to request recognition.","7","16/510268","2019-07-12","2020-0019689","2020-01-16","11062008","2021-07-13","IDEMIA IDENTITY & SECURITY FRANCE","Jonathan Milgram | Stephane Gentric | Franck Maurin","2018-056530","FR","2018-07-13","G06F-0021/32","G06F-0021/32 | G06K-0009/00288 | A61B-0005/1176 | G06K-0009/00885 | G07C-0009/37","G06K-009/00","G06K-009/00 | G06F-021/32 | G07C-009/37 | A61B-005/1171","","","","","","4921029004587"
"US","US","P","B2","Compatibility check for continuous glucose monitoring application","Disclosed are systems, methods, and articles for determining compatibility of a mobile application and operating system on a mobile device. In some aspects, a method includes receiving one or more data values from a mobile device having a mobile medical software application installed thereon, the data value(s) characterizing a version of the software application, a version of an operating system installed on the mobile device, and one or more attributes of the mobile device; determining whether the mobile medical software application is compatible with the operating system by at least comparing the received data value(s) to one or more test values in a configuration file; and sending a message to the mobile device based on the determining, the message causing the software application to operate in one or more of a normal mode, a safe mode, and a non-operational mode.","1. A method comprising: receiving, by at least one processor of a server, one or more output values from a user equipment having a glucose monitoring application installed on the user equipment, the one or more output values representing results from one or more self-tests performed on the user equipment, the one or more self-tests validating that one or more features of one or more of the glucose monitoring application and the user equipment are functioning properly;in response to receiving the one or more output values, determining, by the at least one processor of the server, whether the glucose monitoring application is compatible with an operating environment of the user equipment based at least on a composite score based on the one or more output values, wherein the composite score is a weighted sum of the one or more output values, an average of the one or more output values, a weighted average of the one or more output values, or a statistical mode of the one or more output values, wherein the determining further comprises determining if a core function of the glucose monitoring application is compatible with the operating environment; andbased on the determining, causing the glucose monitoring application to operate in one or more of a normal mode, a safe mode, and a non-operational mode, wherein if the core function of the glucose monitoring application is incompatible with the operating environment, then causing the glucose monitoring application to operate in the non-operational mode, and wherein the core functions include one or more of generating an alert if a glucose level of a user is outside of a target range, displaying a glucose level, or prompting calibration of a glucose sensor assembly.","23","16/728536","2019-12-27","2020-0142804","2020-05-07","11055198","2021-07-06","DEXCOM, INC.","Issa Sami Salameh | Douglas William Burnette | Tifo Vu Hoang | Steven David King | Stephen M. Madigan | Michael Robert Mensinger | Andrew Attila Pal | Michael Ranen Tyler","","","","G06F-0011/3604","G06F-0011/3604 | A61B-0005/6801 | G06F-0008/65 | G06F-0008/70 | G06F-0008/71 | G06F-0009/44505 | G06F-0011/00 | G06F-0011/143 | G06F-0011/1433 | G06F-0011/36 | G06F-0011/3616 | G06F-0011/3688 | G16H-0040/40 | H04L-0067/34 | H04M-0001/724 | H04W-0004/12 | H04W-0004/20 | H04W-0004/70 | A61B-0005/14532 | A61B-0005/743 | H04M-0001/72403 | H04W-0088/02","G06F-011/36","G06F-011/36 | G06F-008/71 | G06F-008/70 | G06F-011/14 | G06F-011/00 | G06F-009/445 | G16H-040/40 | H04W-088/02 | A61B-005/145 | H04W-004/70 | H04L-029/08 | H04W-004/20 | H04M-001/724 | G06F-008/65 | H04W-004/12 | A61B-005/00 | H04M-001/72403","","","","","","4921028004386"
"US","US","P","B2","Intelligent health monitoring","Embodiments are disclosed for health assessment and diagnosis implemented in an artificial intelligence (AI) system. In an embodiment, a method comprises: capturing, using one or more sensors of a device, signals including information about a user's symptoms; using one or more processors of the device to: collect other data correlative of symptoms experienced by the user; and implement pre-trained data driven methods to: determine one or more symptoms of the user; determine a disease or disease state of the user based on the determined one or more symptoms; determine a medication effectiveness in suppressing at least one determined symptom or improving the determined disease state of the user; and present, using an output device, one or more evidence for at least one of the determined symptoms, the disease, disease state, or an indication of the medication effectiveness for the user.","1. A system comprising: an output device;one or more sensors;one or more processors;memory storing instructions that when executed by the one or more processors, cause the one or more processors to perform operations comprising: capturing, using the one or more sensors, signals including information about a user'ss symptoms;collecting other data correlative of symptoms experienced by the user;determining, using a symptom classifier with the signals and the other data as input, one or more symptoms of the user, the symptom classifier including a first neural network trained on a set of symptoms collected from a plurality of individuals;determining, using a disease classifier with the one or more symptoms and one or more sleep features as input, a disease or disease state of the user, the disease classifier including a second neural network trained on the set of symptoms and a set of sleep features collected from the plurality of individuals;determining, using a medication effectiveness classifier with the one or more symptoms and the disease or disease state as input, a medication effectiveness in suppressing the one or more symptoms or improving the disease state of the user, the medication effectiveness classifier including a third neural network trained on the set of symptoms and a set of diseases or disease states collected from the plurality of individuals; andpresenting, using the output device, data for the one or more symptoms, the disease, the disease state, or an indication of the medication effectiveness for the user,wherein the system further includes: a housing having a first portion configured to be inserted inside an ear canal of a user and a second portion worn around the ear of the user;a first microphone or microphone array disposed in the first portion of the housing;a second microphone or microphone array disposed in the second portion of the housing;and wherein the one or more sensors include at least one of a motion sensor, a peripheral capillary oxygen saturation (SPO2) sensor or a heart rate sensor disposed in the housing and configured to detect motion, oxygen levels and heart rate, respectively, of the user during a health-related event;and wherein the one or more processors are coupled to the first and second microphones and the motion sensor, and configured to: obtain a first audio signal from the first microphone;obtain a second audio signal from the second microphone;obtain data from the one or more sensors; anddetermine, based on the first audio signal, the second audio signal and the data, one or more health-related events for the user.","9","16/680429","2019-11-11","2020-0146623","2020-05-14","11055575","2021-07-06","CURIEAI, INC.","Ramin Anushiravani | Sridhar Krishna Nemala | Ravi Kiran Yalamanchili | Navya Swetha Davuluri","","","","G06K-0009/6264","G06K-0009/6264 | A61B-0005/0205 | A61B-0005/0823 | A61B-0005/11 | A61B-0005/14551 | A61B-0005/4803 | A61B-0005/4815 | A61B-0005/4818 | A61B-0005/4842 | A61B-0005/4848 | A61B-0005/6817 | A61B-0005/7257 | A61B-0005/7264 | A61B-0005/7267 | A61B-0005/7275 | G06F-0017/16 | G06F-0017/18 | G06K-0009/629 | G06K-0009/6257 | G06K-0009/6269 | G06K-0009/726 | G06N-0003/04 | G06N-0003/0445 | G06N-0003/08 | G06N-0020/10 | G10L-0015/02 | G10L-0015/16 | G10L-0015/1815 | G10L-0015/22 | G10L-0025/66 | G16H-0010/60 | G16H-0020/10 | G16H-0040/67 | G16H-0050/20 | G16H-0050/70 | G16H-0070/40 | G16H-0070/60 | G16H-0080/00 | A61B-0005/024 | A61B-2562/0204 | G10L-2015/027","A61B-005/00","A61B-005/00 | G06K-009/62 | G06N-003/04 | G16H-010/60 | G16H-080/00 | G16H-040/67 | G16H-050/20 | A61B-005/08 | G06N-003/08 | G10L-015/02 | G10L-015/16 | G10L-015/22 | G10L-025/66 | G16H-050/70 | G16H-020/10 | G06F-017/16 | G06F-017/18 | G16H-070/60 | G16H-070/40 | A61B-005/0205 | A61B-005/11 | A61B-005/1455 | G06N-020/10 | G06K-009/72 | G10L-015/18 | A61B-005/024","","","","","","4921028004759"
"US","US","P","B2","Under-LCD screen optical sensor module for on-screen fingerprint sensing","Devices and optical sensor modules are provided for provide on-screen optical sensing of fingerprints by using an under-screen optical sensor module that captures and detects returned light that is emitted by an LCD display screen for displaying images and that is reflected back by the top surface of the screen assembly.","1. An optical sensor module for detecting a fingerprint pattern of a finger above a light crystal display (LCD) screen, the optical sensor module comprising: an optical sensor array arranged under the LCD screen, the optical sensor array comprising a plurality of optical sensors for receiving probe light that is generated when the finger is illuminated by one or more probe light sources that emit invisible light, and converting the probe light into detector signals representing a fingerprint pattern of the finger;wherein the probe light is transmitted through the LCD screen at a selected region above the optical sensor module, the selected region is structured differently from other regions in the LCD screen so as to transmit more probe light than the other regions in the LCD screen to increase an amount of the received probe light at the optical sensor module.","20","16/529968","2019-08-02","2019-0362119","2019-11-28","11048903","2021-06-29","SHENZHEN GOODIX TECHNOLOGY CO., LTD.","Yi He | Bo Pi","","","","G06K-0009/0002","G06K-0009/0002 | A61B-0005/1172 | G06F-0003/042 | G06F-0003/044 | G06F-0003/0412 | G06F-0003/0416 | G06F-0003/0418 | G06F-0021/32 | G06F-0021/83 | G06K-0009/001 | G06K-0009/0004 | G06K-0009/0012 | A61B-0005/14532 | A61B-0005/7203 | G06F-2203/04106 | G06F-2221/2133 | H04L-0063/0861","G06K-009/00","G06K-009/00 | A61B-005/1172 | G06F-003/041 | G06F-003/042 | G06F-003/044 | G06F-021/32 | G06F-021/83 | A61B-005/145 | A61B-005/00 | H04L-029/06","","","","","","4921027004689"
"US","US","P","B2","Neuro-physiology and neuro-behavioral based stimulus targeting system","An example system includes an analyzer to determine a first distance between (1) a first peak in a first frequency band of first neuro-response data gathered from a subject while exposed to media and (2) a second peak in the first frequency band; determine a second distance between (1) a third peak in the first frequency band and either (2) the second peak in the first frequency band or (3) a fourth peak in the first frequency band and determine a first difference between the first distance and the second distance. The example system includes a selector to determine a modification for the media based on the first difference and a modifier to implement the modification for presentation of the media.","1. A system comprising: an analyzer to: determine a first distance between (1) a first peak in a first frequency band of first neuro-response data gathered from a subject while exposed to media and (2) a second peak in the first frequency band;determine a second distance between (1) a third peak in the first frequency band and either (2) the second peak in the first frequency band or (3) a fourth peak in the first frequency band; anddetermine a first difference between the first distance and the second distance;a selector to determine a modification for the media based on the first difference; anda modifier to implement the modification for presentation of the media.","20","16/790160","2020-02-13","2020-0234329","2020-07-23","11049134","2021-06-29","NIELSEN CONSUMER LLC | CITIBANK, N.A","A. K. Pradeep | Robert T. Knight | Ramachandran Gurumoorthy","","","","G06Q-0030/0244","G06Q-0030/0244 | A61B-0005/4017 | G06Q-0010/0637 | G06Q-0030/0242 | G06Q-0030/0269 | G06Q-0030/0271 | G16H-0010/20 | A61B-0005/0533 | A61B-0005/1176 | A61B-0005/163 | A61B-0005/318 | A61B-0005/369 | A61B-0005/398 | A61B-0005/4035","G06Q-030/00","G06Q-030/00 | G06Q-030/02 | G06Q-010/06 | A61B-005/00 | G16H-010/20 | A61B-005/16 | A61B-005/0533 | A61B-005/1171 | A61B-005/318 | A61B-005/369 | A61B-005/398","","","","","","4921027004919"
"US","US","P","B2","System, computer program product, and method for automated gift determination and delivery","A system and method for automated gift determination and delivery is provided, which include identifying a contact from an electronic contact list for receiving a gift, detecting from first data related to the contact a change in an emotional state of the contact, validating from second data the change in the emotional state of the contact, and automatically selecting the gift from a plurality of identified gifts that is determined to be commensurate with the change status and a current emotional state of the contact.","1. A computer-implemented method for automated gift determination and delivery, comprising: receiving, by a processor of a computer system, a contact from an electronic contact list for receiving a gift;receiving, by the processor, from first data related to the contact a change in an emotional state of the contact from a previous emotional state to a current emotional state;calculating by the processor a confidence level of a reason for the change in the emotional state of the contact, including executing an artificial intelligence computer to train the computer system to identify a plurality of gifts based on the emotional state and previous events involving the contact, comprising: determining by the computer system whether to store at least the first data to train the artificial intelligence computer for generating future decisions regarding the contact;performing an iteration process to modify a result calculated by the artificial intelligence computer; andtraining, updating, and optimizing the artificial intelligence computer by the processing the result;identifying relevant gifts of the plurality of gifts based on the emotional state and previous events involving the contact using the artificial intelligence computer;validating, by the processor, from second data the change in the emotional state of the contact;receiving, by the processor, third data from a data repository that includes information about the contact to identify the gift based on the information about the contact;automatically selecting, by the processor, the gift from the plurality of identified gifts that is determined to be commensurate with the change in the emotional state to the current emotional state of the contact and further commensurate with the information about the contact; anddisplaying, at a computer display of a user purchasing the gift for the contact, in about the gift automatically selected by the processor and an electronic request to approve the selected gift for purchase and delivery to the contact.","18","16/043601","2018-07-24","2020-0034914","2020-01-30","11049169","2021-06-29","INTERNATIONAL BUSINESS MACHINES CORPORATION","Gregory J. Boss | Bernadette Pierson | Cesar Augusto Rodriguez Bravo | Jayashree Vaidyanathan","","","","G06Q-0030/0633","G06Q-0030/0633 | A61B-0005/165 | G06F-0040/30 | G06K-0009/726 | G06Q-0030/0609 | G06Q-0030/0631","G06Q-030/00","G06Q-030/00 | G06Q-030/06 | A61B-005/16 | G06K-009/72 | G06F-040/30","","","","","","4921027004954"
"US","US","P","B2","User care system using chatbot","A user care system accuses a chatbot. According to an embodiment of the present disclosure, not only a current state of a user can be quickly determined on the basis of a content of a conversation between a chatbot and the user, but also the user can quickly receive help from the outside when it is determined that the user is in a dangerous situation. Further, not only the chatbot can quickly determine a current state of the user on the basis of a change in biometric information of the user, but also the user can quickly receive help from the outside when it is determined that the user is in a dangerous situation.","1. A user care system using a chatbot, which determines a current state of a user by using a chatbot mounted to a server, wherein the chatbot includes:a processor;a memory;a conversation content generating module implemented by the processor, the conversation content generating module configured to generate a conversation content information and transmit the same to a user terminal of the user;a conversation content storing module implemented by the processor and the memory, the conversation content storing module configured to receive and store an answer content information in response to the conversation content information from the user terminal, an answer content base information logically matched with the conversation content information being stored in the conversation content storing module;a biometric information storing module implemented by the processor and the memory, the biometric information storing module configured to receive at least one of biometric information, electrocardiogram, blood pressure, oxygen saturation, and body temperature of the user from a wearable device that the user is wearing and store the biometric information at each preset unit time; anda determining module implemented by the processor, the determining module configured to determine, by comparing the conversation content information, the answer content information and the answer content base information, that the current state of the user as a normal state if the answer content information is in a preset matching ratio in comparison to the answer content base information, and the biometric information is in a preset stable range, and, if not, the current state of the user as a dangerous situation.","9","16/648352","2018-05-30","2020-0228470","2020-07-16","11050686","2021-06-29","1THEFULL PLATFORM LIMITED","Seung-Yub Koo","10-2017-0122585","KR","2017-09-22","H04L-0051/02","H04L-0051/02 | A61B-0005/0002 | A61B-0005/165 | A61B-0005/4803 | G06F-0040/30 | G06N-0005/043 | G06Q-0050/265 | G16H-0040/67 | G16H-0050/30 | A61B-0005/02055 | A61B-0005/14551 | A61B-0005/318 | A61B-0005/6801","G06F-015/16","G06F-015/16 | H04L-012/58 | G16H-040/67 | G16H-050/30 | G06F-040/30 | A61B-005/00 | A61B-005/16 | G06N-005/04 | G06Q-050/26 | A61B-005/0205 | A61B-005/1455 | A61B-005/318","","","","","","4921027006456"
"US","US","P","B2","Wearable cardioverter defibrillator (WCD) system having main UI that conveys message and peripheral device that amplifies the message","In embodiments, a Wearable Cardiac Defibrillator (WCD) system is configured to be worn by an ambulatory patient. The WCD system includes a main user interface (UI) output device that can output an image, sound or vibration as a main message about a condition of the patient or the WCD system. The patient may further carry a peripheral device that can also output an image, sound or vibration as a peripheral message about the condition. The peripheral message may mirror the main message at least in part, amplify it, and so on. The availability of the peripheral message provides the patient with the opportunity to better perceive the main message, and the flexibility to receive and react to it discreetly, learn more about the condition, and so on.","1. A wearable cardioverter defibrillator (WCD) system configured to be worn by an ambulatory patient, comprising: an electrode;a support structure configured to be worn by the ambulatory patient so as to maintain the electrode on a body of the ambulatory patient;an energy storage module configured to store an electrical charge, and to discharge at least some of the stored electrical charge via the electrode through the ambulatory patient;a main housing;a main user interface (UI) output device located at least partly within the main housing;a main communication module;a main processor configured to: receive an input about a condition, the condition being about one of the ambulatory patient and a component of the WCD system,cause, responsive to the received input, the main UI output device to output a main human-perceptible indication (HPI), the main HPI being about the condition, andcause, responsive to the received input, the main communication module to transmit an amplification signal, the amplification signal encoding and amplifying a message about the condition; anda peripheral device that is configured to be carried by the ambulatory patient, the peripheral device comprising: a peripheral housing distinct from the main housing;a peripheral communication module (PCM) located at least partly within the peripheral housing, the PCM configured to receive the transmitted amplification signal with the encoded and amplified message; anda peripheral UI output device configured, responsive to the PCM receiving the amplification signal with the encoded and amplified message, to output a peripheral HPI, the peripheral HPI being about the condition.","25","16/283342","2019-02-22","2019-0269930","2019-09-05","11040214","2021-06-22","PHYSIO-CONTROL DEVELOPMENT CO., LLC | TACTILE, INC. | WEST AFFUM HOLDINGS CORP. | STRYKER CORPORATION","Zoie Engman | Aaron Piazza | Christoffer Peter Hart Hansen | David P. Finch | Laura M. Gustavson | Erik L. Schneider | Erick Roane | Amanda K. Hall","","","","A61N-0001/3993","A61N-0001/3993 | A61B-0005/0205 | A61B-0005/6804 | A61N-0001/046 | A61N-0001/0484 | A61N-0001/3904 | A61N-0001/3968 | A61N-0001/3975 | G06F-0003/1454 | A61B-0005/0002 | A61B-0005/024 | A61B-0005/026 | A61B-0005/029 | A61B-0005/0816 | A61B-0005/11 | A61B-0005/1455 | A61B-0005/363 | A61B-0005/7275 | A61B-0007/003 | A61B-0007/04 | G08B-0003/10 | G08B-0006/00","A61N-001/39","A61N-001/39 | A61N-001/04 | G06F-003/14 | A61B-005/00 | A61B-005/0205 | G08B-003/10 | G08B-006/00 | A61B-005/024 | A61B-007/04 | A61B-005/08 | A61B-005/029 | A61B-005/1455 | A61B-005/11 | A61B-007/00 | A61B-005/026 | A61B-005/363","","","","","","4921026001240"
"US","US","P","B2","Sensing system, sensor device, and sensor fixture","Provided is a sensing system, a sensor device, and a sensor fixture. The sensing system includes a sensor fixture that has a mode corresponding to an attachment attitude to a target object and a sensor device that is attachable with the attachment attitude to the target object via the sensor fixture. The sensor device includes a sensor unit that senses information regarding the target object, and an acquisition unit that acquires information indicating the attachment attitude based on the mode of the sensor fixture.","1. A sensing system, comprising: a sensor fixture that comprises a first mode, wherein the first mode corresponds to an attachment attitude of the sensor fixture; anda sensor device attachable to a target object via the sensor fixture, wherein the sensor device is attachable based on the attachment attitude of the sensor fixture,each axis of the sensor device has a different relation with respect to the target object based on the attachment attitude of the sensor fixture, andthe sensor device includes: an acquisition unit configured to acquire first attachment information based on a physical shape of the sensor fixture, wherein the acquired first attachment information indicates the attachment attitude of the sensor fixture on the target object;a sensor setting unit configured to set a resolution of a sensing operation based on the attachment attitude of the sensor fixture on the target object, wherein at least one axis of the sensor device is set to have a different resolution compared to other axes of the sensor device based on the set resolution; anda sensor unit configured to execute the sensing operation to detect sensor information regarding the target object, wherein the execution of the sensing operation is based on the set resolution.","12","15/751054","2016-07-26","2018-0236340","2018-08-23","11040263","2021-06-22","SONY CORPORATION","Sho Murakoshi","2015-191864","JP","2015-09-29","A63B-0069/3658","A63B-0069/3658 | A61B-0005/1122 | A61B-0005/6895 | A63B-0069/38 | G06K-0009/00342 | G06Q-0010/0639 | G08C-0017/00 | G09B-0019/0038 | H04Q-0009/00 | A61B-2503/10 | A61B-2505/09 | A61B-2562/0219 | A63B-2220/40 | A63B-2220/80 | H04Q-2209/40","A63B-069/36","A63B-069/36 | H04Q-009/00 | G06K-009/00 | G06Q-010/06 | A61B-005/00 | A61B-005/11 | G09B-019/00 | A63B-069/38 | G08C-017/00","","","","","","4921026001289"
"US","US","P","B2","Classifying facial expressions using eye-tracking cameras","Images of a plurality of users are captured concurrently with the plurality of users evincing a plurality of expressions. The images are captured using one or more eye tracking sensors implemented in one or more head mounted devices (HMDs) worn by the plurality of first users. A machine learnt algorithm is trained to infer labels indicative of expressions of the users in the images. A live image of a user is captured using an eye tracking sensor implemented in an HMD worn by the user. A label of an expression evinced by the user in the live image is inferred using the machine learnt algorithm that has been trained to predict labels indicative of expressions. The images of the users and the live image can be personalized by combining the images with personalization images of the users evincing a subset of the expressions.","1. A method comprising: capturing images of a plurality of users concurrently with the plurality of users evincing a plurality of expressions, wherein the images are captured using at least one eye tracking sensor implemented in at least one head mounted device (HMD) worn by the plurality of users;forming at least one personalization image for each of the users based on a subset of the plurality of expressions by: calculating a mean neutral image of a user of the plurality of users by averaging corresponding pixel values of a subset of images of the user, wherein the images of the plurality of users comprise the subset of images of the user, and wherein the at least one personalization image includes the mean neutral image;combining the at least one personalization image with the images by subtracting the at least one personalization image from the images; andtraining a machine learnt algorithm to predict labels indicative of emotions based on the images and the plurality of expressions.","12","15/831823","2017-12-05","2018-0314881","2018-11-01","11042729","2021-06-22","GOOGLE LLC","Avneesh Sud | Steven Hickson | Vivek Kwatra | Nicholas Dufour","","","","G06K-0009/00302","G06K-0009/00302 | A61B-0005/165 | A61B-0005/6803 | G01S-0003/00 | G06F-0003/013 | G06K-0009/00671 | G06K-0009/4628 | G06K-0009/627 | G06K-0009/6256 | G06N-0003/04 | G06N-0003/0454 | G06N-0003/08 | H04N-0005/232 | H04N-0005/23219 | A61B-0005/163 | A61B-0005/7267 | A61B-0005/7275 | G06K-0009/00604 | H04N-0013/344","G06K-009/00","G06K-009/00 | A61B-005/16 | G06N-003/08 | G06N-003/04 | H04N-005/232 | G06K-009/62 | G06F-003/01 | A61B-005/00 | G01S-003/00 | G06K-009/46 | H04N-013/344","","","","","","4921026003732"
"US","US","P","B2","Information processing apparatus and information processing method with display of relationship icon","The present invention has an objective to display information more suitable for searching for medical information. An information processing apparatus includes a display control unit configured to display, when a first medical information item included in a plurality of medical information items is associated with a second medical information item, an icon indicating that a medical information item related to the first medical information item is present, on a display unit in conjunction with a thumbnail of the first medical information item, wherein the display control unit performs, when the icon is selected, such control as to display a display information item indicating a relationship between the first medical information item and the second medical information item, on the display unit.","1. An information processing apparatus comprising: at least one memory storing instructions; andat least one processor that when executing the instructions, causes the information processing apparatus to function as:a display control unit configured to display, in a case that it is determined whether a derived image generated on the basis of a first medical test image and a second medical test image which are included in a plurality of medical test images obtained by different medical tests, is or is not present, and it is determined that the derived image is present, an icon visually indicating that the derived image generated on the basis of the first medical test image and the second medical test image is present, on a display unit in conjunction with at least one of a thumbnail of the first medical test image and a thumbnail of the second medical test image,whereinthe display control unit performs, when the icon is selected, such control as to display a display information item indicating a relationship between the first medical test image and the second medical test image, on the display unit.","20","15/995729","2018-06-01","2018-0356958","2018-12-13","11036352","2021-06-15","CANON KABUSHIKI KAISHA","Toshimizu Yamane | Yoshio Iizuka | Gakuto Aoyama | Kazuhito Oka","2017-114689","JP","2017-06-09","G06F-0003/04817","G06F-0003/04817 | A61B-0005/743 | A61B-0005/7435 | G06F-0003/0482 | G06F-0003/0484 | G06F-0016/2453 | G06F-0016/30 | A61B-0005/0013","G06F-017/00","G06F-017/00 | G06F-003/0481 | G06F-003/0482 | G06F-003/0484 | A61B-005/00 | G06F-016/2453 | G06F-016/30","","","","","","4921025004093"
"US","US","P","B1","Method and apparatus for informed personal well-being decision making","An apparatus and method for informed personal-well-being decision making that provides a user with alerts and information, focused on health and wellness, on items they choose for possible consumption. Some embodiments include optical, sonic, smell and other sensors, communications with databases that identify ingredients and effects on health and well-being, as well as user inputs. From user input, GPS, local conditions and alerts, some embodiments determine information specific to the user and their environment. By using established, and creating new, databases, some embodiments compile, compare, transmit and store data on various consumables. Some embodiments provide access to information on the companies, manufacturers, and various other components in an item's trip from dirt to table. Some embodiments establish methods and procedures to ascertain both the point-of-origin and where the consumable has traveled. Some embodiments provide a score for the specified consumable to show the quality of health provided by the consumable.","1. A method comprising: providing a personal computing device (PCD) for a first human user, wherein the PCD includes a camera and a plurality of LEDs that are controllable to emit a plurality of successive flashes in quick succession including a first flash having a first spectrum and a successive second flash having a different second spectrum;obtaining from the camera, first image data corresponding to the first flash and second image data corresponding to the second flash;using the first and second image data from the camera, identifying, in the user'ss personal computing device (PCD), plurality of at least three selected from the group consisting of: a chosen consumable item, the chosen consumable item'ss components, the chosen consumable item'ss current age, and the chosen consumable item'ss condition;communicating, to a database server, the identifications of the plurality of at least three selected from the group consisting of: the chosen consumable item, the chosen consumable item'ss components, the chosen consumable item'ss current age, and the chosen consumable item'ss condition;identifying, in the database server, a plurality of the set consisting of ingredients, allergens, and toxins in the chosen consumable item based on the communicated identifications of the plurality of at least three selected from the group consisting of: the chosen consumable item, the chosen consumable item'ss components, the chosen consumable item'ss current age, and the chosen consumable item'ss condition;looking up, in the database server, a plurality of known detrimental, beneficial and health-quality effects on the first user'ss health for each of the plurality of the set consisting of ingredients, allergens, and toxins in the chosen consumable item;determining an effect on biological aging of the first user resulting from consumption of the chosen consumable item, wherein the effect is determined by starting with a score at a neutral point in a range of values, incrementing the score based on the looked-up beneficial effects of at least some of the chosen consumable item'ss ingredients, and decrementing the score based on the looked-up detrimental effects of at least some of the chosen consumable item'ss ingredients; andpresenting to the first user, from the first user'ss PCD, a comparison of a plurality of parameters of the chosen consumable item to at least one alternative consumable item based on the effect on the biological aging of the first user.","20","17/087581","2020-11-02","","","11037681","2021-06-15","Food2Life, LLC","Fazal Wala | Alexander B. Lemaire | Charles A. Lemaire","","","","G16H-0050/20","G16H-0050/20 | G06Q-0030/0627 | G16H-0010/60","G16H-050/20","G16H-050/20 | G06N-020/00 | G16H-010/60 | G16H-020/00 | G16H-040/67 | G16H-010/20 | A61B-005/145 | G06F-019/00 | G06Q-030/06","","","","","","4921025005410"
"US","US","P","B2","Patient support apparatus having data collection and communication capability","A system and method for collecting, communicating, displaying, and/or analyzing data from multiple medical devices is disclosed. The system includes a local data collection module and a number of medical device adapters. The medical device adapters are coupled to respective medical devices via hardwired connections to receive data from the respective medical devices. The medical device adapters wirelessly transmit the data to the local data collection module. The local data collection module communicates the data received from the medical device adapters to an Electronic Medical Records (EMR) system for automatic entry of at least some of the data in the electronic medical record of a patient associated with the medical devices.","1. A hospital bed for use in a healthcare facility having a network, the hospital bed comprising a frame to support a patient,a controller carried by the frame, the controller having a first transceiver and a second transceiver, the controller being adapted to wirelessly receive information via the first transceiver from a different hospital bed, the information relating to the different hospital bed, and the controller is adapted to forward the information off of the hospital bed via the second transceiver, andan expansion port coupled to the controller and having a bank of ports such that the expansion port is configured for hardwired connection to a plurality of medical devices for receipt of medical device data without involving the network of the healthcare facility, the controller also being adapted to forward the medical device data off of the hospital bed via the second transceiver, whereby the controller of the hospital bed operates as a hub for collection of the medical device data from the plurality of medical devices and the information from the different hospital bed, wherein the at least one expansion port is attached to an upper surface of a frame member of the frame at a head end of the hospital bed and is configured such that the bank of ports face upwardly and are situated between a pair of push handles at the head end of the hospital bed, wherein an underside of the expansion port includes one or more downwardly facing power outlets for receipt of one or more power plugs of at least one medical device of the plurality of medical devices, wherein the expansion port is generally centered on the frame member so that extending portions of the expansion port that extend beyond a front surface and a rear surface of the frame member are approximately equivalent in size.","20","15/672381","2017-08-09","2017-0372025","2017-12-28","11031130","2021-06-08","Hill-Rom Services, Inc.","Williams F. Collins, Jr. | Michael D. Gallup","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/002 | G16H-0010/60 | G16H-0040/20 | G16H-0040/63 | G16H-0080/00 | H04L-0067/12 | H04L-0067/125 | H04L-0069/18 | A61B-0005/021 | A61B-0005/0205 | A61B-2017/00221 | A61B-2034/256","G16H-040/67","G16H-040/67 | H04L-029/08 | H04L-029/06 | A61B-005/00 | G16H-010/60 | G16H-040/63 | G16H-040/20 | A61B-005/0205 | A61B-005/021 | A61B-017/00 | A61B-034/00 | G16H-080/00","","","","","","4921024005455"
"US","US","P","B2","Mount, authentication device, authentication method, and program","A wearable article includes: an annular casing that surrounds a space into which a body of a user is to be inserted; a light-emitting element that is provided in the casing, the light-emitting element emitting light towards the space; an imaging element that is provided in the casing, the imaging element capturing and obtaining an image of the space when the light-emitting element emits light; and an authentication circuit that authenticates the user based on a vein pattern obtained in advance and the image.","1. A wearable article comprising: an annular casing that surrounds a space into which a body part of a user is to be inserted;an imaging element that is provided in the annular casing, the imaging element configured to capture a first image of the space;an authentication circuit configured to authenticate the user based on a second image of biometric information previously stored in a storage and the first image of the space, in a case where there is a change from a first state in which the body part is not present in the space to a second state in which the body part is present in the space;a sensor that is provided in the annular casing and configured to output a sensor value in accordance with a positional relationship between the annular casing and the body part;a wearing determination circuit configured to determine whether the body part is present in the space, based on the sensor value; anda light source configured to emit light based on an authentication result of the authentication circuit to notify the user of the authentication result.","19","16/695730","2019-11-26","2020-0099681","2020-03-26","11032276","2021-06-08","NEC CORPORATION","Hiroshi Fukuda","2015-158540","JP","2015-08-10","H04L-0063/0861","H04L-0063/0861 | A61B-0005/0062 | A61B-0005/1171 | A61B-0005/6821 | A61B-0005/6826 | G06F-0001/163 | G06F-0021/32 | G06F-0021/86 | G06K-0009/00885 | G06K-0009/2027 | H04L-0063/0853 | A61B-2562/0257 | G06K-0009/00912 | G06K-0009/2018 | G06K-2009/00932","H04L-029/06","H04L-029/06 | G06F-021/32 | A61B-005/1171 | A61B-005/00 | G06F-001/16 | G06F-021/86 | G06K-009/00 | G06K-009/20","","","","","","4921024006591"
"US","US","P","B2","User authentication with acoustic fingerprinting","Methods, apparatus, and processor-readable storage media for user authentication with acoustic fingerprinting are provided herein. An example computer-implemented method includes generating, in response to an authentication request from a given device, an instruction for an acoustic output to be emitted and recorded by the given device; obtaining the recorded acoustic output from the given device; creating an acoustic fingerprint by applying one or more signal processing algorithms to the recorded acoustic output; processing the acoustic fingerprint and one or more items of information pertaining to the given device against historical authentication data; and resolving the authentication request in response to a determination that the acoustic fingerprint and the one or more items of information pertaining to the given device match at least a portion of the historical authentication data.","1. A computer-implemented method comprising: generating, in response to an authentication request from a given device, an instruction for an acoustic output to be emitted and recorded by the given device, wherein the acoustic output comprises one or more frequencies below approximately twenty hertz;obtaining the recorded acoustic output from the given device;creating an acoustic fingerprint by converting the recorded acoustic output into at least one frequency response curve using one or more fast Fourier transform algorithms, and attributing the at least one frequency response curve to the given device;processing the acoustic fingerprint and one or more items of information pertaining to the given device against historical authentication data; andresolving the authentication request in response to a determination that the acoustic fingerprint and the one or more items of information pertaining to the given device match at least a portion of the historical authentication data;wherein the method is performed by at least one processing device comprising a processor coupled to a memory.","20","16/259446","2019-01-28","2020-0242224","2020-07-30","11023570","2021-06-01","EMC IP HOLDING COMPANY LLC","Stas Khoroshevsky | Christina Tkachenko | Chen Gantz | Julia Petukhov | Rei Maoz | Liat Ben-Porat","","","","G06F-0021/32","G06F-0021/32 | G06F-0009/30003 | G06K-0009/0002 | G06K-0009/00087","G06F-021/30","G06F-021/30 | G06F-021/31 | A61B-005/00 | H04B-011/00 | G06F-021/32 | G06F-009/30 | G06K-009/00","","","","","","4921023004516"
"US","US","P","B2","Interactive biometric touch scanner","Aspects of this disclosure relate to a biometric sensing device that combines sensing with an actuator for two way communication between a finger on a surface and the device. The sensor can also function as an actuator. A finger can be authenticated based on an image of the finger generated by the sensor and also based on a response to energy delivered to the finger by the actuator. Two way communication can provide more robust authentication than fingerprint sensing alone.","1. A method of interactively authenticating a person, the method comprising: transmitting, by a fingerprint sensor, a signal to a finger of the person;generating an image of at least a portion of the finger based on a received signal associated with the signal transmitted to the finger;delivering energy to the finger;detecting a response to the energy delivered to the finger, wherein the response is indicative of whether the person is alive; andauthenticating the person based on (i) processing the image and (ii) separately processing an indication of the response to the energy delivered to the finger.","28","16/811547","2020-03-06","2020-0257874","2020-08-13","11023704","2021-06-01","ORCHID SOUND TECHNOLOGIES LLC | THE BOARD OF TRUSTEES OF THE LELAND STANFORD JUNIOR UNIVERSITY","Butrus T. Khuri-Yakub | Morten Fischer Rasmussen | Gerard Touma | John N. Irwin, III","","","","G06K-0009/0002","G06K-0009/0002 | A61B-0005/0261 | A61B-0005/02416 | A61B-0005/02438 | A61B-0005/0816 | A61B-0005/1172 | A61B-0005/14552 | G06F-0003/0436 | G06K-0009/0012 | G06K-0009/00107 | G06K-0009/00906 | H04M-0001/03 | A61B-0005/14551 | A61B-0005/489 | G06F-2203/04103 | G06K-0009/228 | G06K-2009/00939","G06K-009/00","G06K-009/00 | A61B-005/026 | A61B-005/024 | A61B-005/08 | A61B-005/1172 | G06F-003/043 | H04M-001/03 | A61B-005/1455 | A61B-005/00 | G06K-009/22","","","","","","4921023004648"
"US","US","P","B2","Worksite risk analysis and documentation system and method","A worksite risk analysis and documentation system and method defines, captures, categorizing, and documents, while analyzing functional physical demands of a plurality of jobs at various worksites, including environmental health and safety risks/opportunities as well as ergonomic risks/opportunities. The system and method prioritizes risk and body part injury based on individual injury costs associated with each body part used in various tasks at the worksite. The system and method creates a report which includes a catalog of engineered solutions and administrative control solutions to various issues and ergonomic risks/opportunities.","1. A system comprising: an Internet-connected computerized appliance having a processor and coupled to a data repository, the processor executing software from a non-transitory storage medium, the software providing an interactive interface to a worksite risk analysis and documentation system, the system enabling a user to:log on;upload video data from a camera or a mobile device;capture a plurality of images from the video data to document physical demands and ergonomic risk/opportunities corresponding to a plurality of tasks at a worksite;analyze each image of the plurality of images to each task of the plurality of tasks as either a physical demand or an ergonomic risk/opportunity;assign each image of plurality of images as either essential or non-essential functions of each task;utilize a database of definitions to define and standardize the essential and non-essential functions of each task;calculate DOT classification levels to produce a category of work level corresponding to each task for each image categorized as physical demands, wherein the category of work level includes at least one of sedentary work, light work, medium work, heavy work, and very heavy work;store the plurality of images in the non-transitory storage medium; andgenerate a final report, wherein the final report includes the plurality of images, the plurality of tasks, the calculated DOT classifications levels, and physical demand descriptions.","17","15/871810","2018-01-15","2018-0204155","2018-07-19","11023841","2021-06-01","Ergonomics International, LLC","Samuel Bradbury | Mark Heidebrecht","","","","G06Q-0010/0635","G06Q-0010/0635 | A61B-0005/11 | G06K-0009/00342 | G06K-0009/00711 | G06K-0009/20 | G06Q-0010/0631 | G06Q-0010/0637 | G06T-0007/20 | G16H-0050/30","G06K-009/00","G06K-009/00 | G06Q-010/06 | G06K-009/20 | G06T-007/20 | G16H-050/30 | A61B-005/11","","","","","","4921023004784"
"US","US","P","B2","Machine learning system for assessing heart valves and surrounding cardiovascular tracts","A machine learning system for evaluating at least one characteristic of a heart valve, an inflow tract, an outflow tract or a combination thereof may include a training mode and a production mode. The training mode may be configured to train a computer and construct a transformation function to predict an unknown anatomical characteristic and/or an unknown physiological characteristic of a heart valve, inflow tract and/or outflow tract, using a known anatomical characteristic and/or a known physiological characteristic the heart valve, inflow tract and/or outflow tract. The production mode may be configured to use the transformation function to predict the unknown anatomical characteristic and/or the unknown physiological characteristic of the heart valve, inflow tract and/or outflow tract, based on the known anatomical characteristic and/or the known physiological characteristic of the heart valve, inflow tract and/or outflow tract.","1. A computer-implemented machine learning method for evaluating at least one characteristic of a heart valve, an inflow tract, an outflow tract or a combination thereof the method comprising: (a) predicting, with a transformation function on a computer, an unknown anatomical characteristic of at least one of a training heart valve, a training inflow tract or a training outflow tract, using at least one of a training known anatomical characteristic or a training known physiological characteristic of the at least one training heart valve, training inflow tract or training outflow tract, wherein said at least one of said training heart valve, said training inflow tract, and said training outflow tract includes at least one of said training heart valve, a training coronary vessel, a training Valsalva sinus, a training sinotubular junction, and a training ascending aorta;(b) using a production mode of a machine learning system on the computer to direct the transformation function and one or more feature vectors, to predict unknown anatomical characteristic of at least one production heart valve, production inflow tract or production outflow tract, based on at least one of patient specific production known anatomical characteristic or production known physiological characteristic of at least one said production heart valve, said production inflow tract or said production outflow tract, wherein said at least one of said production heart valve, said production inflow tract, said production outflow tract includes at least one of said production heart valve, a production Valsalva sinus, a production sinotubular junction, and a production ascending aorta, to generate at least one or more quantities of interest.","55","16/050613","2018-07-31","2018-0336497","2018-11-22","11024426","2021-06-01","STENOMICS, INC.","Michael A. Singer","","","","G16H-0050/20","G16H-0050/20 | A61B-0005/7253 | A61B-0005/7267 | A61B-0005/7275 | A61B-0034/10 | G06F-0017/11 | G06N-0005/04 | G06N-0020/00 | G16H-0050/50","G16H-050/20","G16H-050/20 | A61B-005/00 | G06N-005/04 | G06N-020/00 | A61B-034/10 | G16H-050/50 | G06F-017/11","","","","","","4921023005364"
"US","US","P","B2","Electronic device and method for measuring biometric information","An electronic device and method for measuring biometric information are provided. The electronic device includes at least one light emitter; light receiver; and a processor. The processor is configured to emit light outside of the electronic device through the at least one light emitter, obtain, through the light receiver, light reflected by an external object among the emitted light, obtain a signal generated based on at least a portion of the reflected light and corresponding to the external object, output guide information related to a location of the external object when the signal satisfies a first designated condition, and obtain biometric information about the external object when the signal satisfies a second designated condition. When a finger of a user finger of the electronic device is not accurately located at a bio sensor, by guiding an accurate grip location, biometric information about the user can be accurately measured.","1. An electronic device comprising: at least one light emitter;a light receiver;a digitizer formed at a periphery of the at least one light emitter and the light receiver, the digitizer being configured to detect location information comprising a touch coordinate; anda processor operably connected to the at least one light emitter and the light receiver, wherein the processor is configured to: emit light outside of the electronic device through the at least one light emitter,obtain, through the light receiver, light reflected by an external object among the emitted light,obtain a signal generated based on at least a portion of the reflected light and corresponding to the external object through the digitizer and receive the detected location information from the digitizer,output guide information related to a location of the external object when the signal satisfies a first designated condition, andobtain biometric information about the external object when the signal satisfies a second designated condition,wherein the light receiver comprises a plurality of photodiodes, and the light receiver is configured with at least one of a form in which the photodiodes are integrated, a form in which the photodiodes are disposed parallel to a predetermined direction, or a form in which the photodiodes are disposed at each of an upper portion, a left side portion, a lower portion, and a right side portion based on a central hole of the light receiver.","15","15/937668","2018-03-27","2018-0276448","2018-09-27","11013435","2021-05-25","SAMSUNG ELECTRONICS CO., LTD.","Jihwan Kim | Sehoon Kim | Jungmo Kim | Jooman Han | Choonghee Ahn | Jeongyup Han | Taeho Kim | Jeongmin Park | Seungeun Lee","10-2017-0038756","KR","2017-03-27","A61B-0005/14551","A61B-0005/14551 | A61B-0005/684 | A61B-0005/6898 | A61B-0005/7221 | A61B-0005/7246 | A61B-0005/743 | G06F-0003/0488 | G06F-0003/167 | G06K-0009/0002 | G06K-0009/00067 | G06F-0003/044","A61B-005/1455","A61B-005/1455 | G06K-009/00 | G06F-003/0488 | G06F-003/16 | A61B-005/00 | G06F-003/044","","","","","","4921022001002"
"US","US","P","B2","System and method for providing smart communication device-based low level light therapy service","The present disclosure relates to a system and method for providing a smart communication device-based light therapy service in which a light therapy device is operated in conjunction with a smart communication device and an information providing server to perform a light therapy, and when a user purchases a lightceutical clinical code corresponding to a certain disease using the smart communication device, the information providing server generates the lightceutical clinical code of the disease containing a wavelength of light, an intensity of light, an irradiation time of light, and an irradiation pattern of light and sends the generated lightceutical clinical code to the smart communication device, and the smart communication device sends the received lightceutical clinical code to the light therapy device or generates a light therapy device control signal in accordance with the lightceutical clinical code and transmits the generated light therapy device control signal to the light therapy device to operate the light therapy device, thereby allowing customized therapy of various diseases or symptoms to be performed. The system for providing a smart communication device-based light therapy service includes a smart communication device through which a screen for purchasing a lightceutical clinical code including a wavelength of light, an intensity of light, an irradiation time of light, and an irradiation pattern of light is output and configured to receive a lightceutical clinical code from an information providing server and send the received lightceutical clinical code to a light therapy device or generate a light therapy device control signal in accordance with the lightceutical clinical code and send the generated light therapy device control signal to the light therapy device when a payment for a purchase of the lightceutical clinical code is completed in conjunction with a financial payment server, the information providing server configured to read the lightceutical clinical code from an information providing server DB and send the read lightceutical clinical code to the smart communication device when the payment for the lightceutical clinical code is completed by the smart communication device operating in conjunction with the financial payment server, and the light therapy device configured to operate in accordance with the lightceutical clinical code or the light therapy device control signal received from the smart communication device.","1. A system for providing a smart communication device-based light therapy service, the system comprising: a smart communication device through which a screen for purchasing a lightceutical clinical code including a wavelength of light, an intensity of light, an irradiation time of light, and an irradiation pattern of light is output and configured to receive a lightceutical clinical code from an information providing server and send the received lightceutical clinical code to a light therapy device or generate a light therapy device control signal in accordance with the lightceutical clinical code and send the generated light therapy device control signal to the light therapy device when a payment for a purchase of the lightceutical clinical code is completed in conjunction with a financial payment server;the information providing server configured to read the lightceutical clinical code from an information providing server database (DB) and send the read lightceutical clinical code to the smart communication device when the payment for the lightceutical clinical code is completed by the smart communication device operating in conjunction with the financial payment server; andthe light therapy device including a light source and configured to operate in accordance with the lightceutical clinical code or the light therapy device control signal received from the smart communication device to irradiate light by a light irradiating electrode;wherein a number of times that the lightceutical clinical code is usable is limited, and the number of times that the lightceutical clinical code is usable is stored in the information providing server DB and reduced in accordance with a number of times that the light therapy device is operated;wherein the lightceutical clinical code is different in accordance with a disease or symptom;wherein, before an operation of the light therapy device is started, the smart communication device is configured to display a self-diagnosis screen including questionnaire items related to a drug dose of a certain disease and send a response to the self-diagnosis questionnaire input by a user via the self-diagnosis screen to the information providing server, and the information providing server is configured to store the response to the self-diagnosis questionnaire in the information providing server DB;wherein, when the operation of the light therapy device is ended, the smart communication device is configured to display a therapy result input screen including questionnaire items related to a degree of pain and send a response to a therapy result questionnaire input by the user via the therapy result input screen to the information providing server, and the information providing server is configured to store the response to the therapy result questionnaire in the information providing server DB;wherein the smart communication device is configured to display a disease selection screen through which a name of a disease to be treated is to be selected before the light therapy device is operated, display a notification that a lightceutical clinical code of a selected disease name is not purchased and a pop-up window that asks whether to purchase the lightceutical clinical code of the selected disease name when the lightceutical clinical code of the disease name selected via the disease selection screen has not already been purchased, and output a purchase screen when a switch indicating a decision to purchase the lightceutical clinical code is selected in the pop-up window;wherein the number of times that the lightceutical clinical code is usable is displayed on the smart communication device;wherein, when the number of times that the lightceutical clinical code is usable is exhausted, the smart communication device is configured to display a notification that the number of times that the lightceutical clinical code is usable has been exhausted and a pop-up window that asks whether to purchase the number of times that the lightceutical clinical code is usable and output a purchase screen when a switch indicating a decision to purchase the number of times that the lightceutical clinical code is usable is selected in the pop-up window to allow a payment for an additional purchase of the number of times that the lightceutical clinical code is usable; andwherein the smart communication device is configured to operate the light therapy device through a therapy screen and, upon receiving a lightceutical clinical code, configured to send a changed lightceutical clinical code to the light therapy device or generate a light therapy device control signal in accordance with the changed lightceutical clinical code and send the generated light therapy device control signal to the light therapy device.","17","15/736864","2017-05-10","2018-0154167","2018-06-07","11013932","2021-05-25","COLOR SEVEN CO., LTD. | KIM, NAM GYUN | PARK, KYOUNG JUN | AN, HEA JA | OH, HAN YEONG","Nam Gyun Kim | Kyoung Jun Park | Hea Ja An | Han Yeong Oh","10-2016-0057680 | 10-2017-0056954","KR | KR","2016-05-11 | 2017-05-04","A61N-0005/06","A61N-0005/06 | G06F-0009/44 | G06Q-0020/127 | G06Q-0020/145 | G06Q-0020/325 | G06Q-0030/02 | G06Q-0030/06 | G06Q-0050/22 | G07F-0017/0014 | G07F-0017/18 | G16H-0010/20 | G16H-0020/70 | G16H-0040/63 | H04L-0009/32 | A61N-2005/0626 | A61N-2005/0645 | A61N-2005/0647 | A61N-2005/0651 | A61N-2005/0653","A61F-009/008","A61F-009/008 | A61N-005/06 | G06Q-020/32 | G16H-010/20 | G16H-040/63 | G16H-020/70 | G06Q-020/14 | G06Q-030/06 | G06Q-050/22 | H04L-009/32 | G07F-017/00 | G07F-017/18 | G06F-009/44 | G06Q-020/12 | G06Q-030/02","","","","","","4921022001493"
"US","US","P","B2","Millimeter-wave-radar-based electromagnetic apparatus","A wearable electromagnetic, EM, apparatus includes: at least one antenna operable in a millimeter-wave-radar-based, MWRB, application; at least one computer processor disposed in signal communication with the at least one antenna; an attachment system configured and adapted to attach to an actor; the at least one antenna and the at least one computer processor disposed in a supported relationship with the attachment system, such that the attachment system with the supported at least one antenna and the at least one computer processor at least partially forms a wearable apparatus that is wearable by the actor.","1. An electromagnetic, EM, apparatus that is wearable, comprising: at least one antenna operable in a millimeter-wave-radar-based, MWRB, application;at least one computer processor disposed in signal communication with the at least one antenna;an attachment system configured and adapted to attach to a particular actor;the at least one antenna and the at least one computer processor disposed in a supported relationship with the attachment system, such that the attachment system with the supported at least one antenna and the at least one computer processor at least partially forms the wearable apparatus that is wearable by the actor;wherein the at least one antenna is at least one dielectric resonator antenna, DRA;wherein the DRA is a first dielectric portion, 1DP, having a proximal end and a distal end, wherein the proximal end of the 1DP is disposed on an electrically conductive ground structure;wherein the EM apparatus further comprises a second dielectric portion, 2DP, having a proximal end and a distal end, the proximal end of the 2DP being disposed in direct contact with the distal end of the 1DP to form a dielectric structure, the 2DP comprising a dielectric material other than air; andwherein the dielectric material of the 1DP has an average dielectric constant that is greater than the average dielectric constant of the dielectric material of the 2DP.","41","16/663443","2019-10-25","2020-0133398","2020-04-30","11016574","2021-05-25","ROGERS CORPORATION","Shawn P. Williams | Gianni Taraschi | Sara G. Canzano | Kristi Pance | Christopher Brown | Karl E. Sprentall | Roshin Rose George","","","","G06F-0003/017","G06F-0003/017 | A61B-0005/681 | G06F-0001/163 | G06F-0003/011 | G06K-0009/00335 | H01Q-0001/273 | H01Q-0009/0485","G06F-003/048","G06F-003/048 | G06F-003/01 | G06K-009/00 | H01Q-009/04 | A61B-005/00 | G06F-001/16 | H01Q-001/27","","","","","","4921022004114"
"US","US","P","B2","Primary preview region and gaze based driver distraction detection","A computer-implemented method of detecting distracted driving comprises: determining, by one or more processors, a primary preview region (PPR) in a representation of an environment; determining, by the one or more processors, a gaze point for a driver based on a sequence of images of the driver; determining, by the one or more processors, that the gaze point is outside of the PPR; based on the determined gaze point being outside of the PPR, decreasing, by the one or more processors, an attention level for the PPR; based on the attention level for the PPR, generating, by the one or more processors, an alert.","1. A computer-implemented method of detecting distracted driving comprising: determining, by one or more processors, a primary preview region (PPR) in a representation of an environment;determining, by the one or more processors, a first gaze point for a driver based on a first sequence of images of the driver;determining, by the one or more processors, that the first gaze point is inside of the PPR;based on the first gaze point being inside of the PPR and a reaction coefficient of the driver, increasing, by the one or more processors, an attention level for the PPR using a first function;determining, by the one or more processors, a second gaze point based on a second sequence of images of the driver;determining, by the one or more processors, that the second gaze point is outside of the PPR;based on the second gaze point being outside of the PPR and the reaction coefficient of the driver, decreasing, by the one or more processors, the attention level for the PPR using a second function different from the first function; andbased on the decreased attention level for the PPR, generating, by the one or more processors, an alert.","19","15/882581","2018-01-29","2019-0236386","2019-08-01","11017249","2021-05-25","FUTUREWEI TECHNOLOGIES, INC.","Hai Yu | Fatih Porikli | Yuzhu Wu","","","","G06K-0009/00845","G06K-0009/00845 | A61B-0005/0077 | A61B-0005/18 | A61B-0005/746 | B60W-0010/18 | B60W-0010/20 | B60W-0030/09 | B60W-0050/16 | G06F-0003/013 | G06K-0009/66 | H04N-0005/33 | B60W-2050/143 | B60W-2540/00","G06K-009/00","G06K-009/00 | A61B-005/00 | A61B-005/18 | B60W-010/18 | B60W-010/20 | B60W-030/09 | B60W-050/16 | G06F-003/01 | G06K-009/66 | H04N-005/33 | B60W-050/14","","","","","","4921022004783"
"US","US","P","B2","Intelligent assistant with intent-based information resolution","A method for use with a computing device is provided. The method may include executing one or more programs of an intelligent digital assistant system at a processor and presenting a user interface to a user. At the processor, the method may include receiving natural language user input from the user, parsing the user input at an intent handler to determine an intent template with slots, populating the slots in the intent template with information from user input, and performing resolution on the intent template to partially resolve unresolved information. If a slot with missing slot information exists in the partially resolved intent template, a loop may be executed at the processor to fill the slots. The method may include, at the processor, determining that all required information is available and resolved and generating a rule based upon the intent template with all required information being available and resolved.","1. A method executed by a computing system of one or more computing devices, the method comprising: receiving user input via an interface of the computing device, the user input including natural language user input;performing the following actions in a loop with respect to a first intent template defining a set of slots until the set of slots are both filled and resolved, wherein the first intent template is selected from among a plurality of candidate intent templates: populating one or more slots of the set of slots of the first intent template with information based on the user input,if the first intent template is partially resolved in which a subject slot of the set of slots is not both filled and resolved, performing the following additional actions as part of the loop: determining a state of the subject slot as at least one of unfilled or unresolved,presenting a query for a user to fill or resolve the subject slot based on query selection criteria,receiving a user response to the query,altering the state of the subject slot based on the user response to the query, andre-executing the loop with the user response to the query being incorporated into the user input, wherein an iteration of the loop during re-execution of the loop includes: determining that a second intent template of the plurality candidate intent templates is a closer match to the user input and the user response to the query, based on slots filled and resolved within the loop; andresponsive to determining that the second intent template is the closer match, exiting the loop prior to the subject slot of the set of slots being both filled and resolved;populating one or more slots defined by the second intent template based on the user input and the user response to the query; andperforming an action that is based on the second intent template following populating the one or more slots of the second intent template.","18","16/700308","2019-12-02","2020-0104653","2020-04-02","11017765","2021-05-25","MICROSOFT TECHNOLOGY LICENSING, LLC","Oz Solomon | Christopher Brian Quirk | Han Yee Mimi Fung | Keith Coleman Herold","","","","G10L-0015/1822","G10L-0015/1822 | A61B-0005/0205 | A61B-0005/0507 | A61B-0005/117 | A61B-0005/1113 | A61B-0005/7475 | G01S-0005/18 | G01S-0005/28 | G06F-0001/324 | G06F-0001/3206 | G06F-0001/3231 | G06F-0003/011 | G06F-0003/017 | G06F-0003/0304 | G06F-0003/0482 | G06F-0003/04842 | G06F-0003/167 | G06F-0021/32 | G06F-0021/35 | G06F-0040/211 | G06F-0040/35 | G06K-0009/00 | G06K-0009/00214 | G06K-0009/00255 | G06K-0009/00261 | G06K-0009/00288 | G06K-0009/00295 | G06K-0009/00342 | G06K-0009/00362 | G06K-0009/00711 | G06K-0009/00771 | G06K-0009/00973 | G06K-0009/6254 | G06K-0009/6255 | G06K-0009/6289 | G06K-0009/6296 | G06K-0009/726 | G06N-0005/025 | G06N-0005/047 | G06N-0020/00 | G06T-0007/248 | G06T-0007/292 | G06T-0007/60 | G06T-0007/70 | G06T-0007/74 | G07C-0009/28 | G08B-0013/1427 | G10L-0015/02 | G10L-0015/063 | G10L-0015/08 | G10L-0015/18 | G10L-0015/1815 | G10L-0015/19 | G10L-0015/22 | G10L-0015/24 | G10L-0015/26 | G10L-0015/28 | G10L-0015/32 | G10L-0017/04 | G10L-0017/08 | G10L-0025/51 | H04L-0051/02 | H04L-0063/102 | H04L-0067/12 | H04L-0067/22 | H04N-0005/23219 | H04N-0005/332 | H04N-0007/181 | H04N-0007/188 | H04N-0021/231 | H04N-0021/42203 | H04N-0021/44218 | H04N-0021/44222 | H04R-0001/406 | H04R-0003/005 | H04W-0004/029 | H04W-0004/33 | A61B-0005/05 | A61B-0005/1118 | G01S-0005/16 | G01S-0013/38 | G01S-0013/867 | G01S-0013/888 | G06F-0003/0488 | G06F-0016/70 | G06F-2203/0381 | G06F-2221/2111 | G06K-2209/09 | G06N-0003/0445 | G06T-2207/10016 | G06T-2207/10024 | G06T-2207/20101 | G06T-2207/30196 | G06T-2207/30201 | G06T-2207/30204 | G06T-2207/30232 | G07C-0009/32 | G08B-0029/186 | G10L-0017/00 | G10L-2015/0635 | G10L-2015/088 | G10L-2015/223 | G10L-2015/225 | G10L-2015/228 | H04N-0005/247 | Y02D-0010/00","G10L-015/18","G10L-015/18 | G06K-009/72 | G06K-009/00 | G06T-007/70 | G06K-009/62 | G06F-003/16 | G06N-020/00 | G06T-007/292 | H04W-004/33 | H04W-004/029 | A61B-005/11 | A61B-005/117 | A61B-005/00 | G01S-005/28 | G06F-001/3206 | G06F-001/3231 | G06F-001/324 | G06F-003/01 | G06F-003/03 | G06F-021/32 | G10L-017/04 | G10L-017/08 | H04L-012/58 | H04L-029/08 | H04N-005/232 | H04N-007/18 | H04N-021/422 | H04N-021/442 | G07C-009/28 | G06F-040/35 | G06F-040/211 | G06T-007/73 | G06T-007/246 | G01S-005/18 | G06T-007/60 | G10L-015/22 | G10L-015/28 | H04R-001/40 | H04R-003/00 | H04N-005/33 | G10L-015/02 | G06N-005/02 | G06N-005/04 | G10L-015/06 | G10L-015/24 | G10L-015/26 | G10L-015/19 | G10L-015/08 | G10L-015/32 | G10L-025/51 | H04L-029/06 | A61B-005/0205 | A61B-005/0507 | G06F-021/35 | G08B-013/14 | G06F-003/0482 | G06F-003/0484 | H04N-021/231 | G06F-003/0488 | G06F-016/70 | A61B-005/05 | G01S-005/16 | G01S-013/86 | G06N-003/04 | G08B-029/18 | G10L-017/00 | G07C-009/32 | H04N-005/247 | G01S-013/38 | G01S-013/88","","","","","","4921022005297"
"US","US","P","B2","Information processing device and information processing method","An assessment model for enabling a subjective assessment value to be estimated from a brainwave feature amount is constructed, and assessment data which does not deviate from a subjective assessment result on the basis of an objective brainwave signal by using the assessment model can be acquired. An assessment model representing relevance between a brainwave feature amount of a subject and a subjective assessment value of the subject with respect to a stimulus is constructed by presenting the stimulus to the subject. For example, images with different image qualities and a standard image are alternately displayed on a display unit which is a stimulus presentation unit, brainwave feature amount corresponding to an image quality of a subject observing the displayed images and subjective assessment values corresponding to image qualities are acquired, and an image quality assessment model for enabling subjective assessment values to be estimated from the brainwave feature amounts is constructed by machine learning in which the brainwave feature amounts and the subjective assessment values are used as input data.","1. An information processing device, comprising: central processing unit (CPU) configured to: measure a brainwave of a subject to which a stimulus is presented;calculate a brainwave feature amount based on the brainwave of the subject;acquire a subjective assessment value with respect to the stimulus of the subject; andconstruct an assessment model through machine learning based on input data as learning data, wherein the brainwave feature amount and the subjective assessment value are the input data, andthe assessment model represents relevance between the brainwave feature amount and the subjective assessment value.","18","16/337478","2017-10-23","2019-0254525","2019-08-22","11006834","2021-05-18","SONY CORPORATION","Shuichi Takahashi","2016-217804","JP","2016-11-08","A61B-0005/0042","A61B-0005/0042 | A61B-0005/378 | A61B-0005/4064 | A61B-0005/7267 | G06F-0003/01 | G06F-0016/00 | G06N-0020/00 | G06T-0005/009 | G06T-0005/20 | G06T-0007/0012","A61B-005/00","A61B-005/00 | G06N-020/00 | G06T-005/00 | G06T-005/20 | G06T-007/00 | G06F-016/00 | G06F-003/01 | A61B-005/378","","","","","","4921021001003"
"US","US","P","B1","System and method for identifying and attenuating mental health deterioration","A method for determining a state of mind of a user may include receiving one or more strings of characters composed by the user, and determining, by a processing device, the state of mind of the user by processing the one or more strings of characters. The processing of the one or more strings of characters may include identifying similarities of the one or more strings of characters with other strings of characters indicative of the state of mind. The method may also include determining, based on the one or more strings of characters, a severity of the state of mind of the user.","1. A method for determining a state of mind of a user, comprising: receiving one or more strings of characters composed by the user;determining, by a processing device executing a machine learning model, the state of mind of the user by processing the one or more strings of characters, wherein the processing of the one or more strings of characters comprises identifying similarities of the one or more strings of characters with other strings of characters indicative of the state of mind, and the machine learning model is trained to determine the state of mind of the user using input data comprising:(i) the other strings of characters composed by other users, and(ii) feedback entered by the other users via user interfaces presented on computing devices of the other users, and the feedback comprises: an indication of current states of minds of the other users at the time at which the other users composed the other strings of characters, andan indication of severities of the current states of minds of the other users at the time at which the other users composed the other strings of characters; anddetermining, based on the one or more strings of characters, a severity of the state of mind of the user.","17","16/700344","2019-12-02","2021-0161450","2021-06-03","11006877","2021-05-18","NAVIGATE LABS, LLC","Joseph Scanlin","","","","A61B-0005/165","A61B-0005/165 | G06F-0003/01 | G06N-0020/00 | G10L-0025/63 | H04W-0004/90 | G06F-2203/011","A61B-005/16","A61B-005/16 | G06N-020/00 | G06F-003/01 | H04W-004/90 | G10L-025/63","","","","","","4921021001046"
"US","US","P","B2","Particle filtering for continuous tracking and correction of body joint positions","Skeletal recording devices (e.g., Microsoft Kinect®) has been gaining popularity in home-based rehabilitation solution due to its affordability and ease of use. It is used as a marker less human skeleton tracking device. However, apart from the fact that the skeleton data are contaminated with high frequency noise, the major drawback lies in the inability to retain the anthropometric properties, for example, the body segments' length, which varies with time during the tracking. Embodiments of the present disclosure provide systems that implement a particle filter based approach to track the human skeleton data in presence of high frequency noise and multi-objective genetic technique is further implemented to reduce the bone length variations. Further multiple segments in skeleton are filtered simultaneously and segments' lengths are preserved by considering their interconnection for obtained corrected set of body joint positions which ensures that the body segment length is maintained close to ground truth.","1. A processor implemented method, comprising: receiving, in a coordinate system, an input data comprising a plurality of body joint positions pertaining to a subject, each of the plurality of body joint positions comprising a plurality of joint coordinates, wherein the plurality of body joint positions are captured by an infrared sensing device; anddenoising for each joint, by using a dynamic filtering based technique, the plurality of body joint positions comprising the plurality of joint coordinates to obtain a corrected set of body joint positions pertaining to the subject, wherein the step of denoising the plurality of body joint positions comprises:computing one or more actual body length segments using a plurality of physically connected joints identified from the plurality of body joint positions;forming a state vector based on the plurality of physically connected joints identified from the plurality of body joint positions;tracking at every time instance ‘t’, using the dynamic filtering based technique, a current position of each of the plurality of physically connected joints through an estimation of the formed state vector based on each of the plurality of body joint positions obtained from the input data;generating a plurality of particles associated with the estimation of the formed state vector;updating weight of each of the plurality of particles during transition of each of the plurality of particles from one state to another state;computing a first cost function based on the updated weight associated with each of the plurality of particles;computing a second cost function based on (i) one or more body length segments derived from the plurality of particles and (ii) the one or more actual body length segments;concurrently optimizing, using an optimizing technique, the first cost function and the second cost function to obtain a ranked list of optimized particles based on the plurality of particles; andadjusting the updated weight associated with each of the plurality of particles based on the ranked list of optimized particles to obtain a final state vector comprising the corrected set of corrected body joint positions pertaining to the subject.","18","16/523491","2019-07-26","2020-0054276","2020-02-20","11000222","2021-05-11","TATA CONSULTANCY SERVICES LIMITED","Kingshuk Chakravarty | Aniruddha Sinha | Soumya Rajan Tripathy","201821032408","IN","2018-08-29","A61B-0005/4528","A61B-0005/4528 | A61B-0005/1121 | G06T-0007/30 | G06T-0007/60 | G06F-0003/011 | G06K-0009/00369 | G06T-2207/10028","A61B-005/00","A61B-005/00 | G06T-007/30 | A61B-005/11 | G06T-007/60 | G06F-003/01 | G06K-009/00","","","","","","4921020000992"
"US","US","P","B2","Decoding from brain imaging data of individual subjects by using additional imaging data from other subjects","A computer-implemented method for decoding brain imaging data of individual subjects by using additional imaging data from other subjects includes receiving a plurality of functional Magnetic Resonance Imaging (fMRI) datasets corresponding to a plurality of subjects. Each fMRI dataset corresponds to a distinct subject and comprises brain activation patterns resulting from presentation of a plurality of stimuli to the distinct subject. A group dimensionality reduction (GDR) technique is applied to the example fMRI datasets to yield a low-dimensional space of response variables shared by the plurality of subjects. A model is trained to predict a set of target variables based on the low-dimensional space of response variables shared by all subjects, wherein the set of target variables comprise one or more characteristics of the plurality of stimuli.","1. A system for decoding brain imaging data of individual subjects by using additional imaging data from other subjects, the system comprising: a functional Magnetic Resonance Imaging (fMRI) scanner configured to acquire an fMRI dataset corresponding to a subject, wherein the fMRI dataset comprises brain activation patterns resulting from presentation of a plurality of stimuli to the subject;one or more processors configured to: apply a group dimensionality reduction (G.D.R.) technique to the fMRI dataset to transform it into a low-dimensional space of response variables shared by a plurality of subjects, andapply a machine learning model to the transformed fMRI dataset to predict one or more target variables comprising one or more semantic vectors describing the plurality of stimuli,wherein at least a portion of the fMRI dataset comprises a synthetic dataset, and the one or more processors are further configured to:generate the synthetic dataset using a GAN framework comprising a generator and a discriminator connected using a cyclic consistency constraint that minimizes difference between the plurality of stimuli and generated stimuli produced by the generator.","11","15/793137","2017-10-25","2019-0120918","2019-04-25","11002814","2021-05-11","SIEMENS MEDICAL SOLUTIONS USA, INC.","Francisco Pereira | Ahmet Tuysuzoglu | Bin Lou | Tommaso Mansi | Dorin Comaniciu","","","","G01R-0033/4806","G01R-0033/4806 | A61B-0005/0042 | A61B-0005/055 | A61B-0005/7267 | A61B-0005/0022 | A61B-0005/4064 | A61B-0005/7246 | A61B-0005/7264 | A61B-2576/026 | G06F-0003/015 | G06K-0009/6247 | G06K-0009/6269 | G06K-2209/05 | G06T-0007/0012 | G06T-2207/20081 | G16H-0050/70","G01R-033/48","G01R-033/48 | A61B-005/055 | A61B-005/00 | G06F-003/01 | G06T-007/00 | G16H-050/70 | G06K-009/62","","","","","","4921020003560"
"US","US","P","B2","System and method for automatically triggering the communication of sensitive information through a vehicle to a third party","A vehicle is provided that determines a first triggering event in predetermined triggering event information has occurred, in response to a first event, automatically forwards a first communication to a selected third party vendor to preorder a product or service of the selected third party vendor in connection with a transaction with the user, determines a second triggering event in the triggering event information has occurred, and in response to the later second event, automatically sends an authorization to complete the transaction by providing financial information to the selected third party vendor.","1. A vehicle, comprising: a memory to store triggering event information applicable to communications associated with a user in the vehicle;a processor in communication with the memory, the processor being programmed to: determine, at a first time, that a first triggering event in the triggering event information has occurred;establish, automatically in response to determining that the first triggering event has occurred, a first communication connection with a selected third party vendor of a plurality of third party vendors;separate a token comprising an entirety of transaction information required for the selected third party vendor to conduct a transaction activity with the user into a first half of the token and a second half of the token, the first half of the token comprising a preorder defining a product or service to authorize and a first portion of payment information for the transaction activity, the second half of the token comprising a second portion of the payment information required to complete the payment information and allow the selected third party vendor to complete the transaction activity;send, via the first communication connection, a preorder communication comprising the first half of the token to the selected third party vendor to preorder the product or service of the selected third party vendor in connection with the transaction activity, wherein the first half of the token is sent without the second half of the token in the preorder communication;determine, at a later second time, that a second triggering event in the triggering event information has occurred;establish, automatically in response to determining that the second triggering event has occurred, a second communication connection with the selected third party vendor;receive, from the selected third party vendor via the second communication connection, an identity and order confirmation request to verify an identity of at least one of the vehicle and the user;send, via the second communication connection, an acknowledgement signal to the selected third party vendor that verifies that the identity of the at least one of the vehicle and the user;receive, via the second communication connection, a confirmation signal from the selected third party vendor that the transaction activity is confirmed by the selected third party vendor; andsend, via the second communication connection after the confirmation signal is received, an authorization communication comprising the second half of the token that completes the token and the transaction activity by providing the second portion of the payment information to the selected third party vendor.","20","15/396613","2016-12-31","2018-0012426","2018-01-11","11005657","2021-05-11","NIO USA, INC.","Christopher P. Ricci","","","","H04L-0009/321","H04L-0009/321 | A61B-0005/1171 | A61B-0005/1172 | A61B-0005/1176 | B60L-0053/00 | B60L-0053/65 | B60L-0053/665 | B60L-0053/80 | B60R-0001/00 | B60R-0011/04 | B60R-0025/102 | B60R-0025/2081 | B60W-0040/08 | B60W-0050/08 | G01C-0021/36 | G01C-0021/3617 | G01C-0021/3697 | G05B-0015/02 | G05D-0001/0011 | G05D-0001/0088 | G06F-0003/011 | G06F-0016/248 | G06F-0016/951 | G06F-0021/31 | G06F-0021/32 | G06F-0021/6245 | G06K-0007/10257 | G06K-0007/10316 | G06K-0007/10425 | G06K-0009/00087 | G06K-0009/00832 | G06K-0009/00845 | G06K-0019/0708 | G06Q-0010/20 | G06Q-0020/105 | G06Q-0020/108 | G06Q-0020/14 | G06Q-0020/32 | G06Q-0020/3224 | G06Q-0020/401 | G06Q-0020/405 | G06Q-0020/4012 | G06Q-0030/012 | G06Q-0030/0206 | G06Q-0030/0208 | G06Q-0030/0601 | G06Q-0030/0609 | G06Q-0030/0613 | G06Q-0030/0625 | G06Q-0030/0635 | G06Q-0030/0637 | G07B-0015/063 | G07C-0005/008 | G07C-0005/02 | G07C-0005/0808 | G07C-0005/0816 | G07C-0005/0858 | G07C-0009/00563 | G08G-0001/017 | G08G-0001/0962 | G08G-0001/09626 | G08G-0001/096775 | G08G-0001/096827 | G08G-0001/096838 | G08G-0001/20 | H01Q-0001/325 | H01Q-0001/3266 | H01Q-0001/3275 | H01Q-0001/3283 | H01Q-0001/3291 | H01Q-0021/30 | H02J-0007/0068 | H04B-0005/0037 | H04L-0009/3226 | H04L-0063/0428 | H04W-0004/40 | H04W-0004/44 | H04W-0004/46 | H04W-0004/80 | H04W-0012/001 | H04W-0012/02 | H04W-0012/04 | H04W-0012/06 | A61B-2503/22 | B60K-0006/20 | B60K-0035/00 | B60K-2370/1537 | B60K-2370/334 | B60L-0005/24 | B60L-0007/10 | B60L-0008/003 | B60L-0008/006 | B60L-0009/00 | B60L-0053/12 | B60L-0053/14 | B60L-2240/549 | B60L-2240/70 | B60L-2240/72 | B60L-2270/32 | B60M-0001/00 | B60M-0007/00 | B60R-2011/0003 | B60R-2011/004 | B60R-2300/30 | B60R-2300/804 | B60R-2325/105 | B60W-2040/0809 | B60W-2050/143 | B60W-2050/146 | B60W-2300/34 | B60W-2540/00 | B60W-2540/21 | B60W-2540/215 | B60Y-2200/91 | B60Y-2200/912 | B60Y-2200/92 | B60Y-2300/60 | B60Y-2302/07 | B60Y-2400/92 | G01S-2013/9316 | G06K-0009/00288 | G06K-0009/00885 | G08G-0001/16 | H04L-2209/80 | H04L-2209/805 | H04L-2209/84 | Y02T-0010/70 | Y02T-0010/7072 | Y02T-0010/72 | Y02T-0090/12 | Y02T-0090/14 | Y02T-0090/16 | Y02T-0090/167 | Y04S-0030/14","H04L-009/32","H04L-009/32 | H04W-004/44 | G01C-021/36 | G08G-001/0962 | G08G-001/0967 | G08G-001/0968 | H01Q-001/32 | H04W-012/06 | G06F-021/31 | G06Q-020/10 | G06Q-020/14 | G06Q-020/32 | G06Q-020/40 | B60L-053/80 | B60L-053/65 | B60L-053/66 | G06Q-030/06 | H04W-012/00 | G06F-016/951 | G06F-016/248 | B60R-011/04 | B60R-025/102 | B60R-025/20 | H04W-012/04 | H04W-004/46 | H04W-004/40 | B60R-001/00 | G05B-015/02 | G06K-007/10 | G06K-019/07 | H04L-029/06 | B60L-053/00 | G06Q-010/00 | G06Q-030/02 | G07C-005/00 | H04W-004/80 | G06Q-030/00 | G07C-005/08 | A61B-005/1171 | A61B-005/1172 | G07C-005/02 | G07C-009/00 | G06F-003/01 | G06K-009/00 | H02J-007/00 | H01Q-021/30 | H04B-005/00 | B60W-040/08 | B60W-050/08 | G08G-001/017 | G08G-001/00 | G07B-015/06 | G06F-021/62 | H04W-012/02 | G05D-001/00 | G06F-021/32 | B60L-005/24 | B60M-007/00 | G08G-001/16 | B60L-007/10 | B60L-008/00 | B60L-009/00 | B60L-053/14 | B60L-053/12 | G01S-013/931 | B60K-006/20 | B60K-035/00 | B60R-011/00 | B60W-050/14 | B60M-001/00","","","","","","4921020006373"
"US","US","P","B2","Operating apparatus for medical apparatus","There is provided an operating apparatus for a medical apparatus, the operating apparatus including a ring-shaped magnet magnetically polarized in a circumferential direction or a radial direction and configured to rotate along with rotating operation by a user, and a sensor unit configured to detect a magnetic field and to output a signal depending on the detected magnetic field. The sensor unit outputs two-phase signals having different phases.","1. An operational apparatus for a medical device, the operational apparatus comprising: a ring-shaped magnet polarized in a predetermined direction, the ring-shaped magnet to rotate in response to manual manipulation by a user;at least one sensor to detect a magnetic field and to output a signal based on the detected magnetic field, the at least one sensor being arranged in an airtight and waterproof shielded area, wherein the at least one sensor is entirely encircled by the ring-shaped magnet;a partition that forms the airtight and waterproof shielded area and separates the ring-shaped magnet from the at least one sensor, but does not shield a magnetic field, wherein the partition between the ring-shaped magnet and the at least one sensor is a non-ferromagnetic material; andan operating ring that covers the ring-shaped magnet.","23","14/893344","2015-01-14","2016-0128607","2016-05-12","10993637","2021-05-04","SONY OLYMPUS MEDICAL SOLUTIONS INC.","Naoyuki Ohno","2014-072113","JP","2014-03-31","A61B-0005/062","A61B-0005/062 | A61B-0001/00006 | A61B-0001/00009 | A61B-0001/00039 | A61B-0001/00066 | A61B-0001/00105 | A61B-0001/00124 | A61B-0001/00158 | A61B-0001/00188 | A61B-0001/045 | A61B-0005/05 | A61B-0005/7475 | A61B-0017/00234 | G01D-0005/2451 | G06F-0003/0362 | A61B-2562/0223","A61B-005/06","A61B-005/06 | G01D-005/245 | A61B-001/00 | G06F-003/0362 | A61B-001/045 | A61B-005/05 | A61B-005/00 | A61B-017/00","","","","","","4921019001003"
"US","US","P","B2","Method, system, and apparatus for remotely controlling and monitoring an electronic device","In one embodiment, a system, method, and apparatus are provided which enable remote monitoring and control of a patient's wearable medical device. The wearable medical device may collect bodily data that characterizes their condition and transmits such data to an electronic device viewable by a remote party such as a physician, close family member or friend, or other party communicatively linked in a treatment network. Considering the transmitted bodily data relative to predetermined threshold conditions as a guide, the remote party may cause the wearable electronic device to dispense therapeutic treatments into the patient's body in an effort to immediately facilitate treatment of the patient's condition regardless of geographic proximity. The wearable medical device may be also configured to automatically transmit alerts to electronic devices of remote parties, triggering treatment and other forms of aid in the event of an emergency.","1. A non-transitory, tangible computer-readable medium having stored thereon computer-executable instructions, which, when executed by a computer processor of an electronic wearable medical device of a patient of a private treatment network, the electronic wearable medical device comprising an infusion pump, enable performance of a method comprising: collecting bodily data characterizing a patient'ss condition at the electronic wearable medical device of the patient, wherein the collecting of the bodily data includes monitoring blood glucose levels of the patient over time;periodically transmitting, from the electronic wearable medical device, the bodily data characterizing the patient'ss condition to an electronic device controlled by a remote party of the private treatment network, the private treatment network comprising a communication network connecting the electronic wearable medical device of the patient and the electronic device controlled by the remote party;transmitting, from the electronic wearable medical device, a request for remote treatment of the patient to the electronic device controlled by the remote party, wherein the request is automatically generated by the computer processor responsive to determining that one of the monitored blood glucose levels of the patient falls below or exceeds an acceptable range of blood glucose levels for the patient from a predetermined threshold value;responsive to receipt of the request for remote treatment of the patient at the electronic device controlled by the remote party, receiving control instructions from the electronic device controlled by the remote party at the electronic wearable medical device to automatically start delivering one of glucose or insulin into the patient'ss body, or to automatically stop delivering one of glucose or insulin into the patient'ss body, depending on whether the monitored blood glucose level of the patient falls below or exceeds the acceptable range of blood glucose levels for the patient from the predetermined threshold value;controlling the infusion pump to start delivering one of glucose or insulin into the patient'ss body, or to stop delivering one of glucose or insulin into the patient'ss body, based on the control instructions received from the electronic device controlled by the remote party;transmitting, from the electronic wearable medical device, a confirmation to the electronic device controlled by the remote party that the starting of the delivering of one of glucose or insulin into the patient'ss body, or the stopping of the delivering of one of glucose or insulin into the patient'ss body, was performed successfully via the infusion pump responsive to receipt of the control instructions; andgenerating, by the electronic wearable medical device of the patient, a report characterizing the patient'ss bodily data over time.","12","15/285774","2016-10-05","2018-0092576","2018-04-05","10987032","2021-04-27","Cl?udio Afonso Ambr?sio","Claudio Afonso Ambrosio","","","","A61B-0005/14532","A61B-0005/14532 | A61B-0005/0002 | A61B-0005/0004 | A61B-0005/0205 | A61B-0005/6801 | A61B-0005/746 | A61B-0005/7465 | A61M-0005/14244 | G16H-0010/60 | G16H-0010/65 | G16H-0020/17 | G16H-0040/67 | G16H-0050/70 | G16H-0080/00 | H04L-0067/12 | H04L-0067/125 | H04W-0004/02 | H04W-0004/029 | A61M-2005/14208","A61B-005/145","A61B-005/145 | H04W-004/02 | H04L-029/08 | H04L-029/06 | A61B-005/00 | A61B-005/024 | A61B-005/08 | A61M-005/142 | H04W-004/029 | G16H-010/60 | A61B-005/0205 | G16H-080/00 | G16H-020/17 | G16H-050/70 | G16H-040/67 | G16H-010/65","","","","","","4921018000994"
"US","US","P","B2","Compensating for a movement of a sensor attached to a body of a user","A method of compensating for a movement of a device worn by a user is described. The method comprises measuring, using a first sensor, a motion of a user wearing the device; measuring, using a second sensor, a motion of the device; determining a difference in the motion of the device with respect to the motion of the user; and compensating for the difference in the motion of the device with respect to the motion of the user. An electronic device for monitoring a device worn by a user is also disclosed.","1. A method of compensating for a movement of a device worn by a user, the method comprising: measuring, using a first sensor of the device, a motion of the head of the user wearing the device;measuring, using a second sensor, a motion of the device;determining a difference in the motion of the device with respect to the motion of the head of the user; andcompensating for the difference in the motion of the device with respect to the motion of the head of the user.","20","16/214462","2018-12-10","2020-0183488","2020-06-11","10990168","2021-04-27","SAMSUNG ELECTRONICS","Injoon Hong | Long Le | Sourabh Ravindran","","","","G06F-0003/013","G06F-0003/013 | A61B-0003/113 | A61B-0003/14 | A61B-0005/0205 | A61B-0005/6803 | A61B-0005/721 | G01P-0013/00 | G06F-0003/012 | G06K-0009/0061 | G06T-0003/00 | A61B-0005/0402 | A61B-0005/0476 | A61B-0005/0496 | A61B-0005/14542 | A61B-0005/4806 | A61B-0005/6823 | A61B-0005/6824 | A61B-0005/6828 | A61B-0005/6831","G06F-003/01","G06F-003/01 | G06K-009/00 | G06T-003/00 | A61B-005/0205 | A61B-005/00 | A61B-003/13 | A61B-003/14 | G01P-013/00 | A61B-005/0402 | A61B-005/0476 | A61B-005/0496 | A61B-005/145 | A61B-003/113","","","","","","4921018004097"
"US","US","P","B2","Multi-camera imaging for IV compounding","Systems and methods for compounding pharmaceuticals, for example for intravenous delivery. The devices include an infrared camera and an infrared light source, and can backlight and photograph items such as syringes in infrared light for enhanced clarity. A visible light camera may also be provided, for example a color digital camera, for documenting other parts of the compounding process. The device may have other sensors, for example a bar code scanner and a weight sensor, for collecting other data about the compounding process. In one example implementation, the device is a compounding assistance device that leads a user of the device through a compounding task using instructions shown on a display. In other implementations, the device may be robotic.","1. A compounding assistance device, comprising: a carrier for supporting items, wherein the material of the carrier is not opaque to infrared light;an infrared digital camera positioned to photograph at least a portion of the carrier from above;an area light source positioned under the carrier, the area light source configured to generate infrared light and direct the infrared light through the carrier and toward the infrared digital camera;a display;a controller programmed to guide a user of the compounding assistance device through a pharmaceutical compounding task using one or more prompts shown on the display;a visible light digital camera positioned to photograph at least a portion of the carrier from above;a source of visible light positioned adjacent the visible light digital camera and controllable by the controller to emit light during taking of a photograph using the visible light digital camera;a bar code scanner positioned to read a bar code from an item between the bar code scanner and the carrier; anda gantry spanning the carrier;wherein the infrared and visible light digital cameras, the bar code scanner, and the visible light source are mounted on the gantry.","12","15/865038","2018-01-08","2019-0156697","2019-05-23","10991264","2021-04-27","OMNICELL, INC. | AESYNT HOLDINGS, INC.","Giuseppe Trovato | Andrea Schiavinato | Luca Amato","IT2017-134813","IT","2017-11-23","G09B-0019/003","G09B-0019/003 | A61J-0001/22 | A61J-0003/002 | A61M-0005/002 | B65C-0009/40 | G06K-0007/10722 | G06K-0007/1413 | G06T-0007/62 | H04N-0005/2256 | H04N-0005/247 | A61M-2005/3125 | A61M-2205/3313 | A61M-2205/3393 | A61M-2205/587 | A61M-2205/6072 | B65B-0003/26 | B65C-2009/408 | G06T-0007/70 | G06T-2207/10024 | G06T-2207/10048","G09B-019/00","G09B-019/00 | G06K-007/10 | G06T-007/62 | H04N-005/247 | H04N-005/225 | A61M-005/00 | B65C-009/40 | G06K-007/14 | A61J-001/22 | A61J-003/00 | G06T-007/70 | A61M-005/31 | B65B-003/26","","","","","","4921018005186"
"US","US","P","B2","Method for visualizing a tooth situation","Method for visualizing a tooth situation. The invention relates to a method for visualizing a tooth situation, wherein an image of the face of a patient is recorded and stored as a facial data record (1), feature points (3) within the facial data record (1) are automatically recognized by means of a facial recognition method, a mouth area (4) located between the lips is recognized automatically on the basis of the feature points (3), a position of the face in the facial data record (1) and a three-dimensional direction (5) of an orientation of the face in the facial data record (1) is automatically recognized on the basis of the feature points (3), a virtual tooth data record (2) of the tooth situation is oriented according to the three-dimensional direction (5) of the orientation of the face and is positioned according to the position of the face with respect to the facial data record (1), the mouth area within the facial data record (1) is partially overlaid and/or overlaid and/or replaced by the tooth data record (2), and the result is displayed as a visualization data record.","1. A method for visualizing a tooth situation comprising: generating a visualization data record by: a) recording an image of a face of a patient and storing said image as a facial data record;b) automatically recognizing feature points within the facial data record using a facial recognition method,c) automatically recognizing a mouth area located between the lips on the basis of the feature points,d) automatically recognizing a position of the face in the facial data record and a three-dimensional direction of an orientation of the face in the facial data record on the basis of the feature points,e) orienting a virtual tooth data record of the tooth situation according to the three-dimensional direction of the orientation of the face and positioning said virtual tooth data record according to the position of the face with respect to the facial data record,f) partially overlaying, overlaying or replacing the mouth area within the facial data record by the virtual tooth data record, andg) displaying the result as the visualization data record and responsive to obtaining the visualization data record for the image of the face of the patient, and at defined time intervals, repeating steps a)-g) using a new image of the face of the patient recorded at the defined time intervals to produce a new corresponding visualization data record such that a continuous real-time generation and display of new corresponding visualization data records each corresponding to one new image, is produced in order to provide a virtual mirror image for said face; wherein in each new image the orientation or expression of the face of the patient has changed.","12","15/775601","2016-11-17","2018-0249912","2018-09-06","10980422","2021-04-20","DENTSPLY SIRONA INC.","Sascha Schneider | Evgenij Derzapf | Ravid Aloni","10-2015-222782","DE","2015-11-18","A61B-0005/0077","A61B-0005/0077 | A61B-0001/24 | A61B-0005/0088 | A61B-0005/1176 | A61B-0005/4547 | A61B-0005/7425 | A61B-0034/10 | A61B-0034/20 | A61B-0090/36 | A61C-0009/0053 | A61C-0013/0004 | G06F-0003/012 | G06K-0009/00248 | G06K-0009/00281 | G16H-0010/60 | G16H-0030/20 | G16H-0030/40 | G16H-0050/50 | A61B-2034/102 | A61B-2034/105 | A61B-2034/107 | A61B-2034/2065 | A61B-2034/2074 | A61B-2090/365 | A61B-2090/368 | A61B-2576/02 | G06T-0011/60 | G06T-2210/41 | G09G-2340/12","A61B-005/00","A61B-005/00 | A61B-001/24 | A61B-005/1171 | A61C-009/00 | A61C-013/00 | G16H-010/60 | G16H-030/20 | G16H-050/50 | A61B-034/20 | A61B-090/00 | A61B-034/10 | G06K-009/00 | G06F-003/01 | G16H-030/40 | G06T-011/60","","","","","","4921017000987"
"US","US","P","B2","3-dimensional optical topographical sensing of fingerprints using under-screen optical sensor module","Devices and optical sensor modules are provided for provide on-screen optical sensing of fingerprints by using an under-screen optical sensor module that captures and detects optical transmissive patterns in probe light that transmits through the internal finger tissues associated with the external fingerprint pattern formed on the outer finger skin to provide 3-dimensional topographical information for improved optical fingerprint sensing.","1. An electronic device capable of detecting a fingerprint by optical sensing, comprising: a display panel that displays images;a top transparent layer formed over the display panel as an interface for user touch operations and for transmitting the light from the display panel to display images, the top transparent layer including a designated fingerprint sensing area for a user to place a finger for fingerprint sensing;an optical sensor module located below the display panel and underneath the designated fingerprint sensing area on the top transparent layer to receive light from the top transparent layer to detect a fingerprint, wherein the optical sensor module includes an optical sensor array of optical detectors to convert the received light that carries a fingerprint pattern of the user into detector signals representing the fingerprint pattern;extra illumination light sources, separated from the display panel and located outside the optical sensor module at different locations to produce different illumination probe beams to illuminate the designated fingerprint sensing area on the top transparent layer in different illumination directions, each extra illumination light source structured to produce probe light in an optical spectral range with respect to which tissues of a human finger exhibit optical transmission to allow probe light in each illumination probe beam to enter a user finger over the designated fingerprint sensing area on the top transparent layer to produce scattered probe light by scattering of tissues inside the finger that propagates towards and passes the top transparent layer by transmission through internal tissues of ridges and valleys of the finger to carry both (1) 2-dimensional fingerprint pattern information of a 2-dimensional fingerprint pattern of ridges and valleys of the finger present in the designated fingerprint sensing area and (2) different fingerprint topographical information associated with illumination direction-dependent spatial shadowing patterns due to illumination of internal tissues of the ridges and valleys of the finger at the different illumination directions, respectively, caused by transmission of probe light of each of the different illumination probe beams through a skin of the finger such that scattering of the probe light under the skin of the finger by the internal tissues renders the different fingerprint topographical information associated with illumination direction-dependent spatial shadowing patterns to contain 3-dimensional information of the internal tissues of the ridges and valleys of the finger that is not captured by the 2-dimensional fingerprint pattern information of the 2-dimensional fingerprint pattern of the ridges and valleys of the finger; anda probe illumination control circuit coupled to control the extra illumination light sources to sequentially turn on and off in generating the different illumination probe beams at different times, one beam at one of the different illumination direction at a time, so that the optical sensor module located below the display panel is operable to sequentially detect the scattered probe light from the different illumination probe beams to capture both (1) the 2-dimensional fingerprint pattern information, (2) the different fingerprint topographical information associated with illumination direction-dependent spatial shadowing patterns due to illumination at the different illumination directions, respectively, and (3) to process different fingerprint topographical information to extract a spatial shift between different topographical patterns due to a difference in the different illumination directions and to determine whether the detected fingerprint pattern is from a natural finger based on the extracted spatial shift between the different topographical patterns.","28","16/147855","2018-09-30","2019-0303639","2019-10-03","10984213","2021-04-20","GOODIX TECHNOLOGY INC. | SHENZHEN GOODIX TECHNOLOGY CO., LTD.","Yi He | Bo Pi","","","","G06K-0009/0004","G06K-0009/0004 | A61B-0005/0073 | A61B-0005/1172 | A61B-0005/1455 | A61B-0005/14532 | G06K-0009/0012 | G06K-0009/2027 | G09G-0003/3208 | G06F-0003/0421 | G06K-0009/0002 | G06K-2009/0006","A61B-005/117","A61B-005/117 | G06K-009/00 | G06F-003/041 | G06K-009/52 | A61B-005/00 | G06K-009/20 | G09G-003/3208 | A61B-005/1455 | A61B-005/1172 | A61B-005/145 | G06F-003/042","","","","","","4921017004746"
"US","US","P","B2","Detecting complex user activities using ensemble machine learning over inertial sensors data","A computer implemented method of detecting complex user activities, comprising using processor(s) in each of a plurality of consecutive time intervals for: obtaining sensory data from wearable inertial sensor(s) worn by a user, computing an action score for continuous physical action(s) performed by the user, the continuous physical action(s) extending over multiple time intervals are indicated by repetitive motion pattern(s) identified by analyzing the sensory data, computing a gesture score for brief gesture(s) performed by the user, the brief gesture(s) bounded in a single basic time interval is identified by analyzing the sensory data, aggregating the action and gesture scores to produce an interval activity score of predefined activity(s) for a current time interval, adding the interval activity score to a cumulative activity score accumulated during a predefined number of preceding time intervals and identifying the predefined activity(s) when the cumulative activity score exceeds a predefined threshold.","1. A computer implemented method of detecting complex user activities comprising a plurality of continuous physical actions and brief gestures, comprising: using at least one processor in each of a plurality of consecutive time intervals for: obtaining sensory data from at least one wearable inertial sensor worn by a user;computing an action score for at least one continuous physical action performed by said user, said at least one continuous physical action extends over multiple consecutive basic time intervals and is indicated by at least one repetitive motion pattern identified by analyzing said sensory data;computing a gesture score for at least one brief gesture performed by said user, said at least one brief gesture is bounded in a single basic time interval and is identified by analyzing said sensory data;aggregating said action score and said gesture score to produce an interval activity score of at least one predefined activity of said user for a respective one of said plurality of consecutive time intervals;adding said interval activity score to a cumulative activity score of said at least one predefined activity accumulated during a predefined number of preceding time intervals of said plurality of consecutive time intervals; andidentifying said at least one predefined activity when said cumulative activity score exceeds a predefined threshold.","15","15/716524","2017-09-27","2019-0095814","2019-03-28","10984341","2021-04-20","INTERNATIONAL BUSINESS MACHINES CORPORATION","Oded Dubovsky | Alexander Zadorojniy | Sergey Zeltyn","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/1114 | A61B-0005/1118 | A61B-0005/1126 | A61B-0005/6802 | A61B-0005/7246 | A61B-0005/7264 | G06F-0001/163 | G06F-0003/011 | G06F-0003/017 | G06N-0007/005 | A61B-0005/6814 | A61B-0005/6823 | A61B-0005/6824 | A61B-0005/6828 | A61B-2562/0219 | G06F-0003/0346 | G16H-0050/20","G06N-020/00","G06N-020/00 | G06F-001/16 | G06N-007/00 | A61B-005/11 | G06F-003/01 | A61B-005/00 | G16H-050/20 | G06F-003/0346","","","","","","4921017004873"
"US","US","P","B2","Medical device and method for operating a medical device","A medical device includes a network interface, a processor unit, a memory unit, an actuator physiologically acting on a patient and/or a sensor interface detecting a sensor signal indicative of a patient physiological parameter. The network interface receives a sender network identity data message, a sender authorization level and a sender change request to change an actuator operating parameter and/or a predefined alarm detection value. The memory unit provides a predefined minimum authorization level. The processor unit determines an actual authorization of the sender to change the operating parameter and/or to predefine a value on the basis of the sender authorization level and of the predefined minimum authorization level as well as to change the operating parameter as a function of the result of the determination and/or to perform a detection of an alarm generation state as a function of the indicated predefined value and of the sensor signal.","1. A medical device comprising: a sensor interface for detecting a sensor signal, which indicates a physiological parameter of a patient;a network interface configured to receive at least one data message, which indicates a network identity of a sender of the at least one data message, which indicates at least one authorization level of the sender of the at least one data message, and which indicates a request of the sender to change a set alarm detection predefined threshold value for an alarm detection to a changed alarm detection predefined threshold value by indicating an alarm detection predetermined threshold value type and an alarm detection predetermined threshold value;a memory unit configured to provide one or more predefined minimum authorization level with a corresponding alarm detection predetermined threshold value type and the set alarm detection predefined threshold value;a processor unit configured:to receive the sensor signal and determine therefrom a parameter value of the physiological parameter of the patient on the basis of the signal;to determine an actual authorization of the sender to change the set alarm detection predefined threshold value on the basis of the authorization level of the sender and the alarm detection predetermined threshold value type from the at least one data message and on the basis of the predefined minimum authorization level and corresponding alarm detection predetermined threshold value type provided by the memory unit;as a function of the result of the determination of the actual authorization of the sender, to change the set alarm detection predefined threshold value of the memory to the alarm detection predefined threshold value indicated in the at least one data message; andto perform a detection of the alarm generation state as a function of the determined parameter value and as a function of the alarm detection predefined threshold value indicated in the at least one data message.","17","15/848227","2017-12-20","2018-0182487","2018-06-28","10984908","2021-04-20","DR?GERWERK AG & CO. KGAA","Gotz Kullik | Stefan Schlichting | Alexander Loose | Tim Weinmann | Hinrich Althoff","10-2016-015368","DE","2016-12-22","G16H-0040/67","G16H-0040/67 | A61B-0005/4836 | A61B-0005/4839 | H04L-0009/0841 | H04L-0009/14 | H04L-0009/30 | H04L-0009/3226 | H04L-0063/0435 | H04L-0063/0442 | H04L-0063/105 | H04W-0012/08 | A61B-0005/087 | A61B-0005/0816 | A61M-0005/142 | A61M-0005/1723 | A61M-0016/0003 | A61M-0016/0051 | A61M-0016/026 | A61M-2205/18 | A61M-2205/3553 | A61M-2205/3584 | A61M-2205/3592 | A61M-2205/52 | G06Q-2220/10 | H04L-2209/88","H04L-029/06","H04L-029/06 | G16H-040/67 | H04L-009/14 | H04L-009/30 | H04L-009/32 | A61B-005/00 | H04W-012/08 | H04L-009/08 | A61B-005/08 | A61M-005/172 | A61M-016/00 | A61B-005/087 | A61M-005/142","","","","","","4921017005433"
"US","US","P","B2","Wearable article for determining a task","In aspects of a wearable article for determining a task, a system includes a delivery device usable to administer a liquid responsive to an applied force, and includes a wearable article with multiple sensors to detect movements of the wearable article and handling of the delivery device. A task determination module is implemented to receive sensor data identifying the movements of the wearable article and the applied force, and can determine a task from a set of tasks associated with the delivery device that corresponds to the movements of the wearable article. The task determination module can then determine whether the task was performed correctly based on a comparison of the sensor data to a correct pattern of use for the task.","1. A system, comprising: a tool usable to perform a task responsive to an applied force;a wearable article worn by a user while using the tool to perform the task with the applied force, the wearable article including one or more sensors to detect movements of the wearable article and handling of the tool;a task determination module implemented at least partially in computer hardware to: receive sensor data identifying the movements of the wearable article and the applied force to perform the task; anddetermine, from the sensor data, a type of the task from a set of tasks associated with the tool that corresponds to the movements of the wearable article.","20","16/820187","2020-03-16","2020-0214631","2020-07-09","10973460","2021-04-13","MOTOROLA MOBILITY LLC","Sudhir C. Vissa | Vivek Kumar Tyagi | Scott Patrick DeBates | Douglas Alfred Lautner","","","","A61B-0005/6806","A61B-0005/6806 | A61B-0005/11 | A61M-0005/178 | G06F-0003/014 | G16H-0040/60","G06F-003/00","G06F-003/00 | A61B-005/00 | G06F-003/01 | G16H-040/60 | A61B-005/11 | A61M-005/178","","","","","","4921016001024"
"US","US","P","B2","Method and system for planning and performing arthroplasty procedures using motion-capture data","A method of preparing a surgical plan comprises video capturing a range of motion of an anatomic joint of a patient to obtain a range of motion video, registering the range of motion video to two-dimensional images of a skeleton of the patient, identifying potential impingement and stability issues in the anatomic joint based on the range of motion video, templating a prosthetic implant on two-dimensional images of the anatomic joint using the identified potential impingement and stability issues of the anatomic joint to obtain position and orientation data, and electronically saving the position and orientation data of the prosthetic implant for use with a surgical navigation system. A system for planning and performing a surgical procedure comprises a motion capture system, an x-ray system, an electronic modeling system, and a surgical navigation system.","1. A method of preparing a surgical plan, the method comprising: video capturing a range of motion of an anatomic joint of a patient to obtain a range of motion video while the patient is wearing a body suit having markers located at multiple joints of the patient;x-ray imaging the patient in a pose while the patient is wearing the body suit having the markers and while the patient is adjacent a first scale marker placed into a field of view of a first x-ray imaging machine to obtain directional medical images showing the markers relative to bones of the patient;registering the medical images to templating x-ray images of a skeleton of the patient including a second scale marker in the templating x-ray images, the templating x-ray images being derived independently and directly from the patient separately from the medical images by x-ray imaging the patient with a second x-ray imaging machine while the second scale marker is in a field of view of the second x-ray imaging machine;identifying potential impingement and stability issues in the anatomic joint based on correlated images of the range of motion video and the medical images by comparing locations of the markers in the range of motion video and the medical images;sizing and positioning a prosthetic implant on the templating x-ray images using the identified potential impingement and stability issues of the anatomic joint through the registered medical images to obtain position and orientation data;electronically saving the position and orientation data of the prosthetic implant in a computer readable storage medium, the electronically saved position and orientation data being configured for use with a surgical navigation system in a format that allows the surgical navigation system to guide positioning of the prosthetic implant within a coordinate system maintained by the surgical navigation system during a surgical procedure; andconfiguring an instrument based on the position and orientation data.","8","15/081370","2016-03-25","2016-0278868","2016-09-29","10973580","2021-04-13","BIOMET MANUFACTURING, LLC","Michael E. Berend | Keith R. Berend | Adolph V. Lombardi | Kirk J Bailey | John White","","","","A61B-0034/10","A61B-0034/10 | A61B-0005/1071 | A61B-0005/1121 | A61B-0005/1127 | A61B-0005/4851 | A61F-0002/4609 | G06F-0003/011 | A61B-0005/0035 | A61B-0005/0077 | A61B-0034/25 | A61B-2034/104 | A61B-2034/105 | A61B-2034/107 | A61B-2034/108 | A61B-2034/2048 | A61B-2034/2051 | A61B-2034/2055 | A61B-2034/2065 | A61B-2090/364 | A61B-2090/376 | A61B-2090/3937 | A61B-2090/3966 | A61B-2090/502 | A61B-2505/05 | A61F-2002/30952 | A61F-2002/4633","A61B-034/10","A61B-034/10 | A61B-005/11 | A61B-005/107 | A61B-005/00 | G06F-003/01 | A61F-002/46 | A61F-002/30 | A61B-034/20 | A61B-090/00 | A61B-090/50 | A61B-034/00","","","","","","4921016001143"
"US","US","P","B2","Electronic device for authenticating biometric data and system","The present disclosure provides an electronic device and system that include an electrode interface that can be brought in contact with the body of a user, a memory, and a processor operably coupled to the electrode interface and the memory, in which the processor is set to obtain user information through user authentication, generate a user authentication signal on the basis of the user information, and transmit the user authentication signal or a signal including at least a portion of the user authentication signal through the body of a user being in contact with the electrode interface.","1. An electronic device comprising: an electrode interface comprising one or more electrodes and configured to transmit and receive electrical signals via the one or more electrodes, through a body of a user;a memory; anda processor operably coupled to the electrode interface and the memory, the processor configured to: obtain first biometric information;authenticate the user based on the first biometric information;generate a user authentication signal based on user information, wherein the user information is stored in the memory and includes at least one of a user'ss account, phone number, or information related to the user;transmit, via the electrode interface, a first signal comprising at least a portion of the user authentication signal and a control signal for obtaining second biometric information to a second electronic device attached to the body of the user;receive, via the electrode interface, a second signal comprising the second biometric information and the at least a portion of the user authentication signal, wherein the second signal is transmitted from the second electronic device; andstore, the second biometric information and the at least a portion of the user authentication signal in the memory.","20","15/886620","2018-02-01","2018-0225437","2018-08-09","10977349","2021-04-13","SAMSUNG ELECTRONICS CO., LTD.","Kanghyun Suh | Doosuk Kang | Bokun Choi | Jeongmin Park","10-2017-0015437","KR","2017-02-03","G06F-0021/32","G06F-0021/32 | A61B-0005/0261 | A61B-0005/04 | A61B-0005/0402 | A61B-0005/117 | A61B-0005/681 | A61B-0005/6824 | A61B-0005/6832 | G06K-0009/00885 | H04L-0009/3231 | H04L-0063/0861 | H04W-0012/0605 | H04W-0012/0608 | A61B-0005/053 | A61B-0005/0533 | A61B-0005/4809 | A61B-2560/0204 | A61B-2560/0242 | A61B-2562/0219 | A61B-2562/0257 | G06K-2009/00939 | H04W-0088/02","G06F-021/00","G06F-021/00 | G06F-021/32 | H04L-009/32 | H04L-029/06 | A61B-005/026 | A61B-005/00 | A61B-005/0402 | A61B-005/117 | G06K-009/00 | H04W-012/06 | A61B-005/04 | A61B-005/0533 | A61B-005/053 | H04W-088/02","","","","","","4921016004888"
"US","US","P","B2","Conducting digital surveys that collect and convert biometric data into survey respondent characteristics","This disclosure covers systems and methods that administer a digital survey to respondents who interact with a biometric sensor to collect and convert biometric data into behavioral and physical characteristics of the respondents. In certain embodiments, by converting biometric data into respondent characteristics, the disclosed systems and methods identify various unwritten or nonverbal responses of survey respondents who respond to a digital survey or who interact with a display medium that captures survey data. To facilitate review of respondents' characteristics and responses, in some embodiments, the disclosed systems and methods further categorize the converted respondent characteristics within a response database for the digital survey.","1. A method comprising: providing, to a respondent device, a digital survey question within a first data packet as part of a digital survey, the first data packet comprising a textual query, a biometric query comprising computer-executable instructions that cause a biometric sensor to capture a specific type of biometric data at a time when the respondent device presents the textual query, and a question identifier for the digital survey question;receiving, from the respondent device, a response by a respondent to the digital survey question within a second data packet comprising a reply to the textual query, a biometric dataset corresponding to the specific type of biometric data captured by the biometric sensor in response to the biometric query at the time when the respondent device presents the textual query, and the question identifier;based on the question identifier, selecting a biometric-data analysis corresponding to the specific type of biometric data captured by the biometric sensor;analyzing the biometric dataset utilizing the biometric-data analysis corresponding to the specific type of biometric data;based on analyzing the biometric dataset, converting the biometric dataset to a respondent characteristic; andcategorizing the respondent characteristic with the reply to the textual query within a response database of the digital survey.","20","15/582180","2017-04-28","2018-0315063","2018-11-01","10977674","2021-04-13","QUALTRICS, LLC","Larry Dean Cheesman","","","","G06Q-0030/0203","G06Q-0030/0203 | A61B-0005/0205 | A61B-0005/165 | G06K-0009/00892 | A61B-0005/021 | A61B-0005/024 | A61B-0005/0816 | G06Q-0030/0217","G06Q-030/02","G06Q-030/02 | A61B-005/0205 | A61B-005/16 | G06K-009/00 | A61B-005/021 | A61B-005/024 | A61B-005/08","","","","","","4921016005211"
"US","US","P","B2","Workplace analysis system","The invention relates to a workplace analysis system (1) in a work area (10) for the acquisition and analysis of the presence of a person (P) to be monitored in several differently defined zones (A, B, C) of the work area (10) and of at least the temporal duration of stay (tA, tB, tC) of the person (P) to be monitored within the zones (A, B, C), wherein in the workplace analysis system (1) at least one acquisition device (20) is provided, which is designed to detect and to acquire the presence and the duration of stay (tA, tB, tC) of the person (P) to be monitored in the respective zones (A, B, C), and an analysis device (30) is provided, which is designed to analyze and evaluate the presence and the duration of stay (tA, tB, tC) of the person (P) to be monitored in the respective zones (A, B, C).","1. A workplace analysis system in a work area for the acquisition and analysis of the presence and/or of characteristic values of a person (P) to be monitored in several differently defined zones (A, B, C) of the work area and of at least a temporal duration of stay (tA, tB, tC) of the person (P) to be monitored within the zones (A, B, C), wherein, in the workplace analysis system, at least one acquisition device is provided, which is designed to detect and to acquire the presence and duration of stay (tA, tB, tC) of the person (P) to be monitored in the zones (A, B, C), and an analysis device is provided, which is designed to analyze and evaluate the presence and the duration of stay (tA, tB, tC) of the person (P) to be monitored in the zones (A, B, C), wherein the acquisition device comprises at least one sensor, andwherein, in each case, the at least one sensor is provided for the detection and acquisition of the person (P) to be monitored or for the acquisition of characteristic values of the person (P) to be monitored at least in zone (A) and/or zone (B), andwherein, in each case, the detection and acquisition of the person (P) to be monitored in zone (C) is provided, in each case, by the at least one sensor of zone (A) and/or zone (B), andwherein the sensor is an ultrasound echo sensor or an infrared array sensor, andwherein the analysis device is designed to store data acquired by the acquisition unit, to classify the acquired data depending on the zones (A, B, C) in which it was acquired, to process, evaluate and/or extrapolate the acquired data and, to compare the acquired data with stored target values, andwherein the analysis device is designed to classify, depending on the zones (A, B, C), whether the person (P) to be monitored is sitting, standing, absent or moving.","12","15/814526","2017-11-16","2018-0197126","2018-07-12","10966637","2021-04-06","AERIS GMBH","Josef Glockl","10-2017-100884","DE","2017-01-08","A61B-0005/1116","A61B-0005/1116 | A61B-0005/11 | A61B-0005/6889 | A61B-0005/6898 | G06K-0009/00342 | G06K-0009/00382 | G06Q-0010/0633","A61B-005/11","A61B-005/11 | A61B-005/00 | G06Q-010/06 | G06K-009/00","","","","","","4921015000998"
"US","US","P","B2","Augmented reality information system for use with a medical device","An augmented-reality system for providing information relating to a wearable medical device and/or a patient wearing a medical device. An augmented-reality enabled computing device includes an image acquisition device, a user interface operatively coupled to the image acquisition device, the user interface configured to receive streaming images of a scene having one or more predetermined recognizable features, and a processor operably connected to the user interface. The processor is configured receive the streaming images, analyze the one or more predetermined recognizable features to determine a context of the scene, retrieve information relating to at least one of the medical device and the patient wearing the medical device, the information corresponding to the determined context of the scene, and augment the received streaming images with contextual information relating to at least one of the medical device and the patient wearing the medical device.","1. An augmented-reality system for providing information, the system comprising: a wearable medical device configured to be worn by a patient on a body of the patient, the wearable medical device comprising at least one electrocardiogram (ECG) sensor coupled to the patient and configured to sense at least one ECG signal for the patient,a controller operably coupled to the at least one ECG sensor and configured to receive the at least one ECG signal from the at least one ECG sensor, andproduce one or more cardiac parameters for the patient based upon the at least one ECG signal,at least one therapy electrode configured to couple to the patient and operably coupled to the controller, the at least one therapy electrode configured to produce, based upon the one or more cardiac parameters, a therapy shock,an electrode belt, anda garment comprising the electrode belt, the at least one ECG sensor, and the at least one therapy electrode; andan augmented-reality enabled computing device comprising an image acquisition device,a display operatively coupled to the image acquisition device, the display configured to receive and display a video of a scene having one or more predetermined recognizable features, anda processor operably connected to the display, the processor configured to execute one or more computer-readable instructions to cause the processor to receive the video of the scene, the video comprising the wearable medical device,process the video to analyze the one or more predetermined recognizable features to detect a presence of the wearable medical device within the scene, wherein the recognizable features define at least one of a plurality of components of the wearable medical device, the plurality of components comprising the at least one ECG sensor, the controller, the at least one therapy electrode, the electrode belt, and the garment,process the video to determine a context of the scene comprising an environment of the wearable medical device and at least one of the plurality of components of the wearable medical device,retrieve contextual information relating to the wearable medical device and the at least one of the plurality of components of the wearable medical device based on the context of the scene, the contextual information comprising at least operational information relating to the at least one of the plurality of components of the wearable medical device, andactivity information related to a walk test the patient is performing, wherein the processor is configured to query the patient wearing the wearable medical device to answer one or more questions upon completion of the walk test, the one or more questions comprising questions related to shortness of breath, overall fatigue, and pain or numbness in extremities of the patient, andbased on the retrieved contextual information relating to the wearable medical device and the at least one of the plurality of components of the wearable medical device, augment the received video with at least a portion of the contextual information relating to the wearable medical device and the at least one of the plurality of components of the wearable medical device such that at least a portion of the operational information related to the at least one of the plurality of components of the wearable medical device and at least a portion of the activity information is overlaid on the video of the scene and displayed on the display.","26","15/442280","2017-02-24","2018-0242920","2018-08-30","10969583","2021-04-06","ZOLL MEDICAL CORPORATION","Patrick Hresko | Thomas E. Kaib | Shane S. Volpe | Grace Owens | Trisha A. Pavel | John G. Clark","","","","G02B-0027/0172","G02B-0027/0172 | A61B-0005/0006 | A61B-0005/04085 | A61B-0005/6804 | A61B-0005/743 | A61N-0001/0484 | A61N-0001/36014 | A61N-0001/3904 | G06F-0003/011 | G06K-0009/00671 | G06K-0009/00718 | A61B-0005/021 | A61B-0005/02055 | A61B-0005/1118 | A61B-0005/1176 | A61B-0005/14532 | A61B-0005/14542 | A61B-0005/7405 | A61B-0005/7455 | A61B-0005/7475 | A61B-0090/96 | G02B-0027/017 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0141 | G02B-2027/0178","G02B-027/01","G02B-027/01 | A61B-005/0408 | A61M-001/04 | A61B-005/00 | G06K-009/00 | A61N-001/39 | A61N-001/36 | G06F-003/01 | A61B-090/96 | A61B-005/1171 | A61B-005/11 | A61B-005/145 | A61B-005/021 | A61B-005/0205 | A61N-001/04","","","","","","4921015003927"
"US","US","P","B2","Intelligent sleep system, and user side system and cloud side system thereof","Disclosed herein are an intelligent sleep system, and a client system and a cloud system thereof, wherein the client system comprises a multi-dimensional data acquisition module (101), a local data processing module (102), a client system communication module (103), and a driving execution module (104); and the cloud system comprises a cloud side communication module (201), a data management module (202), and a data mining module (203).","1. A client system for an intelligent sleep system, comprising a multi-dimensional data collection module, a local data processing module, a client system communication module, and a driver execution module; wherein:the multi-dimensional data collection module is used to collect human-machine environment information associated with the sleep activity of users of the client system through a multi-mode sensor cluster, and send the human-machine environment information to the local data processing module, wherein the human-machine environment information includes the user'ss sleep behavior data and sleep environment data;the local data processing module is used to send the human-machine environment information to a cloud system connected to the client system after preprocessing, and write driver data received from the cloud system into the driver execution module;the client system communication module is used for wireless transmission of data between the client system and the cloud system;the driver execution module is used to execute a supporting action according to the driver data and the human-machine environment information.","10","15/737035","2015-01-14","2019-0069838","2019-03-07","10959667","2021-03-30","MOONMARK SMART TECHNOLOGY (SHANGHAI) CO., LTD.","Zhiyu Xin | Liqun Zhang","2014-10025534","CN","2014-01-20","A61B-0005/4815","A61B-0005/4815 | A61B-0005/4806 | A61B-0005/4809 | A61M-0021/02 | G06K-0009/00523 | G16H-0015/00 | G16H-0040/40 | G16H-0040/67 | G16H-0050/70 | H04L-0067/12","G16H-015/00","G16H-015/00 | A61B-005/00 | H04L-029/08 | G16H-040/40 | G16H-040/67 | A61M-021/02 | G16H-050/70 | G06K-009/00","","","","","","4921014001045"
"US","US","P","B2","Wearable device for trusted biometric identity","A wearable device may store a biometric token associated with a wearer of the wearable device, the wearable device including: a wireless communications interface; a processing circuitry; a memory configured to store a biometric token associated with a wearer of the wearable device, the biometric token including a device identifier that is associated with the wearable device, a biometric template for each of one or more biometric scans of the wearer of the wearable device, and an indication of whether or not the biometric token is valid, wherein the biometric token, if valid, establishes a trust that wearer identifying information, linked to the biometric token, is associated with the wearer; an invalidating event detector configured to determine if an invalidating event has occurred; wherein the processing circuitry is configured to invalidate the biometric token in response to detecting that an invalidating event has occurred for the biometric token.","1. A wearable device that is configured to store a biometric token associated with a wearer of the wearable device, the wearable device comprising: a wireless communications interface;a processing circuitry;a memory configured to store a biometric token associated with a wearer of the wearable device, the biometric token including a device identifier that is associated with the wearable device, a biometric template for each of one or more biometric scans of the wearer of the wearable device, and an indication of whether or not the biometric token is valid, wherein the biometric token, if valid, establishes a trust that wearer identifying information, linked to the biometric token, is associated with the wearer;an invalidating event detector configured to determine if an invalidating event has occurred;the invalidating event detector including at least a power-on detector to detect an invalidating event based on detecting that the wearable device has been powered on or booted up;wherein the processing circuitry is configured to: invalidate the biometric token in response to detecting that an invalidating event has occurred for the biometric token; andstore, in the memory of the wearable device, information associated with the invalidating event.","34","16/033945","2018-07-12","2020-0019682","2020-01-16","10963547","2021-03-30","SECURIPORT LLC","Chi Jung Lee | Frank Buscaglio","","","","G06F-0021/32","G06F-0021/32 | A61B-0005/681 | G06K-0009/00892 | H04L-0063/0853 | H04L-0063/0861 | H04W-0012/06 | G06F-0001/163 | G06F-0001/3231 | G06F-0021/35 | G06K-2009/00939","G06F-021/32","G06F-021/32 | A61B-005/00 | G06K-009/00 | H04L-029/06 | H04W-012/06 | G06F-001/3231 | G06F-001/16 | G06F-021/35","","","","","","4921014004890"
"US","US","P","B2","Fishing suggestions","Various implementations described herein are directed to a non-transitory computer readable medium having stored thereon computer-executable instructions which, when executed by a computer, may cause the computer to receive a location, a date, a wind direction, a water temperature, a species, or combinations thereof. The computer may use the location, date, wind direction, water temperature, species, or combinations thereof to retrieve fishing data. The computer may analyze the retrieved fishing data to determine one or more suggested fishing locations.","1. A non-transitory computer-readable medium having stored thereon a plurality of computer-executable instructions which, when executed by a computer, cause the computer to: receive, at a marine electronics device of a marine vessel, one or more environmental measurements during a fishing trip, wherein the one or more environmental measurements comprise at least one of a location of the marine vessel or a water temperature;determine input criteria comprising at least one of a location, a date, a wind direction, a water temperature, or a species, wherein at least a portion of the input criteria is determined automatically without user input and based on the received one or more environmental measurements;retrieve fishing data relevant to the input criteria from a database, wherein the fishing data comprises fishing records from a plurality of users;analyze the retrieved fishing data to determine one or more suggested fishing locations based on the input criteria;retrieve one or more suggestions relevant to the input criteria, wherein the one or more suggestions comprise one or more areas suggested for fishing; anddisplay, on a screen of the marine electronics device, the suggestions on a map such that the one or more areas suggested for fishing are highlighted on the map with respect to remaining areas on the map using at least one of a plurality of repeated-patterned dots, one or more colors, or shading that cover an entirety of each of the one or more areas suggested for fishing.","12","14/461393","2014-08-16","2015-0058323","2015-02-26","10952420","2021-03-23","NAVICO HOLDING AS","Paul Robert Bailey","","","","A01K-0097/00","A01K-0097/00 | A01K-0079/00 | A01K-0099/00 | A61B-0005/1118 | A61B-0005/1123 | G01B-0021/00 | G01C-0021/203 | G06F-0003/014 | G06F-0003/017 | G06F-0003/0231 | G06F-0003/0346 | G06F-0011/3438 | G06F-0011/3476 | G06F-0015/0225 | G06F-0016/9535 | G06K-0009/00342 | G06Q-0010/00 | G06Q-0050/01 | G06T-0007/246 | G06T-0007/292 | G06T-0007/60 | G06T-0011/206 | G08C-0017/02 | G11B-0027/031 | G11B-0027/17 | G11B-0027/28 | G11B-0027/34 | G11B-0031/006 | H04N-0005/91 | H04N-0021/4335 | H04Q-0009/00 | B63B-0049/00 | G01S-0007/003 | G01S-0015/96 | G06F-0011/3013 | G06F-0011/3058 | G06F-2201/835 | G06T-2207/10016 | G06T-2207/30196 | G08C-2201/32 | H04Q-2209/43 | Y02D-0010/00","A01K-097/00","A01K-097/00 | H04N-005/91 | H04N-021/4335 | G06F-016/9535 | A01K-079/00 | G06F-011/34 | G06F-003/01 | G06K-009/00 | G11B-027/28 | G11B-027/34 | G06F-003/0346 | H04Q-009/00 | G01C-021/20 | G06T-007/246 | G06T-007/292 | G08C-017/02 | G06F-003/023 | G06F-015/02 | G06T-011/20 | G06T-007/60 | G11B-027/031 | G11B-027/17 | G11B-031/00 | A01K-099/00 | A61B-005/11 | G01B-021/00 | G06Q-010/00 | G06Q-050/00 | B63B-049/00 | G01S-015/96 | G06F-011/30 | G01S-007/00","","","","","","4921013000783"
"US","US","P","B2","Passive sensors and related structures for implantable biomedical devices","A biomedical implant includes a wall enclosing at least a portion of the implant. The wall includes a first stratum and a second stratum conformal with the first stratum. An interlayer is provided between the first and the second strata, and includes a structure that produces capillary pressure in an infiltrating fluid in response to rupture of the first stratum or the second stratum resulting in entry of the infiltrating fluid into the interlayer. A detector is exposed to the interlayer and configured to detect a presence, if any, of the infiltrating fluid and output a detection state indicator. A communication circuit is communicatively coupled to the detector and configured to communicate the detection state indicator to a reader external to the patient.","1. A biomedical implant for implantation inside a patient comprising: a wall enclosing at least a portion of the implant, the wall including: a first stratum;a second stratum conformal with the first stratum;an interlayer between the first and the second strata, the interlayer including a structure that produces capillary pressure in an infiltrating fluid in response to rupture of at least one of the first stratum and the second stratum resulting in entry of the infiltrating fluid into the interlayer; anda sensor system that includes: a first device including a detector and a first communication circuit, the detector being exposed to the interlayer, and including fluid-sensing circuitry to detect a presence, if any, of the infiltrating fluid, and the first communication circuit being operative to initiate a first wireless transmission containing a detection state indicator of the detector; anda second device physically isolated from the first device, and including a second communication circuit interactive with the first device to receive the first wireless transmission, and to initiate a second wireless transmission to communicate the detection state indicator to a reader external to the patient.","12","15/667375","2017-08-02","2018-0036115","2018-02-08","10952612","2021-03-23","GEISSLER COMPANIES, LLC","Yuri Smirnov","","","","A61B-0005/0031","A61B-0005/0031 | A61B-0005/02444 | A61B-0005/1451 | A61B-0005/4312 | A61B-0005/6869 | A61B-0005/6876 | A61F-0002/12 | A61F-0002/2472 | G06F-0019/3468 | G06K-0007/10366 | G06K-0019/0723 | G16H-0040/67 | G16H-0050/20 | H04Q-0009/00 | A61B-0005/1108 | A61B-2560/0219 | A61B-2562/029 | A61B-2562/0219 | A61B-2562/0247 | A61B-2562/08 | A61F-2250/0002 | A61F-2250/008 | A61F-2250/0085 | A61F-2250/0096 | G06K-0019/0709 | G06K-0019/0716 | H04Q-2209/47 | H04Q-2209/886","A61B-005/00","A61B-005/00 | A61B-005/024 | A61B-005/145 | A61F-002/24 | G16H-050/20 | G16H-040/67 | A61F-002/12 | G06F-019/00 | G06K-007/10 | G06K-019/07 | H04Q-009/00 | A61B-005/11","","","","","","4921013000974"
"US","US","P","B2","Method and apparatus for determining a type of data and an integrity of data","A method of operating an activity tracking system includes collecting activity data from an activity tracking device, receiving an activity type associated with the activity data via a user input, and evaluating the collected activity data to determine whether one or more aspects thereof are within a predetermined range of values assigned to the received activity type.","1. A method of operating an activity tracking system, comprising: generating electronic activity data with an activity tracking device of the activity tracking system, the activity data including speed data generated by a speed sensor of the activity tracking device;receiving a user selection of one of a plurality of activity types with an input unit of the activity tracking device, the user selection is a first activity type;processing the activity data with a controller of the activity tracking system to determine average speed data based on the speed data and whether the average speed data exceed a predetermined average speed assigned to the first activity type, the controller operably connected to the speed sensor to receive the activity data and operably connected to the input unit to receive the user selection;when the average speed data exceed the predetermined average speed (i) preventing association of the first activity type with the activity data using the controller, and (ii) displaying an error message on a display of the activity tracking device to inform a user that a second activity type was performed in generating the activity data that is different than the first activity type,wherein the first activity type includes running, walking, jogging, or hiking,wherein the second activity type includes only activities which a human is not capable of performing without aid of a machine including riding a bicycle and riding in an automobile, andwherein the predetermined average speed is an average speed that the user of the activity tracking system is incapable of achieving while performing the first activity type.","21","15/252536","2016-08-31","2018-0056126","2018-03-01","10952645","2021-03-23","UNDER ARMOUR, INC.","Kyler Eastman","","","","A61B-0005/1118","A61B-0005/1118 | G06K-0009/0055 | G06K-0009/00342 | A61B-0005/024 | A61B-0005/0205 | A61B-0005/681 | A61B-0005/6801 | A61B-0005/6805 | H04L-0067/22","A61B-005/11","A61B-005/11 | G06K-009/00 | A61B-005/0205 | A61B-005/024 | H04L-029/08 | A61B-005/00","","","","","","4921013001007"
"US","US","P","B2","Hearing assist device fitting method and software","A method and software program is used by patients for fitting and refitting of a DSP-based hearing assistance device. An audiologist user interface and patient user interface include soundmaps and slidebars to control collections of hearing parameters within the DSP of the device. Based on cognitive testing and training of the patient, the range of adjustment allowed within at least the patient user interface is limited. The software therefore always maintains 100% safety for the patient and/or user to finetune the hearing aid parameters without worrying about selecting parameter values which would leading to slowing the cognitive learning of the patient.","1. A method for fitting a hearing assist device for a patient, comprising: conducting a test to assess aural cognitive abilities of the patient; andproviding a user interface which allows adjustment of a plurality of sound processing parameter values within a digital signal processor in the hearing assist device, in which the range of sound processing parameter values permitted in the user interface is a limited subset of the values allowed within the digital signal processor, limited based on the assessed aural cognitive abilities.","19","16/601368","2019-10-14","2020-0069224","2020-03-05","10952649","2021-03-23","INTRICON CORPORATION","Andreas Perscheid","","","","A61B-0005/123","A61B-0005/123 | A61B-0005/4836 | A61B-0005/7435 | A61B-0005/7475 | G06F-0003/04847 | G16H-0020/00 | H04R-0025/70 | A61N-0001/36039","A61B-005/12","A61B-005/12 | G06F-003/0484 | H04R-025/00 | A61B-005/00 | G16H-020/00 | A61N-001/36","","","","","","4921013001011"
"US","US","P","B2","Analysis of cognitive status through object interaction","Embodiments of the present invention provide a circuit board enclosed in an encasing with processors and memory, configured to receive and analyze data, and containing computer logic capable of receiving and analyzing data. The apparatus further includes sensors connected to the processors configured to transfer data to the processors, a power source configured to provide power to the processors, memory modules and sensors, one or more of a light source, an audio source, a vibration source, and a video source, a timing device, a wireless component and/or a wired component capable of transferring data, a light sensor capable of determining the intensity of the received light, computer logic capable of generating a report or transferring the data to a source capable of generating the report, where the report is a cognitive assessment, a comparison of cohorts, or a determination of how the subject puts together an apparatus with a second apparatus, and where the cohorts are people with same or similar diseases, conditions, ages, medical histories, social demographics, experience levels, or locations.","1. An apparatus, comprising: a circuit board, enclosed in an encasing, having one or more processors and one or more memory modules;one or more sensors operatively connected to the one or more processors, wherein the one or more sensors are configured to transfer data to the one or more processors;a power source disposed in the circuit board, wherein the power source is configured to provide power to the one or more processors, memory modules, and sensors;one or more features operatively connected to the one or more processors, wherein the one or more features are one or more of: a light source, an audio source, a vibration source, and a video source;a timing device operatively coupled to the one or more features, wherein the timing device switches one or more features into an on or an off state, dependent upon previously determined criteria;one or more of a wireless component or a wired component operatively coupled to the one or more processors, wherein the one or more of the wireless component or the wired component is capable of transferring data between the one or more processors and one or more external communication sources;a cognitive analysis module operatively connected to each of the one or more sensors and processors that is configured to detect cognitive impairments and motor skill functions by analyzing movements of a user during assembly of multiple, assembled apparatuses to identify respective emotional states of the user and wherein the cognitive analysis module transmits collected information to a deep neural net to compare the identified respective emotional states to previously collected emotional states to identify deteriorating or improving emotional states of the user;wherein one of the one or more sensors comprises a light sensor configured to receive light from a light source feature that intensifies in brightness based on pressure exerted on a respective sensor such that an increased pressure results in an increase in voltage and is used by the cognitive analysis module to measure assembly of the apparatus; andwherein the one or more processors contains computer logic capable of one or more of generating an interactive report that includes one or more suggestions that a subject can do to improve performance of an associated task and a query option to determine whether a degree of difficulty and frequency in tests administered to the subject should be modified based on the received and analyzed data, or transferring the received and analyzed data to a source capable of generating the interactive report; wherein the interactive report is one or more of a cognitive assessment of a subject, a comparison of cohorts, or a determination of how the subject puts the apparatus together with a second apparatus based on one or more of a set of instructions, a target structure, an audio aid, or a visual aid; andwherein the cohorts are people with one or more of the same or similar: diseases, conditions, ages, medical histories, social demographics, experience levels, or locations.","11","15/622092","2017-06-14","2018-0360370","2018-12-20","10952662","2021-03-23","INTERNATIONAL BUSINESS MACHINES CORPORATION","Rick A. Hamilton, II | Clifford A. Pickover | Ninad D. Sathaye | Edgar A. Zamora Duran","","","","A61B-0005/4088","A61B-0005/4088 | A61B-0005/0053 | A61B-0005/0059 | A61B-0005/1125 | A61B-0005/165 | G06F-0003/005 | G06F-0003/162 | G06F-0003/165 | G06K-0009/00302 | A61B-0005/0051 | A61B-0005/168 | A61B-0005/4082 | G06K-0009/00248","A61B-005/00","A61B-005/00 | G06F-003/16 | G06F-003/00 | A61B-005/16 | A61B-005/11 | G06K-009/00","","","","","","4921013001024"
"US","US","P","B2","Electroencephalogram bioamplifier","A bioamplifier for analyzing electroencephalogram (EEG) signals is disclosed. The bioamplifier includes an input terminal for receiving an EEG signal from a plurality of sensors coupled to a user. The bioamplifier also includes an analogue-to-digital converter arranged to receive the EEG signal from the input terminal and convert the EEG signal to a digital EEG signal. A data processing apparatus within the bioamplifier is arranged to receive the digital EEG signal from the analogue-to-digital converter and programmed to process, in real time the digital EEG signal using a first machine learning model to generate a cleaned EEG signal having a higher signal-to-noise ratio than the digital EEG signal. The bioamplifier further includes a power source to provide electrical power to the analogue-to-digital converter and the data processing apparatus. The bioamplifier includes a housing that contains the analogue-to-digital converter, the data processing apparatus, the power source, and the sensor input.","1. A bioamplifier for analyzing electroencephalogram (EEG) signals, comprising: a first input terminal for receiving an EEG signal from a plurality of sensors coupled to a user;a second input terminal for receiving images from a camera system;an analogue-to-digital converter arranged to receive the EEG signal from the first input terminal and convert the EEG signal to a digital EEG signal;a data processing apparatus comprising one or more processors arranged to receive the digital EEG signal from the analogue-to-digital converter, the one or more processors being programmed to process, in real time, the digital EEG signal using a first machine learning model to generate a cleaned EEG signal having a higher signal-to-noise ratio than the digital EEG signal and to process the cleaned EEG signal using a second machine learning model to determine a selection, by the user, of one of a plurality of options presented to the user, wherein the first machine learning model employs an artifact recognition process to identify and remove artifacts from the digital EEG signal, andwherein the second machine learning model determines the selection by determining that, within a predetermined time period of the one of a plurality of options being presented to the user, an amplitude of the cleaned EEG signal exceeds a threshold amplitude value;an output arranged to receive output signals from the data processing apparatus;a power source arranged to provide electrical power to the analogue-to-digital converter and the data processing apparatus; anda housing containing the analogue-to-digital converter, the data processing apparatus, the power source, the output, and the input terminal,wherein the one or more processors are arranged to receive images of a machine readable code from the camera system, andwherein the one or more processors are programmed to process, in real time: a) the images of the machine readable code to identify an object associated with the machine readable code and viewed by the user, and b) the cleaned EEG signal using the second machine learning model to determine the selection by the user, wherein the selection is a selection of one of two options associated with the object viewed by the user.","20","15/855870","2017-12-27","2019-0192083","2019-06-27","10952680","2021-03-23","X DEVELOPMENT LLC","Sarah Ann Laszlo | Brian John Adolf | Gabriella Levine | Joseph R. Owens | Patricia Prewitt | Philip Edwin Watson","","","","A61B-0005/7225","A61B-0005/7225 | A61B-0005/0006 | A61B-0005/291 | A61B-0005/369 | A61B-0005/375 | A61B-0005/378 | A61B-0005/6803 | A61B-0005/7203 | A61B-0005/7267 | G06F-0003/015 | G06F-0003/04842 | G06N-0020/00 | A61B-0005/0022 | A61B-0005/7275 | A61B-2562/0209 | G06K-0007/10762 | G06K-0007/1417 | G06N-0003/02","A61B-005/378","A61B-005/378 | A61B-005/00 | G06N-020/00 | G06F-003/01 | G06F-003/0484 | A61B-005/291 | A61B-005/369 | A61B-005/375 | G06K-007/14 | G06N-003/02 | G06K-007/10","","","","","","4921013001042"
"US","US","P","B2","Mobile application to prompt physical action to measure physiologic response in implantable device","A user equipment that includes a touchscreen, and at least one processor configured to generate first timestamp data based upon detection of a first touch event on the touchscreen, and second timestamp data based upon detection of a second touch event on the touchscreen, for calculation by a medical device system of a patient-specific functional status parameter associated with a Sit-To-Stand performance test over a time segment inclusively bounded by a first time defined by the first timestamp data and a second time defined by the second timestamp data.","1. A system comprising: an implantable medical device (IMD) comprising: communication circuitry configured to establish a communication link and transfer data between the IMD intra-corpus and a computing device extra-corpus; andaccelerometer circuitry configured to generate a plurality of signals including a sagittal axis signal, a vertical axis signal and a transverse axis signal; andprocessing circuitry configured to: output, for display by the computing device, a first prompt for a user to indicate a start of a Sit-To-Stand test;output, for display by the computing device, a second prompt for the user to indicate a completion of the Sit-To-Stand test;receive first timestamp data and second timestamp data each one generated by the computing device based on user input;calculate a patient-specific functional status parameter associated with a Sit-To-Stand test from at least one of the sagittal axis signal, the vertical axis signal and the transverse axis signal over a time segment inclusively bounded by a first time defined by the first timestamp data and a second time defined by the second timestamp data, wherein the first timestamp data is generated by the computing device based on a user input indicating the start of the Sit-To-Stand test, and wherein the second timestamp data is generated by the computing device based on a user input indicating the completion of the Sit-to-Stand test; andin response to a command, activate the communication circuitry to transmit the patient-specific functional status parameter from the IMD to the computing device.","20","15/607945","2017-05-30","2018-0035956","2018-02-08","10952686","2021-03-23","MEDTRONIC, INC.","Bruce D. Gunderson | Eduardo N. Warman","","","","A61B-0005/747","A61B-0005/747 | A61B-0005/0031 | A61B-0005/0205 | A61B-0005/0215 | A61B-0005/0422 | A61B-0005/0538 | A61B-0005/1116 | A61B-0005/686 | A61B-0005/6882 | A61N-0001/08 | A61N-0001/37282 | G06F-0019/3418 | G06Q-0050/24 | G08B-0001/08 | A61B-0005/0809 | A61B-2560/0209 | A61B-2560/0462 | A61N-0001/36 | G06F-0019/00","A61B-005/00","A61B-005/00 | A61B-005/0205 | A61N-001/08 | A61N-001/372 | G06F-019/00 | G08B-001/08 | A61B-005/042 | A61B-005/11 | A61B-005/0215 | A61B-005/0538 | G06Q-050/24 | A61B-005/08 | A61N-001/36","","","","","","4921013001048"
"US","US","P","B2","Methods and apparatus for identifying potentially seizure-inducing virtual reality content","Methods and apparatus for identifying potentially seizure-inducing virtual reality content are disclosed herein. An example apparatus includes a neurological data collector to access first neurological response data collected from a user during exposure of the user to first media. The example apparatus includes a predictor to generate a prediction on a likelihood that a portion of second media will trigger an adverse neurological medical event in the user based the portion of the second media and the first neurological response data. The example apparatus includes a content modifier to modify the portion of the second media to create modified media in response to the prediction and output the modified media for presentation to the user.","1. An apparatus comprising: a neurological data collector to access first neurological response data collected from a user during exposure of the user to first virtual reality content;a predictor to generate a prediction that a first portion of second virtual reality content will trigger an adverse neurological medical event in the user based on (a) the first portion of the second virtual reality content, (b) the first neurological response data, (c) one or more of an age, a gender, or a medical history of the user, and (d) neurological event data associated with the adverse neurological medical event in respective ones of a plurality of users, the second virtual reality content to be presented to the user after the first virtual reality content;a content modifier to modify the first portion of the second virtual reality content to create modified virtual reality content in response to the prediction and cause presentation of the modified virtual reality content to the user; anda feedback analyzer to: determine whether the user is experiencing the adverse neurological medical event or whether the modified virtual reality content reduced a risk of the adverse neurological medical event in the user based on second neurological response data collected from the user during exposure of the user to the modified virtual reality content;in response to the adverse neurological medical event during exposure to the modified virtual reality content, cause the presentation of the modified virtual reality content to stop; andin response to the modified virtual reality content reducing the risk of the adverse neurological medical event in the user, store the modified virtual reality content for use in modifying a second portion of the second virtual reality content.","24","16/045370","2018-07-25","2018-0356887","2018-12-13","10955917","2021-03-23","INTEL CORPORATION","Nishanth Ramaprakash | Sreenidhi Koti Ananda Rao","","","","G06F-0003/015","G06F-0003/015 | A61B-0005/04842 | A61B-0005/4094 | A61B-0005/6803 | A61B-0005/6898 | A61B-0005/7275 | A61B-0005/746 | G06F-0001/163 | G06F-0003/011 | G06K-0009/00536 | G06K-0009/00744 | G06T-0005/009 | G06T-0011/60 | G16H-0040/63 | G16H-0050/20 | G16H-0050/30 | A61B-2560/0223 | A61M-2021/005 | A61M-2205/507 | A61M-2230/10 | G06F-2203/011 | G06K-2009/00939 | G06T-2207/20208","G06F-003/01","G06F-003/01 | G06K-009/00 | A61B-005/00 | A61B-005/0484 | G06F-001/16 | G16H-050/30 | G16H-040/63 | G16H-050/20 | G06T-005/00 | G06T-011/60 | A61M-021/00","","","","","","4921013004262"
"US","US","P","B2","Automatic home screen determination based on display device","A mobile computing device, such as a smartphone or tablet device, can be coupled to any one or more of multiple display devices at any given time. The mobile computing device allows the home screen displayed on the display devices to be dynamic, changing based on which display device the mobile computing device is coupled to. The mobile computing device can also be coupled to multiple different display devices concurrently, and different home screens are concurrently displayed on those different display devices. User inputs changing the information displayed on the home screen can also be received, and a record of the change is maintained by the computing device so the changed home screen for a particular display device is again displayed to the user the next time the mobile computing device is coupled to that display device.","1. A method implemented in a mobile computing device, the method comprising: establishing a communication coupling to a display device;responsive to establishing the communication coupling to the display device, determining a type of the display device;determining that the type of the display device is of a same type as an additional type of a different display device;selecting a customized home screen based at least on an association between the customized home screen and the additional type of the different display device, wherein the customized home screen was customized during a previous communication coupling between the mobile computing device and the different display device by rearranging representations of functionality on a default home screen; andcausing display of the customized home screen on the display device.","20","16/102504","2018-08-13","2018-0349001","2018-12-06","10956008","2021-03-23","MICROSOFT TECHNOLOGY LICENSING, LLC","Issa Yousef Khoury | Petteri Jussinpoika Mikkola | Abolade Gbadegesin","","","","G06F-0003/04845","G06F-0003/04845 | G06F-0003/1423 | H04N-0021/4108 | H04N-0021/4122 | H04N-0021/41407 | H04N-0021/4312 | H04N-0021/47 | H04N-0021/4858 | H04N-0021/4882 | H04N-0021/812","G06F-003/0484","G06F-003/0484 | A61M-005/172 | A61B-005/00 | G06F-003/0481 | G06F-003/0488 | G06F-003/14 | H04N-021/41 | H04N-021/414 | H04N-021/431 | H04N-021/47 | H04N-021/485 | H04N-021/488 | H04N-021/81","","","","","","4921013004352"
"US","US","P","B2","Neural networks for biometric recognition","Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for training an encoder neural network having multiple encoder neural network parameters. The encoder neural network is configured to process a biometric data sample in accordance with current values of encoder neural network parameters to generate as output an embedded representation of the biometric data sample. The embedded representation includes: (i) an inter-class embedded representation, and (ii) an intra-class embedded representation that is different than the inter-class embedded representation.","1. A method for training an encoder neural network having a plurality of encoder neural network parameters and being configured to process a biometric data sample in accordance with current values of encoder neural network parameters to generate as output an embedded representation of the biometric data sample, wherein the embedded representation of the biometric data sample defines a set of features representing the biometric data sample, the method comprising: obtaining a positive biometric data sample characterizing an identity of a first person and a negative biometric data sample characterizing an identity of a second person, wherein the identity of the first person is different than the identity of the second person;processing the positive biometric data sample and the negative biometric data sample using the encoder neural network and in accordance with the current values of the encoder neural network parameters to generate: (i) an embedded representation of the positive biometric data sample that defines a set of features representing the positive biometric data sample, and (ii) an embedded representation of the negative biometric sample that defines a set of features representing the negative biometric data sample;determining a gradient of a loss function with respect to the encoder neural network parameters, wherein the loss function includes a first term that encourages a similarity between: (i) a specified proper subset of the set of features defined by the embedded representation of the positive biometric data sample and (ii) a corresponding specified proper subset of the set of features defined by the embedded representation of the negative biometric data sample, to be less than a similarity between: (i) the set of features defined by the embedded representation of the positive biometric data sample and (ii) the set of features defined by the embedded representation of the negative biometric data sample; andadjusting the current values of the encoder neural network parameters using the gradient of the loss function.","20","16/183274","2018-11-07","2020-0143137","2020-05-07","10956704","2021-03-23","ADVANCED NEW TECHNOLOGIES CO., LTD.","Vikas Gottemukkula","","","","G06K-0009/00026","G06K-0009/00026 | A61B-0005/1171 | G06K-0009/00885 | G06N-0003/04 | G06N-0003/08 | G06F-0021/32 | G06K-2009/00939","G06K-009/00","G06K-009/00 | G06N-003/08 | A61B-005/1171 | G06N-003/04 | G06F-021/32","","","","","","4921013005044"
"US","US","P","B2","Method and device for determining action or action part","The present application provides methods and devices for determining an action or an action part, and generally relates to the field of wearable devices. A method disclosed herein comprises: in response to detecting a motion of a first part of a body of a user, acquiring, using a photoelectric sensor, target Doppler measurement information of the first part or a second part corresponding to the first part; determining target velocity related information corresponding to the target Doppler measurement information; and determining the first part or the action according to the target velocity related information and reference information, wherein the target velocity related information comprises target blood flow velocity information or target blood flow information. The methods and devices provide a new scheme for recognizing an action and/or an action part.","1. A method for determining an action or an action part, comprising: in response to detecting a motion of a first part of a body of a user, acquiring, using a photoelectric sensor, target Doppler measurement information of the first part or a second part corresponding to the first part;determining target velocity related information corresponding to the target Doppler measurement information; anddetermining the first part or the action according to the target velocity related information and reference information,wherein the target velocity related information comprises target blood flow velocity information or target blood flow information.","26","16/730478","2019-12-30","2020-0142476","2020-05-07","10948979","2021-03-16","BEIJING ZHIGU RUI TUO TECH CO., LTD.","Yuanchun Shi | Yuntao Wang | Chun Yu | Lin Du","2015-10069988","CN","2015-02-10","G06F-0003/011","G06F-0003/011 | A61B-0005/0261 | A61B-0005/6826 | A61B-0008/06 | A61B-0008/488 | G06F-0001/163 | A61B-0008/5223 | A61B-2503/12 | G06F-0003/017 | G06K-0009/0053 | G06K-0009/00355 | G06K-0009/00543","G06F-003/01","G06F-003/01 | A61B-005/026 | A61B-008/06 | A61B-008/08 | A61B-005/00 | G06F-001/16 | G06K-009/00","","","","","","4921012004311"
"US","US","P","B2","Image classification by brain computer interface","A method of classifying an image is disclosed. The method comprises: applying a computer vision procedure to the image to detect therein candidate image regions suspected as being occupied by a target; presenting to an observer each candidate image region as a visual stimulus, while collecting neurophysiological signals from a brain of the observer; processing the neurophysiological signals to identify a neurophysiological event indicative of a detection of the target by the observer; and determining an existence of the target in the image is based, at least in part, on the identification of the neurophysiological event.","1. A method of classifying an image, comprising: presenting the image to an observer as a visual stimulus, while collecting neurophysiological signals from a brain of said observer;digitizing said neurophysiological signals to provide neurophysiological data;simultaneously processing the image and said neurophysiological data using a convolutional neural network (CNN) to identify a correlation between a computer vision detection of a target in the image and a neurophysiological event indicative of a detection of the target by said observer; anddetermining an existence of said target in the image based said identified correlation;wherein said CNN comprises a first convolutional neural subnetwork (CNS) configured for receiving and processing said neurophysiological data, a second CNS configured for receiving and processing the image, and a shared subnetwork having a neural network layer receiving and combining outputs from both said first CNS and said second CNS.","12","16/413651","2019-05-16","2019-0294915","2019-09-26","10948990","2021-03-16","INNEREYE LTD.","Amir B. Geva | Leon Y. Deouell | Sergey Vaisman | Omri Harish | Ran El Manor | Eitan Netzer | Shani Shalgi","239191","IL","2015-06-03","G06F-0003/015","G06F-0003/015 | A61B-0005/04017 | A61B-0005/04842 | A61B-0005/1103 | A61B-0005/486 | A61B-0005/7264 | G06F-0003/01 | G06K-0009/00496 | G06K-0009/00536 | G06K-0009/00671 | G06K-0009/2054 | G06K-0009/4628 | G06K-0009/627 | G06K-0009/6247 | G06K-0009/6272 | G06N-0003/0454 | G06N-0003/088 | G16H-0020/70 | G16H-0030/40 | A61B-0005/04001 | G06N-0003/04 | G06N-0003/08 | G06N-0007/005 | G06N-0020/10","A61B-005/00","A61B-005/00 | A61B-005/04 | A61B-005/11 | G06K-009/62 | G06F-003/01 | G06N-003/04 | G06N-003/08 | G06K-009/00 | G16H-020/70 | G16H-030/40 | A61B-005/0484 | G06K-009/20 | G06K-009/46 | G06N-007/00 | G06N-020/10","","","","","","4921012004322"
"US","US","P","B2","Electrocardiogram-based face recognition security system and method using smart watch","The present invention relates to an electrocardiogram-based face recognition security system and method using a smart watch, and more particularly, to a security system and a method for enhancing security by simultaneously performing biometric human identification based on an electrocardiogram and biometric human identification using face recognition for user identification in a portable PTT communication device such as smart watch.","1. An electrocardiogram-based face recognition security system comprising: a plurality of user terminals for enabling biometric human identification (HID) using electrocardiogram and face recognition and push-to-talk (PTT) communication, wherein a first user terminal, among the plurality of user terminals, is a smart watch providing a PTT communication service through interworking with a server including an electrocardiogram database and a face recognition database; anda server configured to receive electrocardiogram data, face recognition data and a PTT message from the first user terminal of the plurality of user terminals, transmit the electrocardiogram data to the electrocardiogram database, transmit the face recognition data to the face recognition database, transmit the PTT message to a second user terminal, among the plurality of user terminals, which enters a corresponding PTT channel when the biometric HID is identified using the electrocardiogram database and the face recognition database.","8","16/233668","2018-12-27","2020-0202112","2020-06-25","10949651","2021-03-16","DODOTDO CO., LTD","In Gyeom Kim | So Yeong Sim","10-2018-0164955","KR","2018-12-19","G06K-0009/00288","G06K-0009/00288 | G06F-0021/32 | G06K-0009/00248 | G06K-0009/00302 | G06K-0009/00885 | A61B-0005/0402 | A61B-0005/1176 | G06K-2009/00939","G06K-009/00","G06K-009/00 | G06F-021/32 | A61B-005/1171 | A61B-005/0402","","","","","","4921012004977"
"US","US","P","B2","Health condition monitoring and action","A method, computer program product, and system includes a processor(s) continuously monitoring a current condition of a user of an Internet of Things (IoT) device, via the IoT device. The processor(s) determines that the current condition indicates an issue with the well-being of the user. The processor(s) identifies upcoming event(s), within a given temporal period, in which the user will participate. The processor(s) determines that the current condition of the user will negatively impact at least one event of the upcoming event(s). The processor(s) generates action(s) comprising a change to the event, where the action(s) is executed by interacting with one or more applications deployed on the one or more target computer resources (accessible to the one or more processors via a network connection). The processor(s) automatically executes the at least one action on the one or more target computer resources.","1. A computer-implemented method, comprising: providing, by a cognitive engine comprising one or more processors, to an Internet of Things (IoT) device, an object, wherein the object enables the IoT device to pass data obtained by the IoT device to the cognitive engine, over a secure connection on a trusted network, wherein the IoT device automatically installs the object;based on the IoT device automatically installing the object, continuously monitoring, by one or more processors, a current condition of a user of the IoT device, via the IoT device, wherein the continuously monitoring comprises utilizing the object provided by the cognitive engine to passively monitor the current condition by obtaining data from the IoT device, wherein the data comprises health indicators and sleep activity of the user;identifying, by the one or more processors, over a given period of time, based on the continuously monitoring, wellness patterns of the user in the data;obtaining, by the one or more processors, from a computing resource in the trusted network, demographic data comprising expected health patterns for a demographic relevant to the user;generating, by the one or more processors, based on identifying the patterns and obtaining the demographic data, an electronic model representing an expected health condition of the user;continuously analyzing, by the one or more processors, the continuously obtained data to identify deviations from the expected health condition of the user;determining, by the one or more processors, at a given time, that the current condition indicates a given deviation from the expected health condition of the user;identifying, by the one or more processors, one or more upcoming events, within a given temporal period, in which the user will participate, wherein the identifying comprises: identifying, by the one or more processors, a computer resource of one or more target computer resources comprising a scheduling program, wherein the trusted network comprises the one or more target resources;querying, by the one or more processors, over a network connection, the scheduling program, to obtain events relevant to the user; andanalyzing, by the one or more processors, the events relevant to the user to identify the one or more upcoming events;determining, by the one or more processors, that the current condition of the user will negatively impact at least one event of the more one or more upcoming events;generating, by the one or more processors, at least one action comprising a change to the at least one event to mitigate the negative impact, wherein the at least one action is executed by interacting with one or more applications deployed on the one or more target computer resources, wherein the one or more target computer resources are accessible to the one or more processors via a network connection;automatically executing, by the one or more processors, in real-time, the at least one action on the one or more target computer resources to implement the change in the one or more target computer systems in advance of the negatively impacted at least one event of the more one or more upcoming events, wherein the change comprises implementing a change to data in the scheduling program; andautomatically notifying, by the one or more processors, a given contact of the user of the given deviation, wherein the given contact is one of one or more contacts stored in the IoT device as a trusted party, wherein the notifying is through an interface of a personal device of the contact, and wherein the given contact is notified based on being a contact of the one or more contacts in closest physical proximity to the user at the given time.","6","15/855093","2017-12-27","2019-0156296","2019-05-23","10949811","2021-03-16","INTERNATIONAL BUSINESS MACHINES CORPORATION","Fang Lu | Joseph Lam | Trudy L. Hewitt | William K. Wentworth","","","","G06Q-0010/1095","G06Q-0010/1095 | A61B-0005/024 | A61B-0005/1118 | A61B-0005/165 | A61B-0005/168 | A61B-0005/4815 | G06Q-0010/1097 | G06Q-0050/01","G06Q-010/00","G06Q-010/00 | G06Q-050/00 | A61B-005/00 | G06Q-010/10 | A61B-005/024 | A61B-005/11 | A61B-005/16","","","","","","4921012005136"
"US","US","P","B1","System, computer-readable storage medium and method of deep learning of texture in short time series","A computer-readable storage medium storing program instructions to perform a method of classification of short time series in order to detect a neurodegenerative disorder. The method includes receiving a plurality of sensor data collected from subjects with and without the neurodegenerative disorder over a period of a few seconds as the short time series, generating phase-space vectors from the plurality of sensor data in which each vector is a state of a dynamical system in space and time, transforming the phase-space vectors into a grayscale image representing recurrences of a state-space vector in the same area of the phase space, extracting temporal texture features of the grayscale image to obtain a multi-dimensional time series; inputting the multidimensional time series, without the grayscale image, to the Long Short Term Memory (LSTM) network, and classifying, by the LSTM network, the plurality of the sensor data as the neurodegenerative disorder or not.","1. A method of classification of short time series in order to detect a neurodegenerative disorder, comprising: receiving, by circuitry, a plurality of sensor data collected from subjects with and without the neurodegenerative disorder over a period of a few seconds as the short time series;generating, by the circuitry, phase-space vectors from the plurality of sensor data in which each vector is a state of a dynamical system in space and time;transforming, by the circuitry, the phase-space vectors into a grayscale image representing recurrences of a state-space vector in the same area of the phase space, wherein an intensity distribution of the grayscale image takes values in the range of 0 to 1;for every pixel of the grayscale image, selecting, by the circuitry, a local window, where the respective pixel is at a center of the local window;extracting, by the circuitry, temporal texture features of each window to obtain a multi-dimensional time series of D texture dimensions;inputting, by the circuitry, the multidimensional time series, without the grayscale image, to a Long Short Term Memory (LSTM) network;training the LSTM network by performing in an LSTM layer, inputting to a first LSTM block a first time step of the time series of D texture dimensions and determining a first output and an updated cell state,inputting to a next LSTM block a next time step of the time series of D texture dimensions, a current output and a current cell state and determining a next output and a next updated cell state, andcontinuing the inputting and the determining to LSTM blocks for each time step of the time series of D texture dimensions to output a final cell state;andclassifying, by the LSTM network, the plurality of the sensor data as the neurodegenerative disorder or not.","17","16/932281","2020-07-17","","","10950352","2021-03-16","PRINCE MOHAMMAD BIN FAHD UNIVERSITY","Tuan Duc Pham","","","","G16H-0050/30","G16H-0050/30 | A61B-0005/1128 | A61B-0005/4082 | G06F-0003/011 | G06F-0003/017 | G06K-0009/6232 | G06K-0009/6268 | G06N-0003/0436 | G06N-0003/08 | G16H-0030/40 | G06T-2207/20021","G06K-009/00","G06K-009/00 | G16H-050/30 | G16H-030/40 | G06K-009/62 | G06N-003/08 | A61B-005/00 | A61B-005/11 | G06F-003/01 | G06N-003/04","","","","","","4921012005673"
"US","US","P","B2","Protective medical device faceplate","Devices are provided that include a faceplate for a medical device. In embodiments, an identifier chip adapted to be affixed to an exterior portion of a faceplate provides a unique identifier for the faceplate. Accordingly, the identifier chip enables tracking and monitoring of an associated medical device. And, in embodiments, the faceplate includes a visual communication alert indicator to enable the faceplate to provide visual cues a user. As such, the status of the medical device and the faceplate can be easily communicated to the user. Methods to use the faceplate are also provided.","1. A medical device, the device comprising: a housing and a first communication port;the housing configured to removably affix a faceplate, wherein the faceplate comprises at least one processor, computer readable memory, a power supply, a wireless receiver, a wireless transmitter, and a second communication port, wherein the first communication port is adapted to communicate with the second communication port.","19","15/703526","2017-09-13","2018-0001017","2018-01-04","10940262","2021-03-09","CERNER INNOVATION, INC.","Alan Mark Portnoy","","","","A61M-0005/142","A61M-0005/142 | A61B-0090/96 | A61B-0090/98 | A61M-0005/1415 | G06K-0007/10297 | G16H-0010/60 | G16H-0040/20 | A61M-0005/14 | A61M-2005/14208 | A61M-2205/3584 | A61M-2205/6054 | A61M-2205/6072","G06F-017/00","G06F-017/00 | A61M-005/142 | G06K-007/10 | A61B-090/96 | A61B-090/98 | G16H-010/60 | G16H-040/20 | A61M-005/14","","","","","","4921011001244"
"US","US","P","B2","Prediction of the attention of an audience during a presentation","A method for predicting attention of an audience during a presentation by a speaker. The method includes: measuring vocal or gestural characteristics of the speaker of the presentation in progress and/or of characteristics of content of the presentation in progress; measuring a parameter of duration or of occurrence of the measured characteristics; consulting a database having a correspondence between vocal or gestural speaker characteristics and/or presentation content characteristics, parameters of duration or of occurrence which relate to these characteristics and information relating to the evolution of the attention level for these characteristics and these parameters and recovering the information relating to the evolution of the attention level corresponding to the measurements performed; and presenting to the speaker, a prediction of attention level on the basis of the information recovered relating to the evolution of the attention level. Also provided are a prediction device, learning phase and a learning device.","1. A method for learning information on a change in the attention level of at least one presentation, wherein, the method comprises the following acts performed by a the learning device: collecting attention level measurements from at least one audience for a set of presentations, a presentation being given by at least one speaker;indexing the presentations of the set by the collected attention level measurements;indexing the presentations of the set by measurements of vocal or gestural characteristics of the speakers and/or measurements of characteristics of content of the presentations;synchronizing the respective indexations so as to determine associations between characteristics and attention level measurements for the presentations of the set;determining the change in the attention levels by analyzing associations determined for a set of characteristics or groups of characteristics and in accordance with at least one parameter of duration or of occurrence of these characteristics;recording, in a database, correspondences between the vocal or gestural characteristics of the speaker and/or the characteristics of presentation content, the duration or occurrence parameters linked to these characteristics and information in relation to the change in the attention level of audience for these characteristics and these parameters.","12","16/329429","2017-08-31","2019-0212811","2019-07-11","10942563","2021-03-09","ORANGE","Ghislain Moncomble | Patrick Rondet","2016-058105","FR","2016-09-01","G06F-0003/011","G06F-0003/011 | A61B-0005/1128 | A61B-0005/168 | A61B-0005/4803 | G06F-0003/017 | G06K-0009/00771 | G06N-0020/00 | G09B-0005/062 | G09B-0019/04 | G10L-0025/48 | G10L-0025/63 | A61B-2503/12 | G06F-2203/011 | G06K-0009/00302 | G16H-0050/30","G06F-003/01","G06F-003/01 | A61B-005/11 | G09B-019/04 | A61B-005/16 | A61B-005/00 | G10L-025/63 | G09B-005/06 | G06K-009/00 | G06N-020/00 | G10L-025/48 | G16H-050/30","","","","","","4921011003524"
"US","US","P","B2","User identification via motion and heartbeat waveform data","The disclosure relates to methods, devices, and systems to identify a user of a wearable fitness monitor using data obtained using the wearable fitness monitor. Data obtained from motion sensors of the wearable fitness monitor and data obtained from heartbeat waveform sensors of the wearable fitness monitor may be used to identify the user.","1. A wearable device comprising: one or more heartbeat waveform sensor; andone or more processors configured to: receive heartbeat waveform data of a wearer of the wearable device generated by the one or more heartbeat waveform sensors;determine a heartbeat waveform signature using the heartbeat waveform data;compare the heartbeat waveform signature to one or more reference features of a reference user;determine, based on the comparison, the identity of the wearer of the wearable device with reference to the reference user; andbased on the determined identity of the wearer, perform one or more operations using the wearable device,wherein:the determined identity of the wearer is not the reference user, andthe one or more operations comprise discrediting a fitness metric obtained for the wearer via the wearable device or preventing the wearable device from authenticating a transaction.","16","16/663166","2019-10-24","2020-0167004","2020-05-28","10942579","2021-03-09","Fitbit, Inc.","Shelten Gee Jao Yuen | James Park | Atiyeh Ghoreyshi | Anjian Wu","","","","G06F-0003/017","G06F-0003/017 | A61B-0005/0002 | A61B-0005/02416 | A61B-0005/02438 | A61B-0005/0402 | A61B-0005/11 | A61B-0005/112 | A61B-0005/117 | A61B-0005/1112 | A61B-0005/1118 | A61B-0005/1123 | A61B-0005/441 | A61B-0005/681 | A61B-0005/6802 | A61B-0005/725 | A61B-0005/7257 | A61B-0005/7264 | A61B-0005/7267 | A61B-0005/7271 | A61B-0005/742 | A61B-0005/7475 | A63B-0024/0062 | G01C-0022/006 | G01P-0001/02 | G01P-0013/00 | G01S-0019/00 | G01S-0019/19 | G06F-0001/163 | G06F-0021/32 | G06K-0009/00342 | G06K-0009/00885 | G06Q-0050/01 | G09B-0005/02 | A61B-0005/0022 | A61B-0005/0024 | A61B-0005/0404 | A61B-0005/1032 | A61B-0005/1172 | A61B-0005/6844 | A61B-2560/0223 | A61B-2562/0219 | A63B-0069/36 | G06K-0009/0053 | G06K-0009/00335 | G06K-2009/00939","G06F-003/01","G06F-003/01 | G01S-019/19 | G06F-001/16 | G06K-009/00 | G01S-019/00 | G06Q-050/00 | A61B-005/11 | A61B-005/00 | A63B-024/00 | G06F-021/32 | G09B-005/02 | A61B-005/024 | A61B-005/0402 | G01C-022/00 | G01P-001/02 | G01P-013/00 | A61B-005/117 | A63B-069/36 | A61B-005/0404 | A61B-005/103 | A61B-005/1172 | G16H-050/70","","","","","","4921011003540"
"US","US","P","B2","Monitoring compliance with medical protocols based on occlusion of line of sight","In various embodiments, an image monitoring device (100) may acquire (200) video data that captures a medical room. Image processing may be performed on the acquired image data to perform the following tasks: identifying (202) at least one medical apparatus (402) in the medical room; identifying (204) a medical personnel in the medical room; detecting (206) that the medical personnel has occluded a line of sight between the image monitoring device and the at least one medical apparatus; and determining (206) an amount of time that the medical personnel occludes the line of sight. In various embodiments, the amount of time may be compared to one or more thresholds set forth by the first medical protocol. An alert may be triggered (208) in response to a determination that the amount of time fails to satisfy a first threshold of the one or more thresholds.","1. A computer-implemented method, comprising: acquiring, using a single image sensor, video data that captures a medical room;performing, using an image monitoring device including an image input and one or more processors, image processing on the acquired video data to determine whether it is possible that a medical personnel has complied with a first medical protocol, wherein the image processing includes: identifying at least one medical apparatus in the medical room in a line of sight with the image monitoring device;identifying a medical personnel in the medical room;detecting that the medical personnel has occluded the line of sight between the image monitoring device and the at least one medical apparatus; anddetermining an amount of time that the medical personnel occludes the line of sight;comparing the amount of time that the medical personnel occludes the line of sight to a first predefined time period corresponding to a minimum time required to avoid a false detection;comparing the amount of time that the medical personnel occludes the line of sight to a second predefined time period corresponding to a minimum time required for a medical personnel to accomplish the first medical protocol;disregarding the amount of time that the medical personnel occludes the line of sight when the amount of time that the medical personnel occludes the line of sight is less than the first predefined time period; andtriggering an alert in response to a determination that the amount of time that the medical personnel occludes the line of sight is less than the second predefined time period.","14","16/083949","2017-03-28","2020-0311432","2020-10-01","10943124","2021-03-09","KONINKLIJKE PHILIPS N.V.","Harald Greiner","2016-163547","EP","2016-04-01","G06K-0009/00718","G06K-0009/00718 | A61B-0005/1176 | G06K-0007/10722 | G06K-0007/1413 | G06K-0007/1417 | G06K-0009/00288 | G06K-0009/00362 | G06K-0009/00771 | G06K-0009/46 | G08B-0021/182 | G16H-0040/20 | G06K-2209/01 | G06K-2209/057","G06K-009/00","G06K-009/00 | G16H-040/20 | A61B-005/1171 | G06K-007/10 | G06K-007/14 | G06K-009/46 | G08B-021/18","","","","","","4921011004081"
"US","US","P","B2","Electronic system to romantically match people by collecting input from third parties","Methods and apparatus for a social-oriented computer system for matching daters. Daters establish friend connections with other users, including non-daters. After identifying candidate pairings of daters, the system solicits the opinions of users including the daters, their friends, and matchmakers. The system may analyze the opinions to determine whether a match would result in the satisfaction of both daters. If so, the system matches the daters, inviting them to communicate. The system may gather feedback on the match to help calibrate future matching.","1. A method of electronically evaluating a potential romantic pairing of daters, wherein the method comprises:selecting one or more evaluators;collecting and recording evaluations from the one or more evaluators, whereineach of the evaluations is one of an opinion by one evaluator on a quality ofthe potential romantic pairing or suitability of one of the daters for the other;recording a history of the evaluations;recording at least one evaluator-dater relationship;calculating a score representing the quality of the potential romantic pairing or suitability of one of the daters for the other, wherein calculation of the score is based on the history of the evaluations and the evaluator-dater relationship;calculating a certainty of the score, wherein calculation of the certainty of the score is determined by a number of evaluations received and the evaluator-dater relationship;wherein if the certainty of the score is below a predetermined threshold, collecting and recording one or more additional evaluations; andwherein if the certainty of the score and the score satisfy a predetermined condition indicating a high certainty of the potential romantic pairing being high quality, issuing a match between daters, wherein the matched daters are connected by a system and invited to communicate.","6","15/476953","2017-03-31","2017-0300935","2017-10-19","10943243","2021-03-09","SOCIAL DATA SCIENCES, INC.","Daniel C. Herbst | Sean P. Finegan","","","","G06Q-0030/0201","G06Q-0030/0201 | A61B-0005/167 | G06F-0016/00 | G06Q-0050/01 | G06Q-0050/10 | G06Q-0030/02","G06F-017/00","G06F-017/00 | G06Q-030/02 | A61B-005/16 | G06Q-050/00 | G06Q-050/10 | G06F-016/00","","","","","","4921011004197"
"US","US","P","B2","Diagnosis of migraine via expert system","Systems and methods are provided for identifying, monitoring, and treating migraines and migraineurs. An electroencephalogram (EEG) of a patient is obtained. The EEG comprises a plurality of EEG signals. At least two features are extracted of a network feature across at least one pair of the plurality of EEG signals in the alpha frequency band, a feature derived from a signal decomposition of at least one EEG signal, and a feature representing the power spectrum density of at least one EEG signal. The patient is classified into one of a plurality of classes, each representing one of the presence of migraine symptoms, a response to migraine treatment, a type of migraineur, a current stage of a migraine, and a likelihood that the patient is a migraineur, according to the extracted at least two features.","1. A method comprising: obtaining an electroencephalogram (EEG) of a patient, the EEG comprising a plurality of EEG signals, each of the plurality of EEG signals representing an output of an associated electrode of a plurality of electrodes associated with an EEG device;extracting at least two features of:1. a network feature across at least one pair of the plurality of EEG signals;2. a feature derived from a signal decomposition one of the plurality of EEG signals; and3. a feature representing the power spectrum density one of the plurality of EEG signals; andclassifying the patient into one of a plurality of classes, each class of the plurality of classes representing one of the presence of migraine symptoms, a response to migraine treatment, a type of migraineur, a current stage of a migraine, and a likelihood that the patient is a migraineur, according to the extracted at least two features.","14","15/904956","2018-02-26","2018-0242919","2018-08-30","10932725","2021-03-02","HEADACHE SCIENCES INCORPORATED","Mark S. Doidge | Mario Garingo | Farhang Sahba","","","","A61B-0005/7267","A61B-0005/7267 | A61B-0005/048 | A61B-0005/04014 | A61B-0005/0482 | A61B-0005/4824 | A61B-0005/4839 | G06F-0017/14 | G16H-0050/20 | A61B-0005/726 | G06N-0005/047 | G06N-0020/00","A61B-005/04","A61B-005/04 | A61B-005/00 | A61B-005/048 | A61B-005/0482 | G06F-017/14 | G16H-050/20 | G06N-005/04 | G06N-020/00","","","","","","4921010001052"
"US","US","P","B2","Systems and methods for weight management including virtual reality, artificial intelligence, accountability integration, and biofeedback","A minimally invasive system and method for providing weight loss by inducing the feeling of satiety whereby reconditioning experiences are combined with an intragastric device that is inserted into the gastric lumen via the esophagus and an external magnetic device is used as needed to magnetically attract the intragastric device towards the inner wall of the stomach.","1. A method of stimulating the anterior gastric wall comprising the steps of: positioning an external magnet in close enough proximity to an intragastric device deployed in a stomach of a patient to create a sufficient magnetic pulling force between the external magnet and the intragastric device to move the intragastric device towards, and impart stimulating force upon an anterior gastric wall;and providing a visual interface, wherein the visual interface displays a virtual reality reconditioning experience to the patient as part of a treatment, wherein the reconditioning experience is a primary purpose of the treatment together with the intragastric device.","13","16/809431","2020-03-04","2020-0253769","2020-08-13","10932936","2021-03-02","Appetec, Inc.","Shahriar Sedghi","","","","A61F-0005/0036","A61F-0005/0036 | A61B-0005/486 | A61B-0005/7435 | A61B-0005/7455 | G06F-0003/011 | A61B-2017/00876","A61F-005/00","A61F-005/00 | A61B-005/00 | G06F-003/01 | A61B-017/00","","","","","","4921010001260"
"US","US","P","B2","Method and device for determining head movement according to electrooculographic information","Methods and devices for determining a head movement are provided. A method comprises: acquiring, in response to a head movement performed by a user, a piece of electrooculographic information of the user; and determining information related to the head movement according to the piece of electrooculographic information and at least one piece of reference information. The head movement can be identified according to electrooculographic information. For some devices integrated with electrooculographic sensors, the electrooculographic sensor can be reused to collect the electrooculographic information, and thereby reduce implementation costs.","1. A method for determining a head movement, comprising: acquiring, by a system comprising a processor, in response to a head movement performed by a user, a piece of electrooculographic information of the user; anddetermining, by the system, movement information related to the head movement according to the piece of electrooculographic information and at least one piece of reference information, wherein the movement information comprises a type of the head movement, and the determining comprises: determining a target waveform from the piece of electrooculographic information, anddetermining the type of the head movement according to the target waveform and at least one reference waveform, wherein the reference waveform is a waveform that corresponds to a corresponding head movement obtained through pre-training during a training phase.","22","15/567954","2016-04-20","2018-0144191","2018-05-24","10936052","2021-03-02","BEIJING ZHIGU RUI TUO TECH CO., LTD","Hao Liu","2015-10185762","CN","2015-04-20","G06F-0003/012","G06F-0003/012 | A61B-0003/113 | A61B-0005/0496 | A61B-0005/1123 | A61B-0005/6814 | G06F-0003/013 | G06F-0017/15 | G06K-0009/0053 | G06K-0009/00335 | G06K-0009/00536 | G06K-0009/00604 | A61B-0005/6803 | G02B-2027/0178 | G02B-2027/0187","G06F-003/01","G06F-003/01 | A61B-005/00 | A61B-003/113 | A61B-005/0496 | A61B-005/11 | G06F-017/15 | G06K-009/00 | G02B-027/01","","","","","","4921010004355"
"US","US","P","B2","Vehicle device","A vehicle device receives image data transmitted from a compartment camera photographing a driver. The vehicle device analyzes the image data received from the compartment camera, and detects a heart rate of the driver. The vehicle device wirelessly communicates with one or more wearable devices possessed by one or more people who ride on a vehicle. The vehicle device identifies a wearable device possessed by the driver by comparing biometric information transmitted from the one or more wearable devices with the heart rate of the driver.","1. A vehicle device comprising a processor configured to: receive image data transmitted from a compartment camera that photographs a driver;analyze the image data received from the compartment camera;detect a heart rate of the driver based on the image data;wirelessly communicate with one or more wearable devices possessed by one or more people who ride on a vehicle;determine whether the one or more wearable devices include a wearable device that has been registered together with image data of a person;identify, when the process does not determine that the one or more wearable devices include the wearable device that has been registered, a wearable device, among the one or more wearable devices, possessed by the driver by comparing biometric information transmitted from the one or more wearable devices with the heart rate of the driver, and register the identified wearable device together with the image data of the driver;identify, when the processor determines that the one or more wearable devices include the wearable device that has been registered, that the person registered with the registered wearable device is seated on a driver seat by comparing the registered image data of the person with the image data transmitted from the compartment camera; andactivate, when the processor identifies the wearable device possessed by the driver or identifies that the person registered with the registered wearable device is seated on the driver seat, an application program using the biometric information of the driver.","4","16/211300","2018-12-06","2019-0108407","2019-04-11","10936890","2021-03-02","DENSO CORPORATION","Takafumi Okayasu","2016-115310","JP","2016-06-09","G06K-0009/00885","G06K-0009/00885 | A61B-0005/0013 | A61B-0005/024 | A61B-0005/02416 | A61B-0005/02438 | A61B-0005/1176 | A61B-0005/165 | A61B-0005/18 | A61B-0005/6801 | A61B-0005/6893 | G06F-0003/01 | G06F-0003/011 | G06F-0016/5866 | G06K-0009/00838 | G06K-0009/00845 | H04M-0011/00 | H04W-0004/48 | B60R-0016/037 | G06F-2203/011 | G06K-0009/00255 | G06K-0009/00315 | G06K-2009/00939","G06K-009/00","G06K-009/00 | A61B-005/024 | A61B-005/00 | A61B-005/1171 | A61B-005/16 | A61B-005/18 | H04M-011/00 | G06F-003/01 | H04W-004/48 | G06F-016/58 | B60R-016/037","","","","","","4921010005187"
"US","US","P","B2","Information processing apparatus, system, and method for displaying bio-information or kinetic information","An information processing apparatus includes a bio-information obtaining unit configured to obtain bio-information of a subject; a kinetic-information obtaining unit configured to obtain kinetic information of the subject; and a control unit configured to determine an expression or movement of an avatar on the basis of the bio-information obtained by the bio-information obtaining unit and the kinetic information obtained by the kinetic-information obtaining unit and to perform a control operation so that the avatar with the determined expression or movement is displayed.","1. An information processing apparatus, the apparatus comprising: at least one processor configured to perform a method comprising: determining a user state of a first user based on an image of the first user captured by an image sensor and a movement of the first user detected by an acceleration sensor and/or a gyro sensor, wherein the user state of the first user comprises an emotional state of the first user; andcontrolling a real-time display of an avatar image of the first user, representing the user state of the first user, and an avatar image of a second user, representing a user state of the second user, on a display, wherein controlling the real-time display of the avatar image of the first user and the avatar image of the second user comprises determining the avatar image of the first user based on the emotional state of the first user and determining the avatar image of the second user based on an emotional state of the second user,wherein the user state of the second user is determined via an external device based on an image of the second user and a movement of the second user, wherein the user state of the second user comprises the emotional state of the second user.","18","16/696796","2019-11-26","2020-0098157","2020-03-26","10937221","2021-03-02","SONY CORPORATION","Akane Sano | Masamichi Asukai | Taiji Ito | Yoichiro Sako","2007-204114","JP","2007-08-06","G06T-0013/40","G06T-0013/40 | A61B-0003/113 | A61B-0005/0006 | A61B-0005/0008 | A61B-0005/0022 | A61B-0005/01 | A61B-0005/02055 | A61B-0005/0402 | A61B-0005/0476 | A61B-0005/0488 | A61B-0005/1118 | A61B-0005/1123 | A61B-0005/14551 | A61B-0005/165 | A61B-0005/4266 | A61B-0005/742 | A61B-0005/744 | G06F-0003/015 | G06K-0009/00335 | G06T-0007/20 | G16H-0050/50 | A61B-0005/021 | A61B-0005/024 | A61B-0005/026 | A61B-0005/0533 | A61B-0005/08 | Y10S-0345/95","G06T-013/40","G06T-013/40 | G16H-050/50 | A61B-005/00 | A61B-005/0205 | A61B-005/0402 | A61B-005/0476 | A61B-005/0488 | A61B-003/113 | A61B-005/01 | A61B-005/11 | A61B-005/1455 | G06K-009/00 | G06T-007/20 | A61B-005/16 | G06F-003/01 | A61B-005/024 | A61B-005/08 | A61B-005/021 | A61B-005/026 | A61B-005/0533","","","","","","4921010005517"
"US","US","P","B2","Subject assessment using localization, activity recognition and a smart questionnaire","A method for subject assessment in an environment includes receiving subject sensor data from a sensor positioned adjacent to a subject, receiving beacon signals from a beacons placed at different positions within the environment, processing the beacon signals to determine a location of the subject within the environment based on relative power levels of the received beacon signals, and processing the subject sensor data to determine an activity of the subject.","1. A method for subject assessment in an environment, comprising: receiving subject sensor data from a sensor positioned adjacent to a subject;receiving beacon signals from a plurality of beacons placed at different positions within the environment;processing the beacon signals to determine a location of the subject within the environment based on relative power levels of the received beacon signals;processing the subject sensor data to determine an activity of the subject;determining a timing for providing a question to the subject based on the determined location of the subject and the determined activity of the subject; andproviding the question to the subject at a graphical user interface based on the determined timing.","19","15/736744","2016-06-14","2018-0190382","2018-07-05","10937547","2021-03-02","THE REGENTS OF THE UNIVERSITY OF CALIFORNIA","Ramin Ramezani | Babak Moatamed | Arjun | Arash Naeim | Majid Sarrafzadeh","","","","G16H-0050/20","G16H-0050/20 | A61B-0005/1118 | A61B-0005/7267 | G06F-0001/16 | G06F-0001/163 | G06N-0005/003 | G06N-0005/045 | G06N-0005/047 | G06N-0007/005 | G06N-0020/00 | G16H-0010/20 | G16H-0020/30 | G16H-0040/63 | H04L-0029/08 | H04L-0029/10 | H04L-0067/04 | H04L-0067/12 | H04L-0067/22 | A61B-0005/002 | A61B-0005/0022 | A61B-0005/1116 | A61B-0005/681 | A61B-0005/6898 | A61B-2560/0209 | G06F-0003/0482","H04W-004/20","H04W-004/20 | G16H-050/20 | G06N-020/00 | H04L-029/10 | H04L-029/08 | G06F-001/16 | A61B-005/11 | A61B-005/00 | G16H-020/30 | G16H-040/63 | G06N-005/00 | G16H-010/20 | G06N-005/04 | G06N-007/00 | G06F-003/0482","","","","","","4921010005840"
"US","US","P","B2","Robust classifier","Various embodiments described herein relate to methods and apparatus for robust classification. Many real-world datasets suffer from missing or incomplete data. By assigning weights to certain features of a dataset based on which feature(s) are missing or incomplete, embodiments of the prevention can provide robustness and resilience to missing data.","1. An apparatus for robust classification, the apparatus comprising: a receiver receiving data, the received data having at least one feature from a feature set;a memory configured to store computer-readable instructions for performing robust classification notwithstanding at least one missing feature from the feature set; anda processor in communication with the memory and receiver, wherein execution of the computer-readable instructions for performing robust classification causes the processor to implement: at least one trained low-dimensional classifier in communication with the receiver, wherein each of the at least one low-dimensional classifiers is associated with a respective input feature from the feature set and provides an output based on the presence of the input feature in the received data; anda weighted adder in communication with the receiver and each of the at least one trained low-dimensional classifiers, wherein:the weighted adder assigns weights to each of the at least one trained low-dimensional classifier and provides a weighted sum of the outputs of the at least one low-dimensional classifier and the assigned weights, andthe weighted adder assigns a weight of zero to a classifier whose associated feature is absent from the received data and otherwise assigns a non-zero weight to a classifier whose associated feature is present in the received data.","20","15/549714","2016-02-05","2018-0046942","2018-02-15","10929774","2021-02-23","KONINKLIJKE PHILIPS N.V.","Bryan  Conroy | Larry James  Eshelman | Cristhian  Potes | Minnan  Xu","","","","G06N-0020/00","G06N-0020/00 | A61B-0005/02028 | A61B-0005/7267 | A61B-0005/7275 | G06F-0007/08 | G06F-0016/285 | G06F-0017/18 | G06N-0005/04 | G16H-0050/20 | A61B-2505/03","G06N-020/00","G06N-020/00 | G16H-050/20 | G06F-016/28 | A61B-005/00 | G06N-005/04 | G06F-017/18 | A61B-005/02 | G06F-007/08","","","","","","4921009005278"
"US","US","P","B2","Systems and methods providing a computerized eyewear device to aid in welding","A system to support communication and control in a welding environment is disclosed. In one embodiment the system includes an internet-of-things (IoT) technology platform configured to provide scalable, interoperable, and secure communication connections between a plurality of disparate devices within a welding environment. The system also includes a welding power source configured to communicate with the IoT technology platform. The system further includes a computerized eyewear device. The computerized eyewear device includes a control and communication circuitry configured to communicate with the welding power source via the IoT technology platform. The computerized eyewear device also includes a transparent display configured to display information received from the welding power source via the IoT technology platform while allowing a user to view a surrounding portion of the welding environment through the transparent display.","1. A welding system, comprising: an internet-of-things (IoT) technology platform including at least one server computer having a connection server application, wherein the internet-of-things (IoT) technology platform, includes means for providing scalable, interoperable, and secure wireless communication connections between a plurality of disparate devices, and includes means for enabling protocol-independent deployment of the plurality of disparate devices;at least one welding power source, being at least one of the plurality of disparate devices, including means for wirelessly communicating, two-way, with the internet-of-things (IoT) technology platform using the connection server application, wherein the at least one welding power source is an inverter-based welding power source that includes means for supporting at least one of a gas metal arc welding (GMAW) operation, a pas tungsten arc welding (GTAW) operation, or a shielded metal arc welding (SMAW) operation; andat least one computerized eyewear device, being at least one of the plurality of disparate devices, including: a control and communication circuitry, having a processor and a memory, configured to wirelessly communicate, two-way, with the at least one welding power source via the internet-of-things (IoT) technology platform using the connection server application, anda transparent display configured to display information received by the control and communication circuitry from the at least one welding power source via the internet-of-things (IoT) technology platform using the connection server application while allowing a user to view a surrounding portion of a real world welding environment through the transparent display.","19","15/660525","2017-07-26","2017-0323584","2017-11-09","10930174","2021-02-23","LINCOLN GLOBAL, INC.","Joseph Allen  Daniel | William Thomas  Matthews","","","","G09B-0019/24","G09B-0019/24 | A61F-0009/06 | B23K-0009/0953 | B23K-0009/0956 | B23K-0009/10 | B23K-0009/1006 | B23K-0009/32 | B23K-0009/321 | B23K-0009/322 | B23K-0037/00 | G02B-0027/0172 | G02B-0027/0176 | G06K-0009/00 | G06K-0009/00671 | G09B-0005/02 | G09B-0009/00 | G09B-0019/003 | G02B-0027/017 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0158 | G02B-2027/0178 | G02B-2027/0187 | G06F-0003/04883 | G06F-0003/167","B23K-009/09","B23K-009/09 | G02B-027/01 | G09B-019/24 | G09B-019/00 | B23K-009/095 | G06K-009/00 | B23K-009/32 | G09B-009/00 | B23K-037/00 | B23K-009/10 | G09B-005/02 | A61F-009/06 | G06F-003/16 | G06F-003/0488","","","","","","4921009005671"
"US","US","P","B1","Methods and systems of telemedicine diagnostics through remote sensing","A system for telemedicine diagnostics through remote sensing includes a computing device 104 configured to initiate a communication interface between the computing device 104 and a client device 112 operated by a human subject, wherein the secure communication interface includes an audiovisual streaming protocol, receive, from at least a remote sensor at the human subject, a plurality of current physiological data, generate a clinical measurement approximation as a function of the plurality of current physiological data, wherein generating further comprises receiving approximation training data correlating physiological data with clinical measurement data, training a measurement approximation model as a function of the training data and a machine-learning process, and generating the clinical measurement approximation as a function of the current physiological data and the measurement approximation model, and presenting the clinical measurement approximation to a user of the computing device 104 using the secure communication interface.","1. A system for telemedicine diagnostics through remote sensing, the system comprising: a computing device at a first location, the computing device configured to:initiate a secure communication interface between the computing device and a client device associated with a human subject and at a second location, wherein the secure communication interface includes an audiovisual streaming protocol;receive, from at least a remote sensor at the second location, a plurality of current physiological data associated with the human subject;generate a clinical measurement approximation as a function of the plurality of current physiological data, wherein generating further comprises: receiving approximation training data correlating physiological data with clinical measurement data;training a measurement approximation model as a function of the training data and a machine-learning process, wherein training the measurement approximation model further comprises generating a general model andtraining a subject-specific model as a function of subject-specific training data; andgenerating the clinical measurement approximation as a function of the current physiological data and the measurement approximation model;present, via the audiovisual streaming protocol of the secure communication interface, the clinical measurement approximation to a user of the computing device at the first location;determine a degree of reliability of the clinical measurement;provide the degree of reliability using the communication interface; andidentify a follow-up action as a function of the degree of reliability.","14","16/939373","2020-07-27","","","10931643","2021-02-23","KPN INNOVATIONS, LLC.","Kenneth  Neumann","","","","H04L-0063/04","H04L-0063/04 | A61B-0005/0022 | A61B-0005/7221 | A61B-0005/7267 | A61B-0005/7465 | G16H-0010/20 | G16H-0040/67 | G16H-0050/20 | G16H-0050/30 | G16H-0080/00 | H04L-0065/608 | G06F-0021/6245 | G06Q-0050/01 | H04L-0063/0428","G16H-050/50","G16H-050/50 | G16H-050/70 | H04L-029/06 | G16H-040/67 | G16H-050/20 | G16H-080/00 | G16H-050/30 | G16H-010/20 | A61B-005/00 | G06N-003/08 | G06N-005/04 | G06F-021/62 | G06Q-050/00","","","","","","4921009007131"
"US","US","P","B2","User movement monitoring method and system performing the same","Provided are a method and a system for monitoring a movement of a user. The system includes at least one wearable flexible tactile sensor configured to sense movement of a muscle or bending of a joint at a corresponding location and transmitting a sensed value. The system further includes a monitoring server configured to analyze movement of the muscle or the bending of the joint of the user based on the sensed value received from the flexible tactile sensor motility of the user.","1. A system for monitoring a movement of a user, the system comprising: at least one flexible tactile sensor wearable on a body of the user and configured to sense a movement of a muscle or bending of a joint at a corresponding location of the user and configured to transmit a sensed value; anda monitoring server configured to analyze the movement of the muscle or the bending of the joint of the user based on the sensed value received from the at least one flexible tactile sensor and monitor motility of the user,wherein the at least one flexible tactile sensor includes a tactile sensor array comprising a plurality of tactile sensor modules, andwherein each of the plurality of tactile sensor modules includes: a polymer layer,a first metal layer formed over the polymer layer,a first sensor layer formed over the first metal layer, the first sensor layer comprising a first strain gauge configured to change its resistance based on a first strain and a metal wire connected to the first strain gauge,a first cover layer configured to protect the first sensor layer,a second metal layer formed under the polymer layer,a second sensor layer formed under the second metal layer, the second sensor layer comprising a second strain gauge configured to change its resistance based on a second strain and a metal wire connected to the second strain gauge, anda second cover layer configured to protect the second sensor layer.","9","15/891299","2018-02-07","2018-0160940","2018-06-14","10918311","2021-02-16","KOREA ELECTRONICS TECHNOLOGY INSTITUTE","Kunnyun  Kim | Kwang Bum  Park | Won Hyo  Kim | Yeon Hwa  Kwak","10-2015-0111694","KR","2015-08-07","A61B-0005/11","A61B-0005/11 | A61B-0005/1116 | A61B-0005/45 | G06F-0003/016 | G06K-0009/00496 | A61B-2562/0261 | G06K-0009/00342 | G06K-0009/00355 | G08B-0006/00","A61B-005/11","A61B-005/11 | G06K-009/00 | A61B-005/00 | G06F-003/01 | G08B-006/00","","","","","","4921008001027"
"US","US","P","B2","Method and device for neural implant communication","Communications along a neural pathway are provided. The neural pathway is stimulated at a first location, in order to evoke neural responses which propagate along the neural pathway, the neural responses being modulated with data. At a second location spaced apart from the first location along the neural pathway the evoked neural responses are sensed. The sensed neural responses are then demodulated to retrieve the data. The stimulation could comprise peripheral sensory stimulation, and the second location could be at an implanted electrode array.","1. A method of communicating along a neural pathway, the method comprising: stimulating the neural pathway at a first location, the first location being a peripheral location, in order to evoke neural responses which propagate along the neural pathway, the neural responses being modulated with machine readable binary data;sensing, using an implantable device, the evoked neural responses at a second location spaced apart from the first location along the neural pathway, anddemodulating, using the implantable device, the sensed neural responses to retrieve the machine readable binary data, the machine readable binary data being configured to control or alter the operation of the implanted device.","19","15/544515","2016-01-19","2018-0229046","2018-08-16","10918872","2021-02-16","SALUDA MEDICAL PTY LTD","John Louis  Parker | Mark  Staples","2015-900134","AU","2015-01-19","A61N-0001/37217","A61N-0001/37217 | A61B-0005/0028 | A61B-0005/0031 | A61B-0005/04001 | A61B-0005/686 | A61B-0005/7475 | A61N-0001/36132 | G06F-0003/015 | A61B-2560/0487 | A61N-0001/0526 | A61N-0001/37211","A61N-001/372","A61N-001/372 | A61B-005/00 | A61B-005/04 | A61N-001/36 | A61N-001/05 | G06F-003/01","","","","","","4921008001582"
"US","US","P","B2","Device for storing elements","A device is provided for storing elements, each element including a first wireless communication unit. The device includes at least two drawer units each including: a drawer including a bottom defining at least one location for receiving an element; for each location, at least one second wireless communication unit including an antenna having a radiation-zone field, each antenna having a first state in which the antenna is activated and a second state in which the antenna is deactivated; and a data-processing unit able to control the activation and deactivation of the antenna of each second communication unit according to a control law.","1. A device configured to store a plurality of elements, each element including a first wireless communication unit in which information relative to said element is stored such that a set of first wireless communication units is provided for the plurality of elements, the device comprising: at least two drawer units assembled on one another to form a vertical stack, each drawer unit comprising: a support comprising a housing,a drawer positioned in the housing of the support and configured to slide relative to the support, the drawer comprising a bottom defining at least two locations each configured to receive one of the elements,for each of the at least two locations, at least one second wireless communication unit configured to emit radio waves, each second communication unit being disposed facing the respective location, each second communication unit being configured to communicate with the set of first communication units, each second communication unit comprising at least one antenna having a radiation-zone field, the radiation-zone field of each antenna covering at least:the location facing the respective second communication unit of said antenna, andat least one other location of the at least two locations adjacent to said location facing the respective second communication unit of said antenna,each antenna having a first state in which said antenna is activated and a second state in which said antenna is deactivated, anda data processor connected to each second communication unit, the data processor being configured to command activation and deactivation of the antenna of each of the second communication units according to a control law to determine whether the location facing the respective second communication unit of said antenna is occupied by one of the elements and, when it is applicable, the information relative to said element.","10","16/338928","2016-10-24","2019-0220638","2019-07-18","10922504","2021-02-16","BIOLOG-ID","Jean-Claude  Mongrenier","2016-059522","FR","2016-10-03","G06K-0007/10366","G06K-0007/10366 | A61M-0001/0286 | G06K-0007/10356 | G06K-0019/0723 | G06Q-0010/087 | G07F-0011/62 | G07F-0017/0092 | G07G-0001/009","G06K-007/10","G06K-007/10 | G07F-011/62 | G07F-017/00 | G06Q-010/08 | G07G-001/00 | A61M-001/02 | G06K-019/07","","","","","","4921008005179"
"US","US","P","B2","Electronic component and electronic device comprising same","An electronic component (and/or an electronic device comprising the same) according to various embodiments of the present invention comprises: a substrate having a sensing element mounted on one surface thereof; a flexible printed circuit board that is coupled to the other surface of the substrate so as to face the same and extends to a side of the substrate along a first direction; and at least one recess formed on the edge of the other surface of the substrate, wherein the recess is located in an area, on the other surface of the substrate, which faces at least the flexible printed circuit board, and may extend along a second direction intersecting with the first direction. The electronic component and/or the electronic device comprising the same as described above may be diversified according to embodiments.","1. An electronic component comprising: a substrate having a sensor element mounted on one face thereof;a flexible printed circuit board coupled to face a remaining face of the substrate and extending to one side of the substrate in a first direction; andat least one recess formed at an edge of the remaining face of the substrate,wherein the recess is located at least in a region facing the flexible printed circuit board on the remaining face of the substrate and extends in a second direction intersecting the first direction.","15","16/327212","2017-06-13","2019-0188445","2019-06-20","10922513","2021-02-16","SAMSUNG ELECTRONICS CO., LTD.","Youn-Oh  Chi | Yun-Jang  Jin | Kyung-Hoon  Song | Kwang-Sub  Lee | Se-Young  Jang | Chi-Hyun  Cho","10-2016-0111359","KR","2016-08-31","G06K-0009/00013","G06K-0009/00013 | A61B-0005/1172 | A61B-0005/6898 | G06F-0003/0412 | G06F-0021/32 | G06K-0009/00 | H01H-0011/00 | H01H-0011/04 | H01H-0013/06 | H01H-0013/14 | H01H-0013/48 | H01H-0013/84 | H04M-0001/02 | H04M-0001/23 | H01H-2003/0293 | H01H-2225/002 | H01H-2229/02 | H01H-2229/044 | H01H-2231/022 | H01H-2239/074","G06K-009/00","G06K-009/00 | G06F-021/32 | H04M-001/02 | H01H-011/04 | H01H-013/48 | H01H-011/00 | H01H-013/84 | A61B-005/1172 | A61B-005/00 | G06F-003/041 | H01H-013/06 | H01H-013/14 | H04M-001/23 | H01H-003/02","","","","","","4921008005188"
"US","US","P","B2","Welding system providing visual and audio cues to a welding helmet with a display","A system for aiding a user in at least one of welding, cutting, joining, and cladding operations is provided. The system includes a welding tool and a welding helmet with a face-mounted display. The system also includes a spatial tracker configured to track the welding helmet and the welding tool in 3-D space relative to an object to be worked on. A processor based subsystem is configured to generate visual cues based on information from the spatial tracker and transmit the visual cues to a predetermined location on the face-mounted display.","1. A system for aiding a user in at least one of welding, cutting, joining, and cladding operation, the system comprising: a welding tool for performing at least one of a real-world weld and a simulated weld on one of a coupon or workpiece;a see-through face-mounted display to allow the user to see a view of a welding environment including the one of a coupon or a workpiece, the face-mounted display having a combiner;a spatial tracker configured to track position, orientation and movement of the face-mounted display and the welding tool in 3-D space relative to the one of a coupon or a workpiece and determine 6-D data that corresponds to the tracking;a processor based subsystem configured to receive the 6-D data from the spatial tracker,generate at least one visual cue based on at least one of one or more performance parameters related to the 6-D data and one or more process parameters related to the at least one welding, cutting, joining, and cladding operation, wherein the at least one visual cue corresponds to at least one of a deposition rate and a frequency of TIG filler addition, andmap the at least one visual cue to a respective location on the face-mounted display; anda projector configured to project the at least one visual cue onto the combiner of the face-mounted display such that the face-mounted display reflects and shows the at least one visual cue overlaid onto the view of the welding environment, wherein the combiner is configured to allow adjustment of an angle of reflection by the user, andwherein the at least one visual cue provides information to the user that aids in performing the at least one of a real-world weld and a simulated weld.","31","15/785489","2017-10-17","2018-0126476","2018-05-10","10913125","2021-02-09","LINCOLN GLOBAL, INC.","Brian  Meess | Sarah  Evans","","","","B23K-0009/0953","B23K-0009/0953 | A61F-0009/06 | A61F-0009/064 | B23K-0009/0956 | B23K-0009/1087 | B23K-0009/322 | G02B-0027/0172 | G05B-0019/182 | G06K-0009/00671 | G06T-0011/60 | G08B-0003/10 | G09B-0019/24 | G02B-2027/014 | G02B-2027/0138 | G02B-2027/0141 | G05B-2219/45044 | G05B-2219/45135 | G06F-0003/0482","B23K-009/095","B23K-009/095 | A61F-009/06 | G02B-027/01 | G05B-019/18 | G08B-003/10 | G06T-011/60 | G06K-009/00 | G09B-019/24 | B23K-009/10 | B23K-009/32 | G06F-003/0482","","","","","","4921007001459"
"US","US","P","B2","Electronic device and hardware diagnosis result-based process execution method thereof","A process execution method and apparatus of an electronic device are provided for performing hardware diagnosis and executing a process based on the hardware diagnosis result. The electronic device includes a plurality of hardware components; a display configured to display information on the hardware components; and a processor configured to diagnose a hardware component selected as a diagnosis target among the hardware components, determine, based on a diagnosis result, whether the diagnosis target is operating normally, and display information indicating whether the diagnosis target is operating normally and a link for providing a service related to the diagnosis target.","1. An electronic device, comprising: a plurality of hardware components;a display configured to display information on at least one component of the plurality of hardware components; anda processor configured to: diagnose a hardware component selected as a diagnosis target among the plurality of hardware components, anddisplay information related to a result of the diagnosis and a link for providing a service related to the diagnosis target,wherein the displayed information indicates that the diagnosis target is operating normally or abnormally,wherein the link is to an application configured to provide the service related to the diagnosis target, when the diagnosis target is determined to be operating normally, andwherein the link is to an after service (AS) request service, when the diagnosis target is determined to be operating abnormally,wherein the processor is further configured to transmit, in response to selection of the link to the AS request service, AS request information to an external device, andwherein the AS request information includes at least one of: information related to the diagnosis target, anduser information.","18","16/819680","2020-03-16","2020-0217695","2020-07-09","10914619","2021-02-09","Samsung Electronics Co., Ltd","Minsuk  Jang | Gyoseung  Koo | Seokhee  Na | Kyuok  Choi","10-2016-0005637","KR","2016-01-15","G01D-0018/00","G01D-0018/00 | A61B-0005/02055 | A61B-0005/1172 | G06F-0003/03545 | G06Q-0010/20 | A61B-0005/021 | A61B-0005/024 | A61B-2560/0276 | G06F-0003/0416 | G06F-0003/0418 | G06F-0003/0482 | G06F-0003/04817 | G06F-0011/2221","G01D-018/00","G01D-018/00 | G06Q-010/00 | A61B-005/0205 | A61B-005/1172 | G06F-003/0354 | G06F-003/041 | G06F-011/22 | A61B-005/021 | A61B-005/024 | G06F-003/0481 | G06F-003/0482","","","","","","4921007002943"
"US","US","P","B2","Image processing apparatus, image processing method, and recording medium","An image processing apparatus includes an image analysis unit, a biometric information measurement unit, and a determination selection unit. The image analysis unit detects a plurality of part images of a subject from the image including the subject captured by the imaging unit. The biometric information measurement unit obtains biometric information from at least one of the plurality of part images of the subject detected by the image analysis unit. The determination selection unit determines the detectability of biometric information on each of the plurality of part images of the subject detected by the image analysis unit. The determination selection unit makes a selection regarding acquisition of the biometric information of the subject on the basis of the detectability of the biometric information determined by the determination selection unit.","1. An image processing apparatus comprising a processor, wherein the processor: detects a plurality of part images of a subject from an image including the subject captured by an imaging unit,determines detectability of biometric information for each of at least two of the plurality of detected part images of the subject based on a detected pulse of the subject from each of the plurality of part images of the subject, and performs selection, from a plurality of parts of the subject, of a part from which the biometric information is to be obtained, based on the determined detectability of the biometric information.","17","16/226407","2018-12-19","2019-0197223","2019-06-27","10915617","2021-02-09","CASIO COMPUTER CO., LTD.","Toshihiko  Otsuka | Takahiro  Tomida","2017-246941 | 2018-200206","JP | JP","2017-12-22 | 2018-10-24","G06F-0021/32","G06F-0021/32 | A61B-0005/0077 | A61B-0005/02108 | A61B-0005/1032 | A61B-0005/1176 | A61B-0005/6891 | G06K-0009/00288 | H04N-0005/23222 | A61B-2576/02 | G16H-0020/00","G06F-021/32","G06F-021/32 | H04N-005/232 | A61B-005/021 | G06K-009/00 | A61B-005/00 | A61B-005/1171 | A61B-005/103 | G16H-020/00","","","","","","4921007003931"
"US","US","P","B2","Product solution responsive to problem identification","Aspects map biometric data acquired in real-time from a user to a specific task being performed by the user in generating the biometric data; determine that the user is likely experiencing a problem in performing the specific task as a function of a value of the mapped biometric data; determine a performance context of the experienced problem as a function of the mapped biometric data; and select a solution that is most appropriate to solve the experienced problem as a function of the determined performance context and the mapped biometric data.","1. A computer-implemented method, comprising: mapping biometric data acquired in real-time from a user to a specific task being performed by the user in generating the biometric data;determining that the user is likely experiencing a problem in performing the specific task as a function of a value of the mapped biometric data;determining a performance context of the experienced problem as a function of the mapped biometric data;determining a current cognitive state of the user while experiencing the problem as a function of the mapped biometric data, wherein the current cognitive state is selected from a plurality of cognitive states that comprises an agitated state and a calm state;selecting from a plurality of solutions a solution that is most appropriate to solve the experienced problem relative to others of the plurality of solutions as a function of the determined performance context and the current cognitive state; andpresenting the selected solution to the user as a function of the current cognitive state, said presenting comprising:in response to determining that the current cognitive state of the user is the calm state, presenting the selected solution with additional explanation text content that explains why the selected solution will solve the experienced problem; andin response to determining that the current cognitive state of the user is the agitated state, presenting the selected solution without presenting the additional explanation text content.","14","16/191704","2018-11-15","2020-0160399","2020-05-21","10915928","2021-02-09","INTERNATIONAL BUSINESS MACHINES CORPORATION","Sarbajit K.  Rakshit | Craig M.  Trim | John M.  Ganci, Jr. | James E.  Bostick | Martin G.  Keen","","","","G06Q-0030/0271","G06Q-0030/0271 | A61B-0005/165 | G06K-0009/00664 | G10L-0015/26 | G10L-2015/228","G06Q-030/02","G06Q-030/02 | G10L-015/26 | A61B-005/16 | G06K-009/00 | G10L-015/22","","","","","","4921007004240"
"US","US","P","B2","Utilizing a machine learning model to identify activities and deviations from the activities by an individual","A device receives historical information associated with an individual to be monitored, wherein the historical information includes at least one of information associated with a health history of the individual, health histories of other individuals, activities of the individual, or activities of the other individuals. The device receives monitored information associated with the individual from one or more client devices associated with the individual, and pre-processes the monitored information to generate pre-processed monitored information that is understood by the trained machine learning model. The device processes the pre-processed monitored information, with a trained machine learning model, to identify one or more activities of the individual and one or more deviations from the one or more activities by the individual, and performs one or more actions based on the one or more activities of the individual and/or the one or more deviations from the one or more activities.","1. A method, comprising: receiving, by a device, configuration information associated with configuring an application for monitoring an individual, wherein the configuration information includes at least one of: information identifying physical characteristics of the individual,information identifying medications taken by the individual,personal information of the individual, orinformation associated with a caregiver of the individual;receiving, by the device, historical information associated with the individual, wherein the historical information includes at least one of: information associated with a health history of the individual,information associated with health histories of other individuals,information associated with activities of the individual, orinformation associated with activities of the other individuals;creating, by the device, a training set using the configuration information and the historical information;training, by the device and using the training set, a machine learning model to generate a trained machine learning model;providing, by the device, third-party application programming interfaces (APIs) to a plurality of client devices associated with the individual, at least one of the third-party APIs enabling an activity service to track physical activity of the individual;receiving, by the device and via the third party APIs, monitored information associated with the individual from the plurality of client devices, the plurality of client devices including: a wearable device,an image sensor, andan audio sensor, andthe monitored information including: video captured by the image sensor,audio captured by the audio sensor,health information obtained by the wearable device, andphysical activity information obtained by the wearable device;processing, by the device, the monitored information, with the trained machine learning model, to identify one or more activities of the individual;determining, by the device, a routine associated with the individual based on identifying the one or more activities of the individual;processing, by the device, the monitored information, with the trained machine learning model, to identify one or more deviations from the routine by the individual; andperforming, by the device, one or more actions based on identifying the one or more deviations, the one or more actions including one or more of: causing a robot to provide medication to the individual based on a first deviation of the one or more deviations, orcausing an autonomous emergency vehicle to traverse a route to the individual based on a second deviation of the one or more deviations.","20","16/191363","2018-11-14","2019-0156944","2019-05-23","10916344","2021-02-09","ACCENTURE GLOBAL SOLUTIONS LIMITED","Laetitia Cailleteau  Eriksson | Kar Lok  Chan | Faisal Ahmed  Valli | Christopher Paul  Ashley","","","","G16H-0040/67","G16H-0040/67 | A61B-0005/11 | A61B-0005/6898 | A61B-0005/7267 | G06K-0009/00335 | G06K-0009/6262 | G06K-0009/6269 | G06K-0009/6274 | G06Q-0050/22 | G10L-0025/30 | G16H-0010/60 | G16H-0020/30 | G16H-0050/20 | G16H-0050/30 | A61B-0005/0022 | A61B-0005/021 | A61B-0005/024 | A61B-0005/1112 | A61B-0005/4833 | A61B-0005/681 | A61B-0005/6803 | A61B-0005/7275 | A61B-0005/746 | A61B-2505/07","G16H-040/67","G16H-040/67 | G16H-050/20 | G06K-009/62 | G10L-025/30 | G16H-050/30 | G16H-020/30 | A61B-005/00 | A61B-005/11 | G06Q-050/22 | G16H-010/60 | G06K-009/00 | A61B-005/021 | A61B-005/024","","","","","","4921007004654"
"US","US","P","B2","Camera-guided interpretation of neuromuscular signals","Computerized systems, methods, and computer-readable storage media storing code for implementing the methods are described for providing dynamically-updated musculoskeletal information. One such system includes a processor is programmed to: provide, as an input to a trained inference model, information based on a plurality of neuromuscular signals from a user and information based on at least one image of the user; determine, based on an output of the trained inference model, position information describing a spatial relationship between two or more connected musculoskeletal segments of the user and/or force information describing a force exerted by at least one musculoskeletal segment of the user; and output the position information and/or the force information.","1. A computerized system for providing dynamically-updated musculoskeletal information, the system comprising: one or more neuromuscular sensors configured to obtain one or more neuromuscular signals produced by a user;at least one camera configured to capture one or more contemporaneous images with the one or more neuromuscular signals; andat least one computer processor programmed to: derive contemporaneous contextual information from the one or more contemporaneous images, the contemporaneous contextual information comprising at least one of: an indication of at least one musculoskeletal segment of the user exerting a force against at least one other musculoskeletal segment of the user when the one or more neuromuscular signals were produced by the user;an indication of the least one musculoskeletal segment of the user exerting a force against an object when the one or more neuromuscular signals were produced by the user; oran indication of an environment in which the user produced the one or more neuromuscular signals;determine, based on the contemporaneous contextual information and the one or more neuromuscular signals, at least one of: position information describing a spatial relationship between two or more connected musculoskeletal segments of the user;first force information describing the force exerted by the at least one musculoskeletal segment of the user against the object; orsecond force information describing the force exerted by the at least one musculoskeletal segment of the user against the at least one other musculoskeletal segment of the user; andoutput at least one of the position information, the first force information, or the second force information.","30","16/557383","2019-08-30","2020-0069210","2020-03-05","10905350","2021-02-02","FACEBOOK TECHNOLOGIES, INC.","Adam  Berenzweig | Thomas  Reardon | Christopher  Osborn | Patrick  Kaifosh | Brett  Jurman | Daniel  Wetmore","","","","A61B-0005/0488","A61B-0005/0488 | A61B-0005/04001 | A61B-0005/04004 | A61B-0005/04012 | A61B-0005/6824 | A61B-0090/361 | G06F-0003/011 | G06F-0003/015 | G06K-0009/00355 | G06K-0009/6215 | G06N-0005/04 | G06N-0020/00 | G06T-0007/70 | A61B-0005/1107 | A61B-0005/681 | A61B-0005/7267 | A61B-2562/0219 | G06F-0003/016 | G06F-0003/017 | G06T-0019/006 | G06T-2207/20081 | G06T-2207/30196","A61B-005/0488","A61B-005/0488 | G06F-003/01 | A61B-005/00 | A61B-090/00 | A61B-005/04 | G06T-007/70 | G06N-020/00 | G06K-009/00 | G06K-009/62 | G06N-005/04 | G06T-019/00 | A61B-005/11","","","","","","4921006001039"
"US","US","P","B2","Method and wearable device for performing actions using body sensor array","A method for performing actions by a wearable device is provided. The method includes detecting at least one signal indicating a movement of a muscle of the wrist, via an array of biometric sensors exposed through an inner peripheral surface of a substantially circular band of the wearable device, identifying an orientation of the wearable device corresponding to the at least one signal, and providing, based at least in part on the identification, a function corresponding to the orientation of the wearable device. The method further includes detecting, by the wearable device, an absolute orientation of the wearable device using at least one of an inertial sensor and the one or more body sensors. The method further includes dynamically performing an action, by the wearable device, based on a pre-stored mapping of the at least one physiological parameter and the absolute orientation of the wearable device.","1. A wearable device capable of being worn on a wrist of a user, the wearable device comprising: a substantially circular band having an inner peripheral surface and an outer peripheral surface;an array of biometric sensors exposed through the inner peripheral surface; andat least one processor configured to: detect at least one signal indicating a movement of a muscle of the wrist, via the array of the biometric sensors,identify an orientation of the wearable device corresponding to the at least one signal based on a pre-stored mapping, andprovide, based at least in part on the identification, a function corresponding to the orientation of the wearable device,wherein the pre-stored mapping indicates correspondence between signals indicating movements of the muscle of the wrist and orientations of the wearable device.","20","15/989747","2018-05-25","2018-0338720","2018-11-29","10905374","2021-02-02","SAMSUNG ELECTRONICS CO., LTD.","Gaurav  Gupta | Sonu  Agarwal","201741018465 | 201741018465","IN | IN","2017-05-25 | 2018-05-16","A61B-0005/6824","A61B-0005/6824 | A61B-0005/0488 | A61B-0005/1121 | A61B-0005/681 | A61B-0005/7475 | G06F-0003/015 | G06F-0003/017 | G06N-0020/00","A61B-005/0488","A61B-005/0488 | A61B-005/053 | G06F-003/01 | G06F-001/16 | A61B-005/00 | A61B-005/11 | G06N-020/00","","","","","","4921006001063"
"US","US","P","B2","Wearable assistive jamming apparatus and related methods","Wearable assistive jamming apparatus and related methods are disclosed herein. An example apparatus includes a frame to be worn about a body part of a user and a jamming actuator carried by the frame. The example apparatus includes an electrode to deliver electricity from an electrical source communicatively coupled to the electrode to a muscle of the user. In the example apparatus, the jamming actuator is to transition from a flexible state to a substantially rigid state in coordination with the delivery of the electricity to the electrode to support the body part.","1. An apparatus comprising: the frame structured to be worn about a body part of a user;a jamming actuator carried by the frame;an electrode structured to deliver electricity from an electrical source communicatively coupled to the electrode to a muscle of the user;a sensor structured to detect a first movement of the body part and to detect a second movement of the body part; anda processor in communication with the jamming actuator and the electrical source, the processor structured to: generate a first instruction to cause the jamming actuator to transition from a flexible state to a stiffened state in coordination with the delivery of the electricity to the electrode to support the body part in response to the first movement of the body part; andgenerate a second instruction to cause the jamming actuator to transition from the stiffened state to the flexible state in coordination with the delivery of the electricity to the electrode to release the body part in response to the second movement.","20","15/575269","2016-12-19","2018-0296424","2018-10-18","10905617","2021-02-02","INTEL CORPORATION","Jeremy  Parra | Stephanie J.  Walker | James  Hallam","","","","A61H-0003/00","A61H-0003/00 | A61B-0005/1114 | A61F-0004/00 | A61F-0005/013 | A61N-0001/18 | A61N-0001/36003 | A61N-0001/36014 | A61N-0001/36031 | A61N-0001/36034 | G06F-0003/0346 | A61F-2005/0155 | A61F-2005/0158 | A61F-2005/0169 | A61H-2003/006 | A61H-2201/10 | A61H-2201/1238 | A61H-2201/1409 | A61H-2201/5007 | A61H-2230/00 | A61N-0001/025 | A61N-0001/321","A61H-003/00","A61H-003/00 | A61B-005/11 | A61N-001/36 | A61F-004/00 | G06F-003/0346 | A61F-005/01 | A61N-001/18 | A61N-001/02 | A61N-001/32","","","","","","4921006001305"
"US","US","P","B2","Sentiment adapted communication","Methods, computer program products, and systems are presented. The method computer program products, and systems can include, for instance: examining communication data of a first human user to return one or more sentiment attribute of the communication data; processing the communication data to return sentiment neutral adapted communication data, the processing being in dependence on the one or more sentiment attribute; presenting to a second human user a sentiment neutral adapted communication, the sentiment neutral adapted communication being based on the sentiment neutral adapted communication data; augmenting second communication data to return adapted second communication data, the augmenting being in dependence of the one or more sentiment attribute; and presenting to the first human user an adapted second communication, the adapted second communication being based on the adapted second communication data.","1. A computer implemented method comprising: examining communication data of a first human user to return one or more sentiment attribute of the communication data;processing the communication data to return sentiment neutral adapted communication data, the processing being in dependence on the one or more sentiment attribute;presenting to a second human user a sentiment neutral adapted communication, the sentiment neutral adapted communication being based on the sentiment neutral adapted communication data;augmenting second communication data to return adapted second communication data, the augmenting being in dependence on the one or more sentiment attribute; andpresenting to the first human user an adapted second communication, the adapted second communication being based on the adapted second communication data.","20","16/239596","2019-01-04","2020-0218781","2020-07-09","10909328","2021-02-02","INTERNATIONAL BUSINESS MACHINES CORPORATION","Sawa  Takano | Tadayuki  Yoshida","","","","G06F-0040/40","G06F-0040/40 | A61B-0005/165 | G06K-0009/00302 | G06Q-0030/016 | G10L-0015/1822 | G10L-0015/26 | G10L-0017/26 | G10L-0025/63 | G10L-2015/226","G06F-040/40","G06F-040/40 | A61B-005/16 | G06Q-030/00 | G10L-017/26 | G10L-015/18 | G06K-009/00 | G10L-025/63 | G10L-015/26 | G10L-015/22","","","","","","4921006004991"
"US","US","P","B1","Apparatus, system and method for facilitating tracking of consumable pharmaceutical articles","Embodiments of an apparatus, system and method for facilitating the tracking of consumable pharmaceutical articles are disclosed. A marking apparatus is configured to apply unique indelible identifiers to pharmaceutical articles at the point-of-sale. The indelible identifiers on the articles are visually or electronically readable to allow each respective article to be traced with respect to its manufacturer, prescribing physician, patient to whom it was prescribed, dispensing location, expiration date, dosage, or some combination thereof. An associated system and method may involve the provision of a remote database element configured to be in network communication between the marking apparatus, manufacturer, distributor, prescriber, dispensing location, scanning tool, or some combination thereof. The indelible identifier may initially be generated by the apparatus and then obtained and recorded by the remote database element. Alternative, the indelible identifier may initially be generated and recorded by the remote database element, then provided to the marking apparatus.","1. A portable, point-of-sale marking apparatus for applying indelible identifiers onto discreet consumable pharmaceutical articles, the apparatus comprising: an enclosure;a transport subsystem configured to transport a tray element between a loading position and an application position, the tray element having a multiplicity of pockets each configured to retain a respective said article, wherein (a) when the tray element is in the application position, its pockets are located within an application zone internal to the enclosure, and(b) when the tray element is in the loading position, its pockets are located externally to the enclosure;an identifier application subsystem positioned within the enclosure for applying an indelible identifier onto each said article within the application zone; andan inspection element positioned within the enclosure and configured to verify the quality of the indelible identifier applied onto each respective article.","27","16/976467","2019-02-28","2021-0027307","2021-01-28","10909548","2021-02-02","TRI-STAR TECHNOLOGIES","Alex Aron  Kerner | Igor  Murokh","","","","G06Q-0030/0185","G06Q-0030/0185 | A61B-0005/1176 | A61J-0001/035 | A61J-0007/0069 | B41F-0017/36 | B65G-0049/00 | G06K-0009/00255 | G06Q-0020/202 | G06Q-0020/203 | G06Q-0020/208 | G06Q-0050/265 | G16H-0010/60 | G16H-0020/10 | G16H-0040/20 | A61J-0003/007 | A61J-2200/30 | A61J-2205/60 | B65G-2201/027 | G06K-0019/06028","B41F-017/36","B41F-017/36 | G06Q-030/00 | G16H-040/20 | G16H-020/10 | G16H-010/60 | G06Q-020/20 | G06Q-050/26 | B65G-049/00 | A61J-007/00 | A61J-001/03 | A61B-005/1171 | G06K-009/00 | A61J-003/00 | G06K-019/06","","","","","","4921006005208"
"US","US","P","B2","Verified social media content","In an example, there is a disclosed a computing apparatus, including: a psychological state data interface to receive psychological state data; one or more logic elements, including at least one hardware element, including a verification engine to: receive a requested user action; receive a psychological state input via the psychological state data interface; analyze the psychological state input; and bar the requested user action at least partly responsive to the analyzing.","1. A data loss prevention (DLP) server, comprising: a processor;a trusted input/output (IO) interface to communicatively couple to a user device;a social media interface to communicatively couple to a social media service;a trusted execution environment (TEE); anda memory having stored thereon executable instructions to instruct the processor to provide a DLP engine to: receive from the user device via the trusted IO interface a signed and encrypted user posting for the social media service, the user posting including a signed user state report verifying that the user has passed a biometric screening;cryptographically verify the signed and encrypted user posting comprising performing attestation with a source device of the signed and encrypted user posting; andresponsive to a successful attestation, submit the user posting on behalf of the user to the social media service via the social media interface.","20","16/235828","2018-12-28","2019-0139155","2019-05-09","10909638","2021-02-02","McAfee, LLC","Kunal  Mehta | Carl D.  Woodward | Steven  Grobman | Ryan  Durand | Simon  Hunt","","","","G06Q-0050/01","G06Q-0050/01 | A61B-0005/117 | A61B-0005/165 | A61B-0005/4845 | A61B-0005/6898 | G06F-0021/316 | G06F-0021/552 | G06Q-0010/06395 | H04L-0063/0861 | H04W-0012/0609 | G06F-0021/53 | G06F-2221/2133 | H04L-0067/10","H04L-029/00","H04L-029/00 | G06Q-050/00 | G06Q-010/06 | G06F-021/31 | G06F-021/55 | H04L-029/06 | A61B-005/00 | A61B-005/117 | A61B-005/16 | H04W-012/06 | H04L-029/08 | G06F-021/53","","","","","","4921006005298"
"US","US","P","B2","Method and apparatus for monitoring of a human or animal subject","A method and apparatus for monitoring a human or animal subject in a room using video imaging of the subject and analysis of the video image to derive an estimate of vital signs such as heart rate or breathing rate. The method includes determination of whether the subject in the images is still or moving, and whether any of the regions from which vital signs are being detected are not on the subject. The subject's movement may be manually or automatically detected, and the determination of whether regions from which vital signs are being detected are not on the subject can be input manually, by displaying the regions to the user in a visually distinguishable manner, or automatically. Vital signs measurements are only displayed if the subject is determined as being still and if there are no regions in the image which are returning vital signs signals but are not determined as being on the subject.","1. A method of monitoring a human or animal subject comprising the steps of: capturing a video image of the subject;analysing the video image to determine one or more vital signs detected on at least one of (i) the subject and (ii) a region excluding the subject;displaying to a user a graphical user interface comprising an icon indicative of the monitoring of the subject;determining whether the subject is still;if the subject is determined as still, determining based on input from the user whether all regions of the video image from which vital signs are detected are on the subject by displaying the video image of the subject with an indication in the video image of where vital signs are being detected and accepting user input to the graphical user interface indicating whether there are any regions of the image from which vital signs are being detected that are not on the subject; andonly if the subject is determined as still and all regions of the video image from which vital signs are detected are on the subject, displaying the determined vital signs.","12","16/291728","2019-03-04","2019-0272635","2019-09-05","10909678","2021-02-02","OXEHEALTH LIMITED","Nicholas Dunkley  Hutchinson | Oliver John  Gibson | Peter Richard  Dodds","2018003508","GB","2018-03-05","G06T-0007/0012","G06T-0007/0012 | A61B-0005/02416 | A61B-0005/1128 | A61B-0005/6889 | G06F-0003/0482 | G06F-0003/04817 | G06T-0007/20 | H04N-0007/183 | G06T-2200/24 | G06T-2207/10016 | G06T-2207/20104 | G06T-2207/30196 | G06T-2207/30232","G06F-003/0481","G06F-003/0481 | H04N-007/18 | A61B-005/11 | G06T-007/00 | G06F-003/0482 | G06T-007/20 | A61B-005/00 | A61B-005/024","","","","","","4921006005338"
"US","US","P","B2","Alarm routing optimization strategies in targeted alarm system","A targeted alarm system is described that includes a network probe for sending test data to terminal devices connected to a network and deriving reliability data of the terminal devices and the network. When a targeted alarm message needs to be sent, the system identifies a targeted terminal device based on the reliability data for sending the targeted alarm message. Related methods, apparatus, and non-transitory computer readable media are also disclosed.","1. A targeted medical alarm system comprising: a network adapter configured to communicate with a plurality of terminal devices over one or more networks, each of the terminal devices being associated with a respective caregiver;one or more data processors; anda computer-readable medium storing instructions that when executed by the one or more data processors, performs operations comprising: transmitting test data to each of the terminal devices;receiving acknowledgement data indicating when the test data was received by the respective terminal device;automatically measuring network conditions of the one or more networks by determining, for at least one of the terminal devices, one or more latency-related values associated with each of the test data and the acknowledgement data, the latency-related values including a message latency;automatically updating reliability data based at least in part on the latency-related values including the message latency, the reliability data including at least one of (i) historical latency-related values for the at least one of the terminal devices and (ii) historical response data for a respective caregiver associated with the at least one of the terminal devices, the historical response data including one or more response times of the respective caregiver to one or more previous targeted messages;identifying a targeted terminal device to which a current targeted message is to be sent based at least in part on the updated reliability data including the at least one of (i) the historical latency-related values for the at least one of the terminal devices and (ii) the historical response data for the respective caregiver associated with the at least one of the terminal devices, thereby reducing delay or failure of delivery of the current targeted message; andsending the current targeted message to the targeted terminal device.","28","14/896659","2014-12-18","2016-0371449","2016-12-22","10911891","2021-02-02","DR?GERWERK AG & CO. KGAA","Christopher J.  Brouse","","","","H04W-0004/023","H04W-0004/023 | G08B-0025/001 | G08B-0025/003 | G08B-0025/004 | G08B-0029/185 | G16H-0040/20 | G16H-0040/63 | H04W-0004/90 | A61B-0005/1113 | A61B-0005/746 | G06Q-0010/1091 | G06Q-0050/22 | G08B-0026/006 | G08B-0026/007 | H04L-0067/12 | H04M-2242/04","H04W-004/02","H04W-004/02 | G08B-025/00 | G08B-029/18 | G16H-040/63 | H04W-004/90 | G16H-040/20 | A61B-005/11 | G06Q-010/10 | G08B-026/00 | G06Q-050/22 | H04L-029/08 | A61B-005/00","","","","","","4921006007528"
"US","US","P","B1","Contact tracing via location service","Systems, methods, and computer-executable instructions for contact tracing including receiving a first beacon identifier associated with a first person and a second beacon identifier associated with a second person over a time period from a plurality of readers. A location of the first person and the second person is determined. Health data associated with the first person is received. The first person is determined to be sick and a sick period of time is determined. Two or more people, including the second person, are determined to have been within a proximity of the first person during the sick period of time. A list of the two or more people is generated.","1. A method for contact tracing, the method comprising operations performed using an electronic processor, the operations comprising: receiving a first beacon identifier associated with a first person over a time period from a plurality of readers;receiving a second beacon identifier associated with a second person over the time period from the plurality of readers;determining first locations of the first person during the time period based on the received first beacon identifier;determining second locations of the second person during the time period based on the received second beacon identifier;receiving health data associated with the first person;determining, based on the health data, the first person is sick;determining, based on the health data, a sick period of time for the first person, wherein the sick period of time is within the time period;determining two or more people, including the second person, were within a proximity of the first person during the sick period based on the first locations, wherein the determining the second person was within the proximity of the first person during the sick period of period comprises determining the second person was within a predetermined distance from the first person; andgenerating a list of the two or more people that were within the proximity of the first person during the sick period.","20","16/915417","2020-06-29","","","10911893","2021-02-02","DECURTIS LLC","David  DeCurtis | Derek  Fournier | Matthew  Winans | Debashis  Choudhury | James  Learish","","","","H04W-0004/023","H04W-0004/023 | A61B-0005/02427 | G06K-0009/00771 | G06Q-0010/06311 | G11B-0027/031 | H04L-0063/101 | H04W-0004/021 | H04W-0004/029 | H04W-0004/33","H04W-004/021","H04W-004/021 | H04W-004/029 | H04W-004/02 | H04W-004/33 | G11B-027/031 | G06Q-010/06 | G06K-009/00 | A61B-005/024 | H04L-029/06","","","","","","4921006007530"
"US","US","P","B2","Measuring a length of movement of an elongate intraluminal device","A measurement system for measuring a length of movement of an elongate intraluminal device. Cameras are included to obtain three dimensional video data of movement of an elongate intraluminal device by hand. The video data is processed to track the movement of the elongate intraluminal device in three dimensions to provide the length measurement of movement of the elongate intraluminal device.","1. A measurement and processing system for determining a length of movement of an elongate intraluminal device, comprising a data receiver and at least one processor, wherein: the data receiver is configured to receive video data from at least one camera, the received video data including motion of a user'ss hand interacting directly with the elongate intraluminal device to manually move the elongate intraluminal device; andthe at least one processor is configured to process the received video data to determine the length of movement of the elongate intraluminal device based at least on an analysis of the motion of the hand in the received video data.","14","16/314939","2017-07-06","2019-0151030","2019-05-23","10898274","2021-01-26","KONINKLIJKE PHILIPS N.V.","Michael  Grass | Julien  Senegas","2016-178114","EP","2016-07-06","A61B-0034/20","A61B-0034/20 | A61B-0005/061 | A61B-0005/1114 | A61B-0005/1122 | A61B-0005/1128 | A61B-0090/37 | G06F-0003/017 | G06F-0003/0304 | G06T-0007/246 | A61B-0005/749 | A61B-2017/00207 | A61B-2034/2055 | A61B-2034/2065 | A61B-2034/301 | A61B-2090/064 | A61B-2090/371 | A61B-2090/3735 | A61B-2090/3782 | A61B-2090/3937 | G06T-2200/04 | G06T-2207/10016 | G06T-2207/10028","A61B-034/20","A61B-034/20 | A61B-005/06 | A61B-005/11 | G06F-003/01 | A61B-090/00 | G06T-007/246 | G06F-003/03 | A61B-005/00 | A61B-017/00 | A61B-034/30","","","","","","4921005001155"
"US","US","P","B2","Calculating pace and energy expenditure from athletic movement attributes","Systems and methods configured to process motion data associated with a user. The systems and methods are configured to receive motion data from a sensor, calculate motion attributes from the data, and classify the motion data using one or more mathematical models. Attributes may be calculated without classifying the motion data into an activity type (such as walking, running, swimming, or any specific or general activity). Attributes may be compared to activity models comprising motion data from several individuals, which may not include the user. Motion data within the models and attributes of the user may be independent of any activity type. Attributes may be compared to select an energy expenditure model from one or more energy expenditure models, which may be selected as a best-match to the one or more motion attributes. An energy expenditure associated with the motion of the user may then be calculated.","1. A non-transitory computer-readable medium comprising computer-executable instructions that when executed by a processor, perform at least: generating a plurality of activity models using historical training data associated with a plurality of activities;receiving, from an accelerometer device and via one or more wireless networks, motion data comprising one or more data points as a result of motion of a user;calculating, in-real time and as the motion data is received from the accelerometer device, a first one or more attributes for the motion data;determining, for each activity model of the plurality of activity models, a probability that that activity model provides, from the plurality of activity models, a most accurate estimation of a second motion metric by comparing the first one or more attributes to second one or more attributes of that activity model;selecting, based on results of the comparing, a first model, from the plurality of activity models, as a best-match to the first one or more attributes, wherein the first model is selected as the best-match based on a determination that the first model is associated with a highest probability of each of the probabilities;calculating, using the first model, the second motion metric for the user, wherein the calculating comprises inputting the first one or more attributes into the first model and receiving an output comprising the second motion metric from the first model;calculating, using the second motion metric for the user, an energy expenditure associated with a motion of the user; andsending, to a display screen of a user device and via the one or more wireless networks, the energy expenditure.","21","14/514049","2014-10-14","2016-0045159","2016-02-18","10900992","2021-01-26","NIKE, INC.","Santoshkumar  Balakrishnan | Manan  Goel | Bradley W.  Wilkins | Corey  Dow-Hygelund | Jeff  Hazel | John  Schmitt","","","","G01P-0013/00","G01P-0013/00 | A61B-0005/0022 | A61B-0005/024 | A61B-0005/0205 | A61B-0005/02438 | A61B-0005/1118 | A61B-0005/1123 | A61B-0005/4866 | A61B-0005/6823 | A61B-0005/7235 | A61B-0005/7246 | A61B-0005/7278 | A63B-0024/0006 | A63B-0024/0062 | G01B-0021/00 | G01P-0015/00 | G01P-0015/18 | G06F-0001/163 | G06F-0003/011 | G06F-0003/014 | G06F-0003/017 | G06K-0009/00348 | G06K-0009/00543 | G06N-0005/04 | G06N-0020/00 | G16H-0040/67 | A61B-0005/002 | A61B-0005/0024 | A61B-0005/0059 | A61B-0005/01 | A61B-0005/0833 | A61B-0005/1112 | A61B-0005/4806 | A61B-0005/681 | A61B-0005/6804 | A61B-0005/6807 | A61B-0005/6898 | A61B-2562/0219 | A61B-2562/0247 | A63B-2024/0015 | A63B-2024/0068 | A63B-2230/06 | G06F-0019/3481 | G16H-0020/30","G01P-013/00","G01P-013/00 | G01B-021/00 | G01P-015/00 | G06N-020/00 | A61B-005/00 | A61B-005/0205 | A61B-005/024 | A61B-005/11 | G16H-040/67 | G06F-003/01 | G06K-009/00 | G01P-015/18 | A63B-024/00 | G06F-001/16 | G06N-005/04 | G16H-020/30 | A61B-005/01 | A61B-005/083 | G06F-019/00","","","","","","4921005003858"
"US","US","P","B2","Cadence determination and media content selection","Systems, devices, apparatuses, components, methods, and techniques for cadence determination and media content selection are provided. An example media-playback device comprises a media-output device that plays media content items, a cadence-acquiring device, and a cadence-based media content selection engine. The cadence-acquiring device includes an accelerometer and a cadence-determination engine configured to determine a cadence based on acceleration data captured by the accelerometer. The cadence-based media content selection engine is configured to identify a media content item based on the cadence determined by the cadence-determining engine and cause the media-output device to playback the identified media content item.","1. A media playback device comprising: at least one processing device;a display device connected to the at least one processing device; andat least one computer readable data storage device storing software instructions that, when executed by the at least one processing device, cause the media-playback device to: determine a stable cadence;identify a media content item based on the determined stable cadence;play the media content item; anddisplay a cadence-based content playback screen having a media content item display panel that displays the media content item being played and a cadence information message that displays information about the determined stable cadence, the cadence-based content playback screen further having a pause control that operates to receive a pause input that triggers the media playback device to pause cadence determination while continuing to play the media content item, the pause control displayed adjacent to the cadence information message.","19","16/264350","2019-01-31","2019-0235828","2019-08-01","10901683","2021-01-26","Spotify AB","Tristan  Jehan | Sten  Garmark | Dariusz  Dziuk | Rahul  Sen | Owen  Smith | Lars Christian  Olofsson | Nikolaos  Toumpelis","","","","G06F-0003/165","G06F-0003/165 | A61B-0005/112 | A61B-0005/1118 | A61B-0005/742 | A61B-0005/7405 | G10H-0001/0008 | G10H-0001/40 | G11B-0020/10527 | A61B-2503/10 | A61B-2503/12 | A61B-2562/0219 | G10H-2210/076 | G10H-2220/091 | G10H-2220/351 | G10H-2220/355 | G10H-2220/395 | G10H-2230/015 | G10H-2240/131","G06F-017/00","G06F-017/00 | G06F-003/16 | G11B-020/10 | G10H-001/00 | A61B-005/11 | A61B-005/00 | G10H-001/40","","","","","","4921005004544"
"US","US","P","B2","Methods and systems for remotely determining levels of healthcare interventions","The present subject matter relates to methods and systems utilizing wearable sensor technology to determine when a patient's health may be degrading to trigger progressively higher levels of care and involvement, from ""free"" hands and eyes to skilled clinicians, in order to keep patients in the lowest cost setting of care, the home, for as long as possible.","1. A distributed and reconfigurable healthcare communication system comprising: a wearable sensor configured to continually monitor a wearer, the wearable sensor comprising: at least one sensing device configured to sense activity of the wearer and generate sensed activity data of the wearer, anda processor configured to identify a location of the wearer during the activity and determine a current functional activity of the wearer based on the sensed activity data and the identified location, the current functional activity and the corresponding identified location forming current state information; andat least one remote server, in communication with the wearable sensor via a communication network, configured to receive the current state information from the wearable sensor, and reconfigure the healthcare communication system by selectively incorporating one or more additional devices into said healthcare communication system under particular conditions to provide additional wearer data, by:detecting, by the at least one remote server, an event based on deviation of the current state information from at least one baseline activity pattern established for the wearer,for the detected event, determining, by the at least one remote server, a statistical significance of the deviation of the current state information relative to the at least one baseline activity pattern,for the statistical significance, determining, by the at least on remote server, a degree of the statistical significance, wherein the degree of the statistical significance corresponds to a degree of a care level to be selected,selecting, by the at least one remote server, a care level among a plurality of hierarchical care levels based on the statistical significance of the deviation, the degree of the statistical significance, and a previous health-state of the wearer, wherein the plurality of hierarchical care levels comprise at a lower care level corresponding to a lower degree statistical significance determined for the detected event and a higher care level corresponding to a higher degree of statistical significance determined for the detected event,transmitting, by the at least one remote server, a participation notification to a care level device associated with the selected care level, the participation notification based on at least one of the detected event, the selected care level and the previous health-state of the wearer, to incorporate the care level device into the healthcare communication system,causing, by the at least one remote server, the healthcare communication system to collect the additional wearer data via the incorporated device, andautomatically, by the at least one remote server, updating the health-state of the wearer based on evaluation of the collected additional wearer data, the current state information, the detected event, and predetermined clinical guidelines, to determine whether to perform a further action by the remote server,wherein the further action includes obtaining additional health-state information of the wearer via a care level device of a different care level among the plurality of care levels, based on the updated health-state.","30","15/703507","2017-09-13","2018-0025123","2018-01-25","10902090","2021-01-26","CAREPREDICT, INC.","Satish  Movva","","","","G06F-0019/3418","G06F-0019/3418 | A61B-0005/002 | A61B-0005/0017 | A61B-0005/0022 | A61B-0005/1113 | A61B-0005/1116 | A61B-0005/1121 | A61B-0005/1123 | A61B-0005/1127 | A61B-0005/7275 | A61B-0005/747 | G06Q-0050/22 | G16H-0020/30 | G16H-0040/67 | G16H-0050/70 | A61B-0005/0533 | A61B-0005/1117 | A61B-0005/1118 | A61B-0005/6801 | A61B-0005/6843 | A61B-2562/0219 | A61B-2562/0223 | A61B-2562/0271 | G16H-0010/65 | G16H-0040/63","G06Q-050/22","G06Q-050/22 | G06F-017/40 | G06Q-010/10 | G06F-019/00 | A61B-005/11 | A61B-005/00 | G16H-020/30 | G16H-050/70 | G16H-040/67 | A61B-005/053 | G16H-010/65 | G16H-040/63","","","","","","4921005004947"
"US","US","P","B2","Video monitoring system","An asset tracking system includes a camera adapted to capture images and output signals representative of the images. The camera may include one or more depth sensors that detect distances between the depth sensor and objects positioned within the field of view of the one or more cameras. A computer device processes the image signals and or depth signals from cameras and determines any one or more of the following: (a) whether a patient care protocol has been properly followed; (b) what condition a patient is in; (c) whether an infection control protocol has been properly followed; and (d) whether steps have been taken to reduce the risk of a patient from falling. Alerts may be issued if any conditions of importance are detected.","1. A bed system for a patient care facility comprising: a bed comprising a base, a plurality of wheels coupled to the base, a patient support surface supported on the base and configured to support a patient thereon, a plurality of siderails positioned adjacent the patient support surface and adapted to move between up and down positions;a camera positioned within a room of the patient care facility and configured to capture images of the bed and a patient positioned on the bed, the camera adapted to output signals representative of the images;a database containing shape information regarding a shape of the bed and a shape of the siderails; anda computer device in communication with the camera and the database, the computer device configured to use the signals and the shape information to identify both the bed and a partial skeleton of the patient within the images, the partial skeleton including a plurality of points corresponding to three dimensional locations of the patient'ss head, arms, and hands, the computer device adapted to monitor movement of the points relative to each other and relative to the bed in order to determine at least one of the following: (a) if the patient is eating; (b) is the patient is sleeping; or (c) if the patient is entrapped in any of the siderails.","20","16/524343","2019-07-29","2019-0349554","2019-11-14","10904492","2021-01-26","Stryker Corporation","Richard A.  Derenne | Richard Thomas  DeLuca | Jason James  Wroblewski | Sanjay  Dhall | Xiyu  Duan | Vishal P.  Lowalekar","","","","H04N-0007/185","H04N-0007/185 | A61B-0005/002 | A61B-0005/0013 | A61B-0005/0033 | A61B-0005/0036 | A61B-0005/112 | A61B-0005/1113 | A61B-0005/1128 | A61B-0005/7445 | G01B-0011/026 | G06F-0019/3418 | G08B-0021/043 | G08B-0021/0476 | G08B-0021/245 | G16H-0040/20 | G16H-0040/67 | H04N-0005/33 | A61B-0005/1115 | A61B-0005/1117 | A61B-0005/1123 | A61B-0005/6889 | A61B-0005/6892 | A61B-0005/7475 | A61B-2505/03 | G06K-0009/00335 | G06K-0009/00771 | G06Q-0050/22 | G08B-0021/22","H04N-007/18","H04N-007/18 | A61B-005/00 | A61B-005/11 | G08B-021/24 | G06F-019/00 | G08B-021/04 | G16H-040/20 | G01B-011/02 | H04N-005/33 | G16H-040/67 | G06K-009/00 | G08B-021/22 | G06Q-050/22","","","","","","4921005007325"
"US","US","P","B2","Predictively controlling operational states of wearable device and related methods and systems","A method for predictively controlling an operational state of a wearable device, according to one embodiment, includes collecting historical sensor data acquired by one or more sensors of a wearable device. The historical sensor data is analyzed for detecting a pattern therein. A time to change an operational state of the wearable device is predicted based on the analysis. The change of the operational state is instructed at the predicted time.","1. A method for predictively controlling an operational state of a wearable device, the method comprising: collecting historical sensor data acquired by one or more sensors of a wearable device;in response to a determination that at least some of the historical sensor data is inaccurate, correcting the inaccurate historical sensor data, wherein the inaccurate historical sensor data is corrected using predefined spectral analysis algorithms,analyzing the historical sensor data for detecting a pattern therein, wherein the analyzed historical sensor data includes only accurate historical sensor data and/or corrected historical sensor data; andinstructing change of the operational state at a time that is determined based on the detected pattern.","15","16/457551","2019-06-28","2019-0336009","2019-11-07","10893816","2021-01-19","CAEDEN, INC.","Skip Thomas  Orvis | Nora Elam  Levinson | Jamal Eddine  Mouline","","","","A61B-0005/02438","A61B-0005/02438 | A61B-0005/0004 | A61B-0005/0006 | A61B-0005/0022 | A61B-0005/0205 | A61B-0005/02405 | A61B-0005/087 | A61B-0005/0816 | A61B-0005/486 | A61B-0005/4866 | A61B-0005/6823 | A61B-0005/725 | A61B-0005/7264 | A61B-0005/7278 | A61B-0005/742 | A61B-0005/7405 | A61B-0005/746 | A61B-0005/7455 | G04G-0017/04 | G06F-0001/163 | G06F-0003/011 | G06F-0003/0304 | G06K-0009/00892 | A61B-0005/0245 | A61B-0005/02416 | A61B-0005/02433 | A61B-0005/6824 | A61B-2560/0247 | G06K-2009/00939","A61B-005/0205","A61B-005/0205 | A61B-005/024 | G06F-003/03 | A61B-005/00 | A61B-005/08 | A61B-005/087 | G04G-017/04 | G06F-001/16 | G06F-003/01 | G06K-009/00 | A61B-005/0245","","","","","","4921004000611"
"US","US","P","B2","Determination apparatus, imaging apparatus, driver confirmation system, moveable body, and determination method","A determination apparatus includes a first acquisition unit, a second acquisition unit, and a controller. The first acquisition unit acquires a facial photograph image, included in a medium holding the facial photograph image, as a first facial image. The second acquisition unit acquires a captured image obtained by capturing an image of an imaging area, in which the face of a driver is assumed to be located, at a start of driving and/or during driving. The controller extracts an image of the face of the driver as a second facial image from the captured image acquired from the second acquisition unit, compares the first facial image and the second facial image, and determines whether the driver is the person in the facial photograph image held by the medium.","1. A determination apparatus comprising: a processor configured to, acquire a facial photograph image, included in a medium holding the facial photograph image, as a first facial image,acquire a captured image obtained by capturing an image of an imaging area, in which a face of a driver is located, at a start of driving and/or during driving a moveable body,extract an image of the face of the driver as a second facial image from the captured image,compare the first facial image and the second facial image,determine whether the driver is a person in the facial photograph image held by the medium, anddetermine whether the driver is permitted to drive the moveable body based on one or both of, whether the medium indicates a type of driver'ss license that matches a type of the moveable body, orwhether the medium indicates an identifier of the driver that matches member information for the moveable body.","16","16/328700","2017-08-10","2019-0192055","2019-06-27","10893828","2021-01-19","KYOCERA CORPORATION","Tomoki  Mizobuchi","2016-167403","JP","2016-08-29","A61B-0005/1171","A61B-0005/1171 | B60R-0011/02 | B60R-0016/02 | B60R-0025/25 | G06F-0021/32 | G06T-0007/00 | G06T-0007/0002 | G08B-0013/00","G06F-021/32","G06F-021/32 | G06T-007/00 | A61B-005/1171 | B60R-025/25 | B60R-011/02 | B60R-016/02 | G08B-013/00","","","","","","4921004000623"
"US","US","P","B2","Triage of training data for acceleration of large-scale machine learning","Triage of training data for acceleration of large-scale machine learning is provided. In various embodiments, training input from a set of training data is provided to an artificial neural network. The artificial neural network comprises a plurality of output neurons. Each output neuron corresponds to a class. From the artificial neural network, output values are determined at each of the plurality of output neurons. From the output values, a classification of the training input by the artificial neural network is determined. A confidence value of the classification is determined. Based on the confidence value, a probability of inclusion of the training input in subsequent training is determined. A subset of the set of training data is determined based on the probability. The artificial neural network is trained based on the subset.","1. A method, comprising: providing training input from a set of training data to an artificial neural network, the artificial neural network comprising a plurality of output neurons, each output neuron corresponding to a class;determining, from the artificial neural network, output values at each of the plurality of the output neurons;determining, from the output values, a classification of the training input by the artificial neural network;determining a confidence value of the classification by comparing the output values to an output value that corresponds to a predetermined class of the training input;based on the confidence value, determining a probability of inclusion of the training input in subsequent training;generating a subset of the set of training data based on the probability; andtraining the artificial neural network based on the subset.","19","15/449458","2017-03-03","2018-0253645","2018-09-06","10896370","2021-01-19","INTERNATIONAL BUSINESS MACHINES CORPORATION","Geoffrey W.  Burr","","","","G06N-0003/08","G06N-0003/08 | G06N-0003/084","G01N-033/48","G01N-033/48 | G06F-003/01 | A61B-005/00 | G06N-099/00 | G06N-005/02 | G06N-003/08","","","","","","4921004003148"
"US","US","P","B2","Methods and systems for assessing psychological characteristics","A method for assessing a pre-cognitive emotional response from a test subject, using responses obtained during the first moments of brain activity after presentation of a stimulus, includes exposing the test subject to a visual stimulus for between approximately 500 milliseconds and approximately 1 second, and receiving an input from the subject while the subject is exposed to the visual stimulus or within approximately 300 milliseconds after the subject is first exposed to the stimulus. The method further includes storing, in response to receiving the input, a user response that identifies one of a plurality of emotional reactions that is associated with the visual stimulus. Each of the exposing, receiving, and storing acts is repeated for a plurality of visual stimuli. The method further includes determining, based on each of the stored user responses, one or more dominant emotional characteristics of the subject.","1. A method comprising: maintaining a set of sensory stimuli, each of the sensory stimuli being associated with an identified reaction;presenting, to a subject, by a computer through a presentation interface, the sensory stimuli to the subject, including, for each sensory stimulus:presenting the sensory stimulus during a presentation period having a duration of more than 500 milliseconds and less than 1000 milliseconds, the duration of the presentation period having been previously defined, the duration of the presentation period being enforced by computer,ending the presenting of the sensory stimulus at the end of the presentation period,providing a grace period immediately following the end of the presentation period of time, the grace period having a duration of up to 300 milliseconds during which none of the sensory stimuli is presented, the duration of the grace period having been previously defined, the duration of the grace period being enforced by computer, andpresenting a subsequent sensory stimulus at the end of the grace period;by a computer, receiving a response, by the subject, to at least one of the sensory stimuli during a response period including the presentation period and the grace period, the response of the subject to a particular sensory stimulus representing a reaction of the subject to the particular sensory stimulus in relation to a context;by a computer, storing information that represents the reaction of the subject to each of the sensory stimuli for which a response was received; andby a computer, determining, based on at least some of the stored information, an emotional characterization of the subject in relation to the context.","18","15/419755","2017-01-30","2017-0300930","2017-10-19","10896431","2021-01-19","THE FORBES CONSULTING GROUP, LLC","David Lowry  Forbes","","","","G06Q-0030/02","G06Q-0030/02 | A61B-0005/04842 | A61B-0005/16 | A61B-0005/165 | A61B-0005/167 | G06F-0016/436 | G06K-0009/00308 | G06Q-0010/06398 | G06Q-0030/0203 | G06Q-0030/0242 | G09B-0007/00 | G16H-0050/70 | G16H-0010/20 | G16H-0050/20","G06Q-030/02","G06Q-030/02 | A61B-005/16 | G06Q-010/06 | A61B-005/0484 | G09B-007/00 | G06K-009/00 | G06F-016/435 | G16H-050/70 | G16H-010/20 | G16H-050/20","","","","","","4921004003209"
"US","US","P","B2","Troubleshooting system for remote patient monitoring","The technology herein relates to a troubleshooting system for remote patient monitoring. A plurality of triggering conditions defines a data transmission error between a sensor and a remote location. A data transmission log is configured to receive characterization data defining each successful data transmission between a communicator and the remote location. An input user interface is configured to receive input from a user and an output user interface is configured to provide notification to a user. Processing circuitry is in communication with the input interface and the output interface, where the processing circuitry is configured to compare each of the triggering conditions to the characterization data to identify a data transmission error. Upon identification of the data transmission error, the processing circuitry causes the output interface to present a query to the user.","1. A system comprising: a database defining a plurality of triggering conditions defining a data transmission error in sensor data transmission between a sensor and a remote location;a communicator comprising a network interface and a sensor interface, whereby the communicator is configured to transmit data between the sensor and the remote location;a data transmission log configured to receive characterization data defining each successful data transmission between the communicator and the remote location, wherein the characterization data comprises the time of a most recent successful data transmission, the type of sensor data transmitted in the most recent successful data transmission, and a timespan between successive data points in the sensor data transmitted in the most recent successful data transmission;an input user interface and an output user interface, wherein the input user interface is configured to receive input from a user and the output user interface is configured to provide notification to a user; andprocessing circuitry in communication with the input user interface and the output user interface, wherein the processing circuitry is configured to compare each of the triggering conditions to the characterization data to identify a data transmission error, and wherein upon identification of the data transmission error, the processing circuitry causes the output interface to present a query to the user;wherein the communicator is further configured to send the characterization data to the data transmission log.","18","16/239379","2019-01-03","2019-0206557","2019-07-04","10896754","2021-01-19","CARDIAC PACEMAKERS, INC.","Qi  An | Kimberly Anne  Eridon | Arwinder Pal  Singh | Pratik K.  Pandya | Ranganathan Balasubramanian  Iyer | Viktoria A.  Averina","","","","G16H-0040/40","G16H-0040/40 | A61B-0005/0002 | A61B-0005/0022 | A61N-0001/37 | G06F-0011/0778 | G06F-0011/0784 | G16H-0010/60 | G16H-0040/60 | G16H-0040/67 | H04L-0029/08558 | A61N-0001/37264 | G06F-0008/65 | G16H-0080/00 | H04L-0067/12 | H04L-2209/88","G16H-040/40","G16H-040/40 | A61B-005/00 | G16H-010/60 | G16H-040/67 | G06F-011/07 | G16H-040/60 | A61N-001/37 | H04L-029/08 | G06F-008/65 | A61N-001/372 | G16H-080/00","","","","","","4921004003532"
"US","US","P","B2","Establishing secure communication at an emergency care scene","Among other things, we describe a system that includes a first medical device for treating a patient at an emergency care scene, the first medical device including a processor and a memory configured to detect a request for a connection between the first medical device and a second medical device for treating the patient at the emergency care scene, the request for connection including an identifier of the second medical device, responsive to receiving the request for connection, enabling a wireless communication channel to be established between the first medical device and the second medical device based on the identifier of the second medical device and an identifier of the first medical device; and enabling transmission and/or exchange of patient data between the first medical device and the second medical device via the wireless communication channel. Such communications with more than two devices may also be possible.","1. A system for establishing secure dynamically reconfigurable wireless communications between a patient monitor and a computing device, the system comprising: the patient monitor comprising one or more physiological sensors configured to measure physiological data from a patient and the patient monitor being configured to perform a close proximity wireless communication protocol;the computing device comprising a receiver and a transmitter configured to establish a secure communication channel with the patient monitor according to the close proximity wireless communication protocol, the secure communication channel having an effective range of less than 100 cm to the patient monitor;a sensor configured to detect at least one feature from an immediate environment of the patient monitor; anda processor and a non-transitory computer readable storage medium encoded with a computer program comprising instructions that, when executed, cause the processor to perform operations comprising: detecting a request for connection between the patient monitor and the computing device based at least in part on the at least one feature of the immediate environment,determining whether spatial localization is achieved between the patient monitor and the computing device based at least in part on the at least one feature of the immediate environment,initiating a mutual authentication between the computing device and the patient monitor based at least in part on the spatial localization, andestablishing the secure communication channel between the patient monitor and the computing device for exchanging patient data.","34","16/685613","2019-11-15","2020-0084629","2020-03-12","10888229","2021-01-12","ZOLL MEDICAL CORPORATION","Gary A.  Freeman | Guy R.  Johnson | Frederick J.  Geheb | Mark  Weary | Timothy F.  Stever","","","","A61B-0005/0077","A61B-0005/0077 | A61B-0005/02055 | A61B-0005/0402 | A61B-0005/6801 | A61M-0016/0048 | A61M-0016/0057 | A61M-0016/10 | A61N-0001/37252 | A61N-0001/39044 | G16H-0040/63 | H04L-0063/0869 | H04W-0004/80 | H04W-0004/90 | H04W-0012/0609 | H04W-0072/04 | H04W-0076/10 | A61B-0005/021 | A61B-0005/0816 | A61B-0005/0836 | A61B-0005/1123 | A61B-0005/14532 | A61B-0005/14552 | A61B-2560/0252 | A61B-2560/0257 | A61B-2562/0204 | A61B-2562/0219 | A61M-2205/332 | A61M-2205/3303 | A61M-2205/3368 | A61M-2205/3375 | A61M-2205/3569 | A61M-2205/3592 | A61M-2205/505 | A61M-2230/63 | H04L-0067/12 | H04W-0076/50","H04W-012/06","H04W-012/06 | A61N-001/39 | A61B-005/00 | H04W-072/04 | A61B-005/0205 | A61B-005/0402 | A61M-016/00 | A61N-001/372 | H04W-076/10 | H04W-004/80 | A61M-016/10 | H04L-029/06 | H04W-004/90 | G16H-040/63 | H04L-029/08 | A61B-005/021 | A61B-005/083 | A61B-005/11 | A61B-005/145 | A61B-005/1455 | A61B-005/08 | H04W-076/50","","","","","","4921003000772"
"US","US","P","B1","COVID-19 symptoms alert machine (CSAM) scanners","A COVID-19 Symptoms Alert Machine (CSAM) scanner, or apparatus, is described herein. This apparatus employs Artificial Intelligent (AI) technology in combination with the latest mobile device technology (viz. smart phone/smart watch) to quickly help track down people who have COVID-19 symptoms anywhere and anytime, isolate them, and professionally handle them, not allowing SARS-CoV-2 virus to spread. CSAM automatically measures body temperature and assesses lung conditions such as pulmonary fibrosis and B-lines (for asymptomatic people), and other current health vital information (CHVI), furnished by the participant, such as fever, sore throat, headache, and body ache to generate an alert signal when COVID-19 symptoms are found significant and to send it out to a COVID-19 control center. The alerted participant is then immediately required to go to the COVID-19 control center or be picked up by a special COVID-19 emergency vehicle for isolation and further evaluation and testing. If the testing turns out to be COVID-19 positive, the participant will be quarantined and treated appropriately according to COVID-19 protocol until he/she is tested COVID-19 negative. In the meantime, people who have been in close physical contact with this participant will be alerted and requested to be immediately checked for COVID-19 symptoms. If anyone is found to have COVID-19 symptoms, then he/she must go through the same protocol. The process is repeated until all people in the cluster are tested COVID-19 negative. This will ensure that SARS-CoV-2 virus for this cluster has been completely eliminated. A rapid deployment of this type of apparatus throughout communities where people tend to congregate such as superstores, supermarkets, and any other establishments, small or large, can help to contain the rapid spread of the disease, as well as to give more confidence to the general public. People, who pass through this apparatus without an alert signal, should feel more confident in carrying out their activities, though social distancing and other COVID-19 precautionary requirements should still be maintained. The concept can be further expanded to cover shopping malls, concert halls, sports arenas, and any other large events including highways and freeways with the help of mobile phone technologies, transponders, and other mobile devices. By working on the 0.6% (around 2 million infected people in the US as of June 2020) quickly and effectively, instead of on the 99.4% (330 million, the remaining population) by locking people at home and closing down all businesses and activities; we can save a significant amount of money and hassles. (A long lockdown can also lead to a collapse of our economy and can consequently lead to a worldwide calamity.) In this way the 99.4% will not be burdened with the virus problem and can live normally without having to take any test. It is probably the only effective approach in solving the COVID-19 problem at the moment because vaccines and known COVID-19 cures are not yet available. Even if SARS-CoV-2 vaccines are available presently, they may not be practical to implement economically and operationally in time to contain the virus worldwide due to the massive amount of people (viz. over 7 billion).","1. A COVID-10 Symptoms Alert Machine (CSAM) scanner comprising: a main CSAM scanner, including:a human body temperature scanner for scanning a body temperature of a participant;a current health vital information (CHVI) card, provided by the participant, containing information of the participant and COVID-19, and configured for being read by the CSAM scanner; anda Central Processing Unit (CPU) and a memory having computer readable instructions stored thereon, that when executed by the CPU, cause the CPU to controlling the operation of the CSAM scanner.","14","16/917896","2020-06-30","","","10888283","2021-01-12","Boonsieng Benjauthrit | Sorapod B. Benjauthrit | Vatcharee L. Benjauthrit | Kamolchanok J. Benjauthrit","Boonsieng  Benjauthrit | Sorapod B.  Benjauthrit | Vatcharee L.  Benjauthrit | Kamolchanok J.  Benjauthrit","","","","A61B-0005/746","A61B-0005/746 | A61B-0005/0022 | A61B-0005/01 | A61B-0005/055 | A61B-0005/0816 | A61B-0005/0823 | A61B-0005/4011 | A61B-0005/4017 | A61B-0006/032 | A61B-0008/08 | G06Q-0010/107 | G06Q-0050/265 | G16H-0010/20 | G16H-0010/65 | G16H-0015/00 | G16H-0040/20 | G16H-0040/67 | G16H-0050/30 | G16H-0050/50 | G16H-0050/80 | G06K-0007/10297 | G06Q-2240/00 | G07C-0009/15 | G16H-0050/20","A61B-005/00","A61B-005/00 | G16H-040/67 | G16H-050/30 | G16H-050/80 | G16H-040/20 | G06Q-050/26 | G16H-015/00 | G06Q-010/10 | G16H-010/20 | G16H-050/50 | A61B-005/01 | A61B-008/08 | A61B-006/03 | A61B-005/055 | A61B-005/08 | G16H-010/65 | G07C-009/15 | G16H-050/20 | G06K-007/10","","","","","","4921003000826"
"US","US","P","B2","System and method for identifying and authenticating a user of a medical device, and controlling access to patient data generated by the medical device","A system includes a medical device and an authenticator. The medical device has a medical sensor configured to generate patient data indicative of a sensed physiological characteristic of a patient. The authenticator has at least one biometric sensor configured to generate at least one biometric signal indicative of at least one biometric characteristic of a user of the medical device. The authenticator is configured to (i) identify and authenticate a user of the medical device, (ii) protect patient data generated by the medical sensor of the medical device using a user ID unique to the identified user, and (iii) save the protected patient data to a memory.","1. A system, comprising: an endoscope having a medical sensor configured to generate patient data indicative of a sensed physiological characteristic of a patient; andan authenticator having at least one biometric sensor configured to generate at least one biometric signal indicative of at least one biometric characteristic of a user of the endoscope, the at least one biometric sensor including a palm vein sensor and a ballistocardiography (BCG) sensor;the authenticator configured to (i) identify and authenticate a user of the endoscope, (ii) protect patient data generated by the medical sensor of the endoscope using a user ID unique to the identified user, and (iii) save the protected patient data to a memory.","26","15/907919","2018-02-28","2019-0266344","2019-08-29","10891394","2021-01-12","KARL STORZ IMAGING, INC.","Matteo  Contolini | Juri  Baumberger","","","","G06F-0021/6245","G06F-0021/6245 | A61B-0005/067 | A61B-0005/117 | G06F-0021/32 | G06K-0009/00885 | G16H-0010/60 | G16H-0040/63 | H04L-0063/102 | H04L-0067/306 | A61B-0005/1102 | G06K-2009/00932","G06F-021/62","G06F-021/62 | G16H-010/60 | A61B-005/06 | A61B-005/117 | G06F-021/32 | G06K-009/00 | H04L-029/06 | H04L-029/08 | G16H-040/63 | A61B-005/11","","","","","","4921003003913"
"US","US","P","B2","Information collection system","An information collection system 1 has: a stand-alone-power-type transmitter 10 capable of moving together with a mobile entity (purchaser) A1, the transmitter 10 spontaneously and intermittently transmitting a transmitter ID at a timing corresponding to the distance of movement of the mobile entity A1; a plurality of receivers 20 provided in an area X to be monitored in which the mobile entity A1 can move freely, the receivers 20 receiving the transmitter ID transmitted from the transmitter 10 present in the vicinity of the area X; and a server 30 for managing the transmitter ID, the position information (receiver ID) of the receiver 20 that receives the transmitter ID, and a time of day at which the receiver ID is received, in association with each other.","1. An information collecting system, comprising: a plurality of transmitters of a self-powered type, each transmitter being fitted to a respective shopping cart, a shopping basket, or an admission permit carried by a respective shopper who can move about in a store or a market, each transmitter being operable to transmit intermittently a respective transmitter ID, unique to the respective transmitter, spontaneously with random timing each time the shopper moves a predetermined distance;a plurality of receivers provided at scattered places in the store or the market, the receivers operable to receive the transmitter ID transmitted from one of the transmitters located nearby;a register that associates shopping information of the shopper with the transmitter ID; anda server that manages the transmitter ID, location information of a receiver that received the transmitter ID, a reception time of the transmitter ID, and the shopping information of the shopper in an associated manner, the server thereby operable to investigate a series of shopping activities including what path the shopper took, where the shopper stayed and how long, and what the shopper bought, the server operable to output results of the investigation,wherein each of the transmitters includes: an electric generator configured to convert a kinetic energy, resulting from movement of the shopper about the store or market, into an electric energy;an electric storage configured to store the electric energy generated by the electric generator; anda transmission circuit configured to be supplied with the electric energy, and to transmit the transmitter ID, each time a predetermined amount of electric energy is stored in the electric storage, wherein the transmission circuit transmits the transmitter ID by being supplied with a charged voltage stored in the electric storage each time the charged voltage reaches a predetermined operable voltage of the transmission circuit, andwherein, the system is configured such that, even though there are multiple ones of the transmitters in the store or the market, each particular one of the transmitters is configured to prevent collision by intermittently transmitting the transmitter ID, which is unique to the particular transmitter, spontaneously with timing correlated to amounts of movement of shoppers.","7","15/531016","2015-08-03","2017-0323311","2017-11-09","10891636","2021-01-12","ROHM CO., LTD.","Takashi  Naiki","2014-241627","JP","2014-11-28","G06Q-0030/0201","G06Q-0030/0201 | G01C-0022/006 | G06K-0007/10 | G06K-0007/10415 | G06K-0019/07 | G06K-0019/0707 | G06K-0019/0716 | G06K-0019/0723 | G06Q-0010/087 | G06Q-0010/0833 | G06Q-0020/20 | G06Q-0030/02 | A61B-0005/0002 | A61B-0005/1113 | A61B-0005/1117","G06Q-030/02","G06Q-030/02 | G06K-007/10 | G06K-019/07 | G06Q-010/08 | G01C-022/00 | G06Q-020/20 | A61B-005/11 | A61B-005/00","","","","","","4921003004153"
"US","US","P","B2","Home automation system including user interface operation according to user cognitive level and related methods","A home automation (HA) system may include a cloud server, HA operation devices within a senior living facility, and HA user interface devices for respective users within the senior living facility. Each HA user interface device may include a user input device, a display defining a user interface (UI), and a controller. The HA system may include HA hub devices within the senior living facility to provide communications for the cloud server, the HA user interface devices, and the HA operation devices. The controller may send user interaction data to the cloud server and operate the UI according to a user cognitive level. The cloud server may be configured to determine the user cognitive level based upon the user interaction data received from a given HA user interface device, and send the user cognitive level to the given HA user interface device.","1. A home automation (HA) system comprising: a cloud server;a plurality of HA operation devices;a plurality of HA user interface devices for respective users and each comprising a user audio input device, a touch display defining a user interface (UI), and a controller coupled to the user audio input device and the touch display, the controller of each HA user interface device being switchable between a voice input mode for receiving user input based upon the user audio input device and a touch input mode for receiving user input based upon the touch display; anda plurality of HA hub devices to provide communications for the cloud server, the plurality of HA user interface devices and the plurality of HA operation devices;the controller of each HA user interface device configured to send user interaction data to the cloud server and operate the UI according to a user cognitive level, the user interaction data comprising a current touch display user contact location and historical touch display user contact locations; andthe cloud server configured to determine the user cognitive level based upon the user interaction data received from a given HA user interface device,decrease the user cognitive level based upon the current touch display user contact location being outside a touch location threshold relative to the historical touch display user contact locations, andsend the user cognitive level to the given HA user interface device; andthe controller of each HA user interface device being configured to switch from the touch input mode to the voice input mode based upon the decreased user cognitive level.","20","16/176315","2018-10-31","2019-0182071","2019-06-13","10892907","2021-01-12","K4CONNECT INC.","Jonathan Andrew  Gould | Richard  Clancy | Robert Shannon  Smith","","","","H04L-0012/2803","H04L-0012/2803 | A61B-0005/0022 | A61B-0005/1113 | A61B-0005/4088 | A61B-0005/4842 | G06F-0003/0484 | G06F-0003/0488 | A61B-0005/1101 | A61B-0005/16 | A61B-0005/746 | A61B-2503/08 | A61B-2505/07 | A63F-0013/23 | A63F-0013/25 | A63F-2300/308 | G06T-0007/50 | H04L-0067/12","H04L-012/28","H04L-012/28 | G06F-003/0484 | G06F-003/0488 | A61B-005/00 | A61B-005/11 | A63F-013/23 | H04L-029/08 | G06T-007/50 | A63F-013/25 | A61B-005/16","","","","","","4921003005414"
"US","US","P","B2","System and method for gathering and analyzing biometric user feedback for use in social media and advertising applications","Systems and methods for measuring biologically and behaviorally based responses to social media, locations, or experiences and providing instant and continuous feedback in response thereto are disclosed. An example system includes a first sensor to determine an emotional response of a user exposed to a social media application, a second sensor to determine a current activity of the user, and a third sensor to determine an environment of the user. The example system also establishes a priority schedule based on the emotional response, the current activity, and the environment. The system also correlates, based on the priority schedule, an advertisement with at least one of the emotional response, activity, or the environment. In addition, the example system presents the advertisement based on the priority schedule and the correlation of the advertisement with the at least one of the activity, the environment, or the emotional response.","1. A system comprising: a mobile device configured to access a social media application or internet-accessible application, the mobile device including: a first sensor; anda second sensor;a case for the mobile device;a third sensor in or on the case, the third sensor configured to measure a biometric response of a user of the mobile device while exposed to the social media application or internet-accessible application;memory including instructions; anda processor configured to execute the instructions to: determine a first activity of the user involving the mobile device based on input from the first sensor;determine a location of the user based on input from the second sensor, and determine a second activity of the user associated with the location of the user;determine an emotional response of the user based on the biometric response;establish a priority of the first activity of the user involving the mobile device over the second activity of the user associated with the location of the user;determine an action to be performed with the social media application or internet-accessible application based on the priority and the emotional response;perform, via the social media application or internet-accessible application, the action;select an advertisement based on the first activity of the user involving the mobile device; andpresent, via the mobile device, the advertisement to the user.","21","15/400287","2017-01-06","2017-0112431","2017-04-27","10881348","2021-01-05","CITIBANK, N.A","Brian  Levine | Carl  Marci | Ravi Kanth V.  Kothuri","","","","A61B-0005/486","A61B-0005/486 | A61B-0003/112 | A61B-0003/113 | A61B-0005/0077 | A61B-0005/02055 | A61B-0005/0476 | A61B-0005/0488 | A61B-0005/165 | G06N-0020/00 | G06Q-0050/01 | G09B-0019/00 | G09B-0025/00 | A61B-0005/02405 | A61B-0005/0533 | A61B-0005/0816 | A61B-0005/11 | A61B-2503/12","G09B-025/00","G09B-025/00 | G09B-019/00 | A61B-005/00 | G06N-020/00 | G06Q-050/00 | A61B-003/11 | A61B-003/113 | A61B-005/0205 | A61B-005/0476 | A61B-005/0488 | A61B-005/16 | A61B-005/024 | A61B-005/053 | A61B-005/08 | A61B-005/11","","","","","","4921002001041"
"US","US","P","B2","System and method for automated multi-dimensional network management","Systems, methods, and devices for automated provisioning are disclosed herein. The system can include a memory including a user profile database having n-dimension attributes of a user. The system can include a user device and a source device. The system can include a server that can: generate and store a user profile in the user profile database and generate and store a characterization vector from the user profile. The server can identify a service for provisioning, receive updates to at least some of the attributes of the first user, and trigger regeneration of the characterization vector from the received inputs. The server can: regenerate the characterization vector, determine an efficacy of the provisioned services, and automatically identify a second service for provisioning for a second user based on the efficacy of the provisioned services to the first user.","1. An automated multi-dimensional network management system comprising: a memory comprising: an electronic health records (EHR) database; and a network database comprising a plurality of nodes linked by a plurality of edges, at least some of the nodes corresponding to a user state, a user characteristic, and a remediation; anda processor configured to: identify, via a machine-learning model, a first remediation to mitigate a likelihood of an adverse outcome identified in a risk profile based on the user state of a first user;identify a data insufficiency based on missing data in the EHR database, wherein the data insufficiency prevents identification of a remediation;select a medical service comprising a digital component and a non-digital component for provisioning to the first user, the medical service is selected to generate data to remedy the data insufficiency;resolve the data insufficiency via provisioning of the selected medical service and receipt of electronic data generated from the provisioned medical service;upon resolution of the data insufficiency, identify a second remediation to mitigate the likelihood of the adverse outcome; andprovide the second remediation to the first user.","20","16/505514","2019-07-08","2020-0004767","2020-01-02","10885083","2021-01-05","MAMMOTH MEDICAL, LLC","Tobias  Moeller-Bertram | Christopher A.  McDonald","","","","G06F-0016/337","G06F-0016/337 | A61B-0005/4848 | A61B-0005/7264 | G06F-0009/30036 | G06F-0015/76 | G06K-0009/623 | G06K-0009/6232 | G06N-0005/022 | G06N-0020/00 | H04L-0041/16 | H04L-0041/5054 | H04L-0063/1433 | A61B-0005/0022 | H04L-0067/02 | H04L-0067/18 | H04L-0067/22 | H04L-0067/306","G06F-016/335","G06F-016/335 | G06N-005/02 | G06N-020/00 | A61B-005/00 | G06F-015/76 | H04L-029/06 | G06F-009/30 | G06K-009/62 | H04L-012/24 | H04L-029/08","","","","","","4921002004751"
"US","US","P","B2","Biometric method and device for identifying a person through an electrocardiogram (ECG) waveform","Method for identifying a person through an electrocardiogram, ECG, waveform, said method comprising: capturing ECG signals from a sample population including the person to be identified; computing sample population ECG distances ST, RT and QT from the captured ECG signals; training a computer classification model on the computed sample population ECG distances, provided that no other ECG distances are used; capturing an ECG signal from the person to be identified; computing the person's ECG distances ST, RT and QT from the person's captured ECG signal; using the classification model with the person's computed ECG distances to identify the person to be identified within the sample population. Device for identifying a person through an electrocardiogram, ECG, waveform, said device comprising means for carrying out said method.","1. A method for identifying a person through an electrocardiogram(ECG) waveform, said method comprising: capturing ECG signals from a sample population including the person to be identified;computing ECG fiducial points Q, R, S and T of the sample population from the captured ECG signals;computing the ECG distances consisting of ECG distances ST, RT and QT, or any other three linear-combination distances of ST, RT and QT, from the computed ECG fiducial points Q, R, S and T of the sample population;computing an average of the ECG distance RR from the captured ECG signals from the sample population;normalizing the computed ECG distances ST, RT and QT of the sample population and of the person to be identified, using the average of the ECG distance RR;training a computer classification model on the normalized, computed sample population ECG distances;capturing an ECG signal from the person to be identified;computing the person'ss ECG fiducial points Q, R, S and T from the person'ss captured ECG signal;computing the ECG distances consisting of ECG distances ST, RT and QT, or the same three linear-combination distances of ST, RT and QT used when training the computer classification model, from the person'ss computed ECG fiducial points Q, R, S and T; andusing the classification model with the person'ss normalized, computed ECG distances to identify the person to be identified within the sample population.","17","16/097476","2017-04-28","2019-0147277","2019-05-16","10885361","2021-01-05","INESC TEC - INSTITUTO DE ENGENHARIA DE SISTEMAS E COMPUTADORES, TECNOLOGIA E CI?NCIA","Joao Paulo  Trigueiros Da Silva Cunha | Joana Isabel  Santos Paiva","109357","PT","2016-04-29","G06K-0009/00885","G06K-0009/00885 | A61B-0005/0472 | A61B-0005/117 | G06F-0021/32 | G06K-0009/00503 | G06K-0009/00536 | G06K-0009/6269 | G06N-0020/10 | H04L-0063/0861 | G06K-2009/00939","G06K-009/00","G06K-009/00 | G06F-021/32 | H04L-029/06 | G06N-020/10 | A61B-005/0472 | A61B-005/117 | G06K-009/62","","","","","","4921002005027"
"US","US","P","B1","Methods and systems for treating autism","A method for treating autism is provided. The method includes presenting to affected subjects therapeutic content in the form of images or video in a virtual or augmented reality system and monitoring in real time the behaviors and responses of the subject to the therapy. The virtual or augmented reality system may further include audio, and the monitoring of the therapy may be achieved using one or more tracking sensors, such as a camera.","1. A method of studying a performance of a subject comprising: (a) receiving a selection of a learning module from a plurality of learning modules, wherein the learning module is associated with a set of one or more therapeutic goals, and wherein the learning module is configured to present one or more images or video in a virtual or augmented reality experience;(b) receiving, from one or more sensors, sensory data measured for the subject in response to the virtual or augmented reality experience;(c) determining by one or more processors, individually or collectively, using one or more algorithms, a plurality of metrics based at least in part on the sensory data to quantify a progress of the subject toward the one or more therapeutic goals; and(d) providing the plurality of metrics in an analytics dashboard.","20","16/697816","2019-11-27","","","10885719","2021-01-05","FLOREO, INC.","Vijay  Ravindran | Vibha  Sazawal | Ali  Moeeny | Rita  Solorzano | Sinan  Turnacioglu","","","","G06T-0019/006","G06T-0019/006 | A61B-0005/163 | A61B-0005/168 | A61B-0005/4848 | A61B-0005/742 | A61M-0021/00 | A61N-0001/36025 | G02B-0027/0101 | G06F-0003/011 | A61H-0039/007 | A61M-2021/005 | A61M-2021/0022 | A61M-2021/0027 | A61M-2205/332 | A61M-2205/3306 | A61M-2205/3368 | A61M-2205/3375 | A61M-2205/507 | G01N-2800/28 | G02B-2027/0178 | G06K-0009/00671","G06T-019/00","G06T-019/00 | A61B-005/00 | A61B-005/16 | A61M-021/00 | A61N-001/36 | G02B-027/01 | G06F-003/01 | G06K-009/00 | A61H-039/00","","","","","","4921002005385"
